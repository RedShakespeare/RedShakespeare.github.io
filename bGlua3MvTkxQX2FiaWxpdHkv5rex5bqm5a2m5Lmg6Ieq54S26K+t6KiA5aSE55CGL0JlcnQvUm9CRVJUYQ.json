{"title":"","date":"2024-06-21T03:48:14.275Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:14.275Z","content":"<p>RoBERTa：更大更多更强</p>\n<p>今天分享一个Bert的改进工作<a href=\"https://arxiv.org/abs/1907.11692,\" title=\"RoBERTa: A Robustly Optimized BERT Pretraining Approach\" target=\"_blank\">RoBERTa</a>。RoBERTa是训练充分的Bert。</p>\n<p>主要掌握以下几点，与Bert相比较，RoBERTa预训练的时候：</p>\n<ol>\n<li>动态掩码：comparable or slightly better</li>\n<li>去掉NSP任务并且更改数据输入格式为全部填充可以跨越多个文档</li>\n<li>更多数据，更大bsz，更多的步数，更长训练时间</li>\n</ol>\n<h1 id=\"1.-动态掩码\">1. 动态掩码<a title=\"#1.-动态掩码\" href=\"#1.-动态掩码\"></a></h1>\n<p>首先明确Bert使用的是静态掩码。但是这样会存在一个现象，比如我训练40个epoches，那么每次epoches都是使用同一批数据。</p>\n<p>这其实不是什么大问题，我们在深度学习训练模型的时候，每个epoches基本都没咋变过。</p>\n<p>不过对于Bert，其实本质是一个自监督模型。每次的训练输入如果是不同的，对于模型肯定是更好的。</p>\n<p>比如我们句子为：今天去哪里吃饭啊？</p>\n<p>mask之后为：今天去哪里[mask]饭啊？</p>\n<p>每次训练使用同一个mask样本，那么模型见得就少。</p>\n<p>如果换一个mask：[mask]天去哪里吃饭啊？</p>\n<p>模型对于同一个句子，在预测不同的单词，那么模型对句子的表达能力直觉上肯定是会上升的。</p>\n<p>所以为了缓解这种静态掩码的问题，Bert的操作是这样的：</p>\n<p>复制原始样本10份，每份都做不同的静态mask，然后进行训练。</p>\n<p>我们想一下这个过程：比如我仍然是训练40个epoches，复制了十份样本，相当于每4个epoches使用的是同一个mask的样本。</p>\n<p>这个操作确实缓解了静态掩码的问题，但是毕竟还有重复mask的情况出现。</p>\n<p>这个时候其实有个朴素的思想，为啥不直接复制40份，然后分在40个epoches中进行训练，这个到时候写Bert的时候再说。</p>\n<p>RoBERTa 是咋做的呢？</p>\n<p>动态掩码，也就是不是在最开始的时候的数据处理的过程中就生成mask样本，而是在送入到模型之前才进行mask，这样同一个句子，在40epoches中，每次mask都不同。</p>\n<p>效果直接看图</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-02-113140.jpg\" alt=\"动态mask\" loading=\"lazy\" class=\"φbp\"></p>\n<h1 id=\"2.-nsp和模型数据输入格式\">2. NSP和模型数据输入格式<a title=\"#2.-nsp和模型数据输入格式\" href=\"#2.-nsp和模型数据输入格式\"></a></h1>\n<p>这一点其实很有意思。</p>\n<p>我们先说RoBERTa 的四种输入形式和实验效果，然后再详细分析：</p>\n<ol>\n<li>SEGMENT-PAIR+NSP：就是Bert的输入形式</li>\n<li>SENTENCE-PAIR+NSP：输入的是一对句子，即前后是单个句子</li>\n<li>FULL-SENTENCES：输入为全量的句子，填满512的长度，采集样本的时候可以跨越文章的界限，去除了NSP loss</li>\n<li>DOC-SENTENCES：输入和FULL-SENTENCE类似，但是一个样本不能跨越两个document</li>\n</ol>\n<p>然后看一下实验效果：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-02-113141.jpg\" alt=\"RoBERTa四种输入形式效果对比图\" loading=\"lazy\" class=\"φbp\"></p>\n<p>对上面这个图一个最简单的总结就是NSP没啥用。然后我们来详细说一下这个事情。</p>\n<p>首先Bert的消融实验证明，NSP是应该有的，如果没有NSP，在部分任务上效果有损失。</p>\n<p>但是上图RoBERTa实验证明，NSP没啥效果，可以没有。</p>\n<p>一个直观的解释，或者说猜测是因为，可能是Bert在消融实验去除NSP的时候，仍然保持的是原始的输入，即有NSP任务的时候的输入形式。</p>\n<p>这就相当于，构造了好了符合NSP任务的数据，但是你删去了针对这个任务的损失函数，所以模型并没有学的很好，在部分任务精读下降。</p>\n<p>但是RoBERTa这里不是的，它删除NSP任务的时候，同时改变了输入格式，并不是使用上下两句的输入格式，而是类似文档中的句子全部填满这512个字符的格式进行输入。</p>\n<p>简单说就是，去掉了NSP任务的同时，去掉了构造数据中NSP数据格式。</p>\n<p>比较SEGMENT-PAIR和DOC-SENTENCES两个模式的时候，证明没有NSP效果更好。其实看起来好像并没有控制变量，因为变了两个地方，一个是是否有NSP，一个是输入格式。</p>\n<p>这种情况下，就只能去看在下游任务中的效果了。</p>\n<h1 id=\"3.-数据+bsz+steps\">3. 数据+bsz+steps<a title=\"#3.-数据+bsz+steps\" href=\"#3.-数据+bsz+steps\"></a></h1>\n<ol>\n<li>数据：Bert：16G；RoBERTa：160G；十倍</li>\n<li>bsz：Bert：256；RoBERTa：8K</li>\n<li>steps：Bert：1M；RoBERTa：300K/500K</li>\n</ol>\n<h1 id=\"4.-总结：\">4. 总结：<a title=\"#4.-总结：\" href=\"#4.-总结：\"></a></h1>\n<p>简单总结一下学到的东西：</p>\n<ol>\n<li>动态掩码：comparable or slightly better</li>\n<li>去掉NSP任务并且更改数据输入格式为全部填充可以跨越多个文档</li>\n<li>更多数据，更大bsz，更多的步数，更长训练时间</li>\n<li><strong>动态掩码那里，说到一个复制10份的细节，那里是针对的Bert，RoBERTa是每次输入之前才mask，注意区分，不要搞混</strong></li>\n</ol>\n<p>参考资料：RoBERTa: 捍卫BERT的尊严 - yangDDD的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/149249619\" target=\"_blank\">https://zhuanlan.zhihu.com/p/149249619</a></p>\n","link":"links/NLP_ability/深度学习自然语言处理/Bert/RoBERTa","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/RoBERTa/","toc":[{"id":"1.-动态掩码","title":"1. 动态掩码","index":"1"},{"id":"2.-nsp和模型数据输入格式","title":"2. NSP和模型数据输入格式","index":"2"},{"id":"3.-数据+bsz+steps","title":"3. 数据+bsz+steps","index":"3"},{"id":"4.-总结：","title":"4. 总结：","index":"4"}],"reward":true}