{"title":"","date":"2024-06-21T03:48:17.065Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.065Z","content":"<p>在做中文NER的时候，我们的常规操作是以字为单词。这是因为如果以词为单位，</p>\n<p>很容易造成切分错误，导致误差的积累。</p>\n<p>我举个简单的例子，比如我现在有一句话，【你去北京老哥】</p>\n<p>但是以字为单词，有一个问题就是会忽视词的信息。</p>\n<p>所以，大家很自然就想仍然以字为单词做NER，但是把词的信息补充进来。</p>\n<p>这个时候，一个很朴素的想法就是，我输入的时候过一遍分词，然后把词向量和字向量拼接或者相加或者做别的操作来融合起来。</p>\n<p>这个方法一般来说能够提升准确度，但是不会太多。</p>\n<p>后来还有一种思想就是使用 lattice structure，这种确实做到了词汇信息的增强，但是存在并行化困难以及推理速度慢的缺点，换句话说，方法是好方法，但是落地困难。</p>\n<p>这个论文做了一个什么事情呢？把栅栏式结构通过相对位置编码展平。</p>\n<p>我们知道transformer为了保持位置信息，对于每个token，是使用了位置编码的。在这里，为了这个晶格结构设计了一个巧妙的位置编码，来把复杂结构展开展平：</p>\n<p>如图所示：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051016.jpg\" alt=\" Flat-Lattice Transformer\" loading=\"lazy\" class=\"φbp\"></p>\n<p>看这个图需要注意的是，【重】这个字对应到英文代表的是character-字符，【重庆】这个词组对应到英文代表的是word-单词，这一点，大家在读论文的时候需要注意。</p>\n<p>为每一个token（包含char和word）分配两个位置索引：头位置和尾位置；</p>\n<p>在原来的晶格结构中，比如【店】只能和【人和药店】以及【药店】产生关系，但是在TRM中，由于self-attention的存在，【店】是可以和序列中的每个token都发生关系，不仅仅是和self-matched的词汇。这算是一个意外之喜。</p>\n<p>self-matched的词汇，就是包含当前char的</p>\n<p>谈一下为什么这么转化：</p>\n<p>一般来说，我们有语料，和词典，通过词典，我们可以得到一个晶格</p>\n<h1 id=\"为什么要把晶格结构压平\">为什么要把晶格结构压平<a title=\"#为什么要把晶格结构压平\" href=\"#为什么要把晶格结构压平\"></a></h1>\n<p>头部的索引就是第一个单词的位置，尾部就是最后一个单词所在的位置，如果是一个char，头尾就是相同的。</p>\n<p>通过这个巧妙的设置，我们是可以把展平的东西再重建到晶格模式的，所以认为是可行的。</p>\n<h1 id=\"相对位置编码\">相对位置编码<a title=\"#相对位置编码\" href=\"#相对位置编码\"></a></h1>\n<p>通过头尾索引，我们可以把晶格结构压平。</p>\n<p>现在还面临一个问题，就是对于【人和药店】头尾索引是【3】【6】，但是这并不包含位置信息。</p>\n<p>对于NER来说，位置信息是很重要的。</p>\n<p>对于普通的TRM，使用绝对位置编码保持位置信息，但是有研究表示，这种位置信息在self-attention中使用向量内积的时候，会减弱。</p>\n<p>具体的大家可以看我这个文章：<a href=\"https://mp.weixin.qq.com/s?__biz=MzIyNTY1MDUwNQ==&amp;mid=2247483760&amp;idx=1&amp;sn=c2803e63bdd42e4d1f1f880ce9eda8cc&amp;chksm=e87d3356df0aba40c77356418647856ec135c731fd60122378ed702e1e959c820250c2293e1f&amp;token=588814416&amp;lang=zh_CN#rd\" target=\"_blank\">原版Transformer的位置编码究竟有没有包含相对位置信息</a>；</p>\n<p>所以，我们现在就要考虑使用相对位置信息来表达位置，同时还要把我们头尾索引融合进来。</p>\n<p>对于句子中的两个spans（包含char和words）$x_{i},x_{j}$，它们可能有三种关系：相交，包含，和分离。</p>\n<p>比如上面那个例子，【药店】和【人和药店】就是包含的关系；【重庆】和【人和药店】就是分离的关系。</p>\n<p>我们使用一个向量来描述两个spans之间的关系。</p>\n<p>先说两个spans之间存在的距离关系可以用如下公式去表达：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051014.jpg\" alt=\"实体距离关系公式\" loading=\"lazy\" class=\"φbp\"></p>\n<p>上角标的$(hh)$代表的就是两个spans之间的头部索引差值，其他上角标类似的意思。</p>\n<p>具体的实际是什么样子，大家可以看上面的图c；</p>\n<p>然后我们使用如下的公式去生成相对位置编码：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051015.jpg\" alt=\"相对位置编码\" loading=\"lazy\" class=\"φbp\"></p>\n<p>接下来的问题就是利用这个相对位置编码融入到TRM之中。</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051012.jpg\" alt=\"attention_new矩阵\" loading=\"lazy\" class=\"φbp\"></p>\n<p><strong>简单来说，就是利用相对位置编码，生成了一个包含相对位置编码信息的新的attention矩阵，不再使用原始的attention矩阵</strong></p>\n<p>看到这里，其实有注意到一个很有意思的点就是FLAT使用的是一层encoder。</p>\n<h1 id=\"实验\">实验<a title=\"#实验\" href=\"#实验\"></a></h1>\n<p>实验比较感兴趣的是</p>\n<p>一个是和其他词汇增强的网络结果相比，效果如何。</p>\n<p>还有一个就是使用transformer之后，TRM长距离依赖的优点和每个token之间都可以交互的优点有没有在提升效果上发挥作用</p>\n<p>还有一个其实很自然的会想到能不能使用将FLAT和BERT融合起来。也就是如何将动态的字向量和FLAT这种词向量结合起来。</p>\n<p>先看第一二点</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-051013.jpg\" alt=\"results in different models\" loading=\"lazy\" class=\"φbp\"></p>\n<p>再看第三点</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-08-51014.jpg\" alt=\"BERT+FLAT\" loading=\"lazy\" class=\"φbp\"></p>\n<p>有意思的是，使用了FLAT之后，在Resume和Weibo效果有提升，但是不明显，作者认为可能是因为数据集有点小。在大数据集Ontonotes和MSRA上，效果提升比较明显。</p>\n<p>推理速度的话，和Lattice LSTM相比，BSZ为16的情况下，基本是8倍左右。</p>\n<h1 id=\"总结\">总结<a title=\"#总结\" href=\"#总结\"></a></h1>\n<p>梳理一下怎么把词汇信息加入进去的：</p>\n<ol>\n<li>首先我们知道NER融合词汇信息能提升最终效果，但是一般的Lattice结构落地困难</li>\n<li>然后受TRM位置信息的启发，将Lattice结构展开</li>\n<li>然后由于普通TRM绝对位置信息在self-attention中会被削弱，所以想要使用相对位置信息。</li>\n<li>从头尾索引，我们可以知道tokens之间有三种关系：相交，包含，隔离；从这三种关系，我们可以得到两个tokens的四种距离公式，并且把这个四种距离公式融入到了相对位置信息。</li>\n<li>得到最终的相对位置信息，将相对位置信息融合进入attention矩阵，参与Encoder计算</li>\n</ol>\n","link":"links/NLP_ability/深度学习自然语言处理/命名体识别/FLAT-Transformer","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/FLAT-Transformer/","toc":[{"id":"为什么要把晶格结构压平","title":"为什么要把晶格结构压平","index":"1"},{"id":"相对位置编码","title":"相对位置编码","index":"2"},{"id":"实验","title":"实验","index":"3"},{"id":"总结","title":"总结","index":"4"}],"reward":true}