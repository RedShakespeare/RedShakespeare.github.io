{"title":"","date":"2024-06-21T03:20:48.423Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.423Z","content":"<h1 id=\"speech-to-text\">Speech to Text<a title=\"#speech-to-text\" href=\"#speech-to-text\"></a></h1>\n<p>A deep learning-based approach to learning the speech-to-text conversion, built on top of the <a href=\"http://opennmt.net/\">OpenNMT</a> system.</p>\n<p>Given raw audio, we first apply short-time Fourier transform (STFT), then apply Convolutional Neural Networks to get the source features. Based on this source representation, we use an LSTM decoder with attention to produce the text character by character.</p>\n<h3 id=\"dependencies\">Dependencies<a title=\"#dependencies\" href=\"#dependencies\"></a></h3>\n<ul>\n<li><code>torchaudio</code>: <code>sudo apt-get install -y sox libsox-dev libsox-fmt-all; pip install git+https://github.com/pytorch/audio</code></li>\n<li><code>librosa</code>: <code>pip install librosa</code></li>\n</ul>\n<h3 id=\"quick-start\">Quick Start<a title=\"#quick-start\" href=\"#quick-start\"></a></h3>\n<p>To get started, we provide a toy speech-to-text example. We assume that the working directory is <code>OpenNMT-py</code> throughout this document.</p>\n<ol start=\"0\">\n<li>Download the data.</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget -O data/speech.tgz http://lstm.seas.harvard.edu/latex/speech.tgz; tar zxf data/speech.tgz -C data/</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Preprocess the data.</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_preprocess -data_type audio -src_dir data/speech/an4_dataset -train_src data/speech/src-train.txt -train_tgt data/speech/tgt-train.txt -valid_src data/speech/src-val.txt -valid_tgt data/speech/tgt-val.txt -shard_size 300 -save_data data/speech/demo</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>Train the model.</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_train -model_type audio -enc_rnn_size 512 -dec_rnn_size 512 -audio_enc_pooling 1,1,2,2 -dropout 0 -enc_layers 4 -dec_layers 1 -rnn_type LSTM -data data/speech/demo -save_model demo-model -global_attention mlp -gpu_ranks 0 -batch_size 8 -optim adam -max_grad_norm 100 -learning_rate 0.0003 -learning_rate_decay 0.8 -train_steps 100000</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>Translate the speechs.</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_translate -data_type audio -model demo-model_acc_x_ppl_x_e13.pt -src_dir data/speech/an4_dataset -src data/speech/src-val.txt -output pred.txt -gpu 0 -verbose</span><br></pre></td></tr></table></figure>\n<h3 id=\"options\">Options<a title=\"#options\" href=\"#options\"></a></h3>\n<ul>\n<li>\n<p><code>-src_dir</code>: The directory containing the audio files.</p>\n</li>\n<li>\n<p><code>-train_tgt</code>: The file storing the tokenized labels, one label per line. It shall look like:</p>\n</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;label0_token0&gt; &lt;label0_token1&gt; ... &lt;label0_tokenN0&gt;</span><br><span class=\"line\">&lt;label1_token0&gt; &lt;label1_token1&gt; ... &lt;label1_tokenN1&gt;</span><br><span class=\"line\">&lt;label2_token0&gt; &lt;label2_token1&gt; ... &lt;label2_tokenN2&gt;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>-train_src</code>: The file storing the paths of the audio files (relative to <code>src_dir</code>).</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;speech0_path&gt;</span><br><span class=\"line\">&lt;speech1_path&gt;</span><br><span class=\"line\">&lt;speech2_path&gt;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>sample_rate</code>: Sample rate. Default: 16000.</li>\n<li><code>window_size</code>: Window size for spectrogram in seconds. Default: 0.02.</li>\n<li><code>window_stride</code>: Window stride for spectrogram in seconds. Default: 0.01.</li>\n<li><code>window</code>: Window type for spectrogram generation. Default: hamming.</li>\n</ul>\n<h3 id=\"acknowledgement\">Acknowledgement<a title=\"#acknowledgement\" href=\"#acknowledgement\"></a></h3>\n<p>Our preprocessing and CNN encoder is adapted from <a href=\"https://github.com/SeanNaren/deepspeech.pytorch\" target=\"_blank\">deepspeech.pytorch</a>.</p>\n","link":"links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/speech2text","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/speech2text/","toc":[{"id":"speech-to-text","title":"Speech to Text","index":"1","children":[{"id":"dependencies","title":"Dependencies","index":"1.1"},{"id":"quick-start","title":"Quick Start","index":"1.2"},{"id":"options","title":"Options","index":"1.3"},{"id":"acknowledgement","title":"Acknowledgement","index":"1.4"}]}],"reward":true}