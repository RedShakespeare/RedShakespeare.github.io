{"title":"","date":"2024-06-21T03:20:48.133Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.133Z","content":"<h1 id=\"library\">Library<a title=\"#library\" href=\"#library\"></a></h1>\n<p>For this example, we will assume that we have run preprocess to<br>\ncreate our datasets. For instance</p>\n<blockquote>\n<p>onmt_preprocess -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/data -src_vocab_size 10000 -tgt_vocab_size 10000</p>\n</blockquote>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> onmt</span><br><span class=\"line\"><span class=\"keyword\">import</span> onmt.inputters</span><br><span class=\"line\"><span class=\"keyword\">import</span> onmt.modules</span><br><span class=\"line\"><span class=\"keyword\">import</span> onmt.utils</span><br></pre></td></tr></table></figure>\n<p>We begin by loading in the vocabulary for the model of interest. This will let us check vocab size and to get the special ids for padding.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vocab_fields = torch.load(<span class=\"string\">&quot;data/data.vocab.pt&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">src_text_field = vocab_fields[<span class=\"string\">&quot;src&quot;</span>].base_field</span><br><span class=\"line\">src_vocab = src_text_field.vocab</span><br><span class=\"line\">src_padding = src_vocab.stoi[src_text_field.pad_token]</span><br><span class=\"line\"></span><br><span class=\"line\">tgt_text_field = vocab_fields[<span class=\"string\">&#x27;tgt&#x27;</span>].base_field</span><br><span class=\"line\">tgt_vocab = tgt_text_field.vocab</span><br><span class=\"line\">tgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]</span><br></pre></td></tr></table></figure>\n<p>Next we specify the core model itself. Here we will build a small model with an encoder and an attention based input feeding decoder. Both models will be RNNs and the encoder will be bidirectional</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">emb_size = <span class=\"number\">100</span></span><br><span class=\"line\">rnn_size = <span class=\"number\">500</span></span><br><span class=\"line\"><span class=\"comment\"># Specify the core model.</span></span><br><span class=\"line\"></span><br><span class=\"line\">encoder_embeddings = onmt.modules.Embeddings(emb_size, <span class=\"built_in\">len</span>(src_vocab),</span><br><span class=\"line\">                                             word_padding_idx=src_padding)</span><br><span class=\"line\"></span><br><span class=\"line\">encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=<span class=\"number\">1</span>,</span><br><span class=\"line\">                                   rnn_type=<span class=\"string\">&quot;LSTM&quot;</span>, bidirectional=<span class=\"literal\">True</span>,</span><br><span class=\"line\">                                   embeddings=encoder_embeddings)</span><br><span class=\"line\"></span><br><span class=\"line\">decoder_embeddings = onmt.modules.Embeddings(emb_size, <span class=\"built_in\">len</span>(tgt_vocab),</span><br><span class=\"line\">                                             word_padding_idx=tgt_padding)</span><br><span class=\"line\">decoder = onmt.decoders.decoder.InputFeedRNNDecoder(</span><br><span class=\"line\">    hidden_size=rnn_size, num_layers=<span class=\"number\">1</span>, bidirectional_encoder=<span class=\"literal\">True</span>, </span><br><span class=\"line\">    rnn_type=<span class=\"string\">&quot;LSTM&quot;</span>, embeddings=decoder_embeddings)</span><br><span class=\"line\"></span><br><span class=\"line\">device = <span class=\"string\">&quot;cuda&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span></span><br><span class=\"line\">model = onmt.models.model.NMTModel(encoder, decoder)</span><br><span class=\"line\">model.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Specify the tgt word generator and loss computation module</span></span><br><span class=\"line\">model.generator = nn.Sequential(</span><br><span class=\"line\">    nn.Linear(rnn_size, <span class=\"built_in\">len</span>(tgt_vocab)),</span><br><span class=\"line\">    nn.LogSoftmax(dim=-<span class=\"number\">1</span>)).to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">loss = onmt.utils.loss.NMTLossCompute(</span><br><span class=\"line\">    criterion=nn.NLLLoss(ignore_index=tgt_padding, reduction=<span class=\"string\">&quot;sum&quot;</span>),</span><br><span class=\"line\">    generator=model.generator)</span><br></pre></td></tr></table></figure>\n<p>Now we set up the optimizer. Our wrapper around a core torch optim class handles learning rate updates and gradient normalization automatically.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lr = <span class=\"number\">1</span></span><br><span class=\"line\">torch_optimizer = torch.optim.SGD(model.parameters(), lr=lr)</span><br><span class=\"line\">optim = onmt.utils.optimizers.Optimizer(</span><br><span class=\"line\">    torch_optimizer, learning_rate=lr, max_grad_norm=<span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n<p>Now we load the data from disk with the associated vocab fields. To iterate through the data itself we use a wrapper around a torchtext iterator class. We specify one for both the training and test data.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Load some data</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> itertools <span class=\"keyword\">import</span> chain</span><br><span class=\"line\">train_data_file = <span class=\"string\">&quot;data/data.train.0.pt&quot;</span></span><br><span class=\"line\">valid_data_file = <span class=\"string\">&quot;data/data.valid.0.pt&quot;</span></span><br><span class=\"line\">train_iter = onmt.inputters.inputter.DatasetLazyIter(dataset_paths=[train_data_file],</span><br><span class=\"line\">                                                     fields=vocab_fields,</span><br><span class=\"line\">                                                     batch_size=<span class=\"number\">50</span>,</span><br><span class=\"line\">                                                     batch_size_multiple=<span class=\"number\">1</span>,</span><br><span class=\"line\">                                                     batch_size_fn=<span class=\"literal\">None</span>,</span><br><span class=\"line\">                                                     device=device,</span><br><span class=\"line\">                                                     is_train=<span class=\"literal\">True</span>,</span><br><span class=\"line\">                                                     repeat=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">valid_iter = onmt.inputters.inputter.DatasetLazyIter(dataset_paths=[valid_data_file],</span><br><span class=\"line\">                                                     fields=vocab_fields,</span><br><span class=\"line\">                                                     batch_size=<span class=\"number\">10</span>,</span><br><span class=\"line\">                                                     batch_size_multiple=<span class=\"number\">1</span>,</span><br><span class=\"line\">                                                     batch_size_fn=<span class=\"literal\">None</span>,</span><br><span class=\"line\">                                                     device=device,</span><br><span class=\"line\">                                                     is_train=<span class=\"literal\">False</span>,</span><br><span class=\"line\">                                                     repeat=<span class=\"literal\">False</span>)</span><br></pre></td></tr></table></figure>\n<p>Finally we train. Keeping track of the output requires a report manager.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">report_manager = onmt.utils.ReportMgr(</span><br><span class=\"line\">    report_every=<span class=\"number\">50</span>, start_time=<span class=\"literal\">None</span>, tensorboard_writer=<span class=\"literal\">None</span>)</span><br><span class=\"line\">trainer = onmt.Trainer(model=model,</span><br><span class=\"line\">                       train_loss=loss,</span><br><span class=\"line\">                       valid_loss=loss,</span><br><span class=\"line\">                       optim=optim,</span><br><span class=\"line\">                       report_manager=report_manager)</span><br><span class=\"line\">trainer.train(train_iter=train_iter,</span><br><span class=\"line\">              train_steps=<span class=\"number\">400</span>,</span><br><span class=\"line\">              valid_iter=valid_iter,</span><br><span class=\"line\">              valid_steps=<span class=\"number\">200</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[2019-02-15 16:34:17,475 INFO] Start training loop and validate every 200 steps...</span><br><span class=\"line\">[2019-02-15 16:34:17,601 INFO] Loading dataset from data/data.train.0.pt, number of examples: 10000</span><br><span class=\"line\">[2019-02-15 16:35:43,873 INFO] Step 50/  400; acc:  11.54; ppl: 1714.07; xent: 7.45; lr: 1.00000; 662/656 tok/s;     86 sec</span><br><span class=\"line\">[2019-02-15 16:37:05,965 INFO] Step 100/  400; acc:  13.75; ppl: 534.80; xent: 6.28; lr: 1.00000; 675/671 tok/s;    168 sec</span><br><span class=\"line\">[2019-02-15 16:38:31,289 INFO] Step 150/  400; acc:  15.02; ppl: 439.96; xent: 6.09; lr: 1.00000; 675/668 tok/s;    254 sec</span><br><span class=\"line\">[2019-02-15 16:39:56,715 INFO] Step 200/  400; acc:  16.08; ppl: 357.62; xent: 5.88; lr: 1.00000; 642/647 tok/s;    339 sec</span><br><span class=\"line\">[2019-02-15 16:39:56,811 INFO] Loading dataset from data/data.valid.0.pt, number of examples: 3000</span><br><span class=\"line\">[2019-02-15 16:41:13,415 INFO] Validation perplexity: 208.73</span><br><span class=\"line\">[2019-02-15 16:41:13,415 INFO] Validation accuracy: 23.3507</span><br><span class=\"line\">[2019-02-15 16:41:13,567 INFO] Loading dataset from data/data.train.0.pt, number of examples: 10000</span><br><span class=\"line\">[2019-02-15 16:42:41,562 INFO] Step 250/  400; acc:  17.07; ppl: 310.41; xent: 5.74; lr: 1.00000; 347/344 tok/s;    504 sec</span><br><span class=\"line\">[2019-02-15 16:44:04,899 INFO] Step 300/  400; acc:  19.17; ppl: 262.81; xent: 5.57; lr: 1.00000; 665/661 tok/s;    587 sec</span><br><span class=\"line\">[2019-02-15 16:45:33,653 INFO] Step 350/  400; acc:  19.38; ppl: 244.81; xent: 5.50; lr: 1.00000; 649/642 tok/s;    676 sec</span><br><span class=\"line\">[2019-02-15 16:47:06,141 INFO] Step 400/  400; acc:  20.44; ppl: 214.75; xent: 5.37; lr: 1.00000; 593/598 tok/s;    769 sec</span><br><span class=\"line\">[2019-02-15 16:47:06,265 INFO] Loading dataset from data/data.valid.0.pt, number of examples: 3000</span><br><span class=\"line\">[2019-02-15 16:48:27,328 INFO] Validation perplexity: 150.277</span><br><span class=\"line\">[2019-02-15 16:48:27,328 INFO] Validation accuracy: 24.2132</span><br></pre></td></tr></table></figure>\n<p>To use the model, we need to load up the translation functions. A Translator object requires the vocab fields, readers for source and target and a global scorer.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> onmt.translate</span><br><span class=\"line\"></span><br><span class=\"line\">src_reader = onmt.inputters.str2reader[<span class=\"string\">&quot;text&quot;</span>]</span><br><span class=\"line\">tgt_reader = onmt.inputters.str2reader[<span class=\"string\">&quot;text&quot;</span>]</span><br><span class=\"line\">scorer = onmt.translate.GNMTGlobalScorer(alpha=<span class=\"number\">0.7</span>, </span><br><span class=\"line\">                                         beta=<span class=\"number\">0.</span>, </span><br><span class=\"line\">                                         length_penalty=<span class=\"string\">&quot;avg&quot;</span>, </span><br><span class=\"line\">                                         coverage_penalty=<span class=\"string\">&quot;none&quot;</span>)</span><br><span class=\"line\">gpu = <span class=\"number\">0</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> -<span class=\"number\">1</span></span><br><span class=\"line\">translator = onmt.translate.Translator(model=model, </span><br><span class=\"line\">                                       fields=vocab_fields, </span><br><span class=\"line\">                                       src_reader=src_reader, </span><br><span class=\"line\">                                       tgt_reader=tgt_reader, </span><br><span class=\"line\">                                       global_scorer=scorer,</span><br><span class=\"line\">                                       gpu=gpu)</span><br><span class=\"line\">builder = onmt.translate.TranslationBuilder(data=torch.load(valid_data_file), </span><br><span class=\"line\">                                            fields=vocab_fields)</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> batch <span class=\"keyword\">in</span> valid_iter:</span><br><span class=\"line\">    trans_batch = translator.translate_batch(</span><br><span class=\"line\">        batch=batch, src_vocabs=[src_vocab],</span><br><span class=\"line\">        attn_debug=<span class=\"literal\">False</span>)</span><br><span class=\"line\">    translations = builder.from_batch(trans_batch)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> trans <span class=\"keyword\">in</span> translations:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(trans.log(<span class=\"number\">0</span>))</span><br></pre></td></tr></table></figure>\n<pre><code>[2019-02-15 16:48:27,419 INFO] Loading dataset from data/data.valid.0.pt, number of examples: 3000\n\n\nSENT 0: ['Parliament', 'Does', 'Not', 'Support', 'Amendment', 'Freeing', 'Tymoshenko']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.0983\n\n\nSENT 0: ['Today', ',', 'the', 'Ukraine', 'parliament', 'dismissed', ',', 'within', 'the', 'Code', 'of', 'Criminal', 'Procedure', 'amendment', ',', 'the', 'motion', 'to', 'revoke', 'an', 'article', 'based', 'on', 'which', 'the', 'opposition', 'leader', ',', 'Yulia', 'Tymoshenko', ',', 'was', 'sentenced', '.']\nPRED 0: &lt;unk&gt; ist das &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5950\n\n\nSENT 0: ['The', 'amendment', 'that', 'would', 'lead', 'to', 'freeing', 'the', 'imprisoned', 'former', 'Prime', 'Minister', 'was', 'revoked', 'during', 'second', 'reading', 'of', 'the', 'proposal', 'for', 'mitigation', 'of', 'sentences', 'for', 'economic', 'offences', '.']\nPRED 0: Es gibt es das &lt;unk&gt; der &lt;unk&gt; für &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5128\n\n\nSENT 0: ['In', 'October', ',', 'Tymoshenko', 'was', 'sentenced', 'to', 'seven', 'years', 'in', 'prison', 'for', 'entering', 'into', 'what', 'was', 'reported', 'to', 'be', 'a', 'disadvantageous', 'gas', 'deal', 'with', 'Russia', '.']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5578\n\n\nSENT 0: ['The', 'verdict', 'is', 'not', 'yet', 'final;', 'the', 'court', 'will', 'hear', 'Tymoshenko', '&amp;apos;s', 'appeal', 'in', 'December', '.']\nPRED 0: &lt;unk&gt; ist nicht &lt;unk&gt; .\nPRED SCORE: -0.9623\n\n\nSENT 0: ['Tymoshenko', 'claims', 'the', 'verdict', 'is', 'a', 'political', 'revenge', 'of', 'the', 'regime;', 'in', 'the', 'West', ',', 'the', 'trial', 'has', 'also', 'evoked', 'suspicion', 'of', 'being', 'biased', '.']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -0.8703\n\n\nSENT 0: ['The', 'proposal', 'to', 'remove', 'Article', '365', 'from', 'the', 'Code', 'of', 'Criminal', 'Procedure', ',', 'upon', 'which', 'the', 'former', 'Prime', 'Minister', 'was', 'sentenced', ',', 'was', 'supported', 'by', '147', 'members', 'of', 'parliament', '.']\nPRED 0: &lt;unk&gt; Sie sich mit &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.4778\n\n\nSENT 0: ['Its', 'ratification', 'would', 'require', '226', 'votes', '.']\nPRED 0: &lt;unk&gt; Sie sich &lt;unk&gt; .\nPRED SCORE: -1.3341\n\n\nSENT 0: ['Libya', '&amp;apos;s', 'Victory']\nPRED 0: &lt;unk&gt; Sie die &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5192\n\n\nSENT 0: ['The', 'story', 'of', 'Libya', '&amp;apos;s', 'liberation', ',', 'or', 'rebellion', ',', 'already', 'has', 'its', 'defeated', '.']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.2772\n\n...\n</code></pre>\n","link":"links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/Library","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/Library/","toc":[{"id":"library","title":"Library","index":"1"}],"reward":true}