{"title":"","date":"2024-06-21T03:20:53.423Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.423Z","content":"<p>本文大概需要阅读 5.25 分钟</p>\n<p>大概用三篇文章好好谈一下Word2vec，这篇主要是关于 Word2vec 的两种模型。</p>\n<p><strong>先问大家个问题：一个词通过Word2vec训练之后，可以得到几个词向量？</strong></p>\n<p>如果您有点懵，说明我写对了，您慢慢看下去，码字不易，<strong>请多多点赞，让更多人看到</strong>，谢谢。</p>\n<p><strong>词向量一般被认为是一个词的特征向量</strong>，也就是说可以代表一个词的含义。一个中文词，比如&quot;中国&quot;这个词，是很难被神经网络直接作为输入，我们需要将词向量化（或者说数字化），才能更好的喂进神经网络。</p>\n<p><strong>这个过程很类似于计算机在处理我们输入的文字的时候，会转化为二进制进行处理。</strong></p>\n<p>只不过这个二进制表达规则我们会自己人为规定，而词向量这个向量表达根据不同的方式会有着不同的结果。<strong>想一下，两者是不是很类似</strong>。</p>\n<p>谈到向量表达单词，一般首先想到的都是One-Hot编码。但是它是有缺点的。我这里谈两个缺点。</p>\n<p>首先是<strong>维度灾难</strong>：如果我们有1万个单词，那么你的One-hot去表示每个单词的的时候，会转变为一个1万维度的向量。</p>\n<p>那么在计算的时候会带来巨大的不便，而且向量矩阵极其稀疏，占据了太多不必要的内存。当然对于维度灾难，我们一般可以使用PCA等降维手段来缓解。</p>\n<p>其次是<strong>语义表达不足</strong>。这一点很简单，”娱乐“与”八卦“两个词，通过One-hot编码得到的向量，计算Consine得到相似度为0。</p>\n<p>这显然是不合适的。<strong>One-Hot编码表示出来的词向量是两两正交的</strong>，从余弦相似度的角度来看，是互不相关的，所以 One-Hot 不能很好的表达词语的相似性。</p>\n<p>这个时候，我们再来看 Word2vec。首先，要明确一点，<strong>Word2vec 不是一个模型，而是一个工具</strong>。这个点虽然无伤大雅，但是每每看到有的文章说它是个模型，就有点…</p>\n<p><strong>Word2vec 从整体上理解，就是包含两个模型（CBOW and Skip-gram）和两个高效的优化训练方式（负采样和层序softmax）</strong></p>\n<p>这个文章主要谈一下两种模型。</p>\n<p>先假定我们的窗口大小为2，也就是说中心词前面有两个词，后面有两个词，比如&quot;我/永远/爱/中国/共产党&quot;，抽象出来就是：</p>\n<p>W_{t-2}，W_{t-1}，W_{t}，W_{t+1}，W_{t+2}</p>\n<p>对于Skip-gram，中文我们叫跳字模型，使用中心词预测背景词。也就是用&quot;爱&quot;去预测其他的四个词。</p>\n<p>对于CBOW，中文我们叫连续词袋模型，使用背景词预测中心词，也就是用&quot;我&quot;，“永远”，“中国”，“共产党” 去预测&quot;爱&quot;。</p>\n<p>CBOW的模型架构，从整体上，我们可以分为三层：输入层，投影层，和输出层。</p>\n<p>对于输入层，对应的是窗口中的单词，也就是例子中&quot;我&quot;，“永远”，“中国”，“共产党” 四个词的词向量，在投影层，将四个词的词向量进行相加求平均，输出层在没有优化的前提下，维度为词表大小，随后做 Softmax即可。</p>\n<p>Skip-gram模型架构很类似，只不过可以省去投影层，因为输入为中心词，是一个单词的词向量，无从谈起求和与平均了。</p>\n<p>接下来，我们来详细谈一下Skip-gram。</p>\n<p>对于一个神经网络模型，我们需要确定我们的优化目标或者说目标函数是什么？</p>\n<p>对于Skip-gram，其实很容易理解，就是我要最大化在给出中心词的条件下背景词出现的概率，公式如下</p>\n<p>\\prod_{t=1}^T \\ \\prod_{-m\\leq j \\leq m,j\\neq 0}  P(w^{t+j}|w^{t})</p>\n<p><strong>这个公式对应着两个连乘，第一个连乘是对应 T 个输入样本，也就是说我们文本中的每个单词都要做中心词。第二个连乘代表着给定一个中心词的情况下，窗口中的单词出现的概率，内含相互独立的假设</strong>。</p>\n<p>在这里，<strong>有一个细节点想要提醒大家</strong>，<strong>在词汇表中的每个单词，都是对应两个词向量的</strong>，<strong>一个是在作为中心词的时候的中心词向量，一个是在作为背景词时候的背景词向量</strong>。</p>\n<p>优化目标函数的时候，为了减少复杂度（也就是忽略 T），我们可以使用随机梯度下降，针对每一个样本我们都更新一次参数。</p>\n<p>通过公式推导（略），我们能够观察到一个特点，就是每次更新参数，都会涉及到词典中的全部词汇，这是因为我们在做 Softmax 的时候，是分母是针对所有词汇的操作。</p>\n<p>这样的操作的复杂度是 O(|V|)，其中|V|就是我们词汇表的大小。CBOW 其实是很类似的情况，每次更新都会涉及到我们的全部词汇。</p>\n<p>于是，我们就需要对此进行优化，使用了两种近似的训练方式来高效的训练Word2vec，降低训练的复杂度。</p>\n<p><strong>下一个文章谈一下优化方式。</strong></p>\n<p><strong>如果觉得写的还行，帮忙点个赞或者在看，让更多人关注到我吧，感谢。</strong></p>\n","link":"links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/聊一下Word2vec-模型篇","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/聊一下Word2vec-模型篇/","reward":true}