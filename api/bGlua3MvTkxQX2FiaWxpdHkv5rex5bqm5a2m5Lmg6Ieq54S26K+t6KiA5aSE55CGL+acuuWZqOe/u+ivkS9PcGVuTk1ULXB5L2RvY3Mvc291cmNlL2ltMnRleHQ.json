{"title":"","date":"2024-06-21T03:48:22.175Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:22.175Z","content":"<h1 id=\"image-to-text\">Image to Text<a title=\"#image-to-text\" href=\"#image-to-text\"></a></h1>\n<p>A deep learning-based approach to learning the image-to-text conversion, built on top of the <a href=\"http://opennmt.net/\">OpenNMT</a> system. It is completely data-driven, hence can be used for a variety of image-to-text problems, such as image captioning, optical character recognition and LaTeX decompilation.</p>\n<p>Take LaTeX decompilation as an example, given a formula image:</p>\n<p align=\"center\"><img src=\"http://lstm.seas.harvard.edu/latex/resul](../images/119b93a445-orig.png\"></p>\n<p>The goal is to infer the LaTeX source that can be compiled to such an image:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">d s _ &#123; 1 1 &#125; ^ &#123; 2 &#125; = d x ^ &#123; + &#125; d x ^ &#123; - &#125; + l _ &#123; p &#125; ^ &#123; 9 &#125; \\frac &#123; p _ &#123; - &#125; &#125; &#123; r ^ &#123; 7 &#125; &#125; \\delta ( x ^ &#123; - &#125; ) d x ^ &#123; - &#125; d x ^ &#123; - &#125; + d x _ &#123; 1 &#125; ^ &#123; 2 &#125; + \\; \\cdots \\; + d x _ &#123; 9 &#125; ^ &#123; 2 &#125; </span><br></pre></td></tr></table></figure>\n<p>The paper <a href=\"https://arxiv.org/pdf/1609.04938.pdf\" target=\"_blank\">[What You Get Is What You See: A Visual Markup Decompiler]</a> provides more technical details of this model.</p>\n<h3 id=\"dependencies\">Dependencies<a title=\"#dependencies\" href=\"#dependencies\"></a></h3>\n<ul>\n<li><code>torchvision</code>: <code>conda install torchvision</code></li>\n<li><code>Pillow</code>: <code>pip install Pillow</code></li>\n</ul>\n<h3 id=\"quick-start\">Quick Start<a title=\"#quick-start\" href=\"#quick-start\"></a></h3>\n<p>To get started, we provide a toy Math-to-LaTex example. We assume that the working directory is <code>OpenNMT-py</code> throughout this document.</p>\n<p>Im2Text consists of four commands:</p>\n<ol start=\"0\">\n<li>Download the data.</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget -O data/im2text.tgz http://lstm.seas.harvard.edu/latex/im2text_small.tgz; tar zxf data/im2text.tgz -C data/</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Preprocess the data.</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_preprocess -data_type img \\</span><br><span class=\"line\">                -src_dir da](../images/ \\</span><br><span class=\"line\">                -train_src data/im2text/src-train.txt \\</span><br><span class=\"line\">                -train_tgt data/im2text/tgt-train.txt -valid_src data/im2text/src-val.txt \\</span><br><span class=\"line\">                -valid_tgt data/im2text/tgt-val.txt -save_data data/im2text/demo \\</span><br><span class=\"line\">                -tgt_seq_length 150 \\</span><br><span class=\"line\">                -tgt_words_min_frequency 2 \\</span><br><span class=\"line\">                -shard_size 500 \\</span><br><span class=\"line\">                -image_channel_size 1</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>Train the model.</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_train -model_type img \\</span><br><span class=\"line\">           -data data/im2text/demo \\</span><br><span class=\"line\">           -save_model demo-model \\</span><br><span class=\"line\">           -gpu_ranks 0 \\</span><br><span class=\"line\">           -batch_size 20 \\</span><br><span class=\"line\">           -max_grad_norm 20 \\</span><br><span class=\"line\">           -learning_rate 0.1 \\</span><br><span class=\"line\">           -word_vec_size 80 \\</span><br><span class=\"line\">           -encoder_type brnn \\</span><br><span class=\"line\">           -image_channel_size 1</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>Translate the images.</li>\n</ol>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_translate -data_type img \\</span><br><span class=\"line\">               -model demo-model_acc_x_ppl_x_e13.pt \\</span><br><span class=\"line\">               -src_dir da](../images \\</span><br><span class=\"line\">               -src data/im2text/src-test.txt \\</span><br><span class=\"line\">               -output pred.txt \\</span><br><span class=\"line\">               -max_length 150 \\</span><br><span class=\"line\">               -beam_size 5 \\</span><br><span class=\"line\">               -gpu 0 \\</span><br><span class=\"line\">               -verbose</span><br></pre></td></tr></table></figure>\n<p>The above dataset is sampled from the <a href=\"http://lstm.seas.harvard.edu/latex/im2text.tgz\" target=\"_blank\">im2latex-100k-dataset</a>. We provide a trained model <a href=\"http://lstm.seas.harvard.edu/latex/py-model.pt\" target=\"_blank\">[link]</a> on this dataset.</p>\n<h3 id=\"options\">Options<a title=\"#options\" href=\"#options\"></a></h3>\n<ul>\n<li>\n<p><code>-src_dir</code>: The directory containing the images.</p>\n</li>\n<li>\n<p><code>-train_tgt</code>: The file storing the tokenized labels, one label per line. It shall look like:</p>\n</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;label0_token0&gt; &lt;label0_token1&gt; ... &lt;label0_tokenN0&gt;</span><br><span class=\"line\">&lt;label1_token0&gt; &lt;label1_token1&gt; ... &lt;label1_tokenN1&gt;</span><br><span class=\"line\">&lt;label2_token0&gt; &lt;label2_token1&gt; ... &lt;label2_tokenN2&gt;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n<ul>\n<li><code>-train_src</code>: The file storing the paths of the images (relative to <code>src_dir</code>).</li>\n</ul>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;image0_path&gt;</span><br><span class=\"line\">&lt;image1_path&gt;</span><br><span class=\"line\">&lt;image2_path&gt;</span><br><span class=\"line\">...</span><br></pre></td></tr></table></figure>\n","link":"links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/im2text","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/im2text/","toc":[{"id":"image-to-text","title":"Image to Text","index":"1","children":[{"id":"dependencies","title":"Dependencies","index":"1.1"},{"id":"quick-start","title":"Quick Start","index":"1.2"},{"id":"options","title":"Options","index":"1.3"}]}],"reward":true}