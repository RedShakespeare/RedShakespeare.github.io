{"title":"","date":"2024-06-21T03:48:26.695Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:26.695Z","content":"<h1 id=\"机器翻译竞赛-唱园杯-pytorch代码-baseline\">机器翻译竞赛-唱园杯-Pytorch代码-Baseline<a title=\"#机器翻译竞赛-唱园杯-pytorch代码-baseline\" href=\"#机器翻译竞赛-唱园杯-pytorch代码-baseline\"></a></h1>\n<p>几天前看到一个机器翻译竞赛-唱园杯，奖金60万，真是吓了一跳。</p>\n<p>不过我不是冲奖金，因为这么高奖金可以想一下竞争程度。我本意想要积累一下中英文翻译数据，后来发现是编码之后的数据…</p>\n<p>有点失望，就没有然后了。所以就没有花太多时间去做这个东西，简单跑了一个baselines。</p>\n<p>官方评测指标简单粗暴，一个句子有一个单词翻译错了就pass。这个比赛数据量不小，迭代20万步，目测需要一周。所以现在排行榜的分数都很低，大佬们估计在等后期发力吧。</p>\n<p>没时间打比赛，一些相关代码也不想浪费掉，就分享给大家，希望对您有所帮助。</p>\n<p>Baseline代码很简单，就是用 OpenNMT-py 这个库做的机器翻译，不过中文关于这个库的资料很少，当初啃这个库也是一点点看的源代码，细节还挺多的。</p>\n<p>默默吐槽一句代码组织架构有点乱，有些地方真的是让人摸不到头脑…</p>\n<p>我也用这个文章做一个简单的 OpenNMT-py 的教程。如果是参加竞赛的话，后期可能需要修改源代码，所以建议大家不用安装 OpenNMT-py 库，而是直接下载源代码，方便修改。</p>\n<p>首先，使用环境如下，大家照此下载就可以:</p>\n<p>torchtext==0.4<br>\nOpenNMT-py==1.0<br>\npython==3.5<br>\ncuda==9.0</p>\n<p>如果 OpenNMT-py==1.0 这个版本大家找不到，直接来我github上下载下来用就可以。</p>\n<h2 id=\"数据预处理\">数据预处理<a title=\"#数据预处理\" href=\"#数据预处理\"></a></h2>\n<p>在 /data 目录下，需要包含四个文件，分别是 src-train.txt  src-val.txt  tgt-train.txt  tgt-val.txt</p>\n<p>假如我们是中文翻译成英文，那么我们的 src-train.txt 和 src-val.txt 就是中文文件， tgt-train.txt  和 tgt-val.txt 就是英文文件。</p>\n<p>其中文件内容格式为每行为一句文本，以空格进行分割。</p>\n<p>对于唱园杯的数据，我们需要对其进行一些简单的修改，以满足上面的要求，我这边给出一个简单的处理代码，以字为单位，代码文件名称为「process_ori_data.py」：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">file=open(&#x27;train_data.csv&#x27;,&#x27;r&#x27;)</span><br><span class=\"line\">lines=file.readlines()</span><br><span class=\"line\"></span><br><span class=\"line\">src_train=open(&#x27;src-train.txt&#x27;,&#x27;w&#x27;)</span><br><span class=\"line\">tgt_train=open(&#x27;tgt-train.txt&#x27;,&#x27;w&#x27;)</span><br><span class=\"line\">src_val=open(&#x27;src-val.txt&#x27;,&#x27;w&#x27;)</span><br><span class=\"line\">tgt_val=open(&#x27;tgt-val.txt&#x27;,&#x27;w&#x27;)</span><br><span class=\"line\"></span><br><span class=\"line\">chinese_lists=[]</span><br><span class=\"line\">english_lists=[]</span><br><span class=\"line\">index=0</span><br><span class=\"line\">for line in lines:</span><br><span class=\"line\">    if index ==0:</span><br><span class=\"line\">        index+=1</span><br><span class=\"line\">        continue</span><br><span class=\"line\">    line=line.strip().split(&#x27;,&#x27;)</span><br><span class=\"line\">    chinese=line[1].strip().split(&#x27;_&#x27;)</span><br><span class=\"line\">    english=line[2].strip().split(&#x27;_&#x27;)</span><br><span class=\"line\">    chinese_lists.append(&#x27; &#x27;.join(chinese))</span><br><span class=\"line\">    english_lists.append(&#x27; &#x27;.join(english))</span><br><span class=\"line\">    index+=1</span><br><span class=\"line\">assert len(chinese_lists)==len(english_lists)</span><br><span class=\"line\">split_num=int(0.85*index)</span><br><span class=\"line\"></span><br><span class=\"line\">for num in range(len(english_lists)):</span><br><span class=\"line\">    if num&lt;=split_num:</span><br><span class=\"line\">        src_train.write(chinese_lists[num]+&#x27;\\n&#x27;)</span><br><span class=\"line\">        tgt_train.write(english_lists[num]+&#x27;\\n&#x27;)</span><br><span class=\"line\">    else:</span><br><span class=\"line\">        src_val.write(chinese_lists[num]+&#x27;\\n&#x27;)</span><br><span class=\"line\">        tgt_val.write(english_lists[num]+&#x27;\\n&#x27;)</span><br><span class=\"line\"></span><br><span class=\"line\">src_train.close()</span><br><span class=\"line\">tgt_train.close()</span><br><span class=\"line\">src_val.close()</span><br><span class=\"line\">tgt_val.close()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>在对原始数据进行处理之后，我们还需要进一步处理，代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nohup python preprocess.py -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/data  -src_seq_length <span class=\"number\">500</span> -tgt_seq_length <span class=\"number\">500</span> &gt;preposs_datalog &amp;</span><br></pre></td></tr></table></figure>\n<p>在这里需要提一个很重要的小细节，就是 src_seq_length 参数 和tgt_seq_length 参数的设定问题。默认这里是50。它的含义是如果句子长度小于50，不会被读入dataset！！！因为唱园杯的数据普遍比较长，所以你如果这里保持默认的话，会出现你只处理了一小部分原始数据的问题。</p>\n<p>具体这个数值你设定为多少，看你自己具体情况。因为唱园杯在数据说明中说到已经去掉了特殊字符等，所以我就全部保留了。</p>\n<h2 id=\"模型进行预测\">模型进行预测<a title=\"#模型进行预测\" href=\"#模型进行预测\"></a></h2>\n<p>直接使用 Transformer 进行训练。Opennmt使用特定参数复现了 Transformer 的效果，这里我们直接套用就可以。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">nohup python train.py -data data/data -save_model data-model \\</span><br><span class=\"line\">        -layers <span class=\"number\">6</span> -rnn_size <span class=\"number\">512</span> -word_vec_size <span class=\"number\">512</span> -transformer_ff <span class=\"number\">2048</span> -heads <span class=\"number\">8</span>  \\</span><br><span class=\"line\">        -encoder_type transformer -decoder_type transformer -position_encoding \\</span><br><span class=\"line\">        -train_steps <span class=\"number\">200000</span>  -max_generator_batches <span class=\"number\">2</span> -dropout <span class=\"number\">0.1</span> \\</span><br><span class=\"line\">        -batch_size <span class=\"number\">4096</span> -batch_type tokens -normalization tokens  -accum_count <span class=\"number\">2</span> \\</span><br><span class=\"line\">        -optim adam -adam_beta2 <span class=\"number\">0.998</span> -decay_method noam -warmup_steps <span class=\"number\">8000</span> -learning_rate <span class=\"number\">2</span> \\</span><br><span class=\"line\">        -max_grad_norm <span class=\"number\">0</span> -param_init <span class=\"number\">0</span>  -param_init_glorot \\</span><br><span class=\"line\">        -label_smoothing <span class=\"number\">0.1</span> -valid_steps <span class=\"number\">10000</span> -save_checkpoint_steps <span class=\"number\">10000</span> \\</span><br><span class=\"line\">        -world_size <span class=\"number\">4</span> -gpu_ranks <span class=\"number\">0</span> <span class=\"number\">1</span> <span class=\"number\">2</span> <span class=\"number\">3</span>  &amp;</span><br></pre></td></tr></table></figure>\n<h2 id=\"预测\">预测<a title=\"#预测\" href=\"#预测\"></a></h2>\n<p>在预测之前，我们需要看一下测试数据，发现是双向预测，所以我们需要将上面的数据颠倒过来再来一次，训练另一个模型即可。</p>\n<p>按道理也可以使用全部数据（颠倒混合），这样训练一个模型就可以，不过我没试过，不知道效果如何，感兴趣的可以试一试。</p>\n<p>预测代码如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python  translate.py  -model demo-model_200000.pt -src data/src-test.txt -output pred.txt </span><br></pre></td></tr></table></figure>\n<h2 id=\"优化思路\">优化思路<a title=\"#优化思路\" href=\"#优化思路\"></a></h2>\n<p>因为是编码之后的数据，所有常规的优化没啥用，这里简单提两个：</p>\n<ol>\n<li>\n<p>使用全部数据（训练数据和测试数据）训练Word2vec/Glove/Bert 等，然后作为输入，从而加入先验信息</p>\n</li>\n<li>\n<p>如果不想自己训练，可以使用词频对应到编码之后的数据，得到一个大致的结果，从而可以使用我们正常的word2vec/glove/bert</p>\n</li>\n</ol>\n","link":"links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/README","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/README/","toc":[{"id":"机器翻译竞赛-唱园杯-pytorch代码-baseline","title":"机器翻译竞赛-唱园杯-Pytorch代码-Baseline","index":"1","children":[{"id":"数据预处理","title":"数据预处理","index":"1.1"},{"id":"模型进行预测","title":"模型进行预测","index":"1.2"},{"id":"预测","title":"预测","index":"1.3"},{"id":"优化思路","title":"优化思路","index":"1.4"}]}],"reward":true}