{"title":"","date":"2024-06-21T03:48:26.755Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:26.755Z","content":"<h1 id=\"opennmt-py:-open-source-neural-machine-translation\">OpenNMT-py: Open-Source Neural Machine Translation<a title=\"#opennmt-py:-open-source-neural-machine-translation\" href=\"#opennmt-py:-open-source-neural-machine-translation\"></a></h1>\n<p><a href=\"https://travis-ci.org/OpenNMT/OpenNMT-py\" target=\"_blank\"><img src=\"https://travis-ci.org/OpenNMT/OpenNMT-py.svg?branch=master\" alt=\"Build Status\" loading=\"lazy\"></a><br>\n<a href=\"https://floydhub.com/run?template=https://github.com/OpenNMT/OpenNMT-py\" target=\"_blank\"><img src=\"https://img.shields.io/badge/Run%20on-FloydHub-blue.svg\" alt=\"Run on FH\" loading=\"lazy\"></a></p>\n<p>This is a <a href=\"https://github.com/pytorch/pytorch\" target=\"_blank\">Pytorch</a><br>\nport of <a href=\"https://github.com/OpenNMT/OpenNMT\" target=\"_blank\">OpenNMT</a>,<br>\nan open-source (MIT) neural machine translation system. It is designed to be research friendly to try out new ideas in translation, summary, image-to-text, morphology, and many other domains. Some companies have proven the code to be production ready.</p>\n<p>We love contributions. Please consult the Issues page for any <a href=\"https://github.com/OpenNMT/OpenNMT-py/issues?q=is%3Aissue+is%3Aopen+label%3A%22contributions+welcome%22\" target=\"_blank\">Contributions Welcome</a> tagged post.</p>\n<center style=\"padding: 40px\"><img width=\"70%\" src=\"http://opennmt.github.io/simple-attn.png\" /></center>\n<p>Before raising an issue, make sure you read the requirements and the documentation examples.</p>\n<p>Unless there is a bug, please use the <a href=\"http://forum.opennmt.net\" target=\"_blank\">Forum</a> or <a href=\"https://gitter.im/OpenNMT/OpenNMT-py\" target=\"_blank\">Gitter</a> to ask questions.</p>\n<h1 id=\"table-of-contents\">Table of Contents<a title=\"#table-of-contents\" href=\"#table-of-contents\"></a></h1>\n<ul>\n<li><a href=\"http://opennmt.net/OpenNMT-py/\" target=\"_blank\">Full Documentation</a></li>\n<li><a href=\"#requirements\">Requirements</a></li>\n<li><a href=\"#features\">Features</a></li>\n<li><a href=\"#quickstart\">Quickstart</a></li>\n<li><a href=\"#run-on-floydhub\">Run on FloydHub</a></li>\n<li><a href=\"#acknowledgements\">Acknowledgements</a></li>\n<li><a href=\"#citation\">Citation</a></li>\n</ul>\n<h2 id=\"requirements\">Requirements<a title=\"#requirements\" href=\"#requirements\"></a></h2>\n<p>Install <code>OpenNMT-py</code> from <code>pip</code>:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install OpenNMT-py</span><br></pre></td></tr></table></figure>\n<p>or from the sources:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/OpenNMT/OpenNMT-py.git</span><br><span class=\"line\"><span class=\"built_in\">cd</span> OpenNMT-py</span><br><span class=\"line\">python setup.py install</span><br></pre></td></tr></table></figure>\n<p>Note: If you have MemoryError in the install try to use <code>pip</code> with <code>--no-cache-dir</code>.</p>\n<p><em>(Optionnal)</em> some advanced features (e.g. working audio, image or pretrained models) requires extra packages, you can install it with:</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install -r requirements.opt.txt</span><br></pre></td></tr></table></figure>\n<p>Note:</p>\n<ul>\n<li>some features require Python 3.5 and after (eg: Distributed multigpu, entmax)</li>\n<li>we currently only support PyTorch 1.2 (should work with 1.1)</li>\n</ul>\n<h2 id=\"features\">Features<a title=\"#features\" href=\"#features\"></a></h2>\n<ul>\n<li><a href=\"http://opennmt.net/OpenNMT-py/options/train.html#model-encoder-decoder\" target=\"_blank\">Seq2Seq models (encoder-decoder) with multiple RNN cells (lstm/gru) and attention (dotprod/mlp) types</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-the-transformer-model\" target=\"_blank\">Transformer models</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/options/train.html#model-attention\" target=\"_blank\">Copy and Coverage Attention</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-pretrained-embeddings-e-g-glove\" target=\"_blank\">Pretrained Embeddings</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/options/train.html#model-embeddings\" target=\"_blank\">Source word features</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/im2text.html\" target=\"_blank\">Image-to-text processing</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/speech2text.html\" target=\"_blank\">Speech-to-text processing</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/options/train.html#logging\" target=\"_blank\">TensorBoard logging</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/FAQ.html##do-you-support-multi-gpu\" target=\"_blank\">Multi-GPU training</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/options/preprocess.html\" target=\"_blank\">Data preprocessing</a></li>\n<li><a href=\"http://opennmt.net/OpenNMT-py/options/translate.html\" target=\"_blank\">Inference (translation) with batching and beam search</a></li>\n<li>Inference time loss functions.</li>\n<li>[Conv2Conv convolution model]</li>\n<li>SRU “RNNs faster than CNN” paper</li>\n<li>Mixed-precision training with <a href=\"https://github.com/NVIDIA/apex\" target=\"_blank\">APEX</a>, optimized on <a href=\"https://developer.nvidia.com/tensor-cores\" target=\"_blank\">Tensor Cores</a></li>\n</ul>\n<h2 id=\"quickstart\">Quickstart<a title=\"#quickstart\" href=\"#quickstart\"></a></h2>\n<p><a href=\"http://opennmt.net/OpenNMT-py/\" target=\"_blank\">Full Documentation</a></p>\n<h3 id=\"step-1:-preprocess-the-data\">Step 1: Preprocess the data<a title=\"#step-1:-preprocess-the-data\" href=\"#step-1:-preprocess-the-data\"></a></h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_preprocess -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo</span><br></pre></td></tr></table></figure>\n<p>We will be working with some example data in <code>data/</code> folder.</p>\n<p>The data consists of parallel source (<code>src</code>) and target (<code>tgt</code>) data containing one sentence per line with tokens separated by a space:</p>\n<ul>\n<li><code>src-train.txt</code></li>\n<li><code>tgt-train.txt</code></li>\n<li><code>src-val.txt</code></li>\n<li><code>tgt-val.txt</code></li>\n</ul>\n<p>Validation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.</p>\n<p>After running the preprocessing, the following files are generated:</p>\n<ul>\n<li><code>demo.train.pt</code>: serialized PyTorch file containing training data</li>\n<li><code>demo.valid.pt</code>: serialized PyTorch file containing validation data</li>\n<li><code>demo.vocab.pt</code>: serialized PyTorch file containing vocabulary data</li>\n</ul>\n<p>Internally the system never touches the words themselves, but uses these indices.</p>\n<h3 id=\"step-2:-train-the-model\">Step 2: Train the model<a title=\"#step-2:-train-the-model\" href=\"#step-2:-train-the-model\"></a></h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_train -data data/demo -save_model demo-model</span><br></pre></td></tr></table></figure>\n<p>The main train command is quite simple. Minimally it takes a data file<br>\nand a save file.  This will run the default model, which consists of a<br>\n2-layer LSTM with 500 hidden units on both the encoder/decoder.<br>\nIf you want to train on GPU, you need to set, as an example:<br>\nCUDA_VISIBLE_DEVICES=1,3<br>\n<code>-world_size 2 -gpu_ranks 0 1</code> to use (say) GPU 1 and 3 on this node only.<br>\nTo know more about distributed training on single or multi nodes, read the FAQ section.</p>\n<h3 id=\"step-3:-translate\">Step 3: Translate<a title=\"#step-3:-translate\" href=\"#step-3:-translate\"></a></h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_translate -model demo-model_acc_XX.XX_ppl_XXX.XX_eX.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose</span><br></pre></td></tr></table></figure>\n<p>Now you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into <code>pred.txt</code>.</p>\n<p>!!! note “Note”<br>\nThe predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for <a href=\"http://www.statmt.org/wmt16/translation-task.html\" target=\"_blank\">translation</a> or <a href=\"https://github.com/harvardnlp/sent-summary\" target=\"_blank\">summarization</a>.</p>\n<h2 id=\"alternative:-run-on-floydhub\">Alternative: Run on FloydHub<a title=\"#alternative:-run-on-floydhub\" href=\"#alternative:-run-on-floydhub\"></a></h2>\n<p><a href=\"https://floydhub.com/run?template=https://github.com/OpenNMT/OpenNMT-py\" target=\"_blank\"><img src=\"https://static.floydhub.com/button/button.svg\" alt=\"Run on FloydHub\" loading=\"lazy\"></a></p>\n<p>Click this button to open a Workspace on <a href=\"https://www.floydhub.com/?utm_medium=readme&amp;utm_source=opennmt-py&amp;utm_campaign=jul_2018\" target=\"_blank\">FloydHub</a> for training/testing your code.</p>\n<h2 id=\"pretrained-embeddings-(e.g.-glove)\">Pretrained embeddings (e.g. GloVe)<a title=\"#pretrained-embeddings-(e.g.-glove)\" href=\"#pretrained-embeddings-(e.g.-glove)\"></a></h2>\n<p>Please see the FAQ: <a href=\"http://opennmt.net/OpenNMT-py/FAQ.html#how-do-i-use-pretrained-embeddings-e-g-glove\" target=\"_blank\">How to use GloVe pre-trained embeddings in OpenNMT-py</a></p>\n<h2 id=\"pretrained-models\">Pretrained Models<a title=\"#pretrained-models\" href=\"#pretrained-models\"></a></h2>\n<p>The following pretrained models can be downloaded and used with <a href=\"http://translate.py\">translate.py</a>.</p>\n<p><a href=\"http://opennmt.net/Models-py/\" target=\"_blank\">http://opennmt.net/Models-py/</a></p>\n<h2 id=\"acknowledgements\">Acknowledgements<a title=\"#acknowledgements\" href=\"#acknowledgements\"></a></h2>\n<p>OpenNMT-py is run as a collaborative open-source project.<br>\nThe original code was written by <a href=\"http://github.com/adamlerer\" target=\"_blank\">Adam Lerer</a> (NYC) to reproduce OpenNMT-Lua using Pytorch.</p>\n<p>Major contributors are:<br>\n<a href=\"https://github.com/srush\" target=\"_blank\">Sasha Rush</a> (Cambridge, MA)<br>\n<a href=\"https://github.com/vince62s\" target=\"_blank\">Vincent Nguyen</a> (Ubiqus)<br>\n<a href=\"http://github.com/bpopeters\" target=\"_blank\">Ben Peters</a> (Lisbon)<br>\n<a href=\"https://github.com/sebastianGehrmann\" target=\"_blank\">Sebastian Gehrmann</a> (Harvard NLP)<br>\n<a href=\"https://github.com/da03\" target=\"_blank\">Yuntian Deng</a> (Harvard NLP)<br>\n<a href=\"https://github.com/guillaumekln\" target=\"_blank\">Guillaume Klein</a> (Systran)<br>\n<a href=\"https://github.com/pltrdy\" target=\"_blank\">Paul Tardy</a> (Ubiqus / Lium)<br>\n<a href=\"https://github.com/francoishernandez\" target=\"_blank\">François Hernandez</a> (Ubiqus)<br>\n<a href=\"http://github.com/jianyuzhan\" target=\"_blank\">Jianyu Zhan</a> (Shanghai)<br>\n[Dylan Flaute](<a href=\"http://github.com/flauted\" target=\"_blank\">http://github.com/flauted</a> (University of Dayton)<br>\nand more !</p>\n<p>OpentNMT-py belongs to the OpenNMT project along with OpenNMT-Lua and OpenNMT-tf.</p>\n<h2 id=\"citation\">Citation<a title=\"#citation\" href=\"#citation\"></a></h2>\n<p><a href=\"https://arxiv.org/pdf/1805.11462\" target=\"_blank\">OpenNMT: Neural Machine Translation Toolkit</a></p>\n<p><a href=\"https://doi.org/10.18653/v1/P17-4012\" target=\"_blank\">OpenNMT technical report</a></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;opennmt,</span><br><span class=\"line\">  author    = &#123;Guillaume Klein and</span><br><span class=\"line\">               Yoon Kim and</span><br><span class=\"line\">               Yuntian Deng and</span><br><span class=\"line\">               Jean Senellart and</span><br><span class=\"line\">               Alexander M. Rush&#125;,</span><br><span class=\"line\">  title     = &#123;Open&#123;NMT&#125;: Open-Source Toolkit for Neural Machine Translation&#125;,</span><br><span class=\"line\">  booktitle = &#123;Proc. ACL&#125;,</span><br><span class=\"line\">  year      = &#123;2017&#125;,</span><br><span class=\"line\">  url       = &#123;https://doi.org/10.18653/v1/P17-4012&#125;,</span><br><span class=\"line\">  doi       = &#123;10.18653/v1/P17-4012&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","link":"links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/README_old","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/README_old/","toc":[{"id":"opennmt-py:-open-source-neural-machine-translation","title":"OpenNMT-py: Open-Source Neural Machine Translation","index":"1"},{"id":"table-of-contents","title":"Table of Contents","index":"2","children":[{"id":"requirements","title":"Requirements","index":"2.1"},{"id":"features","title":"Features","index":"2.2"},{"id":"quickstart","title":"Quickstart","index":"2.3","children":[{"id":"step-1:-preprocess-the-data","title":"Step 1: Preprocess the data","index":"2.3.1"},{"id":"step-2:-train-the-model","title":"Step 2: Train the model","index":"2.3.2"},{"id":"step-3:-translate","title":"Step 3: Translate","index":"2.3.3"}]},{"id":"alternative:-run-on-floydhub","title":"Alternative: Run on FloydHub","index":"2.4"},{"id":"pretrained-embeddings-(e.g.-glove)","title":"Pretrained embeddings (e.g. GloVe)","index":"2.5"},{"id":"pretrained-models","title":"Pretrained Models","index":"2.6"},{"id":"acknowledgements","title":"Acknowledgements","index":"2.7"},{"id":"citation","title":"Citation","index":"2.8"}]}],"reward":true}