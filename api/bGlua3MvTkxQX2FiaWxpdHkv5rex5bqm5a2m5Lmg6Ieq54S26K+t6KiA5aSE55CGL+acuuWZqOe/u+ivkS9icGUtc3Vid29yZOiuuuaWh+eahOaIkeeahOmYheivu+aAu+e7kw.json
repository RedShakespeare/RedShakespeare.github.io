{"title":"","date":"2024-06-21T03:48:21.435Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:21.435Z","content":"<p>bpe论文的我的阅读感受</p>\n<p>Neural Machine Translation of Rare Words with Subword Units</p>\n<p>提出这个算法的直觉是这样的，作者发现翻译一个单词，有时候不需要这个单词的全部信息，可能只需要一部分信息就可以知道大致信息。还有一种可能是翻译这个单词与，可能通过组成<br>\n这个单词的多个小单元信息来翻译就可以了。</p>\n<p>这个方法是为了解决稀少单词（也就是频率在人为规定下的单词）和未登录词没有办法有效翻译的问题。</p>\n<p>这里作者在摘要中提到了一个back-off 字典，就是说在之前翻译模型在处理未登录词汇的时候，处理办法是使用一个词典，把source-target中未登陆词汇一一对应起来，我们在翻译过程中<br>\n如果出现了未登录词汇，直接使用字典中的对应关系进行替换就可以了。但是这样存在一个问题，就是说，最低频率是我们人为规定的，有些时候在调参的时候，这个频率是一个超参，，那么<br>\n我们在准备词典的时候，就是一个动态的长度，这样很不方便，但是如果我们准备所有单词的back-off就得不偿失。还有一个问题是我们不确定source-target对应的关系是一一对应的，可能对应不上，可能对应<br>\n是多种，在不同句子环境中，我们需要选择不同的单词翻译，这也是存在的一个问题。</p>\n<p>基于word-level的模型还存在一个问题，就是不能产生没有看见过的单词，也就是说在翻译端，没有出现在词汇表中的在翻译模型测试的时候是不会出现的。这其实是一个很重要的问题，就是我不能确定<br>\n我的训练语料包含所有情况下的翻译。</p>\n<p>作者在摘要中说明，自己使用了字符级的n-gram和bpe方法，在WMT 15 英文德文翻译中提升1.1，在英文俄罗斯中提升1.3。</p>\n<p>翻译是一个开放词汇表的问题，我们在翻译模型中，一般把翻译模型词汇表限制在30000–50000（基于词）。</p>\n<p>作者通过实验发现，使用subeword模型，比使用大量词汇表的模型和使用back-off模型效果很好更简单。</p>\n<p>我在博客中看到了总结这个论文不错的博客，总结在下面<br>\n通过BPE解决OOV问题----Neural machine Translation of Rare Words with Subword Units<br>\n<a href=\"https://blog.csdn.net/weixin_38937984/article/details/101723700\" target=\"_blank\">https://blog.csdn.net/weixin_38937984/article/details/101723700</a></p>\n","link":"links/NLP_ability/深度学习自然语言处理/机器翻译/bpe-subword论文的我的阅读总结","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/bpe-subword论文的我的阅读总结/","reward":true}