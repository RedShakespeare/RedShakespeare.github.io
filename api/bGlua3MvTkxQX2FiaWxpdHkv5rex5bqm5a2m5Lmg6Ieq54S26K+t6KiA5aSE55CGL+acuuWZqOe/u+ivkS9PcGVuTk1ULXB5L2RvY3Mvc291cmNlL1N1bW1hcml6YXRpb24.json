{"title":"","date":"2024-06-21T03:48:23.135Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:23.135Z","content":"<h1 id=\"summarization\">Summarization<a title=\"#summarization\" href=\"#summarization\"></a></h1>\n<p>Note: The process and results below are presented in our paper <code>Bottom-Up Abstractive Summarization</code>. Please consider citing it if you follow these instructions.</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;gehrmann2018bottom,</span><br><span class=\"line\">  title=&#123;Bottom-Up Abstractive Summarization&#125;,</span><br><span class=\"line\">  author=&#123;Gehrmann, Sebastian and Deng, Yuntian and Rush, Alexander&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing&#125;,</span><br><span class=\"line\">  pages=&#123;4098--4109&#125;,</span><br><span class=\"line\">  year=&#123;2018&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>This document describes how to replicate summarization experiments on the CNN-DM and gigaword datasets using OpenNMT-py.<br>\nIn the following, we assume access to a tokenized form of the corpus split into train/valid/test set. You can find the data <a href=\"https://github.com/harvardnlp/sent-summary\" target=\"_blank\">here</a>.</p>\n<p>An example article-title pair from Gigaword should look like this:</p>\n<p><strong>Input</strong><br>\n<em>australia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .</em></p>\n<p><strong>Output</strong><br>\n<em>australian current account deficit narrows sharply</em></p>\n<h3 id=\"preprocessing-the-data\">Preprocessing the data<a title=\"#preprocessing-the-data\" href=\"#preprocessing-the-data\"></a></h3>\n<p>Since we are using copy-attention [1] in the model, we need to preprocess the dataset such that source and target are aligned and use the same dictionary. This is achieved by using the options <code>dynamic_dict</code> and <code>share_vocab</code>.<br>\nWe additionally turn off truncation of the source to ensure that inputs longer than 50 words are not truncated.<br>\nFor CNN-DM we follow See et al. [2] and additionally truncate the source length at 400 tokens and the target at 100. We also note that in CNN-DM, we found models to work better if the target surrounds sentences with tags such that a sentence looks like <code>&lt;t&gt; w1 w2 w3 . &lt;/t&gt;</code>. If you use this formatting, you can remove the tags after the inference step with the commands <code>sed -i 's/ &lt;\\/t&gt;//g' FILE.txt</code> and <code>sed -i 's/&lt;t&gt; //g' FILE.txt</code>.</p>\n<p><strong>Command used</strong>:</p>\n<p>(1) CNN-DM</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_preprocess -train_src data/cnndm/train.txt.src \\</span><br><span class=\"line\">                -train_tgt data/cnndm/train.txt.tgt.tagged \\</span><br><span class=\"line\">                -valid_src data/cnndm/val.txt.src \\</span><br><span class=\"line\">                -valid_tgt data/cnndm/val.txt.tgt.tagged \\</span><br><span class=\"line\">                -save_data data/cnndm/CNNDM \\</span><br><span class=\"line\">                -src_seq_length 10000 \\</span><br><span class=\"line\">                -tgt_seq_length 10000 \\</span><br><span class=\"line\">                -src_seq_length_trunc 400 \\</span><br><span class=\"line\">                -tgt_seq_length_trunc 100 \\</span><br><span class=\"line\">                -dynamic_dict \\</span><br><span class=\"line\">                -share_vocab \\</span><br><span class=\"line\">                -shard_size 100000</span><br></pre></td></tr></table></figure>\n<p>(2) Gigaword</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_preprocess -train_src data/giga/train.article.txt \\</span><br><span class=\"line\">                -train_tgt data/giga/train.title.txt \\</span><br><span class=\"line\">                -valid_src data/giga/valid.article.txt \\</span><br><span class=\"line\">                -valid_tgt data/giga/valid.title.txt \\</span><br><span class=\"line\">                -save_data data/giga/GIGA \\</span><br><span class=\"line\">                -src_seq_length 10000 \\</span><br><span class=\"line\">                -dynamic_dict \\</span><br><span class=\"line\">                -share_vocab \\</span><br><span class=\"line\">                -shard_size 100000</span><br></pre></td></tr></table></figure>\n<h3 id=\"training\">Training<a title=\"#training\" href=\"#training\"></a></h3>\n<p>The training procedure described in this section for the most part follows parameter choices and implementation similar to that of See et al. [2]. We describe notable options in the following list:</p>\n<ul>\n<li><code>copy_attn</code>: This is the most important option, since it allows the model to copy words from the source.</li>\n<li><code>global_attention mlp</code>: This makes the model use the  attention mechanism introduced by Bahdanau et al. [3] instead of that by Luong et al. [4] (<code>global_attention dot</code>).</li>\n<li><code>share_embeddings</code>: This shares the word embeddings between encoder and decoder. This option drastically decreases the number of parameters a model has to learn. We did not find this option to helpful, but you can try it out by adding it to the command below.</li>\n<li><code>reuse_copy_attn</code>: This option reuses the standard attention as copy attention. Without this, the model learns an additional attention that is only used for copying.</li>\n<li><code>copy_loss_by_seqlength</code>: This modifies the loss to divide the loss of a sequence by the number of tokens in it. In practice, we found this to generate longer sequences during inference. However, this effect can also be achieved by using penalties during decoding.</li>\n<li><code>bridge</code>: This is an additional layer that uses the final hidden state of the encoder as input and computes an initial hidden state for the decoder. Without this, the decoder is initialized with the final hidden state of the encoder directly.</li>\n<li><code>optim adagrad</code>: Adagrad outperforms SGD when coupled with the following option.</li>\n<li><code>adagrad_accumulator_init 0.1</code>: PyTorch does not initialize the accumulator in adagrad with any values. To match the optimization algorithm with the Tensorflow version, this option needs to be added.</li>\n</ul>\n<p>We are using using a 128-dimensional word-embedding, and 512-dimensional 1 layer LSTM. On the encoder side, we use a bidirectional LSTM (<code>brnn</code>), which means that the 512 dimensions are split into 256 dimensions per direction.</p>\n<p>We additionally set the maximum norm of the gradient to 2, and renormalize if the gradient norm exceeds this value and do not use any dropout.</p>\n<p><strong>commands used</strong>:</p>\n<p>(1) CNN-DM</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_train -save_model models/cnndm \\</span><br><span class=\"line\">           -data data/cnndm/CNNDM \\</span><br><span class=\"line\">           -copy_attn \\</span><br><span class=\"line\">           -global_attention mlp \\</span><br><span class=\"line\">           -word_vec_size 128 \\</span><br><span class=\"line\">           -rnn_size 512 \\</span><br><span class=\"line\">           -layers 1 \\</span><br><span class=\"line\">           -encoder_type brnn \\</span><br><span class=\"line\">           -train_steps 200000 \\</span><br><span class=\"line\">           -max_grad_norm 2 \\</span><br><span class=\"line\">           -dropout 0. \\</span><br><span class=\"line\">           -batch_size 16 \\</span><br><span class=\"line\">           -valid_batch_size 16 \\</span><br><span class=\"line\">           -optim adagrad \\</span><br><span class=\"line\">           -learning_rate 0.15 \\</span><br><span class=\"line\">           -adagrad_accumulator_init 0.1 \\</span><br><span class=\"line\">           -reuse_copy_attn \\</span><br><span class=\"line\">           -copy_loss_by_seqlength \\</span><br><span class=\"line\">           -bridge \\</span><br><span class=\"line\">           -seed 777 \\</span><br><span class=\"line\">           -world_size 2 \\</span><br><span class=\"line\">           -gpu_ranks 0 1</span><br></pre></td></tr></table></figure>\n<p>(2) CNN-DM Transformer</p>\n<p>The following script trains the transformer model on CNN-DM</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_train -data data/cnndm/CNNDM \\</span><br><span class=\"line\">           -save_model models/cnndm \\</span><br><span class=\"line\">           -layers 4 \\</span><br><span class=\"line\">           -rnn_size 512 \\</span><br><span class=\"line\">           -word_vec_size 512 \\</span><br><span class=\"line\">           -max_grad_norm 0 \\</span><br><span class=\"line\">           -optim adam \\</span><br><span class=\"line\">           -encoder_type transformer \\</span><br><span class=\"line\">           -decoder_type transformer \\</span><br><span class=\"line\">           -position_encoding \\</span><br><span class=\"line\">           -dropout 0\\.2 \\</span><br><span class=\"line\">           -param_init 0 \\</span><br><span class=\"line\">           -warmup_steps 8000 \\</span><br><span class=\"line\">           -learning_rate 2 \\</span><br><span class=\"line\">           -decay_method noam \\</span><br><span class=\"line\">           -label_smoothing 0.1 \\</span><br><span class=\"line\">           -adam_beta2 0.998 \\</span><br><span class=\"line\">           -batch_size 4096 \\</span><br><span class=\"line\">           -batch_type tokens \\</span><br><span class=\"line\">           -normalization tokens \\</span><br><span class=\"line\">           -max_generator_batches 2 \\</span><br><span class=\"line\">           -train_steps 200000 \\</span><br><span class=\"line\">           -accum_count 4 \\</span><br><span class=\"line\">           -share_embeddings \\</span><br><span class=\"line\">           -copy_attn \\</span><br><span class=\"line\">           -param_init_glorot \\</span><br><span class=\"line\">           -world_size 2 \\</span><br><span class=\"line\">           -gpu_ranks 0 1</span><br></pre></td></tr></table></figure>\n<p>(3) Gigaword</p>\n<p>Gigaword can be trained equivalently. As a baseline, we show a model trained with the following command:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_train -data data/giga/GIGA \\</span><br><span class=\"line\">           -save_model models/giga \\</span><br><span class=\"line\">           -copy_attn \\</span><br><span class=\"line\">           -reuse_copy_attn \\</span><br><span class=\"line\">           -train_steps 200000</span><br></pre></td></tr></table></figure>\n<h3 id=\"inference\">Inference<a title=\"#inference\" href=\"#inference\"></a></h3>\n<p>During inference, we use beam-search with a beam-size of 10. We also added specific penalties that we can use during decoding, described in the following.</p>\n<ul>\n<li><code>stepwise_penalty</code>: Applies penalty at every step</li>\n<li><code>coverage_penalty summary</code>: Uses a penalty that prevents repeated attention to the same source word</li>\n<li><code>beta 5</code>: Parameter for the Coverage Penalty</li>\n<li><code>length_penalty wu</code>: Uses the Length Penalty by Wu et al.</li>\n<li><code>alpha 0.8</code>: Parameter for the Length Penalty.</li>\n<li><code>block_ngram_repeat 3</code>: Prevent the model from repeating trigrams.</li>\n<li><code>ignore_when_blocking &quot;.&quot; &quot;&lt;/t&gt;&quot; &quot;&lt;t&gt;&quot;</code>: Allow the model to repeat trigrams with the sentence boundary tokens.</li>\n</ul>\n<p><strong>commands used</strong>:</p>\n<p>(1) CNN-DM</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">onmt_translate -gpu X \\</span><br><span class=\"line\">               -batch_size 20 \\</span><br><span class=\"line\">               -beam_size 10 \\</span><br><span class=\"line\">               -model models/cnndm... \\</span><br><span class=\"line\">               -src data/cnndm/test.txt.src \\</span><br><span class=\"line\">               -output testout/cnndm.out \\</span><br><span class=\"line\">               -min_length 35 \\</span><br><span class=\"line\">               -verbose \\</span><br><span class=\"line\">               -stepwise_penalty \\</span><br><span class=\"line\">               -coverage_penalty summary \\</span><br><span class=\"line\">               -beta 5 \\</span><br><span class=\"line\">               -length_penalty wu \\</span><br><span class=\"line\">               -alpha 0.9 \\</span><br><span class=\"line\">               -verbose \\</span><br><span class=\"line\">               -block_ngram_repeat 3 \\</span><br><span class=\"line\">               -ignore_when_blocking &quot;.&quot; &quot;&lt;/t&gt;&quot; &quot;&lt;t&gt;&quot;</span><br></pre></td></tr></table></figure>\n<h3 id=\"evaluation\">Evaluation<a title=\"#evaluation\" href=\"#evaluation\"></a></h3>\n<h4 id=\"cnn-dm\">CNN-DM<a title=\"#cnn-dm\" href=\"#cnn-dm\"></a></h4>\n<p>To evaluate the ROUGE scores on CNN-DM, we extended the pyrouge wrapper with additional evaluations such as the amount of repeated n-grams (typically found in models with copy attention), found <a href=\"https://github.com/sebastianGehrmann/rouge-baselines\" target=\"_blank\">here</a>. The repository includes a sub-repo called pyrouge. Make sure to clone the code with the <code>git clone --recurse-submodules https://github.com/sebastianGehrmann/rouge-baselines</code> command to check this out as well and follow the installation instructions on the pyrouge repository before calling this script.<br>\nThe installation instructions can be found <a href=\"https://github.com/falcondai/pyrouge/tree/9cdbfbda8b8d96e7c2646ffd048743ddcf417ed9#installation\" target=\"_blank\">here</a>. Note that on MacOS, we found that the pointer to your perl installation in line 1 of <code>pyrouge/RELEASE-1.5.5/ROUGE-1.5.5.pl</code> might be different from the one you have installed. A simple fix is to change this line to <code>#!/usr/local/bin/perl -w</code> if it fails.</p>\n<p>It can be run with the following command:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python baseline.py -s testout/cnndm.out -t data/cnndm/test.txt.tgt.tagged -m sent_tag_verbatim -r</span><br></pre></td></tr></table></figure>\n<p>The <code>sent_tag_verbatim</code> option strips <code>&lt;t&gt;</code> and <code>&lt;/t&gt;</code> tags around sentences - when a sentence previously was <code>&lt;t&gt; w w w w . &lt;/t&gt;</code>, it becomes <code>w w w w .</code>.</p>\n<h4 id=\"gigaword\">Gigaword<a title=\"#gigaword\" href=\"#gigaword\"></a></h4>\n<p>For evaluation of large test sets such as Gigaword, we use the a parallel python wrapper around ROUGE, found <a href=\"https://github.com/pltrdy/files2rouge\" target=\"_blank\">here</a>.</p>\n<p><strong>command used</strong>:<br>\n<code>files2rouge giga.out test.title.txt --verbose</code></p>\n<h3 id=\"scores-and-models\">Scores and Models<a title=\"#scores-and-models\" href=\"#scores-and-models\"></a></h3>\n<h4 id=\"cnn-dm-1\">CNN-DM<a title=\"#cnn-dm-1\" href=\"#cnn-dm-1\"></a></h4>\n<div class=\"φbq\"><div class=\"φbs\"><table><thead>\n<tr>\n<th>Model Type</th>\n<th>Model</th>\n<th style=\"text-align:right\">R1 R</th>\n<th style=\"text-align:right\">R1 P</th>\n<th style=\"text-align:right\">R1 F</th>\n<th style=\"text-align:right\">R2 R</th>\n<th style=\"text-align:right\">R2 P</th>\n<th style=\"text-align:right\">R2 F</th>\n<th style=\"text-align:right\">RL R</th>\n<th style=\"text-align:right\">RL P</th>\n<th style=\"text-align:right\">RL F</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Pointer-Generator + Coverage [2]</td>\n<td><a href=\"https://github.com/abisee/pointer-generator\" target=\"_blank\">link</a></td>\n<td style=\"text-align:right\">39.05</td>\n<td style=\"text-align:right\">43.02</td>\n<td style=\"text-align:right\">39.53</td>\n<td style=\"text-align:right\">17.16</td>\n<td style=\"text-align:right\">18.77</td>\n<td style=\"text-align:right\">17.28</td>\n<td style=\"text-align:right\">35.98</td>\n<td style=\"text-align:right\">39.56</td>\n<td style=\"text-align:right\">36.38</td>\n</tr>\n<tr>\n<td>Pointer-Generator [2]</td>\n<td><a href=\"https://github.com/abisee/pointer-generator\" target=\"_blank\">link</a></td>\n<td style=\"text-align:right\">37.76</td>\n<td style=\"text-align:right\">37.60</td>\n<td style=\"text-align:right\">36.44</td>\n<td style=\"text-align:right\">16.31</td>\n<td style=\"text-align:right\">16.12</td>\n<td style=\"text-align:right\">15.66</td>\n<td style=\"text-align:right\">34.66</td>\n<td style=\"text-align:right\">34.46</td>\n<td style=\"text-align:right\">33.42</td>\n</tr>\n<tr>\n<td>OpenNMT BRNN  (1 layer, emb 128, hid 512)</td>\n<td><a href=\"https://s3.amazonaws.com/opennmt-models/Summary/ada6_bridge_oldcopy_tagged_acc_54.17_ppl_11.17_e20.pt\" target=\"_blank\">link</a></td>\n<td style=\"text-align:right\">40.90</td>\n<td style=\"text-align:right\">40.20</td>\n<td style=\"text-align:right\">39.02</td>\n<td style=\"text-align:right\">17.91</td>\n<td style=\"text-align:right\">17.99</td>\n<td style=\"text-align:right\">17.25</td>\n<td style=\"text-align:right\">37.76</td>\n<td style=\"text-align:right\">37.18</td>\n<td style=\"text-align:right\">36.05</td>\n</tr>\n<tr>\n<td>OpenNMT BRNN  (1 layer, emb 128, hid 512, shared embeddings)</td>\n<td><a href=\"https://s3.amazonaws.com/opennmt-models/Summary/ada6_bridge_oldcopy_tagged_share_acc_54.50_ppl_10.89_e20.pt\" target=\"_blank\">link</a></td>\n<td style=\"text-align:right\">38.59</td>\n<td style=\"text-align:right\">40.60</td>\n<td style=\"text-align:right\">37.97</td>\n<td style=\"text-align:right\">16.75</td>\n<td style=\"text-align:right\">17.93</td>\n<td style=\"text-align:right\">16.59</td>\n<td style=\"text-align:right\">35.67</td>\n<td style=\"text-align:right\">37.60</td>\n<td style=\"text-align:right\">35.13</td>\n</tr>\n<tr>\n<td>OpenNMT BRNN (2 layer, emb 256, hid 1024)</td>\n<td><a href=\"https://s3.amazonaws.com/opennmt-models/Summary/ada6_bridge_oldcopy_tagged_larger_acc_54.84_ppl_10.58_e17.pt\" target=\"_blank\">link</a></td>\n<td style=\"text-align:right\">40.41</td>\n<td style=\"text-align:right\">40.94</td>\n<td style=\"text-align:right\">39.12</td>\n<td style=\"text-align:right\">17.76</td>\n<td style=\"text-align:right\">18.38</td>\n<td style=\"text-align:right\">17.35</td>\n<td style=\"text-align:right\">37.27</td>\n<td style=\"text-align:right\">37.83</td>\n<td style=\"text-align:right\">36.12</td>\n</tr>\n<tr>\n<td>OpenNMT Transformer</td>\n<td><a href=\"https://s3.amazonaws.com/opennmt-models/sum_transformer_model_acc_57.25_ppl_9.22_e16.pt\" target=\"_blank\">link</a></td>\n<td style=\"text-align:right\">40.31</td>\n<td style=\"text-align:right\">41.09</td>\n<td style=\"text-align:right\">39.25</td>\n<td style=\"text-align:right\">17.97</td>\n<td style=\"text-align:right\">18.46</td>\n<td style=\"text-align:right\">17.54</td>\n<td style=\"text-align:right\">37.41</td>\n<td style=\"text-align:right\">38.18</td>\n<td style=\"text-align:right\">36.45</td>\n</tr>\n</tbody>\n</table></div></div><h4 id=\"gigaword-1\">Gigaword<a title=\"#gigaword-1\" href=\"#gigaword-1\"></a></h4>\n<div class=\"φbq\"><div class=\"φbs\"><table><thead>\n<tr>\n<th>Model Type</th>\n<th>Model</th>\n<th style=\"text-align:right\">R1 R</th>\n<th style=\"text-align:right\">R1 P</th>\n<th style=\"text-align:right\">R1 F</th>\n<th style=\"text-align:right\">R2 R</th>\n<th style=\"text-align:right\">R2 P</th>\n<th style=\"text-align:right\">R2 F</th>\n<th style=\"text-align:right\">RL R</th>\n<th style=\"text-align:right\">RL P</th>\n<th style=\"text-align:right\">RL F</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>OpenNMT, no penalties</td>\n<td><a href=\"https://s3.amazonaws.com/opennmt-models/gigaword_copy_acc_51.78_ppl_11.71_e20.pt\" target=\"_blank\">link</a></td>\n<td style=\"text-align:right\">?</td>\n<td style=\"text-align:right\">?</td>\n<td style=\"text-align:right\">35.51</td>\n<td style=\"text-align:right\">?</td>\n<td style=\"text-align:right\">?</td>\n<td style=\"text-align:right\">17.35</td>\n<td style=\"text-align:right\">?</td>\n<td style=\"text-align:right\">?</td>\n<td style=\"text-align:right\">33.17</td>\n</tr>\n</tbody>\n</table></div></div><h3 id=\"references\">References<a title=\"#references\" href=\"#references\"></a></h3>\n<p>[1] Vinyals, O., Fortunato, M. and Jaitly, N., 2015. Pointer Network. NIPS</p>\n<p>[2] See, A., Liu, P.J. and Manning, C.D., 2017. Get To The Point: Summarization with Pointer-Generator Networks. ACL</p>\n<p>[3] Bahdanau, D., Cho, K. and Bengio, Y., 2014. Neural machine translation by jointly learning to align and translate. ICLR</p>\n<p>[4] Luong, M.T., Pham, H. and Manning, C.D., 2015. Effective approaches to attention-based neural machine translation. EMNLP</p>\n","link":"links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/Summarization","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/Summarization/","toc":[{"id":"summarization","title":"Summarization","index":"1","children":[{"id":"preprocessing-the-data","title":"Preprocessing the data","index":"1.1"},{"id":"training","title":"Training","index":"1.2"},{"id":"inference","title":"Inference","index":"1.3"},{"id":"evaluation","title":"Evaluation","index":"1.4"},{"id":"scores-and-models","title":"Scores and Models","index":"1.5"},{"id":"references","title":"References","index":"1.6"}]}],"reward":true}