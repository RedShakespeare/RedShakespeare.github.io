[{"title":"狄魔高根的补丁(2)","date":"2024-06-23T15:41:53.000Z","date_formatted":{"ll":"Jun 23, 2024","L":"06/23/2024","MM-DD":"06-23"},"updated":"2024-06-23T08:45:34.530Z","content":"原作：greyknight\n译：Silencess\n上篇：狄魔高根的补丁(1)\n\n裹上厚厚的外衣，本小心翼翼地藏好自己的右手。他的恐惧成真了，诅咒把仪式刀上牢牢地粘在他的手上，无论多大的力量也不能将二者分开。他走进书店，尽力想表现得一如平常。在昨晚的游戏中，他见到过一家商店出售魔法书之解除诅咒，可惜那本书他买不起。不过，神魔克星在现实世界的出现使他产生了灵感。那无疑是个疯狂的想法，但想想一把锋利的匕首正揣在他的口袋里，这件事本身就足够疯狂了。\n他先和认识的店员闲聊了一小会；在另一个顾客过来要求店员帮忙找东西时，他抓住机会走开了。本知道哪个书架上有几本圣经，于是他径直向那里走去。他翻开圣经，随便选了一段文字开始默读，虽然打心里觉得这样有点蠢。他完全不知道这么做有没有意义，但试试总没坏处。\n\n“待那蒙福的盼望，以及我们那伟大的神、救主耶稣基督荣耀的显现。他为我们献上自己，是为了救赎我们脱离一切罪恶，并且洁净我们做他自己特选的子民，做美好工作的热心人。”\n\n读到“救赎”一词时，他感到手心那把诅咒之刃的重量产生了变化，好像解开了某种束缚。到“洁净”时，那种束缚已经消失得无影无踪了，利刃也随之脱手落到口袋底部。庆幸着自己的机智，他放回了那册经书，离开商店返回了家中。一路上他小心翼翼，生怕自己不小心再摸上那把仪式刀。\n到家以后，他垫着块毛巾隔绝了与神魔克星的直接接触，然后战战兢兢地把它从口袋里拎出来。要是这玩意和游戏中的同位体有相同特性的话，它很有可能也会一不留神就自动变成诅咒的了。他拿毛巾把它包好，丢进抽屉里，试图忘记和它有关的一切。那天他没再玩NetHack。\n\n过了一段时间，其他烦心的事情分散了他的注意力，神魔克星的谜团也随之被抛到了脑后。他不愿再思考这种显然无法解释的事情。渐渐地，他又开始抽出些时间玩NetHack了。当到达任务层时，本颇感惊讶，因为符号学家的任务地点居然是在罗切斯特大学，他的母校？他饶有兴趣地和雷转遍了任务地点的第一层——是ASCII版本的主校区一角，还原度还不错。最后，他发现自己已经站到任务导师“科恩教授”的对面。\n他阅读常规的任务介绍，想看看补丁里符号学者命定的宿敌究竟是何方神圣。他完全没准备好面对导师提到的那个令人震惊的名字。但他找不了借口；答案一直就在他眼前，不是吗？他早该意识到他会在任务中与谁战斗。他早该准备好……面对狄魔高根。\n本心知肚明，尽管在数值上（角色的等级和阵营值）已经通过了游戏引擎的检验，他仍然没资格完成这项任务。对抗“老D”可是个艰巨的挑战。他必须先攒一些装备，嗯，很多装备。于是他离开任务层，投身向命运地牢的更深处。\n接着，一场残酷而毫无意义的死亡降临到他头上。关于雷的逝去没有什么伟大故事可讲，他丧气地想着。他们只是遭遇了一窝火蚁，正在杀出重围，可他那忠实的宠物在战斗中被他推挤开来，径直落入了坑洞陷阱，坑底插着的涂毒尖刺轻易夺去了这唯一战友的性命，徒留他一人面对地牢中的万千敌群。他怀着悲痛屠尽了剩余的蚁群，找来一块巨石填平了那个陷坑，然后取出一支闪电魔杖，把雷的名字烙印在这片新填的地面上。安息吧，小家伙，他默念道。\n\n来到地牢下一层，他发现了另一块奥术符文，这同样为他的角色触发了一次奥术充能。他很好奇自己的角色作为符号学家有没有办法利用这些奇怪的陷阱，但一小段时间的实验后，他没找出什么明确的结论。他的尝试被一个绿精灵打断了，它借着走廊的掩护朝他射出箭矢，昭示着自己的到来。本避开精灵的射击角度，躲到门后，只等敌人出现便突袭上去。精灵在一阵乱刺中倒下了——尽管又有个精灵从房间另一边的门里冒出来。本急中生智，凭着刚刚补充的魔力向这第二个精灵倾泻了一连串法术，于是它也倒毙在那块奥术符文所在的格子里，留下一个表示尸体的百分号（%）。本检查了第一个精灵的尸体，看看是否有有用的物品（精灵靴和精灵斗篷都是极好的魔法物品），然后走到第二个精灵身边。\n“你踩上了奥术符文!  – 更多–” 游戏提示道。哎呀，本想。算了，反正多点法力也没什么坏处。他按下空格键。“绿精灵的尸体在一片火焰中消耗掉了! 血液覆盖了祭坛! – 更多–” 本坐直了身体。他完全没料到这个。这条消息并不像任何陷阱的文本，而更像是……更像是来自祭坛。更准确地说，这是献祭人类的文本。他把心提到嗓子眼，又按了一下空格。\n“狄魔高根出现在你面前!”\n本没有被吓飞到盲目乱动真算他有自制力。相反，他立刻双手脱离键盘，让自己飙升的肾上腺素回归平稳。一头撞上狄魔高根这样的恶魔领主可能会导致角色的迅速死亡，也经常确实如此。他知道自己完全无法与屏幕中窥视着他的紫色“&amp;”对抗，唯一的选择只有逃离。他理性客观地检查了屏幕上的情况。谢天谢地，狄魔高根并没有刷在他的贴身位置，还有一线生机。他翻阅了自己的物品栏，脑中快速闪过各种可能性。由于阅读并不消耗游戏内的时间，他完全可以在下一回合前停下来慢慢思考。接着他找到了机会：他在上一层里捡到过一柄挖掘魔杖！太完美了。并没有急着动作，他又花了点时间检查了其余的道具。没问题，这毫无疑问是最安全的解决方案。他选定了挖掘杖，瞄准角色脚下的地面，然后挥动魔杖。他向下跌落……\n……接着落地，落到了安全的，没有恶魔领主的，下一层的大房间里。本当即长出一口气，这才发现自己无意识地屏住了呼吸。他保存了游戏，抽出一支红笔，在笔记本上记了个大大的警告，标记出这个让他险些没能死里逃生的、潜藏恐怖的楼层，又在上面连画两个圈，然后才关掉电脑，颤抖着爬上床。\n\n到了第二天，本有一种莫名的振奋感。鉴于NetHack的基本游戏机制，既然狄魔高根已经在游戏中生成过了，他就不可能再在其他地方出现。并且，考虑到本已经离开了这位大恶魔存在的楼层，那里的一切都会维持原样，直到玩家重返本层。也就是说，“老D”已经被安全地封印起来了。有了这些知识作保障，本一头扎进地牢深处，放心清理每一层的怪物，收集一切有用的物品。经过几天的游戏，他已经穿上了龙鳞甲，掌握了各种强大的武器和魔法。他觉得自己有足够的信心攻打城堡了。\n命运地牢的最底层是其特殊要素之一。恢弘的城堡层坐落在这里的巨型洞窟中，被一条与无人知晓的奇特地下海洋联通的护城河环绕着。抵达地图左侧的小迷宫后，本迅速解决了守卫在城堡吊桥外的怪物。他凝望着护城河对面的雄伟城墙，思考自己的进攻路线。用随身携带的号角演奏正确的曲调密令就可以打开城门吊桥；与以往的许多次一样，他稍微动点脑子就能推断出正确的曲调。不过，从吊桥上发起攻击有其危险之处，其中之一就是吊桥后塞满怪物的王座大厅。本戴上眼罩，使用心灵感应显示他所面对的敌军。\n不出所料，城堡里挤满了敌人。几只水生怪物一如既往地在护城河中巡逻。他扫视着自己的物品栏，目光落在了水上步行靴上。嗯，怎么不行呢？他暗下决定。后门的防守相对较弱，他可以从护城河的水面走过去，顺路消灭其中的原住民——再次检查了一下他的心灵感应，他欣喜地发现没有克拉肯——然后从背后发动攻击。他有几瓶飘浮药水，可以帮助他越过后门走廊上的陷阱门，然后他就可以从后方向王座厅施放法术和魔杖攻击了。\n他检查着现有的这些怪物，根据它们的强弱项策划他的攻势。他估计自己的寒冰锥法术对付王座厅的许多怪物都很有效，因此他从随身携带的书中重新记忆起这个法术。他做了一些其他的准备工作，穿上了水上步行靴，然后大步朝目标走去。护城河中的鲨鱼和鳗鱼迅速被他的+5剑击败，他闪电般地从后方攻入了城堡。一些卫兵穿过后门走廊查看发生了什么，于是本拔出魔杖，冲进后门开始大发神威。\n他迅速突破了通向城堡的防线。与大恶魔的短暂交锋刺激了他，促使他把武器和防具都攒到极高的水平。他仔细留心每个怪物的位置和动作，坚定不移地向前推进，消灭着沿途的一切生机。心灵感应在其中发挥了宝贵的作用，让他能及时察觉远处正在接近的怪物。等他到达了王座厅，许多前住民已经躺在走廊的地板上，成了战利品的一部分。本一边关注着自己的法力条，一边确认剩余怪物的位置和弱点，开始接连放出火球术、寒冰锥、魔法飞弹和其他法术。对那些有魔法抗性的怪物，他扔出了早有准备的附魔匕首，这些魔法锐化的利刃呼啸着，精准地飞向它们的目标。喝下一瓶治疗药水，本迅速欺入敌阵，提剑猛砍受伤的怪物残部。其中一只怪物，一位泰坦，给他带来了一些麻烦，但不久之后，本昂首屹立，胜利地成为了城堡的新统治者。*要是雷能看到现在的我就好了！*他想道。\n他花了一些时间搜刮尸体，将所有的战利品按照类型和实用性分堆，王座厅里的宝藏堆积成山。他把最好的物品放进次元袋或主物品栏，无用之物则堆放在一旁，期望着之后拿着变形魔杖回来尝试变形。当然，最珍贵的物品之一位于城堡的一个塔楼里；作为城堡里知名的顶级宝物，它锁在一个箱子里，而箱子被其下岩石上铭刻着的伊尔碧绿丝的伟大真名所守护。\nw - 许愿魔杖 (0:3)\n不过，本并没有急着做别的事，而是先把王座厅上了锁，防止其他怪物入侵此地，接着又脱下绝大多数装备放在房间正中的金色王座旁边，只留下几件必需品。他小心翼翼地坐上魔法王座，不知道会发生什么事。玩家都知道王座通常会带来各种离奇的效果，从帮助入座者到伤害他们都有可能。谁知道这版补丁会给王座添加什么奇怪的能力呢？\n“汝之专命, 阁下…”\n是灭绝！王座最好最有用的能力之一。尽管没有许愿那样有用，但反正他已经拿到了许愿杖，没有必要太贪心了。有了灭绝效果，他可以选中几乎任意一种怪物种类，把它们从地牢中完全抹杀。他在坐下之前就已经想好了如果有机会灭绝的话要选什么。\n“清除了所有的拟形怪.”\n*该死的拟形怪。*他之前遇到一家商店，里面藏着两个这种讨厌的家伙，时刻准备着扑向不幸靠近的冒险者。其中一个试图伪装成一本法术书，另一个则伪装成了楼梯。楼梯，真的假的？愚蠢的生物。他时常在想店主是不是和它们勾结好了；毕竟，只要有冒险者在商店里被拟形怪袭杀，掉落的财物都会被店主所继承。不管怎么说，即使他的心灵感应可以轻易识破这些变形者，它们的存在本身也会侵占商店中用于进货的空间。灭绝的候选名单上还有些其他怪物——特别是巫妖——但他已经计划好从几层楼上的储存基地里拿一张灭绝卷轴，再用圣水祝福卷轴来解决它们。\n宝座如往常一样消失了。重新装备好物品后，本仔细检查了自己的背包，然后选择了他的三个愿望。首先，当然是一张受祝福的充能卷轴，它可以为魔杖提供三次额外的使用机会。接下来是艾奇奥匹亚之眼。这件超凡的神器是巫师的任务神器，它可以赋予各种能力，包括魔法抗性和快速魔力再生。但本最喜欢的能力是，它能将他传送到遥远的楼层，赋予他不经过狄魔高根层而再次到达任务层的能力！\n实际上他现在就打算去了。他决定之后再使用剩下的愿望；他并不想把愿望都浪费在之后可能捡到的东西上。本把艾奇奥匹亚之眼安全地放在一个之前准备的袋子里，检查了自己的装备，准备激活这个神器。幸运的话，考虑到狄魔高根已经在主地牢中生成了，任务层的最后一层可能只会刷一些无名恶魔作为任务的宿敌。更好的可能是，本的任务神器也许根本不会任何怪物看守！\n打开哪个地牢的入口?\n任务\n回到大学，本先对自己施放了一个小型治疗术以回到最佳状态；艾奇奥匹亚之眼会感知到他不是巫师，因此每次激活时都会对他释放一些惩罚性的能量。但这仍然是值得的。完成恢复后，他径直走向考通往任务层深处的楼梯旁边、考恩教授站着的地方。\n本不可能想到他的世界从这一刻起开始崩塌了。房间的地板上躺着考恩教授的尸体。谁干的？周围没有随机的怪物，也没有战斗的痕迹。本确定上次离开这个楼层时还一切正常，而他不在的时候所有的活动都理应停滞。他怀着忧虑再次搜索可能的凶手，但什么也没有找到。尽管心里仍然很紧张，他还是走下了楼梯。\n任务层以平淡无奇的方式推进着。他与侵占大学一楼的恶魔战斗，然后走向户外。这里的地图似乎与现实世界的大学校园不太一致，因为外面似乎被一片茂密的森林覆盖着，与建筑物密不可分。事实上，这森林也太茂密了，本根本无法从这些粗壮的树干中穿过。他打倒了一些自己够得着的怪物，然后决定取回之前存起来的一把斧头。从任务层主层返回时，他走过考恩教授莫名其妙的尸体，又感到了一阵不安。在主地牢中短暂步行后，他到达了之前的物品存放点，当即决定保存退出，上床睡觉。\n\n当晚，本做了一个怪梦。他徘徊在一片森林里，一片真实的森林，时间是死寂的深夜。可怕的声响从森林深处传来，他试图逃离，但他似乎一直在兜圈子。突然间，他来到一片空地上，空地中央有一张平坦的石桌，桌上放着一张卷起来的纸片，像是一张卷轴。他展开纸张，上面写着一条信息，他衷心希望那不是用血写的。数字……这意味着什么？他惊醒过来，意识到这些数字的意义。坐标。那是一个地点……一个邀请。\n之后的一天里本一直在想那个梦。当然，他已经在电脑上查找了那些坐标，它们恰好位于不远处的一片森林中心。他希望自己有时间在白天前去；他一点也不想重演梦中的夜晚场景。但他又责备自己是个迷信的傻子，决定顺其自然。\n他到达那里时已经很晚了，但还好他想到要带上一把手电筒。电筒的光束在他周围的高大树木之间扫过，这让他倍感安慰。森林的布局不知为何让他觉得哪里有些熟悉。问题解决了，他想。很显然他之前来过这里，这就是他知道此地坐标的原因。是吧？也许只是游戏中的森林用什么方式唤起了他的记忆。他告诉自己，大脑是个很有趣的东西。终于，他来到一个被两棵大树挡住的地方，它们的树干互相交错，阻碍了他的前进。树的前面有什么东西在土里闪着微弱的光。他弯下腰，发现那是一支普通的箭矢。箭矢……他的血液一下子冷却了，脑海中突然浮现出一个画面。被两棵树挡住的路……那是他掉头的地方……他在那里被背后的半人马用弓箭袭击，然后一路战斗回去，脱离了任务层。\n但那不可能是他现在所在的地方。这个想法太荒谬了。对这支无害的箭引起的疯狂想法感到愤怒，他用力推动那些枯木，满意地看到它们吱嘎一声倒在地上。他向前迈步，深入森林。他要向自己证明，除了可能有几只松鼠外，这里没什么可害怕的。\n借着残月的映照，他来到一片开阔的空地。他拒绝承认这是他梦中的那片空地。甚至在他走近那张低矮的石桌时，他的大脑仍然坚定地认为这一切一定有一个合理的解释。即使石桌的一侧雕刻着难以辨认的非人类符文，这也无法使他动摇。这想法指引他看向桌面，看到桌上摆放的那个东西，然后他意识到自己正在看着什么。那一瞬间他失去了镇静，疯狂地逃回来时的路，逃离那个地狱般的森林，无形的东西在林间发出窃笑。他在某个地方丢掉了自己的手电筒，只能依靠月光摸索前行。在他身后，在那冰冷的石头上，弗雷德里克·考恩教授残破而血腥的头颅无神地望向这片夜色。\n","plink":"http://www.ephesus.top/2024/demogorgon_patch_2/"},{"title":"记一场ToMENet中的死亡","date":"2024-06-19T12:08:26.000Z","date_formatted":{"ll":"Jun 19, 2024","L":"06/19/2024","MM-DD":"06-19"},"author":"H_Hedgehog","updated":"2024-06-19T14:28:06.106Z","content":"投稿：H_Hedgehog\n注意看，这名玩家叫做刺猬，他目前被困在了一款游戏中，而且仅有三次死亡的机会……\n本次涉及到的游戏为ToMENet，不少玩家可能会对其“精神续作”ToME4（马基埃亚尔的传说）较为熟悉，尽管两者最初由同一作者创建，但内部分道扬镳后，它们都有了自己的开发团队，相似之处也只剩下个名字罢了。\nToMENet最主要，也是最为罕见的特点在于它是多人联机的在线Roguelike，玩家需要下载好客户端并连接至在线服务器以进行游戏；虽然底层框架仍是回合制，但为了多人游戏，回合处理是自动且迅速的，某种意义上已经算即时制游戏了，或许将它的类型归至MMO RPG更合适。\n既然是在线联机，那自然也有一些别的真人玩家，对，整个情况差不多就是著名动漫《刀剑神域》里的SAO事件，不过由于这是我们自己架设的私服，大家互相认识，关系也都比较好，起码不用担心恶意PK和抢夺装备的问题。\n\n故事背景以J.R.R.的中土世界为基础，游戏中所有城镇与大多数地牢都源自于此，而玩家的最终目标是摧毁堕落之神米尔寇，祂更广为人知的名字叫黑暗之主，魔苟斯（Morgoth）。\n依照传统Roguelike的惯例，字符画面相当简洁抽象，@表示一名玩家，那些方框前的数字表示不同种类的商店入口，+号则表示功能性建筑的大门，像是“马松屋（Mathom-house）”博物馆；在LotR的设定里“马松”是一种夏尔地区的方言，霍比特人用它指代没有用处但又舍不得丢弃的物品，玩家可以通过往马松屋里捐赠些烂得出奇的诅咒装备，或是已被识别的负面道具，让其他未能鉴定的角色过目后避开危险。\n玩家出生的新手村是位于夏尔东部的布理镇（Bree），商店布局大体上继承于各种band系的前辈游戏，交易则需要在对应的类型中进行，没法把冒险中捡来的长枪短剑卖给魔法店，也别指望在药水店里蹲到出售的法术手稿；镇里还有一家酒馆，多半便是著名的跃马客栈，作为大伙的主要碰头区域，如果不担心小毛贼的话可以跟同伴一起存些常用的探险物资于此。\n\n角色创建方面自由度相当之高，除了老掉牙的龙裔、高等精灵、霍比特人之类，甚至还能选择恩特（Ent）和迈雅（Maia）这些十分具有特色的种族，虽然靠后稀有的种族通常更加强力，但对应的代价也更为沉重；以迈雅为例，他们拥有卓越的初始属性和红外视觉，能够感应善良与邪恶生物，看穿隐形并存在隐匿加成，永远从商店中享受最佳优惠，不需要世俗食物维持生命；50级后还将获得漂浮能力、霜电光环、AC加成以及绝大多数抗性等等，更为重要的除了迈雅以外没有人能够使用星界法术。\n代价则是高达400%的经验惩罚，其他种族可能三四十级了迈雅还在二十多级晃悠，此前还必须完成一项试炼，杀死黑暗仆从以选择开悟，或者杀死持烛者（Candlebearer）踏上腐败之路，但消灭两者或未能击杀任何一方都将导致无处容身，角色会在抵达二十级时被直接抹杀；除此之外，能使用星界法术几乎让迈雅与法系职业绑定，可升级缓慢无法快速点出各学派的强力技能，而没有魔法盾的法师又极为脆弱，前中期死亡率极高。\n\n至于职业我曾在上个角色玩过一阵子Mimic（模仿者），算是作为“封测组”的经验吧，该职业允许玩家变成他们杀死过的敌人，且能使用对应形态的所有技能，比如变成寒冰巨魔获得力量补正以增加BpR（每回合近战攻击次数）且免疫冰霜伤害，变成红龙使用火焰吐息和飞行能力，智力足够甚至可以在变形后随意使用法师类敌人的魔法，还无需法典。\n麻烦的地方在于不是所有怪物都具有人形肢体，玩家变形后很容易损失装备槽，毕竟你没法指望一条蠕虫什么的能穿戴盔甲并挥舞长剑；另外模仿者需要杀死足够的敌人才能学习对应形态，具体数量等同于对方等级，特殊boss与精英怪无法被模仿，这意味着能变形的目标都必然比当前角色弱，实在是有些鸡肋。\n哦，还有一件事，千万别在陆地上变形成大白鲨之类的海洋生物，否则就会和某个倒霉的德鲁伊一样，没人想把变成鱼干的同伴抬到神庙去复活。\n\n一番苦思冥想，为了生存能力我还是按惯例车了一只平平无奇的矮子战士，坚韧但行动略为缓慢，免疫失明，在挖掘和使用斧头上存在加成；考虑到后来的主要工作是叮叮当当地凿石头，或许叫矮人矿工更为准确。\n角色创建完就被扔到了酒馆里，身上揣着几百金币和些最为简陋的装备，可惜不会有古怪巫师来你家门口做奇怪的记号，还硬要拉着你踏上冒险之旅，因为玩家压根就是个买不起房的穷鬼；沿着林间小道离开镇上，周边一带几乎全是房区，除了西面不知道为什么有座该死的火山，开荒时期无数萌新曾误入并被烫死在了里面。\n\n房产的平均价格并非让人望而却步，十余格的小房子只需要几万金币就能置办下来，初期玩家咬咬牙也能负担得起，然而房屋的主要作用仅是储存物品，以阻止内部掉落物被系统刷新后消失，是否要将宝贵的积蓄投入到这上面而不是用来提升实力，相当值得斟酌。\n得益于中洲优秀的社会福利制度，每片房区中都有免费供所有人使用的公共房间，通常还比那些廉价的小房子大，再次凸显了购房的华而不实；不过它们的访问权限不可更改，储存贵重物品的安全性得不到保障，也无法被设置成玩家店铺用于摆摊，就只有全员都是熟人的情况下能大大咧咧地当作仓库用了。\n在酒馆爆满后我们往这些公房里扔了许多暂时用不上，又不舍得卖的珍贵马松，介于里面总是乱糟糟的一团，大伙后来干脆形象地将其简称为了“公厕”。\n\n既然已经出门转了一圈，那说不定已经有牺牲者掉湖里或者火山熔岩里了，脆皮法师可能在街上挂个机都会被小混混袭击致死，眼下不得不面对本游戏最为严峻的问题——永久死亡。\n正常模式下一名角色只有三次灵体出窍的机会，且没有任何手段增加，人物生命值归零后会掉落身上携带的物品，变成没有形体但是能够穿墙的鬼魂，此时可以选择放弃所有装备飘到神庙复活，也可以选择在原地等待他人救援；需要注意，一旦鬼魂再被杀死，下场将与三次复活机会用完的情况一样：永远的从这个世界上消逝，虽然屏幕前的玩家不会被烧毁大脑，但将失去至今积攒的一切，无论是苦练的等级，还是精挑细选的武器盔甲，乃至名下的财富和房产，全部都将在死亡后化作虚无。\n纵使允许再建角色重新开始，服务器后来也被他们改成了99命，但死亡应当有其本身的意义，所以我决定玩完最后的这名3命角色就结束这款游戏。\n\n前往真正的地城之前，通常都建议先去镇子外面的训练塔先提升一下等级，在训练塔中生命值归零不会死亡，而是被治疗好再踢出塔楼，代价为损失部分经验和金币；尽管安全，但训练塔一共只有三层，最强的敌人无非是农夫马戈特（幼年弗罗多经常去他田里偷蘑菇）养的几条大狗，稍微升两级经验就不够看了，龟缩其中也不是长久之计。\n古冢岗（Barrow-downs）是离布理镇最近的地下区域，虽然叫这名字，但里面以普通敌人为主，浅层见不到什么妖魔鬼怪，是多数玩家的初期探索地点；所处深度以英尺表示，每下去一层会降低50ft，而该区域的最终boss尸妖王（Wight-King）则在-1750ft守候。\n出发时记得检查是否买好了口粮，火把和灯油有没有备齐，有闲钱最好携带一些保命措施，比如瞬移卷轴和轻伤药水；探索过程倒没有什么好提的，对于近战职业而言，由于即时进行的自动战斗，整个操作还被简化了不少，但对于需要手动施法的各种术士来说，就相当考验手速……以及网络延迟。\n没错，这毕竟还是款在线网游，网络问题是永远绕不开的一道坎，遇上传输波动轻则卡顿，重则断线失联；某些惊险时刻反应过来后，哪怕慢上一次按键输入都有可能导致生命危险，要是掉线再等人登录回来，角色早就一命归西了。\n\n也是band系的传统设定，ToMENet采用了刷新式地城，已存在的楼层如果没有玩家就会自动消失，再有人进入则重新生成，这样保证了同一地点就算被反复探索，也一直能有怪物和宝藏可供掠夺；不过玩家的鬼魂无法捡起物品，一旦离开死亡地点，掉落的装备和财物将全部消失，高等级角色陨落倒是会让那层暂不刷新，问题在于复活后不光等级降低，还只能使用备用装备，想在时限内报仇并取回遗物相当困难。\n绝大多数时候摇人帮忙是更为可靠的办法，我们将其称为“创伤小组”，主要任务便是替人收尸，标准救援流程为带张复活卷轴下去，杀光全部敌人清场，再把死者从阴间拽回来；万一预算不够或者情况紧急，没法复活鬼魂就帮人收拾些最为值钱的物品背到地表，但危险程度要是过于离谱，那么创伤小组也无能为力，因为救助人员很可能折在半路上。\n在我正式决定开始permadeath前，有过两次印象深刻的团灭经历，一次是我被奇形怪状的法师爆杀后，言静姐姐跑来帮我捡尸，那时大伙貌似都比较穷，还用不起返程卷轴，只能一层层走回去，结果在倒数第二层不知怎么的碰到一大群杂种兽人（Snaga），虽然我靠灵体穿墙提前发现并给出了预警，但最终没起到什么作用，也许是盔甲太沉，静姐的法师背不动，总之牺牲人数+1。\n随后赶来的ham干脆利落地解决了那群杂碎，刚嘲讽完对面，没想到还有个精英怪，转眼间被反杀，牺牲人数再+1，这个故事告诉我们，在确定周围绝对安全之前，千万别放松警惕去打字聊天或者截图留念；后面又陆陆续续地来了几位群友，但除了一只没找到路的翡翠鸟幸存，其他顺利抵达现场的创伤小组全员覆没。\n\n\n另外一次团灭是在古冢岗的-300ft，有几位组团下去的群友感受到了special feeling，该信息通常表示本层存在精英怪携带神器；事发之初我并不在场，不过根据复盘来看他们应该是发现了宝库（vault），这是种特殊的建筑结构，里面一般会有不少怪物和好东西，chacha阅读了侦测卷轴，他们肯定被其中的财富所诱惑，然后凿开了宝库墙壁，也同时打开了通向死亡的大门。\n后来发生了什么没有详细记载，只知道包括创伤小组在内，没有任何活人返回地面，尸体的死状都惨不忍睹，未能成功回收哪怕一件遗物；经统计，本次一共五名成员阵亡，作为事故元凶的那件神器披风，不光因为等级要求无法捡起以查看具体信息，还让我们倒贴了另外一件宝物，真是好奇心害死猹。\n\n\n已经提到了就解释下神器（Artifact）的定义，简单来讲是一类非常强大的装备，通常独一无二有自己的名称，不过并非专指跟神明或神话故事有关的东西，只要足够强，来源是什么都无所谓；ToMENet的神器分为两种，一种由高级词条叠加后随机生成，比如混沌之锤，防御者板甲，还有一种是开发团队根据LotR世界观预先设计的真神器（true-artifact），像是统御众戒的至尊戒。\n它们的词条和各项加成都碾压同级的good和excellent装备，但两者的主要区别在于同一真神器只能存在一件，而随机神器理论上能同时随机出两件相同或差不多的，虽然概率多半比买彩票中头奖还小；举个例子，假如有人得到了安督利尔，那么在这把剑消失之前，其他人不管刷怪开宝库有多么卖力，都不可能掉出第二把圣剑。\n好坏参半的是，无论真神器还是随机神器，都会过期，每件神器的大致使用期限为现实中的三十多天，就算物主没有死外面或者把东西卖给系统商人，时间一到神器也会自己消失得无影无踪；算是防止高阶装备通货膨胀，以及有效阻止囤积的手段，不过角色死亡速度通常比装备过期速度快就是了。\n顺带一提，游戏里真有彩票卷轴，极小概率读完后暴富，我们开出过最高的是六等奖500金币，没中奖就只有一张吐舌头的废纸:-P\n\n\n刚进入地城寻宝的那些日子，大家基本上都是有啥捡啥，再多再烂的垃圾也要统统扛回镇里卖掉，白板装备只能卖个几十金币，基础伤害稍微好点的重型武器也常常只能卖个两百出头；药水和卷轴还好说，即便不认得，卖给商店一次就能自动识别并在下次遇到时认出，但其他物品需要鉴定，要么找法师队友蹭鉴定术，要么得去购买一次性的卷轴或者昂贵的鉴定法杖，这让本就不多的收入更是大打折扣。\n法师要攒钱买各种法术手稿，战士也得凑齐自己的重甲，这些东西动辄上千金币，还没算强化费用，那时候为了节俭，走夜路连油灯都舍不得买，举着个廉价火把满地牢找灯捡；但当我们在店门口蹲97一张的打折卷轴时，翡翠鸟捡到了一双靴子，尽管不属于神器，这双靴子却能够卖出足足12330金币的高价。\n12k，也就是一万多金币，该数额在前期足够武装出三四名重装战士，或者让法师学会好几种新法术，再添点存款甚至能买栋小房子；这对纠结于吃生命体征维持餐还是买食物口粮更省钱的我们来说，完全是跨越阶级的一记重击，持靴人翡翠鸟瞬间成为中洲首富，俯视芸芸众生，仿佛成了天上的皇帝XD\n\n\n当事人后来并没有把靴子卖掉，因为它不但提供毒素抗性还能让使用者完全免疫麻痹，这些都是相当实用且不常见的词条，另外不知道是不是果蝠身体的特质，这靴子还为吸血鬼翡翠鸟增加了巨额AC（盔甲等级，可视作物理防御值，在本游戏中数值越高越好）。\n果蝠身体是开局可选择的特质之一，角色将无法使用任何武器，但是近战攻击能够造成hp吸收的效果，自带飞行能力并获得大量移速加成，专门查了一下发现果蝠不能穿鞋子和手套，那可能是人形+武术等技能给予轻甲的AC加成；虽然听起来很适合搭配吸血鬼种族使用，然而吸血鬼可以在20级时主动变成缺点更少的蝙蝠，实际略为吃亏。\n吸血鬼自然是惧怕阳光和神圣，擅长黑暗系魔法，能够夜视并抵抗邪术，食物方面虽然不用吸血，但必须靠不断击杀活物来维持自身存在；十分有趣的一点在于吸血鬼和地狱骑士无视多数负面词条，作为死者显然不会受到幽魂装备的生命汲取，甚至还能逆转一些Heavily cursed的诅咒装备，从而让其成为正面效果，考虑到吸血鬼免疫黑息，他们甚至能毫无顾虑的使用魔古尔（Morgul）的戒灵武器。\n理论上吸血鬼暴露在日光之下就会被烧成渣，但是我们在这里隆重介绍一款产品：ToMENet裹尸布™，有了它，您可以享受到360°全方位无死角防护，保护脆弱的肌肤不受烈日侵害，最先进的自适应缠绕功能保证无论是人形生物还是娇小果蝠都能完美贴合，丝毫不影响您迅捷灵巧的动作，还在犹豫什么？立刻前往最近的神庙或者黑市订购吧，只要您的长相没有恶心到店主（堕落迈雅也可以用自己的古神之姿把老板吓到给你折扣），不到1000金币即可拿下这条破布！\n下面请欣赏非常好笑的吸血鬼笑话一则👇\n\n新裹尸布用上的比预料中快，除了埋头往地底钻，自然也有人对广阔的中土世界感兴趣，最早是冒险者ham穿越森林希望绕西面的火山一圈再回来，没想到在离镇子一步之遥的山边被巨人敲了一闷棍，当场死亡。\n而在后来，可怜的翡翠鸟希望一路向西南方向飞行，穿过海洋以抵达坐标原点，起初顺顺利利还能在水面上捕鱼玩，结果飞到半路被巨型乌贼喷射的酸液融化成了渣，渔夫秒变落汤蝙蝠，还好附近有座孤岛，灵魂龟缩在岛中央避免了魂飞魄散的下场；ham的新果蝠角色飞去营救，结果刚到目标地点就被两口喷死，但这次不是因为酸液，而是乌贼释放了强劲闪电，一发伤害高达175，作为参考，我这样一名health点满的二十几级重装战士也才400出头的血量。\n巨型乌贼的这几口酸电喷吐根本不是我们这个阶段能硬抗的，如果假设ham受到的伤害经过抗性减免，那原先的数据只会更加离谱；我当时由于没有飞行能力，仅能在后方帮忙跑腿买个裹尸布，移动时猛冲过头给翡翠鸟从屋檐下挤出来差点被阳光烧成灰，罪过罪过。\n\n\n俩果蝠复活之后觉得忍一时越想越气，退一步越想越亏，收拾了点新的“陪葬品”又重新上路要再找大鱿鱼一决高下，结果显而易见，复活后临时准备的道具根本不是对手，两只果蝠彻底葬身大海，再也没回来；装备太烂是一方面，最大的问题在于敌人等级太高，巨型乌贼整整41级，生命骰150d10，吐息计算为hp/3，那么预期伤害在250左右，非常纯粹的数值碾压。\n十来级的果蝠这么干，完全是电子虫找通关战争（能把Nethack杀穿的狠角色）打架，言静姐姐：“战争上限才30级”。\n\n\n外面的世界很危险，但我从开始permadeath起就十分谨慎，显示深度的数字会用颜色表明当前区域的经验倍率，白色是正常，黄色是降低，灰色则是怪物等级太低无法获取任何经验，某种意义上也可以当作危险指示器来使用；我主要在白色与黄色区域的交界处进行采矿作业，下楼时感受到special feeling尚可找楼层头目放手一搏，但如果是challenge或者dangerous around那就只有立马离开保命。\n某些极为罕见的特殊装备可以显示隐藏的矿脉，但考虑到宝物侦测卷轴才二十多金币一张，探矿成本几乎能忽略，反倒得注意ToMENet中的卷轴都相当沉，背多了很容易超重，一张卷轴足有半磅，疑似高档羊皮纸配了卷轴筒；挖矿，或者说挖金币的核心要点在于叠加倍率，隐藏矿脉比裸露矿脉更值钱，每十级digging技能都有机会产生额外的金币掉落，而运气属性可以增加掉落金币的数额，最后也是最重要一点，深度越危险，矿物就越值钱。\n挖掘速度是次要的，但能够大为节省时间，这方面主要取决于工具，镐头铲子什么的本质上算武器的一种，伤害和命中的强化等级都将影响挖掘速度，其次是物品重量，重型挖掘道具的效率自然比小铁锹强（感谢chacha送的pick of digging）。\n相较于找boss杀再捡宝贝去卖，挖矿的安全系数要高很多，毕竟活着才是最重要的，具体收入也不会落后太远，中期一镐下去爆四五千都是很稀松平常的事，另外金币不占背包空间和负重，跟再背些装备回去补贴家用并不冲突。\n\n高贵的符文法师甚至挖岩石墙都有小概率得到符文，除了施法自用以外，这东西的售价接近10k一枚，相当值钱；玩家也可能碰到些奇怪的大块金属和零部件，我曾经在古冢岗挖出过大块的铜（超级沉），后来又在魔多挖出过机械腿，这些东西可以制作某种十分有趣的造物——魔像。\n构造一尊魔像需要一块大型特殊材料，两条机械腿，几张命令卷轴和魔像创造卷轴，魔像胳膊是可选的，最多允许添加四条，但不加的话魔像无法攻击敌人；另外还可以在物品上刻写{@G}，让它们融入材料，并根据适合的词条为魔像提供特质。\n魔像的强度主要取决于材料类型，最高级的精金魔像足有210点AC，10d150的生命骰，基础伤害将和胳膊的强化等级相乘再算得实际输出，总之也低不到哪去；所需卷轴很容易在黑市买到，但大块材料与四肢就很难获取了，我个人要求不高，只想要个硅胶魔像放家里自用罢了（并不存在）。\n\n回顾了一下初期冒险，最为惨重的经济损失跟咕噜有关，事情发生于果蝠惨案前，翡翠鸟捡到靴子的不久后；咕噜，原名Smeagol，正是跟巴金斯玩猜谜游戏顺便送至尊戒的那位，在本游戏中以精英怪的形式出现，尽管能将障碍物视若无物，它倒没有清空我的钱包顺便再偷几件装备走，事实恰恰相反，刚从树丛中现身就被我几斧头当场砍死，掉落了一枚戒指和一条非常不起眼的长鞭。\n小心翼翼地收好戒指，返程路上大家都在猜测它会不会是至尊魔戒，我一边鬼叫着“my precious!”一边处理着包中的其他垃圾，顺手把那条没鉴定的长鞭以20金币的价格卖了，下一秒我在商店里看到了它后面跟着的一长串售价，足足五位数，53k，五万三千金币。\n时间在那一瞬间仿佛停止了流逝，我整个人呆若木鸡，周围的一切事物都变得陌生遥远起来，我口干舌燥汗流浃背，心脏几乎停拍，反反复复数着那个数字，希望是太缺钱导致自己眼花，可不容争辩的事实如山一般压得让人喘不过气，沉默良久，认清现实的我当场发出了尖锐的爆鸣：\n五万三千金币的鞭子啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊————\n\n我成功刷新了chacha以20金币卖出保护披风的记录，荣登中洲第一大冤种的宝座，彻底失心疯了，自那以后逢人就傻笑着问对方有没有见过自己53k的鞭子；至于那枚戒指，不过是个很常见也很便宜的潜行戒指而已，给隐身饰品提鞋都不配。\n出售后显示的物品真名为Chaotic Whip of the Thunderlords，混沌的雷霆主宰之鞭，并且使用等级高达29级；当时我由于红温太久，被店主踢出店导致没有截到详细的物品描述，找了张群友的后期装备截图作为代替，但图中的雷霆主宰匕首只要求17级，还少了一项Chaotic前缀，该前缀通常能提供chaos和confusion的抗性，跟这比起来那条鞭子约莫着只好不坏。\n\n最惨重的损失回忆完了，接下来自然该提一下赚得盆满钵满的那趟，当时我只打算非常普通的回地下挖挖矿，选择的层数并不深，应该在750ft左右，意想不到的是刚返程完毕就收到了special feeling的提示，然而没等我评估风险，一名boss从旁边直接突我脸上了，好像还带着个精英跟班。\n结果我赢得莫名其妙，因为boss罗宾汉（Robin Hood）是弓箭手，贴脸冲上来跟重装战士肉搏完全是嫌死得不够快，射箭和放置陷阱等技能统统没有见到，三两下被我砍翻了，另外那只叫Borshin的精英怪好像是坨血肉混合物，也就多撑了几回合，之后当场变成了肉酱，想细细地给它切成臊子都没机会。\n罗宾汉掉了把长弓，后来经鉴定是一件真神器，名为巴德之弓，对，就是长湖镇那位射杀史矛革的河谷之王，真好奇汉丁顿伯爵怎么会有他的长弓；虽然我非常缺少远程攻击手段，也很想用这把对龙宝具射爆那群喜欢瞬移的魔法师，但身上的重甲严重影响射击精度，而且职业与种族让这名角色的箭术潜力极其之差，甚至还不如丢回旋镖有天赋，它并不适合由我使用，考虑到队里当时也没射手，最终还是打算将其卖掉。\n要是装备上的话倒能增加些敏捷和幸运，另外还能让使用者免疫麻痹，可当时我已经有麻痹免疫的防具了，敏捷对战士的重要程度也没那么高，而幸运？加上两点的回报可能还不如卖了弓的零头，尽管委托了魅力点得比较高的静姐帮忙，但这把弓的拾取等级太高，其他人没一个能捡起来；最终我这名魅力10的矮子，在戴上了一件CHR +3的护符后，将巴德之弓卖出了近90k金币的售价，托它的福，我才发现店主会随机更换并且带有种族歧视，同族之间的优惠程度是最高的，如果两族关系不好，玩家的回收金额会被大幅克扣。\n没想到的是精灵和矮人的关系还挺好，精灵给出的报价比人类都要高出四千多金币，或许这是第一纪元吧，矮人还没杀害辛葛抢走带有宝钻的项链。\n\n\n随着不断深入，宝藏增多的同时，怪物也在变得更加致命，各种攻击手段层出不穷，毒素、麻痹、失明、混乱、恐惧、幻觉等等，还有各式各样的元素和魔法伤害，此时抗性（Resistance）就成了很重要的一类保命属性。\n最为头疼的是法师玩家们，他们的法典很容易很容易被火焰烧毁，同时非常怕水怕酸，独特的心灵工匠（Mindcrafter）倒是不用纸质法典，但他们的法术晶体受到电击也会损坏，而失去施法媒介的法师无异于待宰羔羊，另外高阶魔法非常昂贵，已添加的手稿不能从法典上撕下来，万一整本书被烧了那大致等于倾家荡产。\n元素伤害在拥有抗性后变为1/3，拥有双重抗性则会变为1/9，但物品仍然有概率被元素摧毁，装备与装备之间的抗性不能叠加，双重抗性通常需要饮用药水或者激活某些特殊道具，但这些效果大多只是暂时的，还可能遇上消耗品用完和道具失窃的情况；想要稳定持久且保护物品不被破坏，那就必须获得对应元素的免疫（Immune），最简单的方法是变形成有免疫特质的怪物，比如骷髅和僵尸免疫剧毒，地狱猎犬免疫火焰之类的，不具备相关能力的角色可以找模仿者为其制作变形戒指，当然，防具能带有极为罕见的免疫词条自然是最好的。\n\n\n在缺乏免疫和抗性的情况下自救也是一门学问，致命伤治疗药水能够治愈混乱、失明、以及大多数割伤，似乎还能够缓解短暂眩晕但无法解除麻痹，也许是因为不可行动状态下没办法喝药，十分合理。\n中毒与麻痹是萌新冒险者的主要死因，如果不幸被毒虫咬伤或者触发了涂有毒液的陷阱，那么需要服下化解毒素药水或者精灵口粮兰巴斯，不知道精灵是改进了配方还是在里面下了药，ToMENet中的兰巴斯ProMaxUltraPlus版能够解除非常多状态，祛除疾病、中毒、以及幻觉统统不在话下，当然也能消除饥饿，但这东西带有圣洁属性，对吸血鬼等种族来说跟生吞火炭没啥区别。\n在备齐基础抗性的中期，主要威胁来自混乱与幻觉，混乱将让角色失去控制，表现为分不清移动方向并朝空气胡乱攻击，导致单方面疯狂挨打，雪上加霜的是混乱状态下不可阅读卷轴，这将让角色失去传送或回城的能力，很容易被活活揍死而毫无逃跑机会；虽然治疗药水可以解除混乱，但请保证之后能立即脱身，否则可能像某个忘记携带节育卷轴的冒险者，刚喝完药又混乱，被几百条繁殖出虫子硬控到死。\n幻觉的致命性稍低一些，视野中的所有生物会不断变幻外形，从鼠人到戒灵，从泰坦到青蛙，一切都在闪烁扭曲，幸好外观是虚假的，不会改变怪物本体仍然是坨几刀就死的致幻地衣，问题是你也有可能把巨龙看成蠕虫而忽略，把队友看成水怪然后一发火球术给人炸上天；幻觉的持续时间非常长，而且抗性和善后手段极难获得，我几乎就没见过抵抗幻觉的装备，不像隔壁混乱，只要花几万就能轻松买到抗性戒指，再加几万还能买到同时抵抗chaos和混乱的戒指。\n\n兰巴斯虽然能消除幻觉，但它难以获取，普通商店没有这种精灵食物出售，想要批发估摸着得前往精灵国度，神秘的罗斯洛立安；除此之外也存在一些代替方案，比如让牧师给中幻觉的队友来发祷告，又或者吃上一朵神奇的蘑菇。\n蘑菇大体上是把双刃剑，没有鉴定的情况下乱吃可能会中毒失明，但如果能分辨好坏，那么它可以被视作重量和药效都轻一些的药水；有一种Cure Paranoia的蘑菇可以治愈恐惧和幻觉，还有种叫做Unmagic的蘑菇会移除一切现有状态，尽管是万能药但它也将同时消除正面增益，类似于完全净化自身。\n城镇附近的田里说不定会有蘑菇能摘，地城里也不时能采到一些野生蘑菇，但更为稳妥的方案是寻找随机在特殊楼层中出现的蘑菇商店，等待货物刷新顺便托队友送点钱就能轻松购入大量所需品种；比较有意思的是致幻菇，虽然会引起幻觉，但它也能回复一定的魔法值，这可比几千金币一瓶的魔力药水便宜太多了，大啖蘑菇几十朵，饭后再来口解除幻觉，完美（然而会扣珍贵的理智）。\n\n被某些敌人或法术击中将损失经验和能力值，主要威胁在于战斗时的猝不及防，好在玩家不会永久失去它们，可以去神庙处蹲出售的对应药剂，大约几百金币一瓶，经验掉了就喝等级恢复药水，STR掉了就喝力量恢复药水，诸如此类。\n部分像是那兹古尔（Nazgul）之类的高阶不死者，能够腐朽玩家的非神器级近战武器，另外要是没有Slay Undead or Evil的词条，那将无法对其造成任何伤害；最为恐怖的一点是它们能让角色感染“黑息”，这是一种缓慢消耗经验和属性值的负面状态，它不光无法随时间流逝而自然消散，还会在玩家间互相传染，持续蔓延，必须服用罕见的Sprig of Athelas（阿塞拉斯的嫩枝）才能治疗，刚铎人们将其称之为王叶草，正是阿拉贡拿去救人的那种。\n曾遇到过一次非常惊险的情况，当时好像是带着两位群友的新号下去练级，德鲁伊靠毒雾攻击，结果遇上了一大群无视毒素的不死族，当事人冲上去就是揍，结果被一顿乱啃，撤回来之后还感染了疾病，我和另外一名法师清完敌人只能眼睁睁的看着他血量狂掉，想丢药但来不及，好在后面剩一丝血苟住了。\n德鲁伊恢复完刚上楼梯，法师和我在水边又遇到了一大群魔鬼鱼还是什么来着，远距离疯狂吸收经验，并且给我们叠上了混乱和幻觉，场面一度十分惊险，幸亏没有非常强大的高伤害怪物在附近，我们有足够的时间处理并后撤，最终成功相继返回了地表，一看经验条，被活生生从16级吸到了7级。\n\n后期还有更多更恶心的攻击类型，像是Chaos，Nether，Plasma，Life Drain，Disenchantment……以及Nexus，这玩意轻则将人驱逐出地城，背包里塞满垃圾，重则永久交换玩家的两种属性值，战士当场虚弱，法师当场降智，唯一的解决办法只有再去挨打尝试换回来。\n随后的一次冒险就没那么好运了，德鲁伊离队休整，我和言静姐姐准备再返古冢岗，倒也怪我，忘记解释清不要直接返回，否则没有退路非常容易陷入危险；可打完字扭头回来人已经直接阅读了返程卷轴，在地城上方这样会做会直接前往曾经抵达过的最深层，想要回到指定层数必须在卷轴上刻写{@R}，然后使用/rec指令。\n我赶忙查看玩家信息，指定层数阅读了卷轴，接着暗暗祈祷能在意外之前赶上并且传送位置不要相距太远，不过愿望只实现了一半，我抵达时确实在静姐的法师附近，但她已经被一名帕拉丁残忍咒杀，更让人如坠冰窟的是，通知栏中紧接着弹出了鬼魂被杀的消息，这意味着不可复活的彻底死亡。\n本来灵体出窍后可以立马疯狂上楼逃命，但是显然也会让原先那层被刷新掉，所以静姐为了保护装备并未离开本层，结果碰上了同样能穿墙的幽灵战士，不幸牺牲。\n\n解决那名帕拉丁只需要几斧头，但生命在指尖逝去的绝望与无能为力却是久久不能忘怀的。\nVault：“nice night, expect for the ending”\nHedgehog: “got the staff?”\nVault: “is beginner’s kit too, no need”\nVault: “id call it a day, won’t play it till all exams done”\nHedgehog: “hope cya, but i may die before（”\n\n死亡flag立完了，这种凄美的月下道别，感觉我不死上个几次都对不起剧情发展，但当天悲伤过度的我并没有再次返回深处，之后更是将谨慎与稳妥作为了探索的第一准则，仔细地管理着身上的所有保命道具。\n创伤小组那边依旧业务繁忙，从拯救大兵ham发现自己忘记带灯，再到全员出动前往-1350ft勇战三头恶龙，这应该是规模最大的一次行动，所有冒险者几乎倾巢而出，前仆后继地空降填线，最终以折损数名30级中坚玩家为代价，惨痛地取得了胜利；我也在现场，可惜是负责事后打扫战场的那批人，错过了见证史诗级大战的机会。\n\n\n本次收获颇为丰富，光是真神器就有整整三件，其中最为突出的又非埃兰迪尔之星莫属，这是一颗能够用于照明的精灵宝石，为北方王国阿尔诺的王权象征；遗憾之处在于它并不是瓦尔坦封圣的那几颗精灵宝钻之一，不然可能会珍贵过头了点，我起初还把它和凯兰崔尔的水晶瓶（精灵女王送弗罗多的手电筒）搞混了，后来去查才分清星光瓶里储存的是埃雅仁迪尔之星的光芒。\n这颗宝石可以提供失明抗性，免疫恐惧，看穿隐形，抵抗生命汲取，甚至能增加玩家的行动速度；此外它将产生半径为5的光照且永远不需要添加燃料，该照明范围是我们常用油灯的2.5倍，然而还没完，每隔几十回合便可将它激活，当作魔法地图显示周边一大片区域的详细情况，简直就是一双没消耗的天眼。\n不知道算不算乐极生悲，由于任何神器都不能放置在酒馆或家中，所以那场战斗的核心战利品都必须由活人随身保管，另外考虑到拾取等级限制，能拿起来的也就那么几人，没法随便找个小号存着；结果……揣着所有神器的那只仓鼠信心爆棚，一路高歌猛进，随后不负众望地暴毙在了-1750ft，古冢岗底层。\n\n\n大伙自然是又组织了几场声势浩大的救援活动，奈何位置实在太深太危险，主力队伍又于上一场恶战中元气大伤，除了有个倒霉蛋死在-1600ft送了人头之外，我们暂时未能取得任何进展，几位新冒险者甚至因为陷阱剑和其他玩意在浅层死了又死。\n三龙之战尚未开始的前夕我正在忙于收拾行李，准备将初始城镇周边调查一圈，虽然每座据点都是提前设计好的，不过城镇分布与整个地理环境由随机生成决定，探索世界仍是不可或缺的一环；最先发现的是古代森林，这属于地表无建筑的的野外地城，但由于服务器管理员作弊开全图剧透，这里实际上已经被几名群友简单探索过了，没有什么意义。\n初次远行时走得步履维艰，主要问题出在各种障碍物上，缺少漂浮道具也不会飞就无法穿越森林，得手动一棵棵地伐树取道；即便会飞也不能翻过高山，一定要拥有攀爬工具或者点够Climbing技能，否则只能绕远路，深水区没学游泳技能也可硬闯，但角色在溺水状态下会飞速扣血，想要跨越大型水体，飞行依然是不容忽视一道门槛。\n\n硬趟浑水的时候我曾出现过无法移动的情况，不清楚是网络卡顿还是按键失灵，当时角色的hp疯狂下降，身上大量药剂和卷轴也因为浸水而不断损坏；幸亏提前设置的道具快捷键仍然能用，我玩命地把治疗药水往嘴里灌，第一反应是盔甲超重得脱，但想着之前既然能走进来这么远，那应该不是这方面的问题；况且我一旦停止给药，卸个装备的功夫估计够淹死两遍。\n事发仓促，结束的也挺唐突，在我差点要对自己使用传送法杖的刹那，角色忽然又能行动了，我赶紧冲上岸边然后使劲咳嗽（肺部活动为当事人脑补）；大难不死，心有余悸，刺激程度不亚于腐败湖粪坑蝶泳，说来离奇，在水里快淹死的时候居然可以喝药续命。\n\n\n之后托群友花整整两万四千金币，代购到了漂浮戒指，但海洋仍然是我这名矮子的噩梦，这里不光充斥着大片大片的食人鱼，还有剧毒的箱水母，残暴凶猛的杀人鲸和大白鲨，能远距离发射穿甲水弹的枪鱼（Gunfish），更流传着喷吐闪电的克拉肯传说。\n即使在岸边也能冒出一大群水鬼，那架势如同潮水涌出，随时准备吞没路过的无辜冒险者；虽然危险万分，为了绘制地图而在深夜飞越大洋，依旧犹如家常便饭，但正因如此，见到能够歇脚的小岛时才会发自内心的感到欢欣与喜悦。\n\n要让我回忆最惊心动魄的一次出海经历，那毫无疑问是在探索镇子西北方向的那趟，给我造成了严重的精神损伤（物理层面上），几乎真的患上了海洋恐惧症，对比之下，前阵子差点淹死简直是小儿科。\n当时我正沿着海岸线徐徐前进，碰巧发现了不少水族战士，于是利用他们无法踏上陆地的特性，轻松在岸边大杀特杀，可当我边战斗边检查满地掉落物的时候，没注意到几只海马魔法师混在其他怪物里凑了过来；角色进入射程的一瞬间，铺天盖地的元素箭矢就飞了过来，伤害最高的可以一发带走我将近三分之一的hp，还附赠眩晕和摧毁物品的效果，我立刻启动外置血条，开始猛喝速度与治疗药水，然后拼尽全力将敌人斩于斧下。\n虽然是魔法师，但从手感上讲，这些海马的生命值惊人的高，丝毫不逊色于陆地上某些老练的战士，另外不知道是法术列表中空缺，还是因为其战斗风格本就倾向于近身作战，它们没有使用传送或闪现拉开距离，反倒冲上来贴脸施法，甚至还会咬人；得亏是群近战法师，在消耗了许多药剂后，我惊险地获得了胜利，毕竟肉搏方面爷才是专家，它们要采取风筝战术，那说不定我真得落荒而逃。\n正当我洋洋得意，甚至打算截个图吐槽一下走路上被海马揍了的时候，一股诡异的灵能直冲我的意识袭来，大脑仿佛要爆炸了一般，认知被扭曲还导致眼前出现了幻觉；虽然来不及弄清具体情况，可意识到理智降低后，我毫不犹豫地使用了传送法杖脱战，然后光速逃离了那片区域，直到确认周围安全，才惊出一身冷汗。\n\n\n从战斗记录来看，我是遭到了邪教徒的精神攻击，虽然hp没有丝毫减少，可理智值（Sanity）已经下滑至危险程度，两发心灵冲击将其蒸发了约40%，如果说生命值耗尽等于肉体毁坏，那么彻底丧失理智则是灵魂层面上被完全抹去。\n无论角色是3命还是99命，一旦SN归零，都将不可复活的彻底死去，而更恐怖的一点在于理智值不会自然恢复，虽然有药水能够增加，但理智药剂极为罕见，任何商店都没法订货和买到，只能依靠地城拾取或是用空瓶从喷泉舀水，瓶子会随机装满一种药水，而其中就有小概率roll到理智药剂。\n理智状况最初以非常模糊的形式进行描述，在health技能足够高后才可以显示出百分比，但具体数值取决于感知（WIS）属性，同样是100%，人类牧师的理智值肯定比巨魔战士要高得多，面对不可名状之物时也会更加坚韧。\n\n\n不存在任何道具能够抵御精神污染，哪怕神器也不例外，但有一些办法可以降低传入的精神伤害：\n首先是变形成不死者，僵尸骷髅什么的由于没有脑子，可以拥有T1级别（数字越小越弱）的心灵防护，其次是吸血鬼种族，防护等级为T2，而如果想要拥有效果最好的T3级减伤，那得选择心灵工匠职业并将角色练到Level 40，而其他职业需要花费宝贵的技能点数将Hereticism和Traumaturgy两项技能同时加到45级，部分近战职业没有这方面的技能树，尚未开始努力就已遗憾离场。\n另外玩家要是因为hp过低而死，那么变成鬼魂状态时会自带T3级别的心灵防护，危急关头又还有复活次数，说不定自杀还真是个解决办法，如果来得及的话……\n\n不过事已至此，木已成舟，除了想办法恢复理智之外也没什么可做的，哦，或许可以痛批一下中土邪教组织猖獗的问题，这帮疯子还完全不传教，想加入都没机会，正常人没跑掉的全给送去见了他们的真主。\n外出是不敢再出远门了，地城深处似乎有听说过邪教成员出没的传闻，同样不敢继续深入，我整个人草木皆兵，大部分时间都缩在角落里痴呆地流口水，感受着脑海中闪回的记忆场景，眼前不断冒出各种幻象。\n当然，被迫宅在镇上算是个等待商店刷新的好机会，虽然大部分商人售卖的都是基础物品，但偶尔会出现些物美价廉的高级货，比如带’*'的强力卷轴，就算运气差，趁打折期间批发点常用的消耗物资也总是有用的；攒下一定积蓄后，黑市就成了我们的主要消费场所，听名字便知道价格宰人，但里面能买到真正好用，且对于同级玩家来说难以获取的装备，不像其他商店的“占位符”，开局凑合穿一下就换掉了。\n\n黑市中比较值的一笔交易是我花40k左右的金币买到了反射护符，忘记有无折扣，当时的花销水平完全支撑得起，而且反射词条极其稀有，尽管不如隔壁Nethack那样几乎无视所有远程魔法，它多少可以够弹开一些射弹和元素箭矢，而且光面板提供15点AC就相当于一枚优质的保护戒指，对我这种常常看着远程敌人无能狂怒的战士来讲，无比实用。\n非常血亏的交易则是初期花将近25k购入了一柄白板双头战斧，虽然基础伤害骰面很高，但由于太重，我那名矮人拿起时BpR降低，导致伤害还不如原来的旧斧头，如果转手卖给商店又回收不了多少资金，真是花钱买教训，我后来专门把它扔到杂物间当成了预防冲动消费的警钟。\n有些迷惑人的是，黑市里边经常会卖些很贵又没啥用的怪东西，比如各种皇冠，什么词条都没有，AC也差到几乎可以忽略，唯一的特性疑似是比较值钱，买这玩意或许只能满足一下使用者的role play或者换装需求，但这种游戏里角色就是个@符号，根本不会有人在乎你头上带了顶秘银皇冠还是摆着一缸金鱼。\n\n在我因病休假逛黑市期间，可怜的仓鼠鬼魂还在古冢岗底层飘着，当事人后来用小号车了名巨魔战士，慢慢杀了下去，虽然最终成功抵达目的地，但角色实力不足以回收遗物，再次送命，底下躺着的神器倒又多了一件。\n人美心善的言静姐姐也新建了一名叫Doll的牧师，在训练塔里猛猛刷级，高等级的牧师和超能力者都有办法恢复其他玩家的理智，不过施法成功率极低，而且耗蓝奇高无比，估摸着只有在城镇等安全区域能够进行操作；另外不知道是不是在16级前夕死太多次，给静姐造成了心理创伤，之后很长一段时间都只能看到她蜗在训练塔，要不是等级高到一定程度就没经验了，估计可以练成十里坡剑神。\n如此执着于16级是因为我曾弄到过一根萨鲁曼密探的魔法长杖，鉴定之后没想到还是随机神器，自然就给了队伍中唯一的法师，可那玩意有使用等级需求，法师前期又十分脆弱，静姐就不断挣扎在需求线附近；倒不是我不想babysitting，而是ToMENet有组队限制，两名玩家的等级差距超过8级就无法在同一楼层中共享经验，除了复活之外也找不到能够永久降级的方法，痛苦万分。\n经历了漫长的练级，又戴上一堆增加属性的饰品，甚至靠拿法杖增加蓝量后，静姐的牧师终于达到了施放Faithful Focus的最低条件，此时她的主属性WIS为18/80，作为参考，我的力量也不过18/60；但即便这样，祷告的失败率依然高达91%以上，放一发就得耗尽魔力，需要更换回蓝饰品加速恢复，在不知道经历了多少次失败以后，中洲首例精神病治疗案例出现了！我的理智值从61%增加到76%，随着另外两次成功，SN终于重返100%，彻底归复常人！\n这样一来，就完全摆脱了幻觉与潜在的死亡风险，终于能再次踏上广阔的新天地了，多谢言静姐姐！\n\n\n恢复理智后无论瞧见什么都感到赏心悦目，从未如此觉得活着是种奢侈的享受，哪怕仅是坐在屋里，闲看一下游戏的天气和时间变化，都让人无比放松，安心，或许只有死亡的残酷才能凸显出生命的美好与可贵。\n以及，虽然铺张浪费，我在很早之前就购入了自己的两间房屋，一栋主体与一处杂物间，刚好挨在一起，花了35k左右的金币，大约是古冢岗1000ft两趟的收入；实际使用面积为20格，我非常想把中间的隔墙打通，可以再多两格空间并连通两室，遗憾的是城镇墙体无法挖掘，只好作罢。\n这些空间主要拿来储存较为重要，又必须分场合携带的道具，比如倾销垃圾用的魅力护符，挖矿时佩戴的幸运饰品，还有飞越森林海洋的漂浮戒指等等，另外我自己常用的几种卷轴和药水也都囤积了“至死量”——到死之前应该用不完。\n世界危机四伏，所有冒险者得把脑袋别在裤腰带上，过着刀头舐血的日子，刚刚还在嬉笑的同伴可能下一秒就会变成冰冷的尸体，这种严峻的环境下能有一处安全稳定，充满补给物资的容身之所，夫复何求；抛开各种复杂的访问权限，或是将房屋设置成店铺向其他玩家兜售物品，部分花里胡哨的装修小功能反而更加深得我心，像是能将有颜色的药水作为涂料，粉刷屋子外墙，不小心画错了还可以用清水洗掉痕迹，十分具有生活气息。\n\n如果势力庞大，发展到大后期需要公会馆和活动场地，当然也有更加奢华的选择，比如离城镇较远的超大型豪宅和带有护城河的城堡，虽然本质上仍是个大号方框，但它们的售价可以轻松突破千万级别，这么一看隔壁SAO血盟骑士团的总部塔才要10亿珂尔真是太便宜了。\n不过这方面就不得不提到一项令人头疼的问题，无论城堡还是小茅屋，其所有者一旦彻底死亡，房产与里面存储的全部物品都将直接蒸发，也不能写遗嘱什么的留给其他玩家。\n想避免这类情况有两种选择，第一，任命一名完全不离开安全区的法人角色，将资金交由他进行购置房产，再接着将房屋的访问权限设为队内共享，这样当事人出现意外也不会影响到固定资产；第二，则是组建玩家公会，然后再将房屋变更为公会财产，即便原主和公会长双双殒命，也不会导致公会解散或是资产消失，取得公会钥匙的成员将自动晋升为新任公会长；然而成本方面，创建公会需要一名30级以上的角色和整整2000k的费用，你没看错，需要花费整整两百万金币，仅仅为了注册，不过嘛，都想着买小城堡了，那这点钱应该不会放在眼里吧。\n\n地城里有几率遇到丢钱陷阱，部分敌人也拥有盗窃能力，最离谱的是有些魔法能让你的金币造反，跳出钱包变成活化怪物然后狠狠地揍你，如果不想遭遇意外，那么还是不要腰缠万贯地冲进地城深处为好。\n虽然我个人很喜欢把货币扔在屋里，然后欣赏那种积玉堆金的景色，但要是不想每次都得回屋取钱，商人行会很乐意为您提供存款服务，并在户主死亡后侵吞其所有财产，不需要手续费，但也别想获得利息；另外有便捷的快递代发可供选择，能将物品或金钱寄给其他玩家，不过服务费不便宜，收货人则可以在任意一处行会站点取到包裹。\nham曾经给我寄过一盏不用添加燃料的矮人灯，整个收货体验还是比较奇妙的，但必须前往商人行会的站点仍有些不方便，而且东西越珍贵收费越高；可如果队伍里有使用念力的高阶法师和超能力者，那么可以让收货方open mind，建立起精神联系后直接隔空传送物品；万一没带光源直接前往地下深处是件很危险的事，不仅看不到怪物与周遭地形，种族缺少夜视能力甚至无法在黑暗中阅读返回地面的卷轴，只得困在下方等待营救，这时要是能有人隔空递根火把可就帮大忙了。\n\n之前的探索到也不全是白费功夫，地图的绘制上多少取得了一些进展，ToMENet的世界由区块组成，这些小区域互不干涉，但玩家可以穿过边界抵达相邻区块；即使角色正在被猎豹追杀，一旦跑到其他区域，哪怕怪物与你仅有一步之遥，它们也无法越过区域边界，只能望尘莫及，然而在隔壁会遭遇什么可就不好说……\n矮人30级时将获得翻越一切高山的能力，在此之前也不得不像其他人一样使用攀爬工具，或是在狭窄的岩石缝隙中穿行，而这正是最为惊险的地方；在跨越边界时无法看见对面的落脚点，如果对面是山体，那么角色会被“挤压”到最近的空旷处，可当玩家想回去时，游戏又没这么好心了，必须翻过那块山体才能回去；虽然通常可以绕路从其他连通边界的地方回去，但要是遇上死胡同或是怪物占道，那就必须读卷轴回城（希望包里还剩几张没用完），位于火山的话，那角色脚底下很可能有一池岩浆，在返程卷轴生效前多半会先被活活烧成渣。\n\n前文曾提到过，每个服务器的游戏世界都是随机创造的，所以也没法按照LotR的地图直奔邪黑塔，先简单介绍一下我们的周边环境：\n布理镇的北面是片难以穿越的林子，显示为绿色的#，越过树林则会见到海洋，我就是在那边的海岸线附近遭遇了邪教徒，镇子西侧生成了一座火山（萌新吞噬者），想不到用什么恶毒的语言来攻击这玩意，后续经探索，里面除了岩浆和乱石之外别无他物，并非恐怖的末日火山；自东北向西南有一条大河，或许正是著名的烈酒河，跨过河流可以见到古代森林与一大片沙漠，那里还有机率刮沙尘暴。\n\n我们发现的首座其他城镇是“隐匿之城”——刚多林（Gondolin），至此可以确定游戏内的时间是在第一纪元了，当时我正在北部原野往东探索，走着走着突然发现了不少房屋与菜地，随即反应过来这是房区，那说明周围必定能够找到城镇，一番地毯式搜索后果然发现了刚多林，可惜我溜进来的位置是后门，否则从雄伟的城墙正门漫步至王之广场肯定更加震撼。\n刚多林里的商店全是基础类型，没有高级版的变体，不过数量上更多一些，食物店有整整3家，药水店也有2家，此外存在大量布理镇没有的服务点，像是草药治疗、变形恢复、物品研究、全背包鉴定……都相当实用；我们后面经常背一些神器来这里鉴别，因为某些进阶词条无法用普通的鉴定卷轴揭示，而*identify scroll*很难在基础卷轴店里见到，从黑市中花高价买又太过吃亏，到头来还是跑一趟刚多林实惠。\n\n\n有些让人难以理解的是，跟刚多林配套的地城居然是魔多，大概想表达对抗关系吧，我起初看名字还猜索伦在下面等人，最次也得是炎魔之首或者格劳龙，后来查了才发现这地方居然没有固定的关底boss。\n可这不代表魔多是好捏的软柿子，光是下去的第一层就有35级的强度，要知道在击败魔苟斯前，玩家能达到的等级上限才50级；发现这地方之后有几名群友兴冲冲地来练级，打算变强再把古冢岗底下那几堆神器捞回来，结果不幸遭遇了克苏鲁神话中的外神，当场被秒杀，祂吐一口邪垢的伤害都有515，比我满血开狂暴还高；再后来还听说有人遭遇了近七十级的天使，完全不是一个层次的生物，简直让人怀疑人生。\n\n\n埃兰迪尔之星和其余的几件神器，随着底层那只鬼魂的网络掉线，最终还是没能挽救回来；我之后去过-1750ft，虽然神器早八百年前就给刷新掉了，那一次的经历倒也算得上是创伤小组的光荣事迹。\n起因是ham忘记带灯又直接返回了古代森林的深处，如前文所述，这时要是有人用念力直接远程塞个光源就完事了，遗憾的是我们没人会这种操作，所以只能由创伤小组下去送灯送干粮。\n一路上遇到了三名头目：yeek king、东来者首领罗甘（Lorgan）、以及乌姆巴尔海盗的领袖之一，安加麦提（Angamaite of Umbar），这群人就算加上手底下的杂兵，对我来说也不是啥很难缠的对手，除了最后一位的名字把我吓得不轻，老眼昏花差点看成了乌顿（Udun）的安格玛巫王（Angmar），要不是没来得及跑就给对面砍死了，差点闹出大乌龙。\n\n罗甘给我掉了个能放火球术的护手，清理一些很弱又追不上的东西非常非常好用，比如野生果蝠和法师学徒，这些怪物几乎没威胁，但极其喜欢在玩家周围瞎晃悠，果蝠完全不攻击但一直远距离跟着，法师则时不时放个魔法恶心一下人，你冲过去他又瞬移到旁边继续骚扰。\n玩家附近有敌怪就无法快速移动，只能缓慢地一格一格走，由于速度不够追不上，通常只能靠进入其他楼层或是穿越区块边界来甩掉这些玩意，超级浪费时间，光是想起来就血压高，千人血书请求灭绝果蝠（1/1000）。\n尊贵的远程职业吐口痰就能把他们弄死，但智力为⑨的战士只能干瞪眼，除非你愿意喝瓶几千金币的速度药水就为了砍一只野生果蝠；自从有了火球护手，再也没被乱七八糟的玩意烦过，虽然伤害不高，放完一发还得等cd，但它能无限使用而且没有智力门槛，更没有蓝耗和施法失败率，真是爱死这护手了，赐名“炎拳”。\n\n好装备见多了也绝对不能放松警惕，去找ham的路上就遇见了几件烂到出奇的垃圾，比如这把魔古尔的Tomahawk（一种原始战斧），攻击和命中修正都是负数，拿上之后会让玩家感染恐怖的黑息，降低运气和最高生命上限，同时激怒周围的怪物，导致更加容易遭受围殴；不光如此，它属于严重诅咒的物品，必须用带&quot;*&quot;号的高级解咒卷轴才能从手中卸下，由于它还能重复诅咒自己，且在未解咒时无法丢弃，不小心捡起来的话又得斥巨资再到黑市买张卷轴。\n然而这玩意或许可以考虑给吸血鬼用来逆转，但还有一种无法攻击的Nothingness词缀，那是真正彻头彻尾的垃圾，吸血鬼都用不了，而且光放在包里就会妨碍玩家强化别的装备，让升级变成降级，所以出门在外一定要时刻记得鉴定。\n\n扯远了，把重点转回ham这边，成功救援之后我们回镇上补给了一趟，然后决定顺带把森林打穿，本想着两个人应该很轻松，可返回原位后刷新的地形是一大片空地，四处都是敌人，更不妙的是我们并没有传送到一起，不得不各自为战。\n我当时旁边围着一群猎犬，其中还有只不断狂吠的精英怪，这叫声引来了附近一座vault里的巨量怪物，右面还有几队不知道啥品种的半兽人也赶了过来；光是这些也还好，我借助地形仅需同时面对四五名左右的敌人，盔甲较厚也吃不了多少伤害，甚至有空发消息呼叫增援，问题在于未知法师远距离降低了我的理智，杀出重围后理智值仅剩73%。\n几名远处的法师被ham杀掉后我以为危机解除，便不打算撤退，返回了右下角的vault准备收拾些战利品，在靠近时不知道哪里有株反魔真菌一直在释放干扰立场，由于我根本不用魔法，没有丝毫心灵负担地就找了过去；结果眼中突然天旋地转开始出现幻觉，原来之前攻击角色理智的不是什么普通法师，而是一名高等级的敌对心灵工匠，并且在我突围后他没有跟上来被消灭，意识到这一点我立即对自己使用了传送法杖逃命，但仍然吃了两发灵能冲击，理智值下滑至了危险的25%，整个人离踏进鬼门关只差半步。\n\n\n好在我们命不该绝，虽然一个精神残疾一个血药喝完，最终还是成功的撤了回来，多亏之前舀了大量理智药剂存在酒馆，我灌上几口又重回巅峰，否则还得等牧师睡醒；尽管出师不利，但两名活阎王显然不肯放弃，毕竟也没遇见什么较为离谱的怪物，直接带上了更多的补给道具就下去报仇。\n清点着包里的理智药剂，我干脆绑定了快捷键，大不了硬抗精神伤害也要上去把那帮杀千刀的心灵系术士给剁了；阅读返程卷轴，回到森林深处，使用魔法地图，喝速度药水开狂暴，大杀四方一气呵成，然后……然后就没了，我们把整个楼层全探索完了都未能找到下去的楼梯，也没有再刷新出会施放精神攻击的敌怪。\n仔细一查，发现这层已经是森林的最底部了，而我们一直以为在倒数第二层，白折腾半天，甚至还攒着不少打算到最后关头再用的消耗品，像是启蒙和抗性药水之类的，就这样，我们稀里糊涂地完成了古代森林的攻略。\n\n并不过瘾的两名战狂准备回古冢岗一雪前耻，大致上是见到楼梯就下，直奔最底层，一路上顺风顺水，甚至没有精英怪和楼层头目拦路；可抵达-1750ft之后却受到了大群地狱猎犬的热烈欢迎，虽然它们的吐息喷我并不怎么疼，但背包里的各种药水却因为元素伤害而不断损坏，9瓶宝贵的理智药剂碎了5瓶，治疗和速度药水更是损失惨重，吓得我赶紧跑上楼把剩下的那4瓶塞给了队友。\n被底部一巴掌扇回来，我们还是决定在-1700ft逛逛得了，至少不用担心可能冒出来的尸妖王，一顿slash&amp;loot之后，我们发现了个闪光的字母E，ham先冲上去开怪，但不知道踩了陷阱还是直接被当场暴打，血量蹭蹭往下掉，几番周旋后药水还耗尽了；幸亏我们配合默契，他往旁边跑我往前面冲，成功把怪物拦在路口，那一瞬间简直泪流满面，我这名战士终于有前排站桩的机会了，之前经常是果蝠身体的队友飞太快，我在后面跟不上，或是对可怜的脆皮法师照顾不周，一个疏忽就只剩尸体。\n那个闪光字母是头火焰树人，也不知道它怎么没给自己烧干净，反正现在由我们送它上路，比较麻烦的地方在于攻击附加火焰属性，甚至站旁边都会被它的烈焰光环烧到，好在我有一件火抗披风，还可以激活能力提供双重抗性，打我跟挠痒痒似的；然而就像前文提到过，纵使双重抗性也无法阻止物品被破坏，我包里的药水劈里啪啦碎的像是放爆竹，卷轴也跟烧纸钱一样全成了灰。\n\n另外据受害人ham所述，这玩意还能降低玩家的属性，他几乎全属性全黄了，光是买各种对应的恢复药剂就得花几千金币，我的话比较幸运，之前的yeek king掉了件brigandine armour，提供各种属性维持，还加2点移动速度，我最后只损失了微乎其微的力量，完全无伤大雅。\n本来打完火焰树人就想让队友先回地表补给，结果我继续清怪的时候ham跟了一步，不小心踩到陷阱，刚恢复的血量又蹭蹭狂掉，果蝠身体虽然闪避高，速度也快，但hp上限不高，非常头疼于必定命中的陷阱；当时那叫一个惊险，我连忙回防让他赶紧recall，旁边源源不断有怪物涌来，还潜藏着成堆的陷阱，万幸，卷轴生效速度比死神的脚步快，ham成功撤退，不过这种你先走我殿后的感觉简直燃爆了。\n对准怪群大开Taunt拉仇恨！You shall not pass！\n\n这种光荣事迹就应该记在创伤小组的历史上，配享群太庙；不过总的来讲，最终收获是：被陷阱扎到半死的蝙蝠一只，烤得外酥里嫩的矮子一名。\n地上那些垃圾我顶着陷阱收拾了一下，反正重甲可以肆无忌惮，除了几个戒指以外没啥能看的，火焰树人甚至什么都不掉，还真烧了个一干二净；当天的冒险之旅到返程完就结束了，我们后来一起下过魔多，那条魔像腿就是两个人在魔多的时候挖出来的，ham负责冲在前面开无双，我由于负重高就在后面捡垃圾顺便挖矿。\n在魔多也遇上过危急情况，不过是有惊无险的那种，当时我不小心踩到了活板门，然后直接掉到下方楼层的boss脸上，他叫乌法斯特（Ulfast），是东来者族长的儿子，不强，比较轻松的干掉了，印象中没有给好东西；但刚摔下楼又看到闪光头目的时候真冷汗下来了，毕竟来不及仔细看是谁，就结果而言，只在前两层溜达，felling又正常的话，应该不至于撞见古神什么的，深处就不好说了。\n\n\n风险越大回报也就越大，魔多虽然不是啥好地方，但各种板甲龙皮甲随处可见，像Kolla这种高级袍子，哪怕是无前缀的基础款也自带一堆修正，卖掉能值50k金币；地板上甚至可以捡到野生的属性药水，喝下去永久增加角色的对应属性，放黑市里要卖近十万一瓶。\n实力足够的话，在这也能爽快练级，不死族和亡灵较少，经常能见到大群的半兽人与巨魔，砍翻一屏幕的o相当解压，德鲁伊和法师放毒更是一死死一片，由于第一层就足够危险，非常适合stair-dance，指的是在楼梯口不断上下刷新地形，挑选最近最稳妥的怪物进行清理，一旦感觉不对便可立马上楼回地表；但或许值得注意，我在这遇到过几乎整层都是岩浆的地形，即使能够飞行，在岩浆上方仍然会遭受伤害，多半需要有火焰免疫才能安全经过。\n\n\n再后来的那些日子里，我主要都在为绘制地图而探索世界——\n旅途总体上是平淡乏味的，听着音乐穿过大片大片的旷野，飞越海洋，翻过高山，作为大自然的回报，旅行者也能欣赏到一些美景，像是林中小湖、沙漠绿洲、海洋中的灰森林岛屿等等，尤其在遭受过精神创伤后，这些美丽的事物格外令人治愈。\n\n\n第二座被发现的城镇是LotR中著名的白城，但此时它还不叫米那斯提力斯，而叫米那斯阿诺尔（Minas Anor），有些混乱的是，ToMENet中已经出现了魔古尔武器等描述文本，而白城的姊妹城市米那斯伊希尔（Minas Ithil）实际上要在第三纪元才被索伦攻占，变成戒灵要塞，改名米那斯魔古尔（Minas Morgul），也许游戏里是什么平行时空吧，否则我们不可能在见到这些的同时看到刚多林，还有精灵矮人一家亲之类的。\n白城展现于世人面前的过程也历经艰辛，它被一座座火山和高峰环绕的严丝合缝，房区在火山外围，我第一趟费了好大劲，几次重度烫伤都没能穿过那些高耸的尖峰，直到30级获得种族攀岩能力才最终得以如愿。\n从正门进入，观赏完城内美景后，我去执政厅觐见了一下当时的国王，一看人名Aragorn Dunadan懵了，大步佬不应该没出生吗，怎么白城还没改名就已经登基了？后续查阅得知：Aragorn是辛达语，意为“可敬的国王”，由表示荣耀的前缀ara和“可敬的”gorn组成，Dunadan则可以指“杜内丹”这一人种，所以王座上坐着的大概率不会是我们熟知的阿拉松之子，阿拉贡。\n此处对应的地城是The Paths of the Dead，死者之路，里面几乎全是亡灵和不死族，据说还有那兹古尔，攻略起来应该会被各种负面状态烦得不轻，另外光是第一层就有40级的强度，我下楼梯瞅了一眼立马跑了，根本惹不起。\n\n\n抛开到处都有的基础商店，白城这最为特殊的是有一家高级黑市，里面出售各种顶级的武器盔甲，威力惊人的法杖，甚至还有不同学派的法术全本，这些东西随便弄到一件咱都得抱着傻笑一晚上；然而它们的售价却也那么高不可攀，统统都是六位数起步，随便一本法典都要近四十万的金币，把我卖了估计都没这么多钱。\n服务点方面基本和刚多林相同，不过这里有座赌场，应该是游戏中唯一的一座，里面除了摇灌铅骰子猜点数之外，居然还可以下围棋，不过这种围棋变体的棋盘只有9x9大小；对手的难度等级分为8档，尽管ToMENet由于开源的关系一直保持着更新，但棋艺方面肯定比不过近几年的机器学习，而且浏览到玩家手册上一份零几年的围棋入门视频时，我更加断定了这种猜想。\n群里非常热烈地讨论了要用哪家的ai模型，然后……然后大家就忘了这茬，可能是我们道德素质太好，不忍心把赌场刷破产，最后连ascii的下棋界面长啥样都没见着（其实是不小心忘了）。\n\n\n人有悲欢离合，月有阴晴圆缺，再长的旅途也终将抵达尽头，我的故事到这里不得不告一段落了。\n那是在野外遇到的一大群Orge，虽然我个人更习惯将其译作“食人魔”，但这些家伙并不是比尔博在森林里遭遇的那三只食人妖，那三名怪物在《霍比特人》一书的原文中使用的是“Troll”一词，也就是我们现在常说的“巨魔”；纠结名称也不是很重要，只要分清我遇上的是Orge，老比尔博遇上的是Troll，而Ogre不会在阳光下变成石头就行。\n\n我低估了敌人的危险程度，也高估了自己的风险承受能力，对着这些食人魔就冲了上去，殊不知这却葬送了自己的后路，四面八方的攻击如潮水般涌来，血量急速下滑让我开始感觉不对劲，想后撤穿过地图边界离开，但背后早已被敌人包围得水泄不通。\n曾经无数次救过我的药水，在这次却成为了间接的死亡原因，因为在被包围的情况下，角色行动一轮，周围将存在整整8名敌人进行近战反击，更加不妙的是，这群食人魔还有大祭司远程施放魔法，以及前排偷窃戒指等装备的盗贼；致命伤治疗药水的回复量抵不上每轮损失的血量，根本无济于事，习惯性地不停喝药让我错失了最佳的逃跑时机，等我想起使用传送法杖时，hp早已岌岌可危。\n最后关头我记得有按过传送的快捷键，可不知道是网络延迟还是同时喝药导致操作冲突被吞，我最终未能脱困，不幸殒命当场。\n\n双拳难敌四手，更何况是十几名食人魔，由于即时制，事情从发生到结束不过短短十秒，很多东西没有反应过来，我便因为肉体损毁变成了鬼魂；虽然hp耗尽，但我这名3命角色还一次复活机会都没用过，故事本不该在此落幕，但游戏为了防止鬼魂被原先的敌人继续杀死，玩家死亡后会随机转移到同地图的其他位置，而我旁边不幸的出现了一头远古白龙。\n理解白色的字母D代表什么之后，我试图逃跑，但这条远古白龙的移动速度显然更快，何况它还能够喷吐超远距离的龙息，转眼间，我便再次失去了意识……\n\n“救命啊，训练塔里有好多老鼠”\n“来楼梯口蹲着揍老鼠”\n“啊？有什么怪把我的法术书烧了”\n“哪里有蠕虫啊——我这里一只都看不到”\n“二楼右下角的房间里有”\n“有人要这把剑吗，1d5”\n。。。。。。\n“四人小队，出发！”\n“楼梯在这里”\n“被麻痹了”\n“等一下队友过来”\n“我来啦”\n“&gt;&gt;”\n\n“法杖储能快没了，有没有人帮忙充一下”\n“我可以提供免费鉴定和充能”\n“把法杖彻底放干好像就不会爆”\n“下次试试”\n“（我也充爆过”\n“反正都不是什么值钱的杖”\n。。。。。。\n“ham ham，me dead”\n“u dead, dead where”\n“south of the volcanic hall”\n“aww”\n#dig#dig\n“found a vault，f**k wiz and volcano”\n\n“大灯哥画个果蝠”\n#SuperBulb瞬间出图\n“传神”\n“有艺术的”\n“用4090生成37000张表情包”\n“大灯哥拿到了4090脚就离开了地后边忘了”\n\n\n“imsb”\n“忘记带灯了”\n“还有武器”\n“ahhhh”\n“巧了，我下来也是”\n“帮我点个灯”\n\n“怎么房区还能刷果蝠，这不得给烦死”\n“业主呢，都出来维一下权”\n“什么业主，物业”\n“78物业，我在房区杀过兽人小队”\n“bree治安不是很好”\n“房子都建水里了”\n“有人要变银色小老鼠吗”\n\n“捡到张召唤死灵的卷轴，能拿来刷经验吗？”\n“应该可以（”\n“召唤怪物卷轴也刷经验”\n“就是 死灵可能会抽经验”\n“我刚刚差点用召唤杖把自己弄死”\n“出的东西很恐怖”\n“最好在走廊读，或者墙角”\n“死灵没脑子应该没法师吧”\n“巫妖也是死灵”\n“草，对哦”\n#捏着传送法杖在村长旁边读了然后发现安全区不能召唤怪物\n\n“v我50k”\n“等我把我的神器弓卖了就有了”\n“再去赚5k，hxh！”\n*Cap face\n*Whip*\n“唉，资本家”\n=。=\n“现在就给我去赚钱！”\n“v你5k先”\n\n“喝酒伤身，明儿见”\n#You notice Shinie the bat trying to steal from you！\n#Shinie the bat is seized by the guards and thrown into jail！\n“opz”\n“哈哈哈哈哈嗝”\n“等你出狱v你100”\n。。。。。。\n#翡翠鸟光速出狱买了酒\n“喝，你为什么不喝”\n“都给你买了一瓶了！”\n“喝酒掉san”\n“san掉没了脑死亡变植物人”\n“那这个酒有什么用”\n“谁喝谁倒霉”\n\n“happy”\n“i got slime mold juice”\n“want one？”\n“of course”\n“chicken nugget juice”\n“here”\n“that was a delicious slime mold”\n“thx，iden +1”\n\n\n无数回忆掠过脑海，但一切都结束了，眼前仿佛出现了远古白龙的寒霜吐息，将我的灵魂灼烧殆尽。\n哦，没错，我为了绘制地图走得太深，魂飞魄散无法复活了，此处的危险程度相当于地牢的51级，太过贪婪招致自身灭亡，某种意义上也是很符合矮人精神的死法。\n死亡时得分为168065，等级由于被吸掉一级，变成灵体又降了两级，最终显示为31级；另外由于鬼魂状态，没能在记录文件中留下装备信息，略有些遗憾，名下绝大部分财产都因物主死亡而烟消云散，仅剩的少量遗物放在“公厕”里留给了队友，这也是我最后能尽的绵薄之力。\n\n\n在生命活动终止前，我传回了最后的几张图像，并将它们拼在了之前测绘的地图上，往后的空白就要靠队友们填充了。\n起先为了搞清世界的具体大小，我将东南西北方向都探索至了尽头，然后再开始逐渐填充四个象限，所以最终留下的地图像个十字墓碑，或许我的归宿在这时就早已注定。\n\n\n逝者已矣，生者如斯，这只是些关于不知名探索者的事迹，至于成功讨伐魔苟斯凯旋而归的故事，则是一段应当由其他生还者讲述的传奇伟业了。\n这里长眠着刺猬，它为了探索广阔的世界而牺牲\n不能与大家一起挑战米尔寇了很抱歉，但那具魔像的残骸希望你们能拼好\n愿后世旅人的路途不再充满迷雾\n","plink":"http://www.ephesus.top/2024/BoMENet-arc/"},{"title":"Avanor游记：从充满恶性bug的粪作中艰难通关","date":"2024-05-22T03:43:58.000Z","date_formatted":{"ll":"May 22, 2024","L":"05/22/2024","MM-DD":"05-22"},"updated":"2024-06-19T14:27:42.966Z","content":"投稿: H_Hedgehog\n故事要从一只没电又找不到充电线的鼠标说起，尽管我的旧ThinkPad有指点杆，但指望用那种东西玩即时制游戏还是太勉强了，毕竟在下不叫路明非。\n不过这也是玩老游戏难得的好机会，大概几个月前我因为别的缘故囤积了一批Roguelike——是那种画面由字符构成的回合制游戏，而不是有点随机要素就往自己身上贴标签的现代工业垃圾；在失去鼠标却充满无限可能的美好夜晚，我从存货里挑了款叫Avanor的游戏并毫无防备地开始了游玩，殊不知即将遭遇的……是堪称地狱般痛苦的游戏体验……\n\n进入游戏要做的第一件事必然是rtfm，也就是读说明书；但这玩意上来就给了我个下马威，提示“在目录下找不到帮助手册”，然而手动切出去一看文件夹，手册安安稳稳地躺在该待的位置，居然还是用html写成的，也不知道程序为什么死活不承认它的存在；识别不出路径倒也无妨，直接本地用浏览器看甚至更方便。\n\n\n\n依据手册内容简单介绍一下Avanor，这是一款奇幻风格的角色扮演游戏，有着类似于ADOM（传统roguelike佳作之一）的操作界面，除了UI设计上“借鉴”不少之外，一些机制也几乎照搬，比如同样的DV PV（闪避和防御值）设定以及从Coward到Berserker的几种战术风格；不知道该不该庆幸，Avanor里的时间观念倒没有那么重要，因为不会有腐化或者以太病什么的赶着玩家跑，但考虑到各种崩溃、闪退、还有成堆的bug，一个存档的游玩记录越长就越容易永久损坏，某种意义上这个世界也算是危在旦夕，正在逐步崩坏之中。\n剧情方面相当老套，也禁不太住推敲和考据：阿瓦诺王国在古代十分强盛，而今它已没落，突然隆起的山脉围困住了王都以及周边一带，许多可怕的生物趁着阿瓦诺孤立无援之时大肆破坏，您作为幸存下来的英雄，将毫不迟疑的拯救王国于水深火热之中，解决这一前所未有的危机……真的吗？\n\n创建角色的过程比较乏善可陈，选择种族、性别、职业，再输入姓名后就完成了；种族并不丰富，总共只有7种，都是奇幻世界观中常见的类型，除了会影响初始属性的区间和最大属性的上限之外，还会影响角色对食物的评价；比如一份普通人勉强能够下咽的食物，野蛮粗犷的半兽人尝了会觉得是美味珍馐，而对食物极度挑剔的半身人尝了可能会当场呕吐，导致饱食度不增反降；另外还有一项速度属性也是种族决定的，但作者并未对不同种族设置不同数值就停止了更新。\n性别在那个年代流行的还是简单的二元论，Avanor中不存在“沃尔玛塑料袋”之类让玩家眼花缭乱的性别选择，男性会增加一点力量，女性会增加一点体质，仅此而已；所选职业会提供相应属性的固定修正值，并与种族共同决定玩家的初始装备和习得技能，例如人类战士会拿到长剑并拥有烹饪技能，而矮人战士则会得到战斧以及一项挖掘技能。\n听起来不错是吗？但设定得多么饱满，表现出来的就多么贫乏，因为游戏中压根就没有对这些选项进行任何描述，手册同样没有，仅仅介绍了各种技能；可绝大多数技能无法后天习得，玩家要是对某一技能感兴趣，就必须控制变量并不断创建角色，挨个尝试不同选项再到游戏里摸索效果，尽量获得较为理想的技能与属性；然而就像前面提到的，不同类型的选项之间还会产生组合效果，这极大地提高了猜测难度；我是怎么能了解得这么清楚的？因为后来迫不得已把它的源码翻了个底朝天……往下看吧。\n\n玩家的初始位置处于一座村落当中，那些由#号构成的方框就是房子，东边有一条汇入湖泊的小溪，桥对面则能看见不少强盗；村子里有一家食品商店，货物种类不多，无非是大中小三种口粮（Ration）以及精灵的旅行面包（Waybread），这种食物的辛达语名称更广为人知，叫兰巴斯（Lembas），正是《指环王》里那种轻巧又抗饿的美味旅粮；食品的物价在Avanor中相当低廉，最贵的旅行面包才卖15金币，让人不禁怀疑隔壁萨鲁曼是否真的实现了工业化，不过刚开局玩家穷得叮当响，也只买得起一份就是了。\n手册里关于商店的介绍相当有意思：在邪恶降临这片土地之前那个更加幸福的时代，店主很乐意让人们捡起想要的全部东西再一并交钱，可某些流氓和恶棍利用了店主的善意，所以阿瓦诺的商人行会制定了条规则，您必须在捡起任何一件货物后为其付款，别想再装满购物篮后就逃离商店；换个角度理解其实等于允许零元购但是限购一件，只不过玩家没有给钱便离开商店的话，店主会计算玩家的债务利息并在下次要求一同支付。\n还有一种……姑且称之为特性吧，角色能够直接吃掉地上的可食用物品，不需要捡起来，而未拾起的物品就算消失也不会计入玩家的“购物篮”，这恰巧跳过了交易的检测环节，即使在店里吃到撑也不用付出任何代价，甚至把其他地方捡来的食物卖给店主后再吃掉也未尝不可；药剂卷轴等消耗品并不包含在食物当中，必须捡起来才能使用，对比下来这条特性似乎成了种防止玩家饿死的低保，然而考虑到获取食物的轻松程度，还有赶路过来的时间成本，这多半是项漏洞。\n本来我寻思着自己捏了个纯近战向的矮人战士，砍上几名新手村外的强盗那局势应该是一面倒才对，事实证明这完全正确，的确是一面倒的单方面爆杀，只不过倒下的人是我。\n\n\n出师未捷身先死，我就这样迎来了游戏中的首次死亡，愣是没想明白村口的强盗为何如此之强悍，幸好Avanor不像其他传统Roguelike那样会在死亡后强制删档，试错成本还处在尚可接受的范围内。\n介于跨过桥后强盗们便会主动攻击，走正路是行不通了，我只好在村里尝试与所见到的每位居民对话，试图获取有用的信息，但每次都仅能得到一句“你没有收到任何答复”的系统消息（后来发现是bug），直到我与村中长老交谈，他告诉我村子西边有个大家经常去采蘑菇的山洞，但前阵子一头恶魔占据了那里，湖泊南面说不定有人能帮上忙。\n对话系统做得稀烂也就罢了，最大的问题在于任务引导上，刚才这段指引内容在npc口中只会讲述一遍，并且因为可显示区域不多还是分段展示的，如果玩家没有看清或想再次确认，那么抱歉，长老仅会重复一句“那头邪恶的怪物仍然活着”；仔细思考前面的几句指引其实不难发现，长老是让我去湖泊对岸找人帮忙，但玩家要是读过手册会查看自己的任务面板，上面写着的却赫然是长老请求玩家杀死那头恶魔。\n\n\n初入这个世界我没有考虑太多，下意识认为新手任务应该不至于离谱过头，洞里没准是只imp那类的小恶魔，会让村民烦恼也很正常，随即拎着斧头便准备下去大展身手。\n山洞离村子非常近，就位于突然隆起的山脉前，第一层安静得吓人，空空荡荡没有任何生物，我警戒着走向第二层，刚拐过个弯就看见了一道蓝色的身影，正是任务当中提到的恶魔；从看见它的瞬间我便开始发怵，字符&amp;在大多数Roguelike中代表的都是血统纯正的恶魔系怪物，绝对不是imp那类亚种可以比拟的，况且这家伙有名有姓绝对是大恶魔级别以上的存在。\n通常情况下玩家应当立刻掉头就跑，但我仍怀有一丝侥幸，将战术模式切换成完全防御状态缓缓凑了过去，万一对方是魅魔之类可以交涉的种族呢；在靠近的过程中，它的确没有突然暴起并以惊人的速度冲过来将我撕成碎片，反而当我不存在似的，自顾自地在那闲庭信步，直到我接近它身旁时才随手一击清空了我的生命值，说是清空但其实不准确，从负数来看，这一击造成的伤害是我hp上限的十倍有余；杀我对它而言就轻松得如同捏死只虫子，难怪之前忽视我的行动，一只虫子的存在与否确实不值得它放在眼里。\n\n死亡接踵而至，一切又回到了原点，没有狗洞没有发疯的木匠，谁能料到新手任务要去杀死一头古代种的恶魔；也不知道这个世界中的蘑菇是不是什么硬通货，亚种就算了，连古代恶魔都来抢，更让人好奇的是这村子究竟有何神通，居然能在周围这群煞星手下平安待到现在。\n眼下最重要的是寻找出路，我四处转悠，很快在村子南边发现了一座通向湖心的小桥，走到尽头能够看见一扇传送门，我隐约回想起长老曾说过南面和湖泊，便义无反顾的走进了传送门中，转眼间我来到了一处充满建筑物的地方，后来才知道这是王城。\n出传送门往东走几步就能看见一座大理石建筑，里面有黄金砌成的地板与同样是黄金铸成的栅栏，在它们之间的，是一团被称为永恒烈焰的不灭之火，旁边站着的则是被称为火焰之主的魔法师；从介绍文本中得知他还是国王的顾问，然而这样一位实力高强又有权有势的大人物现在却满脸忧愁，与他对话后了解到有名邪恶的法师需要被消灭，目标就位于东南方向的地下城中，地表上方则有着邪法师塔楼的遗迹。\n\n能让皇家首席魔法师感到头疼，必然不会是什么省油的灯，事实上那名邪法师正是最终的反派头目，难以想象勇者刚出新手村就被安排去讨伐魔王，关键在于这魔王还不是一般的近，简直是刚出油锅又入火坑。\n王城当中有一家消耗品店，出售各种卷轴药剂还有魔法书，值得一提的是Avanor的法术系统与ADOM大相径庭，ADOM中阅读有次数限制的咒语书可以获得咒语知识，施法会消耗它，而知识一旦消耗完毕该咒语便无法再次使用，必须寻找新的咒语书来读；Avanor中的魔法书只能阅读一次，但习得的法术并不会随着使用而遗忘，魔力没有消耗殆尽即可不断使用，找到同样的魔法书再次阅读则能够提高法术等级。\n两款游戏最核心的差异在于阅读书籍的成功率，ADOM中读书需要考虑职业、等级、技能、以及书籍状态，一旦失败就得承担包括物品损坏或者属性永久降低在内的负面效果；但Avanor只要玩家角色不是文盲便能够阅读任何书籍，读写技能仅影响阅读所需要的时间，安全区域中唯一可能中断过程的大概只有饱腹度过低，如果阅读中断，书籍并不会被消耗，吃撑之后再读便是。\n这样的设定让Avanor中物理系与魔法系职业的同质化程度相当高，职业不提供加成搭配上属性培养也不困难，玩家到了后期基本都样样精通，远近攻守兼备；作者意识到了问题，从待办列表中不难看出他在考虑为不同书籍加入最低的属性与技能要求，可这毕竟是款停已久的古董，有生之年恐怕都没办法看见它们被实现了。\n\n自王城向东南方向行走几十余步便能看见一处建筑残骸，顺着正中央的楼梯向下前行就抵达了地下城；虽然是邪恶大反派所在的藏匿处，但里面的怪物却意外适合初出茅庐的玩家练手，浅层怪物主要为骷髅还有老鼠蝙蝠之类的小动物，都是很常见的种类。\n到第二层我还遇到了只ghoul，它的攻击有小概率造成麻痹，隔壁ADOM也有这种怪物但拼写方式为ghul，新手碰上的话还算比较致命；同样在不少游戏里出现另一位“常客”叫作灰色软泥（gray ooze），ADOM与Nethack中的这种怪物会腐蚀武器和盔甲，至于Avanor里这小东西则是攻击带毒，以及不像ADOM的亲戚那样会自我增殖。\n\n值得一提，Avanor的怪物间也有着仇恨关系，我曾在与一只老鼠搏斗时突然窜出来条褐蛇，但这条蛇不仅没有先攻击我反而一口咬向了同为怪物的老鼠，也算是意外之喜。\n\n打怪升级的过程整体上还算轻松愉快，逛上三四层地牢就够我升到7级，只不过随处可见的陷阱实在是折磨人；陷阱依据其类型能够造成物理或者元素伤害，可对于前期的玩家来说，不管踩到什么种类的伤害陷阱，下场通常都是被秒杀，“Killed by fooself”则会被记录为死因；更令人头疼的是这游戏并没有几乎每款Roguelike必备的搜索功能，甚至连等待一回合的功能都没有，我自己摸索了好一阵子才发现拾取物品的操作可以在不离开原位的情况下消耗回合，勉强有了替代品。\n\n由于不能主动搜索，被动探测就成了唯一的办法，具体就是在可疑位置附近来回走动或是不断按下拾取键，如果陷阱被发现便会立刻失效，能够安全地经过或是拾取战利品。\n发现陷阱的概率取决于技能等级，玩家升级后可以获得少量改进点数，先不提将宝贵的点数投入到非战斗技能上是否值得，每项技能所能使用的点数存在上限，而这个上限需要通过相应行为锻炼才会提高，直到最大值15；换句话说，想要增加探测概率以避开陷阱，就得先找出大量陷阱积累经验。\n\n与陷阱斗智斗勇已经不可避免，基于个人感受，它们主要集中在房间门后以及财宝室里，狭窄的走廊中通常不会设有陷阱，至少我的印象如此；最应当警惕的是平白无故躺在地上的宝物或装备，这些很可能是诱饵，或来自上一名踩中陷阱的倒霉蛋，必须要在周围仔细检查；由于怪物对陷阱没有豁免权，加上它们会主动捡起物品，我后来还有过在一格陷阱上捡到五六套装备的经历。\n当然，时刻记得在开门后不要急于冒进，说不定冲过来的怪物转眼就将旁边的陷阱踩了个正着。\n\n随着我四处磨蹭，一路疑神疑鬼地走走停停，饱食度很快见了底，更糟糕的是我并没有在出发前买足补给，这下不得不品味一番舌尖上的地下城了；实际上这种环节是大多数Roguelike中的保留节目，通过食用某些特殊生物还能获得额外加成与能力，不过眼下最重要的还是确保自己别饿死；很遗憾我不是一只会做饭的矮人，也不会有精灵队友在一旁吐槽食材，所以怪物尸体只能生吃，连调料都没得放。\n这并不意味着Avanor缺少烹饪功能，如果玩家在某个固定层级捡到了烹饪套件，便可以把尸体加工成更优质的食物，具体是指停止腐烂，让尸体的营养翻倍，且重量至多变为原来的十分之一；无法后天习得的烹饪技能不是必须的，但相当重要，它会影响成功概率与消耗时间，玩家没有该技能的情况下，保底成功率只有30%；考虑到烹饪失败不会返还原料，以及购买现成食品的轻松程度，就算提升这项技能也相当吃亏。\n虽然做不出花式菜肴和满汉全席，但至少聊胜于无，估计还有项缺点在于没法修改食物种类，如果东西本身难吃到吐，即使经过烹饪也于事无补；我自己只在机缘巧合的情况下得到过一次烹饪套件，可惜那个存档后来损坏了，而我读到相关源码的时间远比我首次完成游戏要晚，所以当时不知道刷新位置的我就这么错过了它，没能将全世界有头有脸的npc做成料理装进背包通关可太遗憾了（不是\n\n\n第一种被我品尝的是老鼠尸体，尽管描述为tasty但明显非常难吃，角色哇的一声就连带着前几顿全吐了出来；接着我尝试了灰色软泥的尸体，能吃，可不提供抗性就算了，关键是吃下去也有毒；本来只是饥饿状态，现在不仅因为呕吐而濒临饿死，还因为中毒导致生命值也在蹭蹭往下掉，附近似乎仍有具僵尸的遗骸，但那显然不是活人该碰的食物。\n天无绝人之路，背包中还有部分战利品可供食用，几条老鼠尾巴，几片蝙蝠翅膀，以及成堆的骨头；事到如今也管不了太多，只能闭着眼睛都往嘴里塞，至于结果嘛，我成功的活了下来，虽然每件物品都回复不了多少饱食度，也谈不上多好吃，但架不住骨头实在是量大……不管饱，勉强能让角色别饿死；返程路上还碰到了能增加元素抗性的甲虫，无需多言，三两下就进了我肚子。\n靠着这些难以言喻的食物，我挣扎着爬回城里并填饱了肚子，由衷希望店主没有被我饥不择食的模样吓到；不得不承认，在浅层地城里因食物而走投无路时，抓几根骨头啃居然是最为保险的方案。\n\n\n\n清理完一定数量的怪物后，我带回了不少武器装备，甚至有一把秘银制成的刺剑，可惜并不如我手中的斧头好用，我也没有相关武器的熟练度；Avanor并不像大多数Roguelike那样有着BUC状态，同时无需对武器及盔甲进行鉴定，也就是说玩家不必担心装备上一件未知盔甲后，发现它不仅属性烂得要命还因为诅咒而无法脱下。\n卷轴与饰品却并不遵循这一规则，使用或鉴定前无法得知其效果，我面临的正是这样的问题，虽然没有会勒死角色的护符，但当时知之甚少的我并不敢贸然戴上未知饰品，生怕捅出什么大娄子；另外与ADOM一样，商店里出售的物品都相当于被鉴定过，玩家浏览后即使在其他地方遇见同种物品仍然可以将其认出，这点真的要比前辈Nethack方便太多。\n当我在商店中买下一张鉴定卷轴后，才想起来这名角色是没有读写技能的文盲，即便拥有卷轴也无法使用；手册中提过可以花钱找人学习读写，但并没有说明详细位置，我只好寄希望于在后续的流程中碰到那位贵人，并揣着一背包未鉴定的戒指与护符重返探索之旅；介于店主只会收购与其出售物品类型相同的物品，无法卖掉又较为沉重的盔甲等杂物则被我留在了一间空置的小屋中，毕竟Avanor也有负重过高会降低速度的设定，npc不会打开关上的房门，掉落物品也不会自然消失，所以能安全地往屋里存放物资。\n\n尽管猜到了装备没有诅咒与祝福状态，但获得水瓶后我还是忍不住想尝试制作圣水；在王城传送门往北走几步便能看到一座大理石教堂，里面有几排教堂长椅和一块同样是大理石砌成的祭坛，阿瓦诺的大祭司则位于旁边祈祷，与她对话只能得到“祝福着你”的系统消息，并没有什么任务或交互选项。\n基于Nethack式的制作方法，我先将水瓶放在了祭坛上，它们并没有表现出任何异常，我随即站在相同的位置使用了祈祷功能，一道耀眼的白光一闪而过，成功了？不对……我水呢？变不出圣水在意料之中，没想到的是水瓶直接成为祭品被献祭掉了。\n\n既然说到了教堂与祈祷，那就不得不提Avanor中的两位神明，Marduk（马尔杜克）和Tiamat（提亚玛特），都源自于美索不达米亚神话，隔壁Nethack里要夺回的Yendor护符在设定上即是马尔杜克的宝物，当然，护符的传说来自后人虚构；游戏作者可能受到DND等流行文化影响较重，将祂们简单地设定为了死亡与生命之神，如果玩家在游戏中杀死有生命的活物便会引起生命神的反感，消灭骷髅亡灵等不死族则能够增加祂的好感，死亡神设定上大致相同，只不过喜好完全相反。\n通过献祭高价值物品的方式也可以增进与神明的关系，并且不会惹恼另外那一位，对神的信仰越虔诚，可以向神请求的帮助也就越多，小到治疗轻伤，大到直接对玩家的敌人降下神罚；由于信仰上限远比魔法上限要高，走神官系职业的角色不仅能对自己进行源源不断的治疗，还能够向敌人释放出狂风骤雨般的远程打击，单论持久能力远比纯粹的魔法师要高得多。\n听起来似乎很强，但上面提到的一切都建立在拥有足够的信仰之上，毕竟帮助不是没有代价的，可信仰这东西并不像魔力那样会自动回复，必须得让神满意才行；反映到游戏中就是这不能杀那不能杀，捡到点值钱的玩意还得先考虑献给神明，更痛苦的是就算玩家想要与怪物和平相处，也改变不了它们仍然会主动攻击玩家的事实，体验下来估计相当折磨。\n\n还有个问题，游戏内的信仰系统并不怎么完善，能祈求的神术种类少得可怜，所产生的攻击与辅助效果经常是复用普通法术充数；事实上这部分也确实只是个半成品，根据作者在定义中写下的注释来看，Avanor本来应当还有“地”“水”“风”“火”四方神明，然而随着游戏的停更，这一切也化作了泡影。\n直到通关我也没有想成为神职人员的意愿，绝大多数时间跟两位神明的关系都非常差，幸好祂们胸襟宽广，不会拿闪电劈我，也不会派遣神使把我彻底抹去。\n\n重新上路探索，我从王城离开后，在森林中一路向西穿行，本来只是想看看附近还有没有类似的地城，结果走了一阵子之后迎面撞上群强盗，虽然我的实力和装备都有所进步，但仍然寡不敌众被乱刀砍死；临终前我意识到了一件让我极度震惊的事情，这个世界，竟然是无缝连接的，而刚遇见的强盗也正是村口那伙人。\n解释一下为何我会如此震惊，像是同样拥有完整世界的ADOM及其后继者Elona，它们都有两套地图系统，一张是用于快速旅行但是操作有限的大地图，另一片是更加细致，可供正常游玩的区域；两种地图的核心差异在于比例尺不同，大地图上移动一格相当于游玩区域走上数百格，但后者有着空间限制，即使两处地点在大地图上相邻，玩家也无法在游玩区域直接行走过去，必须经历走到区域边缘、选择离开、在大地图上移动、进入新区域这几个步骤。\n而Avanor并没有大地图，玩家能在世界中无缝访问各个地点，包括在同类型游戏中很难见到的地下城表层建筑，即便时至如今，拥有这一特性的Roguelike仍然屈指可数；新手村的那扇传送门真的相当误导玩家，让人下意识地认为游戏切换了地图场景，最终发现真相的震撼，与近几年发售一部RogueLite作品《Noita》非常相似。\n无缝的开放世界也为玩法带来了更多的可能性，比如能将追逐玩家的怪物引到城内交给守卫解决，又或者是在大片的林地中边跑边攻击，把敌人当作风筝拉扯；利用不会被刷新的机制还可以将多余的战利品藏匿在野外，不必像其他游戏一样担心进出随机区域后导致里面的物品消失。\n\n读档后角色重新复活在了王城，还没从刚才的震撼中缓过神来的我决定先把自身的实力提上去，不然连探索世界的资格都没有，转头便带足补给到邪法师的地城里接着练级。\n向下深入的路途上基本没遇见什么麻烦，但在第5层走下楼梯后，周遭的环境突然一改画风，出现了极为广阔的空间；角色背后是面石墙，在昏暗的灯光下仿佛无边无际，眼前则是由黑曜石铺成的道路，似乎在引导着来访者前进，顺路走上几步便能听到潺潺的流……好吧，Avanor并没任何声音，音效全靠玩家脑补；忽略掉流水声，在前方静待玩家的，是条狂澜暗藏的地下河流，绵延的黑曜石凌空跨于两岸，通向一座规模庞大的地下宫殿。\n走过黑曜石构成的桥梁，推开地宫大门，一间地板同样是黑曜石铺成的前厅出现在了视线中；厅中没有任何陈设，空旷得让人感到不安，我在门口反复确认了许久，断定不可能有陷阱后才缓缓向里走去，打开第二扇门，藏在后面的是条狭窄且一眼望不到头的走廊，与外面的空旷形成了强烈的反差；我小心翼翼地继续前行，走到尽头隐约能看见大厅时才发觉到，有两名全副武装的黑骑士正守在出口，就如同等待猎物走进圈套一般。\n\n它们一手拿长剑，一手持塔盾，浑身上下都被漆黑的重甲所包裹着，这正是邪法师的精锐部队，没有感情只崇拜死亡的杀戮机器——死亡骑士。\n我顿时如临大敌，摆好架势做足了与它们决一死战的准备，回合制的优点在此体现的淋漓尽致，玩家即便身处险境也仍然拥有充足的时间来思考；首先它俩处于我的左右前方，如果我率先发起的攻击不能一击必杀的话，将遭到敌人的双重反击，从装备来看这当然不现实，必须另寻他法。\n根据先前与古代恶魔交手的经验，可以推测Avanor中的高阶敌人不一定会具有压倒性的速度优势，而面前这两位显然是注重防御的重装战士，自然就更不可能拥有远超于我的速度；那么后退就是此时最佳的决策方案，狭窄的过道只能容下一人并行，势必只有一名敌人能够紧跟着追上来，并刚好阻挡在我与另外那名敌人之间。\n无论如何，这至少能让局面从二对一变得更加有利，要是死亡骑士的速度与我接近，那我甚至可以撤退到楼梯处再逃之夭夭；虽然某些敌人似乎会追着玩家上下楼，但现在不想坐以待毙也只有放手一搏，确认完战术模式是全力防御后，我便操纵角色猛地向后一撤，是生是死就看这一步险棋……吗？\n奇怪的是，身后并没有任何东西跟上来，我愣了一下，随即小心翼翼地又等待了数回合，才确认了根本就没有敌人在追，毕竟它们速度再慢也不可能连玩家的几十分之一都不到；我壮着胆子再凑了上去，两名死亡骑士仍然一动不动，静静地矗立在走廊尽头的两旁，好像刚才不过是只苍蝇飞来罢了；结合它们的行动与之前疏漏的事实，我心中大致有了种猜想，便试探性的朝着一名骑士的位置移动，果然，系统弹出了“您是否真的要攻击……”的提示，这死亡骑士竟然是彻头彻尾的中立单位。\n\n也对，如果它们与角色敌对，那我走到尽头后再轮到其他生物行动的一瞬间，我便会身首异处，压根就没有继续思考的机会；捡回条命后总算是能把悬着的心放下来了，地宫的主人似乎并不打算加害于我，这番“盛情难却”之下也不免让人好奇后面究竟还有什么，眼前即使是龙潭虎穴也得闯上一闯。\n大厅中央是座黑曜石祭坛，跟周遭环境几乎融为一体，其供奉对象不用想都能猜到是死亡神马尔杜克；后边不远处有六名死亡骑士整齐的列于过道两侧，装备类型与之前那俩位别无二致，它们簇拥着游戏的反派头目，邪法师Ahk-Ulan；遇见大魔王也倒是意料之中，毕竟任务目标就是除掉这家伙，但整整八名精锐战士配上一名深不可测的施法者，我这样羽毛未丰的冒险者只有脑子不开窍才会动手。\n敌我实力悬殊，可试着交谈过后，这位大魔王居然也有求于我，他声称一帮法师摧毁了他的塔楼，他现在正为了复仇而积攒力量，需要玩家为他取得三块古代机器的部件，并答应事成之后给予丰厚的报酬。\n怀着复杂又矛盾的心情我原路返回了城中，苦思冥想也没搞懂正邪两方咋都喜欢给我这样一位无名小卒派任务，还件件都涉及到世界的危急存亡般重大；不过目前想太多也没用，邪法师我现在弄不死，古代部件找起来也没有一点头绪，到头来貌似依旧停留在原点。\n\n转念一想，邪法师完全不提部件位置，这说明任务设计者在潜意识里就笃定玩家能轻易将其找到，或者后续会有内容引导玩家；另外既然是邪法师委托我，那就表示部件所处位置对于他而言难以进入，而对我来说能够轻易抵达；结合这两种因素，嫌疑就重点指向了戒备森严的皇宫，虽然村子西面那座蘑菇洞也很可疑，但邪法师一人估计就能把那村里村外屠干净，不太可能将我当做战力派过去。\n所以眼下的首要目标是摸清楚王城，在教堂与永恒之火的东边就是皇宫，相较于邪法师朴素的藏匿处，这里明显更加奢华且富有生活气息；花园，餐厅，起居室等日常设施一应俱全，还能见到不少皇家守卫在各处站岗，但他们并没有对我在皇宫里到处晃悠提出任何异议，说不定是因为里面确实没啥好偷的。\n唯一让我在意的是皇宫花园的西南角附近，那里有处被守卫严密把守的下行楼梯，与他们交谈没法得到任何反馈，想下去的话战斗似乎不可避免，只好暂且作罢。\n\n到王座厅找国王对话可以领取到清理墓穴的任务，虽然这位君主身边同样有不少守卫护驾，不过这种奇幻世界里能位居一国之君，其自身实力必然不可小觑，本人很可能是前代勇者之类的人物，据我后来的交手经历，即便城内所有卫兵加上大魔法师和祭司也不会是国王的一合之敌，守卫更多是作为权力象征顺便起装饰作用；邪法师就算带齐部下估计也很难从国王手底全身而退，这可能解释了为什么他不敢主动出击而需要积攒力量。\n令人费解之处在于国王为什么不亲自去墓穴把入侵的不死族处理干净，这对他来说应当是举手之劳，或许是懒得动以及不想跟死亡神搞坏关系吧，另有隐情也说不定。\n\n墓穴位于王城的西南方，离邪法师塔的遗迹很近，地表建筑像是一座石丘；穿过甬道和几扇墓门就进入了大约是前厅的地方，此处现在充满了骷髅，它们察觉到活物存在后便一窝蜂地涌了上来，我起初应付得还比较谨慎，站在甬道里等它们挨个冲到跟前再逐一解决；可这堆没有武器盔甲且不知道腐烂了多久的骷髅实在是构不成什么威胁，攻击迟缓容易避开不说，即使命中了被防具阻挡后也产生不了多少伤害，我后来干脆换成狂战模式边往前冲锋边砍瓜切菜，结果hp都没怎么动弹。\n清理完表层我便顺着楼梯来到了地下，整个墓穴除去表层建筑后只有一层，里面看似空无一人但实际上却充满了各种幽灵系怪物，正常情况下玩家无法用肉眼看见它们，必须佩戴足够强力的see invisible戒指或护符才行；然而正如前文提到的，我这名角色是文盲，鉴定不出饰品，这导致隐形怪物对我来说相当棘手，只能将它们引到狭窄的墓道里，同时不断向空气挥斧以便“凑巧”击中这些鬼魂。\n然而事情并不总会按照预期那般一番风顺，地下的清理过程堪称胆颤心惊，各种鬼魂不仅行动诡谲，伤害还奇高无比；有时候拉扯不到位，砍了半天空气发现没鬼过来，有时候又不小心走得太深，直接被数只怨魂围殴，要是运气不好碰上几只特别厉害的惧魔（Dread），生命值三两下就能见底。\n\n光是看不见敌人倒也还能接受，更大的问题在于出现了极为严重的bug，本来只是十几级再升一级，可游戏在升级完成后依然反复弹出升级界面导致角色的等级一路飞窜至了近百级；刚开始我以为是技能加满后剩余的改进点数无法使用而造成的，毕竟这游戏前中期的等级提升速度高于技能上限的增加速度，但后续流程中该bug不定期触发，以及在新建的低技能角色上同样出现的情况否决了我这一猜想。\n结合触发bug的大致情形，似乎是输入过快以及移动键的问题，因为角色仅仅连升几十级就停了下来，差值范围远比类型错误以及死循环等情况要小；游戏会在升级过程中读取玩家输入用于技能加点，而升级前我正巧在不断按下方向键以砍杀怪物，Avanor推荐使用数字键进行全向移动并且不支持键位映射，但这台笔记本没有NumPad，所以大部分时间我都用方向键进行移动，那么很可能是我的输入队列卡到缓冲区影响了调用或表达式，让升级函数多执行了几十次；考虑到我和游戏之间隔着一层虚拟机和几套不同的字符编码，这一切也不是说不通。\n后来翻阅源码时倒没找到符合我这一猜测的相关片段，反而发现程序有块部分一直在监测等级最高的生物，仿佛有种惊天大阴谋的感觉。\n\n\n嘛，就结果来看，我轻而易举地超越了等级的设计上限，这项数值大于50级再查看经验界面就只会显示一片漆黑；虽然技能全满后升级仅能依照属性增加些许生命与魔法的上限，但这样的bug还是极为影响游戏体验，我后续游玩中因为此漏洞在内的各种运行错误而重开过不下几十趟，不过为了叙述方便，本文还是尽量将其串成一场完整的旅程。\n杜绝类似的问题不太可能，最好还是经常保存（尤其在升级前），并对记录文件进行定期备份，尽管类似行为就Roguelike这种常是永久死亡的游戏而言较为“可耻”，但现在为了减少程序问题产生的损失也顾不上那么多了。\n\n幽灵系怪物无比难缠，好在角色可以随时退回地表恢复状态，毕竟任务没有限时，不断消耗之下还是艰难地完成了清理工作，可扫兴的是这些鬼魂没有形体，自然也不会掉落任何战利品；大失所望的我将目光投向墓中陈设，整座墓穴当中没有任何陪葬品能够拾取，甚至连装饰物都没有，除了建筑结构之外就只剩四丘坟墓，墓碑上刻有死者的姓名与生卒年份。\n意外发生于我脑子一抽后，鬼使神差般地对一间耳室内的坟墓按下了使用键，结果哐当一下掉出把长剑，这把剑在黑暗中闪烁着微光，一看就不是凡品；捡起剑后我整个人都懵了，因为这把长剑实在是让人难以置信的强悍，手中战斧骰满得到的最大总和，还不如这剑的伤害加成，再一对比命中修正，我的斧头仿佛成了根极为笨重的棒槌，另外长剑的属性增幅还能让我的力量和体质翻上近一倍，这些数值就算到后期都不太可能有其他武器超过。\n此剑名为阿瓦诺之防卫者，乃是该游戏中不可多得的unique item，这些独特物品大多都性能优异，固定生成且仅有一件；我拿到它之后脑子里没有开心或激动，想的净是得赶紧去把其他几座坟给掘了，说不定有整套盔甲和盾牌呢，结果是除了几根骨头之外一无所获。\n\n说实话，我一周目里清完怪物就走了，并没有拿到这把神兵利器，后来重开的时候虽然在读源码，但也没有翻到与之相关的部分，能从四座坟墓中一次就选对这座靠后的，纯属机缘巧合。\n\n当作是同一场旅程吧，我兴冲冲的跑回皇宫交任务，没想到国王居然一眼认出了角色手上的长剑，并以打扰先祖安息为由，直接痛下杀手；作者整这一出也不知道该夸人心细还是说人恶毒，无奈，只好再次读档。\n将武器卸下藏回包里才能正常领取报酬，国王会感谢玩家并给予1000金币，再次对话还可以接到一项后续任务，他说几年前有名非常受信任的仆人偷走了一件强大的神器，Eye of Raa，那名仆人后来企图将其藏到南方的一处地洞里，但人们声称他在藏匿途中就被杀害了；国王希望玩家能够找到这件神器并带回给他，不过一件神器丢了几年才派人找，究竟是心眼大还是暗藏玄机，莫非这件神器就是邪法师要找的部件？当时的我不得而知。\n\n路过塔楼遗迹，再向南穿过一片密林便可来到地穴上方，如果再往前走一段的话可以看到环形山脉的南部；地穴没有什么显眼的地标，看到下行入口直接走过去就行，然而，即将迎来的是游戏中最为折磨的部分。\n地穴主要有两种类型，一种是房间式的地下城，另一种是不规则状的洞窟，但里面的怪物类型倒没有什么差异，主要威胁来自于妖精（goblin），他们的装备良莠不齐，有全身上下都被精金秘银所包裹的，也有装备栏位还空着的；危险之处在于这些家伙拥有智慧，不仅会从地上拾取武器盔甲并替换穿戴物品，还会主动喝下隐形等药剂为己方在战斗中增加优势，甚至能向玩家一样对神明祈祷和献祭。\n\n\n怪物的ai和背包系统并不是问题的元凶，这种合理的强度反而能为游戏增添趣味；真正值得诟病的是随机地城的设计，我初见时幸苦杀穿整座洞窟后并没有找到什么所谓的神器，对着截图回忆了一番任务文本才猛然领悟到了“one of the caves”和“was killed while hiding it”的含义；一座地穴约有十层，而这样的地穴有整整三座，那名仆人可能死在其中的任何一层，并且怪物会拾取装备，这使得玩家必须地毯式清空每个层级的每个角落，一只怪物都不能放过。\n从todo list不难看出，搜索和暗门属于未实现的内容，然而玩家却可以在地城中遇到完全密闭的房间，直接被困死；算上初见清了两趟，地图生成错误又再找了几趟，通关又是一趟，累计下来我估摸着扫荡了百余层地牢，这个战绩放到隔壁Moria（以流程拖沓而著名）都够通关了；我也是从这一阶段开始被迫审查源码以继续游戏。\n\n无独有偶，Avanor也有gelatinous cube（凝胶方块）这种怪物，且在随机地城中大量分布，它不像在其他游戏的表亲那样人畜无害，而是能麻痹玩家后继续攻击并叠加麻痹效果，直到死亡，在生命值因为升级bug而来到四位数时，它仍然可以一套连招让角色直接去世，丝毫没有还手的机会。\n处于麻痹状态时，玩家无法进行任何操作，甚至无法键入退出游戏的指令，想主动读档都做不到，只能拍空格以继续显示战斗消息，主打一个无助，过高的hp又使得该过程变得更加漫长，我后来都养成了在被麻痹时干脆重启整个虚拟机的习惯。\n\n应对方法也有，那便是佩戴free action类的饰品，先不考虑当前角色是名文盲，无法识别出道具功效，即使正确佩戴了相关饰品的情况下玩家依然可能会被麻痹，因为饰品提供的抗性强度是一个随机值，而该数值不会直接展示，想判断强弱只能通过在店里试图出售的方式查看价格，价高者的效果自然更好。\n但Avanor并没有类似于Nethack那样能对装备起别名的功能，如果玩家移动高属性饰品时没有清空同类物品，那么它们会直接混到一起，得手动再次鉴别；我后期嫌麻烦，把两个戒指位和一个护符位全装上了free action，以量取胜且干脆不换了，毕竟其他攻击的致死率远远无法相提并论。\n\n头几回我都无功而返，不是杀到最底下啥也没找着，就是走一半到了死胡同，所以便打起了其他地方的主意；从已知的情报来看，被山脉围住的区域里西北角是村庄，东北角是王城，东南角则是森林地穴，那么西南方向有什么呢？于是我转头一路向西，途中除了密林之外别无他物，相当单调，最终在横跨了大半个区域后遇上了一群orc（半兽人），他们看到孤单的旅行者路过自然是不分青红皂白的攻了过来。\n开放世界的好处体现出来了，尽管这群半兽人在前中期都相当致命，但只需稍微往北跑上一段路就会见到一座军营，而这里的士兵与半兽人有仇恨关系，玩家可以利用他们御敌或者脱身，运气好还能渔翁得利，趁乱捡些死者的遗物。\n这里有家武器店，允许出售和购买武器，如果与守卫队长交谈将得知他一直在担心半兽人们会随时攻打过来，玩家消灭掉所有敌人则可以得到一柄独特的匕首，名为Death Hack，除了基础属性不错之外，对恶魔还有整整三倍的额外伤害加成，难怪新手村长老让玩家到湖对岸寻求帮助。\n拿到它的过程也挺有意思，当时的军营被半兽人们杀了个片甲不留，而我再经过此地时已是一身神兵利器，非常随意的就报了曾经的追杀之仇，找了几圈没有看见队长的遗物，还以为任务失败了武器不生成，没想到临近通关回王城的时候，在护城河外见到了这名踌躇不前的逃兵队长，他交出匕首的那一刻仿佛放下了千斤重担，也不知道是什么支撑着他横穿整片地图，从最西跑到最东边，希望他之后能以普通人的身份，回到城内与家人团圆吧；纵使我能直面源码，知道这只是npc随机移动的结果，但程序生成的情节加上脑补，依然是这类游戏最为迷人的特点之一。\n\n\n\n营地上方的一排房屋中，最左边那间有条下去的路，通向一层似乎是牢房的地方，在源码中被标记为“老鼠地窖：1”，虽然有个数字但并不存在第二层或者其他的地窖（与墓穴相仿），也许是本打算在未来进行拓展的内容；里面确实有非常多的老鼠，还固定生成两只鬼魂，错综复杂的小道后面能找到一箱武器盔甲，旁边还有口箱子则装了一大堆魔法书和卷轴，唉，只可惜吾乃文盲，目不识丁。\n四角的大房间内还固定能找到一件森林兄弟……或者说绿林好汉的披风，很可能来自被处极刑后遭到老鼠啃食的受害者，它的防护能力约等于没有，我头回见到的时候还差点当垃圾给扔了，但它却是件定义在unique item当中的独特物品，关于它的用途与故事，则暂时先留个悬念。\n\n离开军营往南，略过那群半兽人之后将看到一座残丘，上面本该有个入口能直通半兽人洞窟，但它只存在于作者的todo list，并未实装至游戏；\n弄错了，后来读源码发现那个入口通往独眼巨人巢穴，翻山进去击杀boss可以得到全游戏最重的武器：黑曜石大棒，质量为8000游戏内单位。\n往西是环形山脉的一部分，无法逾越，但如果顺着山脉的走势往北前进，小心翼翼的穿过湖泊与山体夹缝之间的窄路，就又回到了新手村，没错，虽然看似不可通行，但湖泊旁边确实有条小路可供行走，并且这条路靠山的一侧还有处通往矮人王国的密道。\n\n\nL:PDC3于源码中的完整描述是&quot;Path to the Dwarven City Level 3&quot;，需要穿过6层这样的地方以抵达矮人王国，并完成那里的任务以获得pickaxe（十字镐）用于挖掘墙壁；但眼下在接任务的半路上遇到了完全闭塞的房间，陷入了拿不到镐子无法前进，无法前进就拿不到镐子的死循环。\n不死心的我拾取了半天空气也只搜出了一个无关紧要的陷阱，周围的墙壁依然纹丝不动，这种地图生成问题曾在之前的随机地城出现过，获取镐子的主要原因之一便是需要挖开那边的洞穴；现在的情况约等于宣告死档，只能重开，缘由可能是作者打算添加隐藏门，改完生成机制后忘记相关功能还没做就直接发布了游戏。\n除了十字镐这件工具之外，还有一项挖掘技能，该技能无法后天习得，只可通过开局创建角色时选择矮人或地精时自带，我一度以为缺少技能便无法使用相关工具，还庆幸自己刚开始就选对了种族，不过后来阅读源码才了解到，技能仅会加快挖掘速度，并不影响能否进行挖掘。\n\n\n\n从叙事角度考虑，还是当作一切正常继续把故事讲下去，路上都是些常见的怪物与陷阱，随着深度而逐渐变得更加危险，快到目的地时还能捡到烹饪套件，没有看见的话多半是被妖精什么的捡走了，需要满地图追杀。\n矮人之城相较于人类的王城显得十分朴素，大体上都是灰白色的岩石基调，规模也小不少，路过居民区和中央广场的生命祭坛就抵达了矮人王的居所；另外城里有位铁匠特别容易错过，通常他会呆在商店旁边的屋子里，但他也可能和广场上的人群混到一起，我通关后翻阅npc的对话文本才发现有这么号人，花钱可以让他为武器添加元素伤害，以及对半兽人的额外攻击倍率。\n\n与矮人王对话将接到收复矿坑的任务，不过这家伙的脾气非常暴躁，没完成任务再与其对话时他会让玩家滚出去；矿坑里倒没有半兽人和炎魔，全是从岩石中渗透出来的神秘毒气，玩家需要前往最底层启动一台气泵把毒气抽干。\n设计比较蠢的地方在于它纯粹是个“数值检测器”，任务未被标记完成前每走一步都有概率会强制扣上2到7hp，不存在任何豁免，尽管游戏中有着毒素抗性和魔法戒指等各类道具，玩家能做的却只有走上一段再喝口治疗药剂让生命值别见底，要是恢复能力足够高甚至连喝药都免了；毒气矿坑因为可以稳定掉血且没有怪物，刚好还非常适合刷恢复技能的等级上限，代价只是几份食物和一点微不足道的时间，就可以将上限刷到最高值，突出一个无脑粗暴。\n\n完成任务获得十字镐之后也没什么留在矮人之城的必要了，本来打算立即动身，但矮人王右手边紧锁的门扉勾起了我的好奇，门旁一左一右两名精壮的矮人战士更凸显了它的不凡；靠近后玩家会立即遭到他们驱逐，可我现在不仅神剑在手，数值也经历了相当的积累，警告只当作耳旁风，毕竟此处不光没有了利用价值，矮人王的态度还十分恶劣，早就看不顺眼了。\n随着我一脚将门踹开，两名门卫意识到劝说无果，挥舞着兵器就和其他士兵一起攻了过来，可这些攻击要么被闪开，要么被防具阻挡了绝大部分伤害；我三下五除二解决了他们，顺着前方的阶梯走下，展现在我眼前的，正是矮人王国的宝库，但遍地的黄金与宝箱之间，潜藏着大量的传送及其他陷阱，不幸触发便会被转移到没有出路的幽暗洞穴中，万一没有挖掘工具，将会活活困死在洞里，不得不说很有矮人作风。\n抛开那些司空见惯的财物装备，宝库中最为让人惊喜的是一块古代机器部件，正是邪法师苦苦寻找的三件之一，结合门外的状况，几乎可以断定皇宫花园那处被严加看守的地方就是王城宝库了，里面多半也有古代部件。\n\n简单的置换一下身上的盔甲，又挑了点暂时还看不懂的魔法书带走，这座宝库便搜刮完毕，返回刚才的事故现场，门卫尸骸仍倒在血泊之中，虽然逻辑上是进门时他们对我先动的手，但在我“不正当防卫”后，角色依然与整个矮人王国产生了敌对关系，场外的其余士兵甚至市民都在蜂拥而来；令人啼笑皆非的是矮人王非但没有率先发难，反倒逃去了墙角，可惜游戏没有更详尽的姿态描述，是否在角落瑟瑟发抖我们无从得知，姑且当作他是担心腹背受敌吧，哈哈。\n战斗过程没什么可说的，几乎一面倒的屠杀，唯一一点小插曲大概是有名士兵喝下隐形药剂，让我不得不与空气搏斗一番；矮人王虽然躲挺远，但其战斗能力依然大于所有卫兵之和，交战过后我的hp定格在了164/267，损失的近一半生命值中，估计90%都是由他造成；npc的战斗ai要是懂得合围，重伤后撤，利用远程击破走廊战术，那鹿死谁手还不一定，眼下这帮乌合之众可改变不了我完胜的局面。\n不出所料，堂堂矮人王者果然有不少独特装备，最为引人注目的是一柄重1200单位的巨斧，该重量相当于10件钢铁鳞甲，其威力自然不言而喻，除此之外还有一顶王冠与一面Torin之盾，它们穿戴后都能够增加属性，尤其是体质方面。\n\n\n\n遗憾在于我最不缺的便是体质属性，因为初始种族选择了矮人，不但体质基础值高，还可以安全地通过食用怪物尸体继续增加，更重要的是在我随机地城的战利品中有一双“至高王之靴”，它能提供整整25点体质，此数据比穿戴矮人王全套装备的加值还高，可惜这双靴子仅是带有高级词条的随机物品，而非独特的unique item。\n\n\n离开了矮人王国，准备利用传送门快速回到王城，虽然西侧小路可以直接抵达村庄，可湖泊东面的那片未知着实引人好奇，于是我向东从军营与湖泊之间穿过，打算绕湖一周从村子正门回去。\n没走出去多远，一座模样怪异的建筑出现在了面前，似乎有几分眼熟，凑近之后猛地一拍脑门想起来了，这不就是邪法师塔楼的完好无损版吗；顺楼梯上到塔顶，见到位名字非常日系的精灵巫师叫Yohjishiro，与她对话可以花500金币学习识字，没错，我几乎把整个世界绕了一圈才终于在回到原点的时候摆脱了文盲身份，而此时背包里的金币已是数以万计。\n很大程度上要怪角色视野不足，巫师塔就位于军营东北方向不远处，甚至开局都可以非常安全地从小路绕过来，但我却三番五次与它失之交臂，并因为不识字而在冒险之旅上吃了许多暗亏；除了能教识字以外，这位巫师还可以为玩家鉴定携带物品，而代价只是一条老鼠尾巴或者一片蝙蝠翅膀，还记得我曾在初出茅庐没有食物时而吃过它们吗？这相当于我在把珍贵的全背包鉴定卷轴当饭吃。\n\n\n\n那两样东西倒不是说有多难得，顾名思义只要杀老鼠和蝙蝠就有可能掉落，而且概率高达整整四十分之一（捧读），问题是这两类生物几乎不会在中高等级的地城中出现，如果专门为了刷它们而前往没什么经验的低级地城又十分吃亏，显然是种仅对新人友好的机制，使得我如鲠在喉般难受。\n与之类似，军营中还有条叫Gekta的大狗，给予骨头后它会从地里刨出一些东西作为回报，可由于军营被半兽人血洗，我直到通关后翻阅源码时才留意到这一隐藏功能。\n\n\n识字后终于可以让积压的卷轴与魔法书派上用场了，虽然看书学了很多乱七八糟的的法术，不过真正常用的却只有鉴定术与无属性的魔法箭，前者用于探索时辨别高价值战利品，后者适合在绝大多数情况下作为远程辅助手段。\n绕着湖泊来到村庄正门，作为新人杀手的强盗们依然是二话不说就杀了过来，可对如今的角色来说犹如以卵击石；将他们当靶子试验完新法术后，在其掉落物里居然发现了绿林好汉的披风，与我之前在军营地下室捡到的unique item一模一样。\n结合他们在村口却不攻击村庄的情况，这些所谓的“强盗”应该是劫富济贫的绿林好汉才对，那军营地下室里的鬼魂是他们的同胞吗？为何只留下了一件披风，有人成功逃脱了吗？两口宝箱是原有的储备物资还是哪次行动被缴获的赃物？当地正规军与他们还发生过哪些摩擦？悬念太多让人一头雾水，但完全有理由怀疑这是什么依托于物品的碎片化叙事。\n看着眼前的披风，我突然冒出了个大胆的猜想，随即将角色的服饰卸下，换上捡来的绿林披风，并试着朝着角落残余的“强盗”走去，结果如图设想一般，他们居然真的没有再主动发起攻击，伪装成功了；虽然一周目我不小心杀了他们不少弟兄，但后来我专门戴上隐形戒指，不伤一人的情况下将他们同伴的遗物送还了回去，意想不到的是他们竟然对我放下的披风进行祈祷，然后把它献祭给了死亡之神马尔杜克；虽然我知道这是npc处理多余物品的常规行为，但我更倾向将其认作是送还失物，悼念亡者的一种仪式。\n\n\n山洞里的恶魔老朋友也当然不能忘记，经过数座地城洗礼，现在的角色早已今非昔比，不费吹灰之力便可手刃仇敌，考虑到手上盗墓弄来的先王剑对它也有额外伤害，最终斩杀恶魔只用了两刀。\n这头邪恶生物的尸体似乎化为了飞灰，掉落物仅有一枚戒指，名为“伟大元素之戒”，游戏内没有途径显示物品描述，但从源码中可以了解到它提供几种元素抗性与看破隐形，具体数值以毕业装的角度还算过得去，不过为了预防麻痹至死，大多数时候我并不戴它。\n\n恶魔所在的一层有两处向下的道路，一处通往钴鬼（kobold）洞穴，另一处通往蘑菇山洞，先不提我没有草药学技能，摘了蘑菇也没法加以利用，光是站在这鬼地方就有概率永久降低角色的属性，也不知道恶魔和那些村民怎么会看上这种凶险之处。\n至于隔壁自然是有成群的钴鬼，细分下来还存在萨满和弓箭手等远程杂兵，对初期玩家应该算不小的威胁，然而它们的掉落物基本上都是些破烂，源码里也表明此处仅会生成一只没有特殊装备的头目，但玩家都有能力击杀恶魔了，实在是想不出有什么来这探索的理由，或许让下去的玩家倒霉就是它唯一的意义。\n\n村里的待办事项全部解决，向长老报告完成情况，好，如我所料没有任何奖励，毕竟恶魔已经“给”过戒指了，然后……然后游戏就又崩溃了。\n不过这次崩溃与以往的任何一次都不同，它非常稳定的出现在与长老交谈的瞬间，如果不交任务那么大家相安无事，但一旦对话游戏程序便会炸成一团灿烂的乱码，找不着解决办法，报错信息也看不出个所以然，倒是视觉冲击力相当震撼；翻了翻源码，发现既没奖励也没后续任务，那少听一句感谢也损失不大，其他任务别出问题就行，可墨菲定律告诉我们，越担心什么它就越有可能发生，等着瞧吧。\n\n既然已经提到了程序崩溃，就顺便再说说存档损坏的情况，问题出在保存进度是通过直接覆写存档文件完成的，没有创建副本；本来无伤大雅，但这游戏有概率在储存时崩溃，而文件覆写中断会导致格式失效并无法再被读取，意味着永远丢失进度只能从头再来。\n储存时坏档的概率大致上取决于数据复杂程度，刚开局那会儿一般遇不上，但到了后期，即使在森林里跑一跑都有可能乱码，这时候存个档堪称心惊胆战，还好我曾因为升级bug而备份了巨量存档，坏档问题虽然也碰上了不少次，幸亏都没造成太大损失。\n长话短说，重返王城沿着熟悉的路线再次下到随机地城底部，依次挖开封堵的岩壁以寻找道路，功夫不负有心人，三座随机地城里有两座都存在隐藏洞穴，一路往下地毯式搜刮，最后我成功于一只惧魔附近找到了Eye of Raa这件神器。\n\n历经艰难险阻，我自然希望这件神器有个惊天动地泣鬼神的外观，最好在拿起来的时候再出现一道直冲天际的金色闪光，遗憾的是在这种字符世界里它的长相就是个‘*’，甚至连物品描述都没得看；从源码中得知它能提高精神相关的属性，还能施放一种不耗mp的远程攻击，但最大的优点是它属于工具类别，不用跟其他饰品抢装备栏位。\n与随机地城永别，到皇宫提交任务，时刻记得在这里要将盗墓弄来的先王剑藏好，不然一对话国王会翻脸；给出神器之后国王戴上它，深刻地感谢了玩家两句话，接着呢？接着就没下文了，不光把东西拿走连奖励也不给，感觉就差一句“最珍贵的回报正是你一路上的成长”。\n发送消息，标记任务完成，给出物品，结束操作；四行代码让我起了杀心，之前清理墓穴好歹给一千让人尝个甜头，这次忙活这么久居然一毛不拔，那就不要怪我心狠手辣了；拔剑斩击一气呵成，但是手感有些不对，臆想中血溅五步的场景也并未出现，查看战斗记录，国王居然躲开了突如其来的一剑，还反手几招将我击伤，其速度之快，破甲力道之强，着实令人汗流浃背，雪上加霜的是周围一队皇家守卫也围了上来，虽然几乎无法对我造成有效伤害，但致命之处在于他们堵住了皇宫唯一的逃脱路线，最终害我饮恨当场。\n\n几番读档与国王“切磋”后，我大致掌握了他的信息，首先他是一名纯粹的物理战士，拥有非常高的速度与闪避率，尽管我防护能力也不低，但我一剑的功夫够他反击三四次，实在吃亏，其次是虽然有相当程度的元素抗性，但他并不免疫射弹伤害与恐惧状态，不会使用刚刚交出的神器，也造成不了麻痹等状态。\n综上所述，我的策略转变成了穿戴增加智慧的饰品，然后在极限射程发射无属性的魔法箭进行消耗，必定命中但是伤害较低，肯定不足以直接击杀国王，所以等他冲过来后势必需要近身作战，好在这个世界持剑穿重甲也不影响施法，可以无缝切换近战模式；接下来我在死亡前大概率能将他的hp削减至恐惧范围，他会逃跑，但此时千万不能追，角色状态较差容易被他回身几棍子送走，保持使用魔法压制，运气好可以将国王的hp消耗完，运气差让其脱离恐惧状态则可以在他冲回来之前喝药，这下再面对重伤国王应该能轻松取胜。\n战前也当然要给自己叠好状态，力量药水英雄卷轴什么的有就都用上，多少增加一些容错，另外不太建议在国王未恐惧之前喝那些轻伤治疗药水，按照对方的速度，hp恢复量可能还没一回合吐出去的多；国王身上也会固定生成不少healing药水，最好提前偷走，不然会增加翻车风险。\n\n最终击杀国王时角色hp还剩四分之一不到，再吃两下重击便会暴毙，mp也空了大半，堪称惊险万分，这离谱的强度除了他是前代勇者外，我想不出还能有什么合理的解释，至于那些烦人的皇家守卫，国王一死早就跑到角落里逃命去了，打扫战场时直接挨个手起刀落。\n除去Eye of Raa，掉落物中还有两件unique item，一顶阿瓦诺的王冠与一柄阿瓦诺的权杖，王冠和矮人王那顶几乎是同款，除了外观不同，所提供的加成都完全一致；权杖则没想到是近战武器，骰面比先王剑大概要低一半，但是能额外增加16点力量，可以考虑拿在副手，不过国王拿根棍子都这么强，要是我这剑给他拿去了那强度想都不敢想。\n从花园旁边的神秘入口下去，底下果然是王城宝库，也如同预料般躺着古代部件，而且还是两块，先前在矮人王国“捡”来的那件在去随机地城的时候就顺路给邪法师送去了，既然三件都已浮出水面，那么激动人心的时刻马上就要到了……吗？在我捡起第二块部件时，游戏程序又崩溃了，这次崩溃得也非常稳定，固定在捡起两块部件时发生。\n\n\n起初我以为这是不让玩家一次性将所有部件带上逃走，必须击败国王能再来一趟拿剩下的那件，但转念一想谁会用进程崩溃当作阻止手段，这肯定又是什么恶性bug；当时还乐观的我想着一件件拿也无所谓，就多跑一趟的事，可等我找到邪法师给出部件时，游戏再次崩溃。\n一部分原因得怪所有生物都有背包设定，包括敌人和npc，这表示我给出的第一块部件真真切切的躺在邪法师包里，而并非交完任务便消失，即使我将第二块部件扔到地上尝试让邪法师自己拾取也无济于事，因为任何物品栏只要含有复数块的部件就会导致崩溃，别无例外。\n也不知道这玩意是不是铀235做的，两坨凑一起就发生临界反应；在反复读档以及试验失败后我决定将目光投向广大的互联网，首先是Roguelike的大书库RogueBasin，其中记载了Avanor在dosbox上存在缺乏长文件名支持的问题，也就是无法在游戏内打开帮助手册的元凶，我尝试切换了模拟器，但文件名与崩溃的问题依旧未能解决，仅能调整出过于清晰的字体。\n\n\n根据官网的更新日志与irc上的求助信息，可以推断出游戏的原生平台应该是win32，但即使我连后续移植的linux版都找到了，也没能发现任何windows版本的踪影。\n曾经还有些人在游玩过Avanor后也留下了自己的观点，从Ben Power在大书库中的评价来看，原生环境下程序运行得堪称完美，他甚至请了病假在凌晨玩到了第二天晚上；个人角度更好奇Jeff那篇充满负面言论的评论在哪里，猜测他和我肯定遇到了不少一样的问题。\n\n\n正当我考虑是否要用源码自己编译个破解版出来的时候，我突然想起了曾经看到过的一篇俄语冒险记录，帖主似乎没有遇到相关问题，只是随口提到过一块古代部件也能完成任务，给出去再偷回来就行。\n再看到这句话我顿时犹如醍醐灌顶，简单阐明一下工作原理，虽然所有生物都拥有背包设定，可任务不会检测npc持有的物品数量以判断完成情况，而是在玩家执行“给予”这一操作时更新状态；换个说法，该任务的完成条件其实可以看作是将部件交给邪法师三次。\n即使物品存在bug无法堆叠，但这一方法自始至终都不会出现两块古代部件，依然是由于背包空间，只需要利用偷窃将部件盗回，对方物品栏中的对应物品也就真的不翼而飞，如此重复至计数器归零，任务即可完成。\n一切由bug引起又以bug终结，理解到背后的运作逻辑时我不由得拍案叫绝，利用漏洞欺骗游戏系统以绕过故障，酷毙了。\n\n不出意外的话又该出意外了，查完源码发现能教玩家偷窃技能的大师在村口那群绿林好汉里……已经不小心横尸村头了，只得被迫回档，本文姑且当作无事发生吧。\n随着一次次给出刚偷回来的物品，任务进度变成了3/3，与邪法师交谈他会称赞玩家并提出最后一个请求，杀死阿瓦诺之王Roderick，这样就没人能阻止他的计划了；慢着，国王不已经是具尸体了吗，我突然有股不详的预感，难道要领完任务之后击杀才能检测到？\n我赶忙再次对话，试图说明情况，结果表示不过是虚惊一场，游戏成功弹出了统计信息，通关！\n\n很难说明当时是什么感受，毫无疑问，游戏体验相当之“粪”，但里面确实又有可取之处，不然我也不会坚持到结束。\n如果让我总结，我会认为这是一款相当“遗憾”的作品，它有很多基础框架都十分扎实，像是显示方式、物品生成、射击寻路、各种怪物的特质与风味文本等等，UI虽说是仿的，但也确实做得有模有样；每个生物都有背包，都有技能，甚至都可以学习法术，同时期的ADOM都未能做到这一点。\n\n事实上，2002年ADOM停更在1.1.1之后的那些日子里，irc上甚至有人认为Avanor能够取代它的地位；Avanor于2001年发布首个版本，持续开发至2003年后公布了源代码，接着要是能吸收开源和社区的力量，那它也许会成为一个“CddADoM”式的玩意，可没想到它跟前辈有样学样，同样陷入了停更。\n不知道是幸运还是不幸，制作组回光返照得比较快，两三年后突然又更了次新，一副要恢复长期开发的架势，较为重大的一项改动是使用LUA脚本，以便让游戏世界更容易扩展，这本有可能让它吸引elona系里喜欢omake mma的受众。\n能保持住这个势头的话，或许它真会成为新一代中流砥柱，因为忙于JADE的Thomas要到2012年才恢复ADOM更新，顺便赚上一笔巨款；但说不定是这次诈尸依旧反响平平，Avanor再后来就彻底没了消息。\n它的基础设定充满潜力，无缝开放世界，所有生物同时移动，如果作者再想着加入z轴并3d化，没准我们今天流行的就是“AvanorCraft”之类的东西，毕竟《矮人要塞》在2006年才发布，而受此启发的notch要在2009年才创作出《Minecraft》的原始版本。\n\n\n抛开未来的发展形态不谈，光是现有的内容就还有很多疑问和可拓展之处：古代恶魔与蘑菇洞有什么秘密？国王为何不肯离开王城一步？丢失数年的神器怎么才开始找？与现在的异变是否存在关系？\n能在灾难之中存活下来，玩家这名角色的真实身份是什么？散尽力量又怎会被正邪双方一起看上？\n外面的世界多半早已天翻地覆，围住这一带的环形山脉有没有可能不是牢笼，而是最后的壁障？\n邪法师与国王莫非是更高位存在的棋子？元凶真有能力造成山脉隆起这样的天地异象吗？从未现身的神明对世界动荡又持怎样的态度？\n另外Avanor的游戏流程太短，对整个世界犹如管中窥豹，凑齐部件后完全可以来段山崩地裂的演出，然后从山体缺口展开一副更为广阔的外界画卷；曾经那个dos与软盘的时代可能会受限于机能，但现如今这种字符世界能有多大几乎完全取决于创作者的想象能力。\n\n该项目的主要作者是Vadim Gaidukevich，在详情页面上还有另外两位程序，其中一位的github主页至今仍然活跃，除此之外应该还有许多其他成员也参与了开发，像是撰写了生物描述的Uriah Otting与许多汇报了bug的玩家。\n翻源码的过程中也能感受到些许码风差异，几位程序里肯定有一名狂热的OOP爱好者，比如对于游戏内的物品，多数情况下定义个item class或者struct就差不多了，剩下的数据打表或者从本地文件读取；尽管部分药水列表啥的让人神清气爽，但Avanor中有非常多的物品与生物都单独以一个派生类的形式出现，如果将继承关系用树状图画出来应该是朵蒲公英，由衷希望当事人有配置良好的自动补充模板，否则我会非常佩服ta的毅力。\n\n那些人共同创作了这部作品，他们曾有过一座论坛，一张响应迅速的bug收集表，肯定还度过了几段欢乐的时光；纵使时间流逝，绝大部分存在过的痕迹都抹平，也总会有些美好的东西留下来，就像irc上那些发自内心的赞扬与冒险分享。\n\n\n游戏实际上还有另外一种隐藏结局，将国王和邪法师全部击杀后，回到初入王城的地方找火焰之主对话，他便会为玩家加冕，封角色为新一任的Avanor之王，看样子这家伙精明的很，懂得将自己隐藏在幕后；我后来专门再去完成了这一结局，通关的统计数据放到了本文末尾，如果感兴趣的话可以下载看看。\n可惜Avanor这部作品的结局则早已注定，未来也几乎不可能有机会“复活”，至少希望本篇短文能够成为它的一份Memory File，记录曾有过这么一款游戏存在。\n非常感谢读到这里的各位，能撑着看下来可真不容易。\nPS：多亏了Simon Swerwer的Danger Room才让我在枯燥的rng地牢里没有当场Bo毙，如果没听过这首歌请一定要抽空听听\n👉本文首发于言静rl网，点我前往以弗所查看角色统计👈\n","plink":"http://www.ephesus.top/2024/Avanor/"},{"title":"使用SIMH模拟器安装4.2 BSD（以Win11为例）","date":"2023-05-08T12:50:09.362Z","date_formatted":{"ll":"May 8, 2023","L":"05/08/2023","MM-DD":"05-08"},"updated":"2023-04-25T14:01:30.000Z","content":"最近在Archive RL S写Moria的时候非常想玩一下VMS上的原版Moria，拿SIMH折腾了很久没成功（能启动VMS 2.0，但是无法在宿主机和虚拟机之间传输文件）。卡住期间顺手试了试别的教程，成功在SIMH的VAX-11/780虚拟机上启动了自带Rogue 5.3的4.2 BSD系统（让Rogue在1980年代风靡全美大学生的功臣之一）。在这里复述一下教程并记录安装经过。\n文件需求\n为了安装4.2 BSD，你需要准备以下文件：（所有下载文件的zip包，省得一个一个点链接）\n\nPerl解释器，用来制作4.2 BSD的磁带镜像；\ngzip（或者随便什么可以解压.tar.gz文件的解压工具，我用的是Bandizip）；\nboot42文件，使用Linux的uudecode命令从boot42.uue文件中解码，或下载解码好的文件（sourceforge/本地镜像）；\n编译好的SIMH vax780.exe，以及SIMH vmb.exe（不过vmb.exe全程好像都没用到？不知道有什么用……）。你也可以在SIMH官方repo下载自动编译的版本（vmb.exe在src里，最新编译的vax780.exe文件在Actions中任意一个run里的Artifacts里）；\n\n也可以直接下载制作好的4.2 BSD磁带镜像，需要bzip2（或者随便什么可以解压.bz2文件的解压工具，Bandizip仍然适用）。\n此外，你还需要以下4.2 BSD的目录文件（教程作者建议从tuhs.superglobalmegacorp.com下载，但是这个站点目前似乎挂了，我用了时光机才下下来，非常折腾……建议直接用上面那个全套zip包）：\ningres.tar.gz\nminiroot.gz\nnew.tar.gz\nrootdump.gz\nsrc.tar.gz\nsrcsys.tar.gz\nstand.gz\nusr.tar.gz\nvfont.tar.gz\n安装准备\n准备4.2 BSD磁带镜像（42.tap）\n如果你安装了Perl解释器，打算自己制作4.2 BSD镜像：\n首先解压所有用来制作镜像的文件：\n1gzip -d *.gz\n接着命令行运行mkdisttap.pl，将输出导入42.tap文件（教程提供的是Linux命令行示例，我自己运行脚本报错找不到stand，所以用的是制作好的镜像）：\n123% ./mkdisttap.pl &gt; 42.tap% ls -l 42.tap-rw-r--r--  1 Neozeed  +SYSTEM  69943640 Feb  6 12:39 42.tap\n如果你选择信任预编译，直接使用做好的4.2 BSD镜像：\n解压4.2bsd.tap.bz2，然后将文件名改为42.tap即可。\n准备启动程序（boot42）\n解码boot42.uue文件，如果你使用解码后的boot42请无视这一步：\n1234567% ls -l boot42.uue-rw-r--r--  1 Neozeed  None_ploc  9117 Feb  6 12:09 boot42.uue% uudecode boot42.uue% ls -l boot42-rw-------  1 Neozeed  None_ploc  6600 Feb  6 12:28 boot42% file boot42boot42: data\n第一阶段\n在安装的第一阶段，我们需使用如下的 install.ini 配置文件（复制到文本文档后重命名为install.ini，也可以直接下载）：\n12345678910111213141516171819set rq0 ra81at rq0 minirootset rq1 ra81at rq1 rq.dskset rq1 disset rq2 disset rq3 disset rp disset lpt disset rl disset tq disset tu disatt ts 42.tapset tti 7bset tto 7bload -o boot42 0d r10 9d r11 0run 2\n启动虚拟机\n使用 install.ini 启动我们的vax780.exe虚拟机。\n12345678910111213141516171819202122232425262728293031323334C:\\4.2BSD\\work&gt;vax780 install.iniVAX780 simulator V3.8-0RQ: creating new file1.txt&gt; set rq1 disCommand not allowedloading ra(0,0)bootBoot: ra(0,0)vmunix199488+56036+51360 start 0x11a04.2 BSD UNIX #9: Wed Nov 2 16:00:29 PST 1983real mem  = 8384512avail mem = 7073792using 102 buffers containing 835584 bytes of memorymcr0 at tr1mcr1 at tr2uba0 at tr3hk0 at uba0 csr 177440 vec 210, ipl 15rk0 at hk0 slave 0rk1 at hk0 slave 1uda0 at uba0 csr 172150 vec 774, ipl 15ra0 at uda0 slave 0ra1 at uda0 slave 1zs0 at uba0 csr 172520 vec 224, ipl 15ts0 at zs0 slave 0dz0 at uba0 csr 160100 vec 300, ipl 15dz1 at uba0 csr 160110 vec 310, ipl 15dz2 at uba0 csr 160120 vec 320, ipl 15dz3 at uba0 csr 160130 vec 330, ipl 15root on ra0WARNING: clock gained 94 days -- CHECK AND RESET THE DATE!WARNING: should run interleaved swap with &gt;= 2Mberase ^?, kill ^U, intr ^C#\n恢复rootdump\n现在我们正在rq0磁盘上的miniroot上运行。为了将系统从磁带镜像安装到虚拟机的磁盘上，我们需要运行磁带中的xtr程序以恢复rootdump（知识水平不足，这部分不太理解）：\n1234# cd /dev# ./MAKEDEV ra1# cd /# disk=ra1 type=ra81 tape=ts xtr\n执行结果如下：\n123456789101112131415161718192021222324252627282930313233343536erase ^?, kill ^U, intr ^C# cd /dev# ./MAKEDEV ra1# cd /# disk=ra1 type=ra81 tape=ts xtrBuild root file systemWarning: 538 sector(s) in last cylinder unallocated/dev/rra1a:     15884 sectors in 23 cylinders of 14 tracks, 51 se        8.1Mb in 2 cyl groups (16 c/g, 5.85Mb/g, 1856 i/g)super-block backups (for fsck -b#) at: 32, 11520,Check the file system** /dev/rra1a** Last Mounted on** Phase 1 - Check Blocks and Sizes** Phase 2 - Check Pathnames** Phase 3 - Check Connectivity** Phase 4 - Check Reference Counts** Phase 5 - Check Cyl groups2 files, 9 used, 7420 free (20 frags, 925 blocks)Rewind tapeRestore the dump image of the rootWarning: ./lost+found: File exists** /dev/rra1a** Last Mounted on /a** Phase 1 - Check Blocks and Sizes** Phase 2 - Check Pathnames** Phase 3 - Check Connectivity** Phase 4 - Check Reference Counts** Phase 5 - Check Cyl groups313 files, 3568 used, 3861 free (21 frags, 480 blocks)Root filesystem extractedIf this is a 780, update floppyIf this is a 730, update the cassette#\n在此之后，执行几次sync以保证数据同步写入磁盘（是这样吗？），然后按CTRL+E退出虚拟机并关闭simh：\n12345678910If this is a 730, update the cassette# sync# sync# syncSimulation stopped, PC: 80001620 (FFS #0,#20,8003ED44,R0)sim&gt; qGoodbyeC:\\4.2BSD\\work&gt;\n第二阶段\n现在系统磁盘已经准备好了，我们可以使用一个新的配置文件 boot.ini 来启动虚拟机。（也可以直接下载）\n1234567891011121314151617set rq0 ra81att rq0 rq.dskset rq1 disset rq2 disset rq3 disset rp disset lpt disset rl disset tq disset tu disatt ts 42.tapset tti 7bset tto 7bload -o boot42 0d r10 9d r11 0run 2\n启动虚拟机\n使用 boot.ini 启动虚拟机：\n1234567891011121314151617181920212223242526272829303132333435C:\\4.2BSD\\work&gt;vax780.exe boot.iniVAX780 simulator V3.8-0TS: creating new fileloading ra(0,0)bootBoot: ra(0,0)vmunix199488+56036+51360 start 0x11a04.2 BSD UNIX #9: Wed Nov 2 16:00:29 PST 1983real mem  = 8384512avail mem = 7073792using 102 buffers containing 835584 bytes of memorymcr0 at tr1mcr1 at tr2uba0 at tr3hk0 at uba0 csr 177440 vec 210, ipl 15rk0 at hk0 slave 0rk1 at hk0 slave 1uda0 at uba0 csr 172150 vec 774, ipl 15ra0 at uda0 slave 0ra1 at uda0 slave 1zs0 at uba0 csr 172520 vec 224, ipl 15ts0 at zs0 slave 0dz0 at uba0 csr 160100 vec 300, ipl 15dz1 at uba0 csr 160110 vec 310, ipl 15dz2 at uba0 csr 160120 vec 320, ipl 15dz3 at uba0 csr 160130 vec 330, ipl 15root on ra0WARNING: should run interleaved swap with &gt;= 2MbAutomatic reboot in progress...Mon Feb  6 05:05:49 PST 1984Can&#x27;t open checklist file: /etc/fstabAutomatic reboot failed... help!erase ^?, kill ^U, intr ^C#\n准备磁盘\n配置硬件设备（这里也不太明白）：\n123456# disk=ra# name=ra0h;type=ra81# cd /dev# ./MAKEDEV ts0;sync# cd /# newfs $name $type\n执行结果如下：\n123456789101112131415161718# disk=ra# name=ra0h;type=ra81# cd /dev# ./MAKEDEV ts0;sync# cd /# newfs $name $typeWarning: 28 sector(s) in last cylinder unallocated/dev/rra0h:     759668 sectors in 1064 cylinders of 14 tracks, 51 sectors        389.0Mb in 67 cyl groups (16 c/g, 5.85Mb/g, 2048 i/g)super-block backups (for fsck -b#) at: 32, 11512, 22992, 34472, 45952, 57432, 68912, 80392, 91872, 103352, 114832, 126312, 137792, 149272, 160752, 172232, 182816, 194296, 205776, 217256, 228736, 240216, 251696, 263176, 274656, 286136, 297616, 309096, 320576, 332056, 343536, 355016, 365600, 377080, 388560, 400040, 411520, 423000, 434480, 445960, 457440, 468920, 480400, 491880, 503360, 514840, 526320, 537800, 548384, 559864, 571344, 582824, 594304, 605784, 617264, 628744, 640224, 651704, 663184, 674664, 686144, 697624, 709104, 720584, 731168, 742648, 754128,#\n恢复usr分区\n现在usr分区还没有格式化。我们挂载并重建usr分区，然后再将其卸载，运行 fsck 以确保执行正确。\n12345678910111213141516# mount /dev/$name /usr# cd /usr# mkdir sys# cd sys# mt rew# mt fsf 3# tar xpbf 20 /dev/rmt12# cd ..# mt fsf# tar xpbf 20 /dev/rmt12# cd /# chmod 755 / /usr /usr/sys# rm -rf sys# ln -s /usr/sys sys# umount /dev/$name# fsck /dev/r$name\n执行结果如下：\n1234567891011121314# chmod 755 / /usr /usr/sys# rm -rf sys# ln -s /usr/sys sys# umount /dev/$name# fsck /dev/r$name** /dev/rra0h** Last Mounted on /usr** Phase 1 - Check Blocks and Sizes** Phase 2 - Check Pathnames** Phase 3 - Check Connectivity** Phase 4 - Check Reference Counts** Phase 5 - Check Cyl groups3735 files, 25550 used, 336310 free (166 frags, 84036 blocks)#\n配置fstab\n现在创建fstab并格式化home分区，然后我们就可以重启虚拟机、进入多用户模式了：\n12345# cd /etc# cp fstab.ra81 fstab# newfs ra0g ra81# sync# reboot\n执行结果如下：\n123456789101112131415# cd /etc# cp fstab.ra81 fstab# newfs ra0g ra81Warning: 30 sector(s) in last cylinder unallocated/dev/rra0g:     82080 sectors in 115 cylinders of 14 tracks, 51 sectors        42.0Mb in 8 cyl groups (16 c/g, 5.85Mb/g, 2048 i/g)super-block backups (for fsck -b#) at: 32, 11512, 22992, 34472, 45952, 57432, 68912, 80392,# sync# rebootsyncing disks... doneReboot requested, PC: 80021716 (ADDL2 #8,SP)sim&gt; qGoodbye\n多用户模式！\n启动虚拟机\n再次使用 boot.ini 启动vax780.exe，我们就可以以多用户模式启动4.2 BSD了！可以使用root用户登录，默认无密码。由于系统第一次启动时会自动执行 fsck 指令，可能需要等待一段时间（与宿主机性能有关）。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748C:\\4.2BSD\\work&gt;vax780.exe boot.iniVAX780 simulator V3.8-0loading ra(0,0)bootBoot: ra(0,0)vmunix199488+56036+51360 start 0x11a04.2 BSD UNIX #9: Wed Nov 2 16:00:29 PST 1983real mem  = 8384512avail mem = 7073792using 102 buffers containing 835584 bytes of memorymcr0 at tr1mcr1 at tr2uba0 at tr3hk0 at uba0 csr 177440 vec 210, ipl 15rk0 at hk0 slave 0rk1 at hk0 slave 1uda0 at uba0 csr 172150 vec 774, ipl 15ra0 at uda0 slave 0ra1 at uda0 slave 1zs0 at uba0 csr 172520 vec 224, ipl 15ts0 at zs0 slave 0dz0 at uba0 csr 160100 vec 300, ipl 15dz1 at uba0 csr 160110 vec 310, ipl 15dz2 at uba0 csr 160120 vec 320, ipl 15dz3 at uba0 csr 160130 vec 330, ipl 15root on ra0WARNING: should run interleaved swap with &gt;= 2MbAutomatic reboot in progress...Mon Feb  6 05:21:02 PST 1984/dev/ra0a: 326 files, 3569 used, 3860 free (20 frags, 480 blocks)/dev/rra0h: 3735 files, 25550 used, 336310 free (166 frags, 84036 blocks)/dev/rra0g: 2 files, 9 used, 77750 free (14 frags, 9717 blocks)Mon Feb  6 05:21:15 PST 1984check quotas: done.local daemons: routed telnetd ftpd talkd syslog sendmail.preserving editor filesclearing /tmpstandard daemons: update cron accounting mail    Mon Feb 6 05:21:17 PST 1984 printer.starting network: rshd rexecd rlogind rwhod.Mon Feb  6 05:21:18 PST 19844.2 BSD UNIX (myname)login:\n现在就可以直接用root用户登录了。\n123456789104.2 BSD UNIX (myname)login: root4.2 BSD UNIX #9: Wed Nov 2 16:00:29 PST 1983Would you like to play a game?You have mail.Don&#x27;t login as root, use sumyname#\n一些常用配置\n修改系统hostname\n安装盘中的hostname默认为myname（很怪），我们可以编辑/etc/rc.local文件修改它。\n1myname# vi /etc/rc.local\n将/bin/hostname 后的myname改为自己想要的名字（我改成了bsd），重启后即可生效。\n添加用户\n4.2 BSD还没有adduser指令，我们需要执行 vipw 指令打开密码文件手动添加。\n1bsd# vipw\n打开后可以看到形如\n1rogue::35:31:&amp;:/usr/guest/rogue:/bin/csh\n的用户条目，其中各项的含义如下：\n1用户名:加密后的用户密码（为空代表无密码）:用户代码（应与现有条目不重复）:用户组代码（10代表管理员，20代表学生？31代表访客？）:用户个人信息（可不填，显示在finger指令中）:用户启动的默认路径:用户使用的shell（留空默认是/bin/sh，也可以是/bin/csh）\n我们在文件末尾添加新行，将新用户的信息填好（密码留空），保存退出。（建议用户路径为/usr/guest/[用户名]）\n然后我们创建对应的用户路径，并将其所有者改为新用户。\n123bsd# cd /usr/guest/bsd# mkdir [路径名]bsd# chown [用户名] [路径名]\n然后重启系统，再次启动就可以用新用户登录了。\n登录后可以执行 passwd 修改当前用户密码。\n修改shell配置文件\n/bin/sh的配置文件为/.profile，/bin/csh的配置文件为/.cshrc。可以将用户使用的shell对应的配置文件复制到用户的启动路径里加以修改，语法和Linux类似。下面是我的/.cshrc文件：\n12345678910111213141516171819202122232425262728293031323334353637383940414243alias mail Mailset history=40# directory stuff: cdpath/cd/backset cdpath=(/a/wnj)alias   cd      &#x27;set old=$cwd; chdir \\!*&#x27;alias   back    &#x27;set back=$old; set old=$cwd; cd $back; unset back; dirs&#x27;# sccs stuff: sd/co/ci/allout/out/uneditalias   sd      &#x27;sccs get -p \\!* | diff - \\!$&#x27;alias   co      sccs get -ealias   ci      sccs delgetalias   allout  &quot;(cd ..; echo */SCCS/p.*|sed s/SCCS\\\\/p.//g)&quot;alias   out     &quot;echo SCCS/p.*|sed s/SCCS\\\\/p.//g&quot;alias   info    sccs infoalias   unedit  sccs unedit# spms stuff: chproject/cpdalias   chproject &#x27;eval `&quot;chproject&quot; -d \\!*`&#x27;alias   cpd     &#x27;eval `&quot;pd&quot; \\!*`&#x27;alias   z               suspendalias   area    &#x27;grep \\!* /usr/games/lib/quiz.k/areas&#x27;alias   tel     &#x27;grep -i \\!* /usr/bill/bin/telno&#x27;alias   x       exitalias   pd      pushdalias   pd2     pushd +2alias   pd3     pushd +3alias   pd4     pushd +4alias   j       jobs -lalias   l       &quot;echo \\!*&#123;*&#125;&quot;set path=(/etc /usr/ucb /bin /usr/bin /usr/local /usr/hosts . /usr/new /usr/games)if ($?prompt) then        set prompt=&quot;`hostname | sed s/ucb//`# &quot;endifecho &quot;Type in &#x27;rogue&#x27; to play rogue in 4.2 BSD.&quot;echo &quot;        &#x27;rogue [savefilename].save&#x27; to load savefile.&quot;echo &quot;__________ ________    ________ ____ ______________&quot;echo &quot;\\______   \\\\_____  \\  /  _____/|    |   \\_   _____/&quot;echo &quot; |       _/ /   |   \\/   \\  ___|    |   /|    __)_ &quot;echo &quot; |    |   \\/    |    \\    \\_\\  \\    |  / |        \\&quot;echo &quot; |____|_  /\\_______  /\\______  /______/ /_______  /&quot;echo &quot;        \\/         \\/        \\/                 \\/ &quot;echo &quot;&quot;echo &quot;&quot;echo &quot;&quot;\n登录效果如下：\n123456789101112131415Last login: Tue Apr 24 04:49:10 on tty004.2 BSD UNIX #9: Wed Nov 2 16:00:29 PST 1983Would you like to play a game?You have mail.Type in &#x27;rogue&#x27; to play rogue in 4.2 BSD.        &#x27;rogue [savefilename].save&#x27; to load savefile.__________ ________    ________ ____ ______________\\______   \\\\_____  \\  /  _____/|    |   \\_   _____/ |       _/ /   |   \\/   \\  ___|    |   /|    __)_ |    |   \\/    |    \\    \\_\\  \\    |  / |        \\ |____|_  /\\_______  /\\______  /______/ /_______  /        \\/         \\/        \\/                 \\/\n远程用户连接\n在 boot.ini 中添加如下内容可以使我们的虚拟机在tcp端口8888开放telnet连接，允许8个远程用户同时登入。（需要添加在启动指令之前，否则无法生效；直接添加在文件开头都行）\n123set dz lines=8set dz 7batt dz -m 8888\n建议使用putty或syncterm等远程连接终端。Windows自带的telnet效果非常糟糕，我使用的时候甚至不能刷新屏幕……\n玩Rogue！\n开始新游戏\n执行\n1bsd# /usr/games/rogue\n如果和我一样将 /usr/games 添加到配置文件中的系统路径里，只需要执行\n1bsd# rogue\n从上次存档继续游戏\n在存档文件（假设为rogue.save）所在路径执行\n1bsd# /usr/games/rogue rogue.save\n或\n1bsd# rogue rogue.save\n这是Rogue，它也没失踪也没怎样，只是很可爱大家都来看看.jpg\n123456789101112131415161718192021222324                                   a) Some food                                   b) +1 ring mail [protection 4] (being worn)                                   c) A +1,+1 mace (weapon in hand)                                   d) A +1,+0 short bow                                   e) 33 +0,+0 arrows                                   --Press space to continue--                             --------------                             |...........*|                             |...S........+                             |............|                             +......@.....|                             |............|                             -----+--------Level: 1  Gold: 0      Hp: 12(12)  Str: 16(16)  Arm: 4   Exp: 1/0\n","plink":"http://www.ephesus.top/2023/win10_4.2_BSD/"},{"title":"狄魔高根的补丁(1)","date":"2023-03-30T17:02:33.000Z","date_formatted":{"ll":"Mar 30, 2023","L":"03/30/2023","MM-DD":"03-30"},"updated":"2024-06-23T07:44:17.039Z","content":"原作：greyknight\n译：Silencess\n摘要：\n本在地铁上发现一张神秘的软盘，而它打开了一个陌生的新世界。\n作者寄语：\n致Synoddiane。\n我一般不会这么吓人的！但一个关于狄魔高根的故事确实值得这样处理。\n本是在地铁上捡到那张改变他人生的软盘的。当时他刚从女朋友家里回来，好不容易才赶上当晚的末班车，车厢里除了他只有寥寥几人。刚一坐下，他发现自己的鞋带松了，于是弯腰去系。他的余光瞥见了什么东西：一块方形塑料片掉在邻座下面，大约有五英寸长宽。他把这玩意勾出来，重新坐正开始好好检查。\n那是一张古旧的软盘，他很多年没见过这种型号的软盘了。软盘上贴了张发黄的标签，上面记着手写的描述，字体略显老派，有点像德文黑体。泛棕的墨水记着这样一行字： “NetHack: 狄魔高根的补丁” 。本的注意立刻被吸引了。NetHack是他本人时不时会玩的一款电脑游戏；很显然，这张软盘里就装着一个什么人自制的、这款游戏的版本补丁。根据这个标题，他猜测这版补丁扩展了狄魔高根这一角色的内容：狄魔高根是这游戏里较为出名的致命敌人之一，是一名强大的恶魔王子。他环视整节车厢；把这东西落下的人很可能已经下车了。他想着是不是应该将它交给失物招领处，好让失主能寻回这东西。但也许他也可以先保留一段时间，把里面的文件复制一份？之后他仍然可以再去失物招领处交还这件物品，而且这样一来他就有机会试试这个补丁到底改了什么内容了。\n他把软盘滑进口袋里，带着罪恶感环顾四周。反正也没有人在看他这边。他想着，自己得去什么地方才能找到一台能读这种古董软盘的机子呢。 也许刘易斯能想到办法 ，他对自己说。\n\n直到第二天晚上，他才终于见到了刘易斯。介绍了自己捡到这卷设备的经过之后，他问刘易斯能不能帮帮忙。\n“这玩意太古老了！”刘易斯感叹道，“我认真的，我这儿连读3.5英寸盘的软驱都没有，更别说这么大的老东西了。”\n“呃，行吧……那你还认识什么够宅的家伙会有这种东西吗？”\n“哈？我看你就是我认识的最宅的家伙了。不过……诶，我还真想到个在这事上也许能帮到我们的人。”\n“不错！你需要我把这张盘给你之类的吗？”\n“确实，最好把它拿上。我们得先搞清楚这张盘是什么格式的，然后才能帮你找到能读取它的机器。”\n“嗯，是这样。我猜肯定得是什么特别古老的操作系统吧？说不定现在都没人用的那种。”\n“对的，像是Windows ME之类的。说不定我明天就能给你消息。”\n那天晚上，本还坐下来翻了几个网站，看看能不能找到点和这版补丁内容相关的线索。他找到几个跟狄魔高根相关的补丁，但没有一个是叫这名字的。这倒不是什么很难想象的事：有很多单人开发的NetHack补丁可能现在还躺在开发者的电脑里，根本没机会上传到“比利厄斯补丁数据库”或者互联网上的其他任何地方呢。显然，都藏在这么一张本身就是旧日遗物的软盘里了，也不会有什么人考虑优先把它送到现代世界来的。他也只是想知道这个补丁到底是基于游戏本体的什么版本。NetHack是个历史悠久的游戏，而这张盘里的内容也很有可能和它本身一样古老。\n他在IRC和USENET上问了问其他玩家，可仍然没找到任何线索。大概只能选择更艰难的那条路了：等他们解密了这张软盘，然后自己找出答案。他打开几个包含游戏剧透信息的站点，开始按时间顺序查询这游戏漫长一生中的诸多重大变化。要是他捡到的是某些特别古老的版本的话，他至少应该知道自己在和什么东西打交道。而假如这补丁基于的版本和他最熟悉的那个版本大相径庭的话……好吧，众所周知NetHack并不是个容错率很高的游戏。\n\n几天之后，本坐在自己的房间里，看着一大坨该进博物馆的奇怪发黄塑料玩意。刘易斯的朋友从他的古董收藏里翻出来了这台东西，它正是能运行那种软盘的设备。这台来自旧时代的电脑有一个陌生的品牌——一张模糊不清的“阿尔哈兹雷德科技”标签。他耸耸肩，按照机器送来时演示的那样启动了它。尽管经历了好一番利诱和宣誓——如果这台破旧的老机器有什么三长两短，他将遭受严厉的惩罚——但他最终还是成功达成了协议，把机器借到了手。\n在这台巨兽推进它长长的启动流程时，他在自己的电脑上调出了他的剧透笔记。启动一完成，他立刻将那张“狄魔高根的补丁”软盘插入了这台设备。参照一张记有所有机器指令的纸条，他敲下对应的命令，屏住呼吸，然后按下了回车。\n“NetHack: 狄魔高根的补丁”，标题栏显示着这么一行字，并没有提到版本号。本快速翻阅了角色职业列表。常规的选项都在这里：盗贼、巫师、医生、骑士、游侠、游客……但还有一个新增的选项。怀着好奇，本选择了“符号学家”作为自己的职业。很快，他已经读完了常规的开局介绍文本，并与他所控制的中立男性人类符号学家一起进入了命运地牢。开局分配的宠物是一只小猫，本没什么犹豫便将其命名为“雷”。接着他开始检视自己的物品栏。 太棒了！ 他想， 符号学家开局就带着一支魔笔！ 这是很有用的工具，可以让他有能力写出魔法卷轴和魔法书。\n当晚剩下的时间里，他小心翼翼地探索着地牢，随时警惕着这个补丁可能加入的、令人不快的意外情况。开局自带的魔法书之魔法飞弹在消灭哥布林和其他袭击他的地牢住民时非常有用，很快他便清理了好几个楼层，还攒下了一小组留作后用的道具。他的小猫，雷，也在奋不顾身的战斗中很快成长为可以独当一面的勇士。\n事实上，在玩了好几天以后，本才对将要发生的事情有了点最基本的了解。那时他刚刚到达神谕的居所，并惊奇地发现所有通常矗立在那里的半人马雕像已经全部被砸成了碎石。当然，他自己本来也打算看看这些雕像里面有什么，因为雕像经常会藏有魔法书，但这件事超出了他的预料。他无可奈何地向前走去，进入了本层的主厅。在那片喷泉之间，只躺着一个孤独的百分号（%）：那是神谕的尸体。\n一瞬间他便进入了戒备状态。这只可能是一个“骨档”文件：这层的状态在某个前任角色死亡时被存储在磁盘里，而他现在被随机选中来见证这位玩家的选择所带来的后果。是那个角色砸碎了雕像，杀死了神谕，现在可能正躺在附近什么地方的一处坟墓中。并且，理所应当地，那杀死了他的什么人——或者什么东西——也是他要面临的问题。\n实际上，这一层静得有点令人毛骨悚然了。自他从上一层下来之后，他还没有见过任何一个随机生成的怪物。提起万分小心，本谨慎地摸过这里的每一个角落，探明了地图上的每一片未知的地块。终于，他找到了自己预想中的那块墓碑。阅读碑上的铭文，他知道这地下埋葬着一具腐朽的尸体，它属于罗宾，另一位符号学家。“被狄魔高根的不悦杀死”。嗯……这听起来很不祥。不过，至少在这格地面上放着的东西是不错的。本贪婪地按下逗号键（,），几乎想都没想便将其拾起。\nm - 仪式刀名为神魔克星\n当然，墓碑上关于“老D”的只言片语令他的警戒心还要更进一步了。他几乎是踮着脚尖挪到了向下的楼梯处，目光不断移向每个进入视野的方格。感觉简直像过了一年之后，他终于到达了下楼的楼梯间，可以逃离这一层了。他意识到，在这整整一层里，他还连一个怪物都没有见过；甚至连“罗宾”的鬼魂都没露过一次面。只有他的宠物猫，雷，随他穿过这片德尔斐神庙的残垣断壁。\n两层之后，在途经一个显然空无一物的房间时，本被吓了一跳。“你踩上了奥术符文!  – 更多–”游戏这样显示。后半部分代表他误入的这个陷阱会带来进一步的后果。他在脑中迅速过了一遍原版NetHack中所有可能的陷阱。通常游戏中不存在“奥术符文”这个东西；很显然这是补丁的一部分。他试探着按下空格，打算看看后续的文本。“你感觉充满了力量!  – 更多–”数据实际上已经更新，游戏显示他的魔法能量满了。结果并不是坏陷阱嘛！他再次按下空格。“你感觉好像少了什么东西.”嗯……不太明白这是什么意思。也许当他拿来某种东西的时候这种陷阱会有不同的表现？但是要带什么，带了又会发生什么呢？他在笔记上记下这里，然后继续赶路。那个有陷阱的格子现在显示为一个插入符号（^），和通常一样代表这里有陷阱。奥术符文是紫色的，他注意到这点。\n他打开一扇门，突然惊恐地倒退一步，所有关于这种奇怪陷阱的想法瞬间挥之一空——一个蓝色的小写e飘进了房间。是浮眼！他无声地祈祷了一下，感谢至少没有其他怪物也加入这场聚会。只有浮眼的话还好处理；要是和别的怪物一起来的话，它的麻痹凝视就相当危险了。幸好，本已经获得了游戏中最令人惊讶的有用工具之一：眼罩。\n将雷留在房间外，他避开移动缓慢的浮眼，关上房间里的所有门以防不速之客的打扰。接着，他戴上那块保护性的布料，将角色与全世界的视线隔断。在一片黑暗中，他挥动武器击向浮眼最后出现的位置。由于魔法麻痹凝视已经被他阻挡在外，那漂浮着的圆球无力作出任何反抗，很快便被他杀死了。然后，本飞快地摘下眼罩，以防有其他什么东西前来偷袭。幸而房间仍是空的……或者说几乎是空的，因为浮眼还留下了可食用的尸体！\n本激动地低呼一声，快速移动到标着尸体符号的格子上开始进食——浮眼的尸体可是种有魔力的美味。自然，他刚刚吃完这一餐，游戏立刻提示道他“感觉奇怪的精神敏锐”。他再次戴上眼罩，这次他的“目盲”状态强化了某种心灵感应的视角，使他足以看清整层的所有怪物。 妙啊！ 他想着， 这么前期就获得了心灵感应！多棒的发现！\n然而他的兴奋并不长久，因为一道黑色光环这时缠绕上了他手中的神魔克星。哦，怎么又这样！他不满地嘟囔道。在原版游戏中，名为神魔克星的这把武器有许多实用效果，而其中最为知名的是它那抵御诅咒的能力；可在这个补丁里，它却似乎一直在招引诅咒。这已经是他捡起它之后第三次莫名其妙被诅咒了。现在它又牢牢地粘在他的手上了，他只有找到下一张解除诅咒卷轴才能把它松开。 现在我算是知道那支魔笔是干嘛用的了， 他在心里嘀咕着， 用来写一万张解诅咒卷轴！\n是时候保存退出去睡觉了，他这样决定。那天夜里他做了个怪梦，梦见自己被困在一片ASCII字符构成的迷宫中，字母们徘徊在线条风的长廊间，猎捕着他。一大波大写Z和W追逐着他，如浪潮般涌入走廊中。突然间，他看见地面上落着一个小括号（(），正要伸手去捡，脊背却传来幽灵的冰冷气息。他迅速转身，举起自己刚入手的新武器——然后在这一瞬间苏醒了过来。\n本汗流浃背、心跳加速，努力平缓自己的呼吸。只是个愚蠢的噩梦，只是最近玩这游戏玩太多了，他这样想着，想要挥手抹去额前的汗水。然而，当他抬起手臂时，有什么冰冷的、金属质的闪光晃了他的眼睛。他连忙去摸眼镜，不敢相信自己看到的一切，但那副情形即使他戴上眼镜也没有消失、不容置疑。这绝不可能发生，可面前这把波浪形边缘的匕首也不可能有第二个名字——它那雕琢着复杂花纹的淡蓝色刀柄令人隐约瞥见另一个几何学和神秘符文的陌生世界。\n被诅咒的+2 神魔克星（拿在手上）\n\n","plink":"http://www.ephesus.top/2023/demogorgon_patch_1/"},{"title":"ChatGPT提示的艺术：制作清晰有效咒语","date":"2023-03-30T15:40:23.000Z","date_formatted":{"ll":"Mar 30, 2023","L":"03/30/2023","MM-DD":"03-30"},"updated":"2023-03-30T08:58:51.000Z","content":"\n本文转自https://github.com/wikiden/Awesome-ChatGPT-Prompts-CN 侵删\n\n当ChatGPT首次推出时，我立即被它的功能所吸引。我以各种方式尝试了该工具，并一直对结果感到惊讶。当我看到其他人找到使用 ChatGPT 的创造性方法并了解更多关于如何优化其潜力的信息时，我受到启发，创建了的有效提示存储库。发现和探索 ChatGPT 的功能，然后与他人分享我的发现，这确实令人兴奋。\n在我为 ChatGPT 制作提示的经验中，我偶然发现了一些有助于提高提示有效性的技巧。例如，我了解到使用特定和相关语言的重要性，以确保 ChatGPT 理解我的提示并能够生成适当的响应。我还发现了为对话定义明确目的和重点的价值，而不是使用开放式或过于宽泛的提示。当我继续与 ChatGPT 合作时，我对如何以富有成效的方式与 AI 交互有了更好的理解，并最大限度地发挥了该工具的潜力。总的来说，这是一次有益和启发性的经历。\n前言\n欢迎来到 “ChatGPT 提示的艺术：制作清晰有效的提示指南” ！在本综合指南中，您将了解有关制作清晰有效的 ChatGPT 提示以推动引人入胜且信息丰富的对话的所有信息。\n首先ChatGPT并非无所不知无所不能，首先它的知识限制在2021年某个时间点之前，之后的很多新的信息都没被录入，另外一些专业领域的文档文案也很少，需要用户自己去提供资料，后面也会提到。\n无论您是初学者还是经验丰富的 ChatGPT 用户，这本电子书都能满足每个人的需求。从理解有效提示的原则到掌握构建清晰简洁提示的艺术，您将获得将 ChatGPT 对话提升到新水平 所需的技能和知识。\n在接下来的章节中，我们将介绍从 ChatGPT 的基础知识及其工作原理到制作引人注目的提示和解决常见问题的高级技术的所有内容。在此过程中，您将找到真实世界的示例和专家见解，以帮助您掌握 ChatGPT 提示的艺术。\n所以让我们开始吧！凭借您从本电子书中获得的知识和技能，您将像专业人士一样推动高效且引人入胜的 ChatGPT 对话。\n\n介绍\n在本综合指南中，您将了解有关制作清晰有效的 ChatGPT 提示以推动引人入胜且信息丰富的对话的所有信息。\n\n您可以通过浏览器访问 ChatGPT：https://chat.openai.com\n\n但首先，让我们从回答这个问题开始：什么是 ChatGPT？\n\nChatGPT（生成式预训练转换器）是 OpenAI 于 2022 年 11月推出的聊天机器人。它建立在OpenAI的GPT-3.5系列大型语言模型之上，并通过监督和强化学习技术进行了微调。\nChatGPT 于  2022年 11月30日作为原型推出，并因其在许多知识领域的详细回复和清晰的答案而迅速引起了关注。其事实准确性参差不齐被认为是一个重大缺点。— 维基百科。\n\nChatGPT 是一种聊天机器人，允许用户与基于计算机的代理进行对话。它通过使用机器学习算法来分析文本输入并生成旨在模仿人类对话 的响应。ChatGPT 可用于多种用途  ，包括回答问题、提供信息和进行随意对话。\n决定 ChatGPT 对话成功与否的关键因素之一是用于启动和指导对话的提示的质量。定义良好的提示有助于确保对话保持正常并涵盖用户感兴趣的主题。相反，定义不明确的提示可能会导致对话脱节或缺乏重点，从而导致参与度和信息量降低的体验。\n这就是这篇说明的做用。  接下来，您将学习清晰沟通的原则以及如何将它们应用于 ChatGPT 提示，以及如何制作有效的提示以推动引人入胜且信息丰富的对话的分步指南。您还将了解要避免的常见陷阱以及解决使用 ChatGPT 时可能出现的常见问题的提示。\n因此，无论您是 ChatGPT 的新手还是希望将您的技能提升到新水平的经验丰富的用户，这本电子书都能满足您的需求。让我们开始吧！\n这就是ChatGPT界面的样子。\n\n                                     这就是ChatGPT界面的样子。\n\nChatGPT 的主要优点之一是它能够理解和响应自然语言输入  。这意味着用户可以使用与人类交谈时使用的相同语言和语法与 ChatGPT 进行通信。ChatGPT 还能够理解和响应上下文，使其能够对用户输入生成更合适和更相关的响应。\n除了自然语言处理功能外，ChatGPT 还具有许多其他特性和功能，使其成为推动对话的强大工具。其中包括：\n\n定制：  ChatGPT可以定制以满足用户的需求和偏好。这可以包括自定义 ChatGPT 回复的语气和风格，以及它能够讨论的信息和主题类型。\n个性化：  ChatGPT 可以使用机器学习算法根据用户过去的交互和偏好个性化其响应。这可以使对话感觉更自然，并根据用户的需求和兴趣量身定制。\n多语言支持：  ChatGPT 能够理解和响应多种语言的输入，使其成为国际用户或想要用多种语言交流的用户的有用工具。目前ChatGPT已经可以用中文直接问答，而且非常完美，但是速度比英文还是慢。\n可扩展性：  ChatGPT 能够处理大量流量，并可用于同时推动与多个用户的对话。这使其非常适合客户服务或在线社区等应用程序。\n\n如您所见，ChatGPT 是一个功能强大且用途广泛的工具  。在接下来的章节中，我们将探讨如何通过制作清晰有效的提示来推动引人入胜且信息丰富的对话，从而充分利用这些功能。\n什么是ChatGPT，它是如何工作的？\n现在您已经大致了解了 ChatGPT 及其功能，让我们更深入地了解 ChatGPT 是什么以及它是如何工作的。\n那么ChatGPT是如何工作的呢？概括地说，该过程可以分解为以下步骤：\n\n用户将文本输入到ChatGPT界面中。  这可能是一个问题、一个信息请求或一个随意的陈述。\nChatGPT 系统分析输入并使用机器学习算法生成响应。\n响应以文本形式返回给用户。\n然后，用户可以输入其他文本，ChatGPT 系统将再次分析和响应这些文本。此过程一直持续到对话结束。\n\n决定 ChatGPT 对话成功与否的关键因素之一是用于启动和指导对话的提示的质量。定义良好的提示有助于确保对话保持正常并涵盖用户感兴趣的主题。相反，定义不明确的提示可能会导致对话脱节或缺乏重点，从而导致参与度和信息量降低的体验。\n在接下来的章节中，我们将更详细地探讨如何制作有效的 ChatGPT 提示，以推动引人入胜且信息丰富的对话。\n那么，它与其他聊天机器人有何不同？\nChatGPT 是市场上可用的几种聊天机器人之一。那么是什么让 ChatGPT 与其他聊天机器人区分开来，又是什么让它与众不同呢？\n一个关键的区别是ChatGPT是一个巨大的语言模型  。这使得 ChatGPT 能够以类似于人类的方式理解和响应输入。其他聊天机器人可能依赖于预先编程的响应或简单的关键字匹配，这可能会导致对用户输入的响应不太自然或相关。\n另一个区别是ChatGPT的学习能力  。通过使用机器学习算法，ChatGPT 能够分析用户输入并根据过去的对话改进其响应。这可以导致对用户输入的更个性化和相关响应。\n另一个关键区别是 ChatGPT 处理更复杂或开放式对话的能力。 由于 ChatGPT 能够理解和响应上下文，因此它能够更好地处理涵盖广泛主题或需要更深入响应的对话。\n总体而言，ChatGPT 对自然语言处理和机器学习算法的使用使其与其他聊天机器人区分开来，并使其成为推动引人入胜且信息丰富的对话的强大工具。在以下章节中，我们将探讨如何通过制作清晰有效的提示来充分利用这些功能。\nChatGPT可以用来做什么？\n鉴于其理解和响应自然语言输入的能力，ChatGPT 具有广泛的潜在应用。ChatGPT 的一些常见用途包括：\n\n顾客服务：  ChatGPT 可用于实时回答客户问题、提供信息和解决问题。这对于希望为客户提供 24/7 全天候支持的企业特别有用。\n教育：  ChatGPT 可用于在各种教育环境中提供信息或回答问题。例如，它可以用作导师或提供有关特定主题的信息。\n信息提供：  ChatGPT 可用于提供有关广泛主题的信息，例如天气、新闻或本地企业。\n私人助理：  ChatGPT 可以用作个人助理，以帮助完成调度、组织和管理信息等任务。\n社交互动：  ChatGPT 可用于进行随意对话或提供娱乐，使其成为社交媒体或在线社区的有用工具。\n\n总体而言，ChatGPT 的潜在用途广泛而多样，使其成为适用于广泛应用的多功能和强大工具。在接下来的章节中，我们将探讨如何制作有效的 ChatGPT 提示，以推动各种目的的引人入胜且信息丰富的对话。\n提示在 ChatGPT 对话中的作用\n正如我们前面提到的，ChatGPT 对话中使用的提示质量会显著影响对话的成功。定义明确的提示有助于确保对话保持正常并涵盖用户感兴趣的主题，从而提供更具吸引力和信息丰富的体验。\n那么，什么是好的 ChatGPT 提示，您如何制作有效的提示来推动引人入胜且信息丰富的对话？有几个关键原则需要牢记：\n\n清晰： 清晰简洁的提示将有助于确保 ChatGPT 了解手头的主题或任务，并能够生成适当的响应。避免使用过于复杂或模棱两可的语言，并在提示中尽可能具体。\n重点： 定义明确的提示应具有明确的目的和重点，有助于指导对话并保持对话正常进行。避免使用过于宽泛或开放式的提示，这可能会导致对话脱节或注意力不集中。\n关联： 确保您的提示与用户和对话相关。避免引入不相关的话题或切线，以免分散对话的主要焦点。\n\n通过遵循这些原则，您可以制作有效的 ChatGPT 提示，以推动引人入胜且信息丰富的对话。在接下来的章节中，我们将更详细地研究这些原则，并探索制作清晰简洁提示的特定技术。\n制作清晰简洁的提示的好处\n制作清晰简洁的提示有很多好处，可以帮助确保您的 ChatGPT 对话引人入胜且信息丰富。一些主要优势包括：\n\n提高理解： 通过使用清晰而具体的语言，您可以帮助确保 ChatGPT 理解手头的主题或任务，并能够生成适当的响应。这可以产生更准确和更相关的响应，从而使对话更具吸引力和信息量。\n增强专注力： 通过为对话定义明确的目的和重点，您可以帮助指导对话并保持对话正常进行。这有助于确保对话涵盖用户感兴趣的主题，并避免切线或干扰。\n更高的效率： 使用清晰简洁的提示也有助于提高对话效率。通过专注于特定主题并避免不必要的切线，您可以确保对话保持正轨并更及时地涵盖所有关键点。\n\n总体而言，制作清晰简洁的提示有助于确保您的 ChatGPT 对话引人入胜、信息丰富且高效。在以下章节中，我们将探讨利用这些优势制作有效提示的特定技术。\n有效和无效的 ChatGPT 提示示例\n为了更好地理解制作有效 ChatGPT 提示的原则，让我们看一些有效和无效提示的示例。\n有效的聊天GPT提示：\n\n“你能总结一下’运动的好处’一文的要点吗？” 此提示重点突出且相关，使 ChatGPT 可以轻松提供请求的信息。\n“巴黎最好的素食餐厅是什么？” 此提示是特定且相关的，允许 ChatGPT 提供有针对性和有用的响应。\n\n无效的聊天GPT提示：\n\n“你能告诉我关于这个世界的什么？” 此提示过于宽泛和开放，使 ChatGPT 难以生成重点突出或有用的响应。\n“你能帮我做作业吗？” 虽然此提示清晰而具体，但它过于开放，无法让 ChatGPT 生成有用的响应。更有效的提示将指定手头的特定主题或任务。\n“你怎么样？” 虽然这是一个常见的对话开始，但它不是一个定义明确的提示，也没有为对话提供明确的目的或重点。\n\n通过比较这些示例，您可以了解制作有效 ChatGPT 提示的原则。在接下来的章节中，我们将更详细地研究这些原则，并探索制作清晰简洁提示的特定技术。\n清晰沟通的原则\n清晰的沟通是确保您的 ChatGPT 提示有效并推动引人入胜且信息丰富的对话的关键。在制作提示时，应牢记清晰沟通的几个关键要素：\n\n清晰： 使用清晰而具体的语言，使 ChatGPT 易于理解。避免使用可能导致混淆或误解的行话或模棱两可的语言。\n简洁： 提示尽可能简洁，避免不必要的单词或切线。这将有助于确保 ChatGPT 能够产生重点突出且相关的响应。\n关联： 确保提示与对话和用户需求相关。避免引入不相关的话题或切线，以免分散对话的主要焦点。\n\n通过遵循这些清晰沟通的原则，您可以制作有效的 ChatGPT 提示，以推动引人入胜且信息丰富的对话。在接下来的章节中，我们将探讨利用这些元素制作清晰简洁的提示的特定技术。\n如何编写清晰简洁的提示\n现在我们已经探讨了制作清晰简洁的提示的重要性以及清晰沟通的要素，让我们深入研究一些编写有效 ChatGPT 提示的特定技术。\n\n定义对话的目的 和焦点  。在开始编写提示之前，重要的是要清楚地了解要通过对话完成的任务。您的目标是提供信息、回答问题还是进行随意交谈？定义对话的目的和焦点将帮助您制作一个具体且相关的提示，从而产生更具吸引力和信息性的对话。\n使用特定 和相关的语言  。为了确保 ChatGPT 理解您的提示并能够生成适当的响应，使用特定且相关的语言非常重要。避免使用可能导致混淆或误解的行话或模棱两可的语言。相反，目标是尽可能清晰简洁，使用与手头主题相关的语言。\n避免开放式或过于宽泛的提示  。虽然为了获得更全面的回答而提出开放式或过于宽泛的问题可能很诱人，但这些类型的提示通常会导致脱节或不集中的对话。相反，在提示中尽可能具体，为对话定义明确的目的和重点。\n保持对话正常进行  。当您进行 ChatGPT 对话时，重要的是要专注于手头的主题，避免引入切线或不相关的主题。通过使对话保持正常进行，您可以帮助确保它涵盖用户感兴趣的主题并提供有用且相关的信息。\n对话式 不断深入。对话式的找答案式人类理解和互相沟通的方式，循序渐进的问你关注的问题，不断深化，最后还可以让ChatGPT整个做个总结。对你\n\n通过遵循这些技术，您可以制作清晰简洁的 ChatGPT 提示。\n避免行话和歧义的提示\n编写有效的 ChatGPT 提示的关键挑战之一是避免行话和歧义。行话或专业语言可能会让不熟悉主题的用户感到困惑或不清楚，而歧义可能会导致误解或误解。为了帮助确保您的提示清晰易懂，请记住以下一些提示：\n\n定义任何行话或技术术语  。如果需要在提示中使用行话或技术术语，请确保为这些术语提供明确的定义或解释。这将有助于确保 ChatGPT 和用户在同一页面上，并且可以避免误解。\n避免使用模棱两可的语言  。对多种解释持开放态度的语言可能会令人困惑并导致误解。为避免歧义，请在提示中尽可能具体，并避免使用具有多种含义的单词或短语。\n使用清晰简洁的语言。 为了帮助确保您的提示易于理解，请尽可能清晰简洁。避免使用不必要的单词或短语，以免分散提示要点的注意力。\n\n通过遵循这些提示，您可以帮助确保您的 ChatGPT 提示清晰易懂，从而产生更具吸引力和信息丰富的对话。\n不好的例子：\n\n“嘿！你能给我一些关于互联网最新动态的情报吗？我正试图掌握时代精神。\n\n\n“Hey there! Can you give me some intel on the latest happenings in the interwebz? I’m trying to get a handle on the zeitgeist.”\n\n此提示使用行话（例如“intel”、“interwebz”、“zeitgeist”）而不对其进行定义，这可能会使不熟悉这些术语的用户感到困惑或不清楚。此外，“最新事件”一词的使用是模棱两可的，因为它可以指任何数量的事情，并且可以有多种解释。因此，ChatGPT 很难理解此提示并生成有用的响应。\n好例子：\n\n“巴黎最好的素食餐厅有哪些？我计划去巴黎旅行，我正在寻找一些满足我饮食需求的好地方。\n\n\n&quot;What are the best restaurants in Paris that serve vegetarian food? I’m planning a trip to Paris and I’m looking for some good places to eat that cater to my dietary needs.”\n\n此提示清晰而具体，使 ChatGPT 易于理解并生成适当的响应。提示指定用户感兴趣的特定位置（巴黎）和食物类型（素食），这有助于确保响应相关且有重点。此外，提示避免使用行话或模棱两可的语言，使用户易于理解。因此，此提示可能会导致更具吸引力和信息丰富的对话。\n构建有效的提示\n制作有效的 ChatGPT 提示的步骤\n现在我们已经探讨了制作清晰简洁的 ChatGPT 提示的原则以及避免行话和歧义的重要性，让我们深入研究制作有效提示的特定过程。以下是您应该遵循的步骤：\n\n确定 对话的目的和重点。在编写提示之前，必须清楚地了解您希望通过对话实现的目标。您想提供信息、回答问题或进行随意交谈吗？通过确定对话的目的和焦点，您可以制作一个具体且相关的提示，从而与 ChatGPT 进行更具吸引力和信息丰富的对话。\n使用特定 和相关的语言。为了确保 ChatGPT 理解您的提示并能够提供适当的响应，使用特定和相关的语言至关重要。避免使用可能引起混淆或误解的行话或模棱两可的语言。相反，努力尽可能清晰简洁，使用与手头主题相关的语言。\n避免使用开放式或过于宽泛的提示。虽然为了获得更全面的回答而提出开放式或过于宽泛的问题可能很诱人，但这些类型的提示通常会导致与 ChatGPT 的对话脱节或不集中。相反，在提示中尽可能具体，为对话定义明确的目的和重点。\n查看 并修改 您的提示。在将您的提示发送到 ChatGPT 之前，请花点时间查看和修改它，以确保它清晰易懂。考虑语言是否具体和相关，以及提示是否集中并避免歧义。\n\n通过执行这些步骤，您可以制作有效的 ChatGPT 提示，以推动引人入胜且信息丰富的对话。在以下章节中，我们将探讨一些高级技术，用于制作有效的提示和解决常见挑战。\n举个例子：\n\n定义对话的目的和焦点：此对话的目的是为罗马适合有小孩的家庭 的旅游景点提供建议。\n选择具体和相关的语言：“你能推荐一些适合有小孩的家庭的罗马旅游景点吗？ 此提示清晰而具体，使 ChatGPT 易于理解并生成适当的响应。\n避免开放式或过于宽泛的提示：此提示重点突出且具体，避免使用开放式或过于宽泛的语言，以免导致对话脱节或不集中。\n查看和修改您的提示：经过审核，此提示清晰易懂，并且侧重于罗马适合有小孩的家庭的旅游景点的具体内容。 无需修改。\n\n通过执行这些步骤，您可以制作一个有效的 ChatGPT 提示，该提示可以推动有关罗马旅游景点的信息丰富且引人入胜的对话，这些景点适合有小孩的家庭。\n引导对话朝着有意义的方向发展的最佳实践\n为了推动与 ChatGPT 进行引人入胜且信息丰富的对话，重要的是要清楚地了解您希望对话进行到哪里，并引导它朝着有意义的方向发展。以下是执行此操作的一些最佳做法：\n\n从清晰简洁的提示开始。正如我们之前所讨论的，重要的是要制作清晰简洁的提示来定义对话的目的和重点。通过从重点突出的特定提示开始，可以帮助确保对话保持正常并涵盖用户感兴趣的主题。\n鼓励 ChatGPT 扩展其响应。 虽然 ChatGPT 能够提供有用且相关的信息，但有时鼓励它扩展其响应以提供更深入的信息或深入研究相关主题会有所帮助。您可以通过提出后续问题或提供其他上下文或示例来帮助指导对话来做到这一点。\n注意对话中使用的语气和语言。 为了保持有意义和引人入胜的对话，重要的是要注意对话中使用的语气和语言。避免使用过于随意或不屑一顾的语言，因为这可能导致沟通中断。相反，以尊重和专业的语气为目标，并使用清晰易懂 的语言。\n监控对话的方向并根据需要进行调整。 随着对话的进行，重要的是要监控对话的方向，并根据需要进行调整以使其保持在正轨上。如果对话开始偏离主要主题，您可以使用提示或后续问题将其引导回更相关的方向。\n\n通过遵循这些最佳实践，您可以帮助引导 ChatGPT 对话朝着有意义的方向发展，并推动更具吸引力和信息丰富的对话。\n骇客艺术\n制作有效的 ChatGPT 提示的最有用的技术 之一是一些咒语提示词如：“装作”、“充当”、“假设”、”像个xxx“。此技术涉及在提示中使用”假假设”（”Act as”来告诉 ChatGPT 在对话中承担特定角色或角色。这对于创建更具吸引力和身临其境的对话或模拟真实场景特别有用。\n例如，您可以使用“充当”黑客告诉 ChatGPT“充当旅行社”，并根据用户的偏好提供度假目的地的建议。或者你可以告诉 ChatGPT“充当侦探”并解决虚构的犯罪。可能性是无穷无尽的，“充当”黑客可以成为创建引人入胜且身临其境的 ChatGPT 对话的强大工具。\n要使用“充当”黑客，只需包含短语“充当”，然后描述 ChatGPT 在对话中应承担的角色或角色。例如：“我希望你充当旅行社。你能根据我的喜好推荐一些度假目的地吗？\n通过使用“充当”黑客，您可以创建更具吸引力和身临其境的 ChatGPT 对话，这些对话是根据用户的特定兴趣和需求量身定制的。\n举个例子：\n\n我希望你充当JavaScript控制台。我将键入命令，您将回复JavaScript控制台应显示的内容。我希望你只回复一个唯一代码块中的终端输出，没有别的。不要写解释。除非我指示你这样做，否则不要键入命令。当我需要用英语告诉你一些事情时，我会通过将文本放在大括号内{像这样}来做到这一点。我的第一个命令是控制台.log（“Hello World”）;\n\n让我们深入研究这个例子：\n\n“我想让你充当JavaScript控制台。 这句话使用“act as”hack来告诉ChatGPT在对话中扮演JavaScript控制台的角色。\n“我将键入命令，您将回复javascript控制台应显示的内容。 这句话解释了用户在对话中的角色，以及 ChatGPT 在响应用户键入的命令方面的作用。\n“我希望你只回复一个唯一代码块中的终端输出，没有别的。” 这句话为 ChatGPT 提供了进一步的说明，指定它应该只回复一个唯一代码块内的终端输出，并且在其响应中不包含任何其他内容或解释。\n“不要写解释。” 这句话是对上一句指令的重复，强调 ChatGPT 不应在其回复中写任何解释。\n“除非我指示你这样做，否则不要键入命令。” 这句话为 ChatGPT 提供了进一步的说明，指定除非用户指示它不应键入任何命令。\n“当我需要用英语告诉你一些事情时，我会把文本放在大括号里{像这样}。 这句话通过将文本括在大括号中，为用户提供了如何用英语与 ChatGPT 进行通信的说明。\n“我的第一个命令是控制台.log（”Hello World“）;” 这句话提供了提示符的第一个命令，所以 ChatGPT 将首先运行。\n\n制作 ChatGPT 提示时要避免的常见错误\n制作有效的 ChatGPT 提示需要仔细考虑和关注细节。但是，很容易犯错误，从而妨碍提示的有效性和对话的整体质量。以下是制作 ChatGPT 提示时要避免的一些常见错误：\n\n用过多信息使提示信息过多 - 为 ChatGPT 提供足够的信息以了解对话的上下文和目的非常重要，但过多的信息可能会让人不知所措和混乱。请务必保持提示简洁明了，并避免包含不必要的详细信息或说明。\n使用行话 或模棱两可 的语言 - 使用清晰易懂的语言非常重要，尤其是在与 ChatGPT 等机器学习模型进行通信时。避免使用 ChatGPT 可能不熟悉或模棱两可的行话或语言。\n过于模糊或开放式  虽然开放式问题有助于鼓励更详细的回答，但过于模糊或开放式的提示可能会使 ChatGPT 感到困惑和难以理解。请务必提供足够的上下文和方向，以有意义的方式指导对话。\n忽略包含必要的说明或约束  - 向 ChatGPT 提供对话有效所需的任何必要说明或约束非常重要。例如，如果您希望 ChatGPT 充当特定电影或书籍中的角色，则应在提示中指定此角色。\n\n通过避免这些常见错误，您可以帮助确保您的 ChatGPT 提示清晰、简洁且有效。\n如何避免开放式问题和过多的信息\n在制作 ChatGPT 提示时，请务必避免包含过多信息或使用过于开放的问题，因为这些可能会使 ChatGPT 感到困惑且难以理解。以下是避免这些陷阱的一些策略：\n\n使用具体的、有针对性的问题而不是开放式的问题 - 与其问一个广泛的、开放式的问题，比如 “你对这个话题有什么看法？” ，不如试着问一个更具体的问题，专注于主题的特定方面。例如，“这种方法 的主要好处是什么？” 或 “你认为这种方法有什么挑战？”\n简明扼要 - 避免在提示中包含不必要的详细信息或说明。坚持基本信息，避免漫无边际或偏离主要主题。\n使用清晰、简洁的语言 - 仔细选择您的单词，避免使用行话或模棱两可的语言。请务必使用易于 ChatGPT 理解的语言。\n\n通过遵循这些提示，您可以帮助确保您的 ChatGPT 提示清晰、简洁和有效，并且对话顺利自然。\n保持清晰度和专注力的提示\n\n从明确的对话目标或目的开始。牢记特定目标将有助于保持对话的专注和正常。\n使用具体的、有针对性的问题，而不是开放式的问题。这将有助于将对话引导到特定方向，避免漫无边际或偏离主要主题。\n避免在 单个提示中包含过多信息。保持提示简洁明了，避免包含不必要的细节或说明。\n使用清晰、简洁的语言，便于 ChatGPT 理解。避免使用行话或模棱两可的语言。\n使用过渡短语从一个主题顺利切换到另一个主题。这有助于保持连贯性并保持对话顺畅。\n请注意 ChatGPT 的功能和局限性。 避免要求它做超出其能力的事情，并准备好在必要时调整提示。\n测试和调试提示，以确保它们清晰有效。重置线程，从头开始以帮助识别和解决任何问题。\n使用“充当”黑客来帮助 ChatGPT 了解其在对话中的作用。 通过指定它应该“充当”特定角色或实体，您可以为其提供明确的方向和指导。\n\n故障 排除\n使用聊天GPT时可能出现的常见问题\n使用 ChatGPT 时，您可能会遇到一些常见问题。以下是一些示例：\n\nChatGPT 无法理解提示或提供不相关或不适当的响应  如果提示不清晰、含糊不清或包含 ChatGPT 不熟悉的行话或语言，则可能会发生这种情况。如果 ChatGPT 缺乏理解提示的必要上下文或信息，也会发生这种情况。\nChatGPT 提供通用或无信息的响应  - 如果提示过于宽泛或开放，或者 ChatGPT 缺乏对主题的必要知识或理解，则可能会发生这种情况。\nChatGPT 不遵循提示中提供的说明 或约束 - 如果说明或约束不明确或与对话的总体目标不一致，则可能会发生这种情况。\nChatGPT 提供重复或不相关的响应  如果提示缺乏足够的指导或对话缺乏方向或焦点，则可能会发生这种情况。\n\n为了避免这些问题，请务必制作清晰、简洁的提示，为 ChatGPT 提供必要的上下文、说明和约束。了解 ChatGPT 的功能和限制，并测试和调试提示以确保它们有效也很重要。\n技术问题\n使用 ChatGPT 时，有时您可能会遇到技术问题或错误。以下是排查这些问题的一些提示：\n\n检查设备或浏览器 是否存在兼容性问题。确保 ChatGPT 与您的设备和浏览器兼容，并且您有稳定的互联网连接。\n使用各种提示测试 ChatGPT 模型，以查看问题是否仍然存在。 这有助于缩小问题的原因范围。\n检查日志或错误消息以获取有关该问题的任何信息。这些通常可以提供有关问题原因的线索。\n查看在线论坛或社区以获取建议或支持。可能还有其他人遇到过类似的问题 并找到了解决方案。\n\n通过执行以下步骤，您可以帮助在线解决 ChatGPT 的技术问题，并使其再次顺利启动和运行。\n案例研究\n在本章中，我们将探讨一些案例研究，说明如何有效地使用 ChatGPT 以及如何制作清晰、简洁的提示来实现特定目标。我们还将研究使用 ChatGPT 的最佳实践以及如何避免常见错误。\n案例研究 1：使用 ChatGPT 提高语言技能\n在本案例研究中，我们将了解如何使用 ChatGPT 来帮助提高语言技能。通过使用有针对性的提示并专注于语言的特定方面，例如语法、词汇和发音，ChatGPT 可以成为语言学习的有效工具。\n最佳实践：\n\n从语言学习课程的明确目标或目的开始。这将有助于引导对话并保持重点。\n使用特定的、有针对性的提示来关注语言的特定方面，例如语法、词汇或发音。\n鼓励 ChatGPT 提出问题或提供反馈，以保持对话的互动性和吸引力。\n使用“充当”（Act as来指定 ChatGPT 应该“充当”导师或语言教练，提供明确的方向和指导。\n\n案例研究 2：使用 ChatGPT 改善客户服务\n在本案例研究中，我们将研究如何使用 ChatGPT 来改善客户服务。通过提供清晰、简洁的提示并保持专业和有用的语气，ChatGPT 可以成为与客户互动并解决他们的需求和疑虑的有效工具。\n最佳实践：\n\n从客户服务交互的明确目标或目的开始。这将有助于引导对话并保持重点。\n使用特定的、有针对性的提示来解决特定的客户需求或顾虑。\n在整个对话过程中保持专业和乐于助人的语气。\n使用“充当”黑客来指定 ChatGPT 应“充当”客户服务代表，提供明确的方向和指导。\n\n通过遵循这些最佳实践，您可以有效地使用 ChatGPT 来改善客户服务并为客户提供积极的体验。\n案例研究 3：使用 ChatGPT 生成内容\n在本案例研究中，我们将研究如何使用 ChatGPT 生成用于各种目的的内容，例如社交媒体帖子、博客文章或营销材料。通过提供清晰、简洁的提示和保持一致的语气，ChatGPT 可以成为生成内容的有效工具。\n最佳实践：\n\n从内容生成的明确目标或目的开始。这将有助于引导对话并保持重点。\n使用特定的、有针对性的提示来关注内容的特定方面，例如语气、样式或目标受众。\n在整个对话过程中保持一致的语气，以确保生成的内容具有凝聚力和专业性。\n使用“充当”黑客来指定 ChatGPT 应该“充当”内容编写者或编辑者，提供明确的方向和指导。\n\n通过遵循这些最佳实践，您可以有效地使用 ChatGPT 为各种目的生成高质量的内容。\n成功的 ChatGPT 提示的真实示例\n在本章中，我们将查看用于实现特定目标的成功 ChatGPT 提示的真实示例。这些示例将说明清晰、简洁的提示如何帮助引导 ChatGPT 对话朝着有意义的方向发展并实现特定结果。\n示例 1：英语翻译和改进者\n\n提示：我希望您充当英语翻译，拼写校正者和改进者。我会用任何语言和你说话，你会检测语言，翻译它，并用我的文本的更正和改进版本回答，用英语。我希望你用更漂亮、更优雅的高级英语单词和句子代替我简化的 A0 级单词和句子。保持含义相同，但使它们更具文学性。我希望你只回复更正，改进，没有别的，不要写解释。我的第一句话是“爱伊斯坦布尔和城市”\n\n在此示例中，ChatGPT 用作英语翻译器和改进器，提供英语文本的更正和改进版本。提示是具体和有针对性的，清楚地概述了对话的目标和期望。使用“充当”黑客有助于为 ChatGPT 提供明确的方向和指导。\n示例 2：面试官\n\n提示：我希望你扮演面试官。我将成为候选人，你会问我这个职位的面试问题 。我希望你只以面试官的身份回答。不要一次写下所有的保护。我希望你只接受我的采访。问我问题并等待我的回答。不要写解释。像面试官一样一个接一个地问我问题，然后等待我的回答。我的第一句话是“嗨”position\n\n在此示例中，ChatGPT 用作面试官，提出问题并等待答案。提示是具体和有针对性的，清楚地概述了 ChatGPT 的角色和对对话的期望。使用“充当”黑客有助于为 ChatGPT 提供明确的方向和指导。\n示例 3：JavaScript 控制台\n\n提示：我希望你充当JavaScript控制台。我将键入命令，您将回复JavaScript控制台应显示的内容。我希望你只回复一个唯一代码块中的终端输出，没有别的。不要写解释。除非我指示你这样做，否则不要键入命令。当我需要用英语告诉你一些事情时，我会通过将文本放在大括号内{像这样}来做到这一点。我的第一个命令是控制台.log（“Hello World”）;\n\n在此示例中，ChatGPT 用作 JavaScript 控制台，为特定命令提供输出。提示是具体和有针对性的，清楚地概述了 ChatGPT 的角色和对对话的期望。使用“充当”黑客并包含有关如何与 ChatGPT 通信的具体说明有助于提供明确的方向和指导。\n示例 4：Excel 工作表\n\n提示：我希望您充当基于文本的 excel。 您只会回复我基于文本的 10 行 Excel 工作表，其中行号和单元格字母作为列（A 到 L）。第一列标题应为空以引用行号。我会告诉你要写什么到单元格中，你只会将excel表格的结果作为文本回复，没有别的。不要写解释。我会给你写公式，你会执行公式，你只会把Excel表格的结果回复为文本。首先，回复我空纸。\n\n在此示例中，ChatGPT 用作基于文本的 Excel 工作表，提供特定公式和命令的结果。提示是具体和有针对性的，清楚地概述了 ChatGPT 的角色和对对话的期望。使用有关如何与 ChatGPT 通信的具体说明有助于提供明确的方向和指导。\n示例 5：英语发音助手\n\n提示：我希望你担任土耳其语人士的英语发音助理。我会给你写句子，你只会回答他们的发音，没有别的。答复不能是我句子的翻译，而只能是发音。发音应使用土耳其拉丁字母进行语音。不要在回复上写解释。我的第一句话是“伊斯坦布尔的天气怎么样？\n\n在此示例中，ChatGPT 用作土耳其语使用者的英语发音助手，提供特定句子的发音。提示是具体和有针对性的，清楚地概述了 ChatGPT 的角色和对对话的期望。使用“充当”黑客并包含有关如何与 ChatGPT 通信的具体说明有助于提供明确的方向和指导。\n示例 6：旅行指南\n\n提示：我想让你充当旅行指南。我会写给你我的位置，你会建议一个靠近我的位置的地方。在某些情况下，我也会给你我将要去的地方的类型。您还会向我推荐靠近我的第一个位置的类似类型的地方。我的第一个建议请求是“我在伊斯坦布尔/贝伊奥卢，我只想参观博物馆。\n\n在此示例中，ChatGPT 用作旅行指南，根据特定位置和地点类型提供要访问的地点的建议。提示是具体和有针对性的，清楚地概述了 ChatGPT 的角色和对对话的期望。使用“充当”黑客并包含有关如何与 ChatGPT 通信的具体说明有助于提供明确的方向和指导。\n示例 7：抄袭检查器\n\n提示：我希望你充当抄袭检查员。我会给你写句子，你只会在给定句子的语言的抄袭检查中回复而不被发现，没有别的。不要在回复上写解释。我的第一句话是“为了让计算机像人类一样行事，语音识别系统必须能够处理非语言信息，例如说话者的情绪状态。\n\n在此示例中，ChatGPT 被用作抄袭检查器，提供特定句子的抄袭检查结果。提示是具体和有针对性的，清楚地概述了 ChatGPT 的角色和对对话的期望。使用“充当”黑客并包含有关如何与 ChatGPT 通信的具体说明有助于提供明确的方向和指导。\n要查看更多示例，您只需访问 https://prompts.chat 即可。\n\n结论\n正如我们在本电子书中看到的那样，为 ChatGPT 对话编写清晰简洁的提示对于成功和有意义的交互至关重要。通过制作有针对性的特定提示，您可以引导 ChatGPT 朝着您希望对话的方向发展，并确保输出相关且有用。\n编写有效的 ChatGPT 提示的一个关键技术是使用“充当”hack，它允许您指定 ChatGPT  在对话中应扮演的角色。通过清楚地概述对 ChatGPT 角色的期望以及您希望接收的输出类型，您可以为对话提供明确的方向和指导。\n除了使用“充当”技巧外，避免提示中的行话和歧义也很重要。通过使用简单、直接的语言并避免开放式问题，您可以帮助确保 ChatGPT 能够提供相关且准确的回答。\n最后，重要的是要记住，ChatGPT 是一种工具，与任何工具一样，它的有效性取决于使用它的人。通过遵循最佳实践来制作有效的提示并引导对话朝着有意义的方向发展，您可以充分利用 ChatGPT 并使用它来实现您的目标。\n总之，编写定义明确的 ChatGPT 提示需要清晰的沟通、特异性以及对工具功能和局限性的清晰理解。通过遵循本电子书中概述的提示和最佳实践，您可以制作有效的提示，帮助您充分利用 ChatGPT 并实现您的目标。\n关于定义明确的 ChatGPT 提示重要性的最终想法\n制作定义明确的 ChatGPT 提示对于与该工具进行成功和有意义的交互至关重要。清晰简洁的提示为 ChatGPT 提供方向和指导，帮助它产生相关且有用的输出。\n但是，定义明确的 ChatGPT 提示的重要性不仅仅是提高工具的有效性。通过制作有针对性的特定提示，您还可以帮助确保以合乎道德和负责任的方式使用 ChatGPT。\n例如，开放式或不明确的提示可能会导致 ChatGPT 做出意外或不适当的响应。通过避免这些类型的提示并注意您提出的问题类型，您可以帮助确保 ChatGPT 不会提供可能有害或令人反感的响应。\n此外，定义明确的 ChatGPT 提示也有助于提高沟通中的清晰度和理解力。通过为 ChatGPT 提供清晰具体的说明，您可以帮助确保输出相关且易于理解，从而有助于促进人与人之间更好的沟通。\n总体而言，定义明确的 ChatGPT 提示的重要性怎么强调都不为过。通过遵循本电子书中概述的提示和最佳实践，您可以制作有效的提示，帮助您充分利用 ChatGPT，并以负责任和合乎道德的方式使用它。\n掌握 ChatGPT 提示艺术的后续步骤\n现在您已经更好地了解了定义明确的 ChatGPT 提示的重要性以及制作有效提示的技术，您可能想知道掌握这门艺术的下一步应该是什么。以下是有关如何继续提高技能的一些建议：\n\n练习，练习， 再练习！  您使用 ChatGPT 并尝试不同提示的次数越多，您就越擅长制作有效的提示。\n寻求他人的反馈。请朋友或同事查看您的提示并提供建设性的批评。这可以帮助您确定需要改进的领域并完善您的技能。\n向他人学习。在线查找成功的 ChatGPT 提示示例，或向其他 ChatGPT 用户寻求建议和提示。您还可以加入专门针对 ChatGPT 的在线社区或论坛，向他人学习并分享您自己的经验。\n尝试不同的风格和方法。不要害怕尝试新事物，看看什么最适合你。您可能会发现某些技巧或方法对于某些类型的对话更有效。\n随时了解 ChatGPT 和人工智能的最新发展。随着技术的不断发展，ChatGPT 的功能也将不断发展。通过随时了解最新进展，您可以确保对 ChatGPT 提示使用最佳技术和方法。\n\n通过遵循这些步骤并继续学习和提高您的技能，您可以成为制作有效 ChatGPT 提示的大师，并充分利用这个强大的工具。\n\nepub版本：chatgpt-prompt.epub\n","plink":"http://www.ephesus.top/2023/ChatGPT-receipt/"},{"title":"写在蒂尔之书中的又一场悲哀的死亡","date":"2022-04-10T23:56:02.000Z","date_formatted":{"ll":"Apr 10, 2022","L":"04/10/2022","MM-DD":"04-10"},"updated":"2023-03-30T16:33:31.000Z","content":"原作：Kastaka\n译：Silencess\n作者寄语：\n致soundingsea。\n杰丝敏大步流星地踏上地牢入口氤氲潮气的石阶，身边陪着她的只有她可靠的小猫伊墨儿。她的身侧挂着母亲的佩剑；她的臂上戴着父亲准备的圆盾——那是他为她的旅程从矮人手上买来的。她身上还剩了把餐刀，但所有的光源和生火工具都已经毁在先前一场渡河事故中了。\n地牢的洞穴闪烁着诱人而奇特的光芒。一切都是那么轻而易举。杀死狗头人。杀死壁虎。杀死蝙蝠。搜索暗门。她饥饿地盯着她唯一的口粮——长途跋涉已经几乎让她的补给消耗殆尽了——可她还是改吃了那条壁虎和一种奇奇怪怪、看起来有害的地衣。伊墨儿独自打倒了一头郊狼，因此它毫无争议地享用了它的战利品。\n披甲的地精撩动着她的物欲，但她坚决克制了穿上那件地精盔甲的想法，唯恐那上面其实带有她所听闻的那些肮脏诅咒。她坚实可靠的盾牌尚能做好保护的工作。她也确信，自己更愿意把随意丢弃在地上的青玉戒指（旁边还有一大堆金币——那又是什么情况？）卖个好价钱，而不是冒险戴上它、迎接未知的馈赠或危险。除了这些以外，她还在地牢里听到过守卫的行进声和不知何人的数钱声，尽管在好一番搜寻后，这些声音仍然捉摸不定。\n在她差点刺穿一位霍比特人之前，她还没意识到这一切可能比她以为的更加复杂。她刚刚打开了一扇门，里面是个矮小的人形身影，她举剑就刺……\n“你干嘛呢？”霍比特人说，看起来有些恼怒。\n“噢。”她惊愕地回应，慌忙停下战斗姿态，“呃。抱歉打扰您了，霍比特人先生。”\n“你也确实该觉得抱歉。”那霍比特人抱怨着，“你们这些疯狂的冒险家，总是攥着凶器一路横冲直撞，下到这里来妨碍我们地下民族的正常生意。”\n“啊……”她说，“呃。你听说过岩德护符吗？”\n“哦，你也在做那个该死的愚蠢任务，是不是？”他回道，“又是带着小猫跑到这儿来的，我懂了。很遗憾，我实话跟你说吧小姐，你大概已经活不久了。在这地方我遇到过很多人，绝大多数都带着那种自负和青春的活力东奔西走，他们没一个能走多远的；只有那些回到这里安家立业的人才有希望，丫头。我劝你还是赶紧从地牢里逃出去吧，趁你还有机会。”\n“还有其他人在寻找护符？”她问，简直不敢相信自己的耳朵。\n“哦废话，”霍比特人说，“这种人一大群。不过我怀疑你一个都遇不上。都是独来独往的家伙，从来没见过哪里会同时出现两个。当然，死了的就更少了。”\n她还想问其他问题，但他很不耐烦地打断了她。\n“听着，我真是谢谢你没直接把我戳个窟窿，但我也有自己的生活要对付，明白？”\n“您还能告诉我什么事吗？”她绝望地发问，“任何能帮我完成任务的事都行？”\n“别喝泉水。”他撂下一句话，随后径直转身离开。\n她回想起之前楼层遇到的泉水，心在自责中一点点往下沉。泉水上层尝起来有些污浊，确实，但是第二口水还挺振奋人心的啊。长途跋涉了这么久，她真的需要清澈宜人的流水，需要洗掉嘴里那些尸体的味道。\n那次偶遇之后，她有一段时间几乎感觉自己像个英雄，一路踢开房门，还收集了一大把兽人匕首。但当她遭遇了某种僵尸时，令人眩晕的饥饿终究逼迫她咽下了最后一块口粮。现在地衣和黏液对她的口腹已经无能为力了。她狼吞虎咽地吃下两具丘陵兽人的尸体、差点噎住，随后才意识到自己刚刚把一块巨石推撞在一扇门上（也许是另一块巨石上），没办法把它移开了。她想哭。这会是她胜利进军的终点吗？就只是被岩石卡住，永远不可能完成任务？\n她试着从其他分支下楼，又路过了刚才那位霍比特人，但他只是怜悯地看着她。她幻想着冲上去摇晃他、逼问他从哪里来的食物，甚至做一顿烤霍比特腿来犒劳自己；但她守序的天性在脑海中大喊，于是她只是对他礼貌地点了下头。她烦躁地跳着脚等小猫靠近身边。等伊墨儿终于慢悠悠地晃过来，她立即从楼梯上冲下去，不敢与霍比特人悲哀的眼神对视。\n下面好黑。她摸着墙壁小心翼翼地选着前进的路，有水从很高的地方落在了她的头顶。侏儒们借着影子朝她投掷有害的药水合剂。她咽下它们中的一员，努力不去思考它有多么像人。伊墨儿叼回一串项链，她弯下腰摸了摸这可怜的小家伙。“我希望我有东西能喂你，”她轻叹，“可我没有。我连自己都喂不饱了。我还得背着这么多武器，我都不知道它们会不会有什么用处，也许有些其实只是垃圾。它们快把我压垮了。”\n她看到一个侏儒掉进了陷坑里，但它挡住了她的去路，因此她没办法留它听天由命。吞食尸体现在已经成了日常。她杀死一头小马，把它生吞活剥，模模糊糊地想着也许她本可以拥有一匹战马，要是她手中有一点胡萝卜的话。虽然即使真有胡萝卜也早该在路上被她吃掉了，何况现在还以为一个人能在这种深处保持文明开化的话实在过于好笑。她搞丢了伊墨儿，然后又遇到了一只小野猫，露着满嘴尖牙朝她哈气。有一瞬间她差点把它当成了她那忠诚的伙伴，还以为伊墨儿终于被那一路追踪他们至此的狰狞饥饿彻底野化了。但当她结果了这只小猫之后她又找到了伊墨儿，它看起来瘦骨嶙峋，却比之前还要健康。\n接着便是一场旷世大战，对手是个头超大的老鼠和蝙蝠群。当她终于擦掉眼中的鲜血、喘匀呼吸之后，她发现自己的小猫不再像小猫了，更像是一只独立自主的家猫。可怜的伊墨儿，要成长得这么快才行。\n不久之后，一个矮人穿过了她前进的路线。她对他呼喊，希冀着在这片漆黑的地方找到一个盟友。毕竟，矮人们以对蒂尔的侍奉闻名，不是吗？\n“你好，善良的矮人！”她喊。矮人也转过身来问候她。可令她万分惊恐的是，伊墨儿突然向这位绅士猛扑过去，开始激烈地攻击他，利爪在黑暗中闪着寒光。\n“伊墨儿，不！”她失声高叫，“坏孩子！快回来！”\n但伊墨儿完全没有理会她的呼号，依然极其狂燥顽固地吼叫着，杰丝敏只能无助地退开。她在绝望中误入了一个箭矢陷阱，而她甚至没怎么留意这件事。伊墨儿是被饥饿逼疯了吗，还是说她的猫猫朋友看出那个矮人其实是显而易见的敌人、而不是潜在的盟友了呢？他们俩究竟谁会赢呢？\n至少第二个问题很快就有了答案。伊墨儿从黑暗中窜了出来，志得意满地把那矮人曾经穿着的斗篷丢在杰丝敏脚下。杰丝敏麻木地拾起那件斗篷，走回那片黑暗去寻找矮人的遗体。他的尸体，唉，已经被撕成碎片、完全没用了，但他那双铁鞋还是完好无损的，而且——奇迹中的奇迹——他还带着一把鹤嘴锄。现在她可以回去弥补她的错误了，在之前的冒险中关于岩石的那个错误。\n“你真不应该这么做的，伊墨儿。”她责骂道，但猫猫早已经转头踏入那片黑暗，继续寻找下一个猎物去了。\n\n文中的绝大多数译名取自小鱼儿行者（SunnyYuer）的NetHack-cn repo（参见https://github.com/SunnyYuer/NetHack-cn） ，包括标题里的“写在蒂尔之书中”。除了猫猫（猫猫！）。\n感谢原作者Kastaka的授权。很难想象这是一篇关于NetHack的同人，但一切都非常合情合理、真实而残酷。我玩女武神的时候觉得基本是挥舞咖喱棒+银剑一路杀到星界无敌手，能从这样一个角度去重新看待战争的故事，确实也很受触动。有幸翻译，希望大家能喜欢这篇作品。\n继续感谢NetHack群的大家陪伴我飞升xd\n继续感谢Arlene Libitina姐的激励，这篇翻起来非常顺畅！\n特别感谢Facebook人工智能研究所的机器学习专家何恺明老师，是他提出了残差网络让我痛苦不堪因此才来做这个翻译（\n也非常感谢看到这里的你。\n","plink":"http://www.ephesus.top/2022/another_yasd/"},{"title":"“仅仅因为它写着‘READ ME’并不意味着你必须这么做”","date":"2022-04-10T15:36:15.000Z","date_formatted":{"ll":"Apr 10, 2022","L":"04/10/2022","MM-DD":"04-10"},"updated":"2023-03-30T16:29:42.000Z","content":"原作：molybdomantic\n译：Silencess\n\n摘要：\n​\t神谕每天都在做些什么呢？\n\n作者寄语：\n​\t致dizmo。\n​\t（更多寄语见文末）\n\n~~~~~\n我的日记\n~~~~~\n神谕\n~~~~~\n &nbsp;&nbsp;&nbsp;不许看！\n~~~~~\n拾到请归还至：命运地牢，7层左右，神谕层，神谕\n~~~~~\n1月1日\n亲爱的日记：\n​\t我决定开始写日记啦！你可是第一个知道这件事的哦（废话）。伊扎克教授当生日礼物送我的笔记本实在是太可爱了，我觉得我一定得用它做点真正有价值的事儿。再说了，现在可是新年，新年新气象，开启新计划再合适不过了。所以，大概就是这样，我不是正在写你嘛！之前我只能整天编造永恒的真理和语焉不详、直到“鉴明你所有的物品”之前都半懂不懂的预言，现在这种生活终于要改变了。琐碎的日常，我们来啦！\n1月2日\n亲爱的日记：\n​\t我还收到了其他的生日礼物，其中一件是弗拉德大公赠送的台历。台历的每一页上都写着一句不同的、据说很有智慧的格言，今天的格言是“土巨怪的凝视能混乱敌人.”我一定得在例行工作中实践一下试试！它们虽然看起来并没有什么用，但又好像很有帮助。顺便，昨天的格言是“我闻到了有扭曲的羊肠小道的迷宫的味道。”哈哈哈哈哈太好笑了我的天！\n1月3日\n亲爱的日记：\n​\t我和今天来的冒险者说“如果你迷路了, 试试下次在商店买张地图.”，她完全没觉得好笑。但她确实只付了我50金币啊，我都不知道她在期待些什么。最后我躲到喷泉后面去了，直到她离开才出来。\n1月4日\n亲爱的日记：\n​\t今天清理了一下喷泉，它们看起来有点死气沉沉的。拔掉一大堆水藻啊，把水池到处都擦擦啊，现在喷泉里的少女石像光亮得能照出我的脸了。也许我需要培养个兴趣爱好什么的？日历上现在写着“有人给坑里装尖刺!”——好像不是个很好的兴趣建议，这太恶意了。阿波罗在上，冒险家们的生活已经够艰难了。\n1月5日\n亲爱的日记：\n​\t今天没做什么大事，只按死了几只电子虫（烦人的跑来跑去的小东西）以及给一个金纳迦指了路。日历说“双锂晶体十分珍贵.”但这作为建议来说也挺糟糕的，因为我根本没时间也没钱去收集什么宝石嘛。还是省钱些的兴趣更好。\n1月6日\n亲爱的日记：\n​\t终于有一句有用的建议了！“据说美杜莎想要把你放在基座上.”我可以去学习石雕呀！这里到处都是石头，而且我还可以向美杜莎请教。还有还有，神庙如果多一点雕像的话看起来也会更庄严的。我在想，恩尼斯科西那里会不会有和这方面相关的书呢？我已经预见到一次矿镇之旅了。\n1月7日\n亲爱的日记：\n​\t今天打算徒步去矿镇买东西。虽然在找到需要的东西之前我不得不跑遍这儿所有的店铺，但我在阿西多霍波那里发现了鹤嘴锄（他不太愿意给我）和几张大地卷轴，所以也不算白跑一趟吧。“在商店里你最好看仔细商品的价签.”——太真实了，日历，简直太真实了。对了，我还去见了伊扎克教授，顺便多买了几根蜡烛来给我的工作室照明。\n1月8日\n亲爱的日记：\n​\t今天一个冒险者死在我家楼层了，他还没有准备好面对一头幼红龙。然后，我平时真的不会这么做的但但但是我刚好看到他背包里伸出来一截挖掘魔杖……“挖坟者有时候很有钱”，日历上也许会这么写。（而且它今天早上写的确实是这句话，真巧。说不定我就是因此才会想摸尸体的？）嗯，雕像计划可以开始了！\n1月9日\n亲爱的日记：\n​\t今天真是平静的一天呢，正好开始我的雕刻初体验。我选了电子虫作为雕刻对象，一部分原因是受到了日历的启发：“据说电子虫在你斜跨过它时不会攻击你.”另一部分原因是电子虫有漂亮的直线、形状也很简单，作为初次尝试非常完美。最后雕像看起来有点怪怪的，不过总体来说我还挺满意啦。\n1月10日\n亲爱的日记：\n​\t今天我想尝试另一种简单的形状——浮眼。唔，不太确定怎么才能让它浮起来，我的飘浮药水全用光了，浮眼显然也没法穿戴戒指或者靴子。“据说对于浮眼来说, 擦自己的脸是不可能的.”谢谢你啊，日历。\n1月11日\n亲爱的日记：\n​\t我以为自己已经相当有信心雕出第一个四足动物了。我让日历来帮我选材，它告诉我“你的狗变成石头的话你会极其伤心.”哎，比起拿一个鸡蛇蛋和一条狗来，我的雕刻方法要麻烦太多了，最后也搞得一团糟。开始考虑是不是干脆放弃雕刻、花钱雇一些石像鬼站在壁龛里算了。我真的不太擅长这个。\n1月12日\n亲爱的日记：\n​\t放弃之前再试最后一次吧。日历上说“据说龙鳞相当迷人”所以我就选龙……虽然但是好像目标有点定太高了。不过事实上最后看起来还不错！真是意外之喜。\n1月13日\n亲爱的日记：\n​\t邀请美杜莎来喝下午茶，“顺便”炫耀了一下我刻的石龙。我想那一定给她留下了很深刻的印象！她谈到自己偏爱更自然的造型，而且喜欢“捕捉事物的原本样貌”。唔，我猜这对她的某些雕刻对象来说是相当幸运的事。“据说在地牢里你通常不会有任何运气.”美杜莎还邀请我去她那层看看她的作品；有时间一定会应邀去的！\n1月14日\n亲爱的日记：\n​\t今天没什么事做（显然，已经一个星期没有冒险者来了）所以突然决定下楼去找美杜莎了。她的作品简直让我肃然起敬！看看那些神态各异、主题丰富的雕像，我猜这大概就是所谓的天分吧。（“嘘!  镜子就能解决!”）她那里还有一座特别漂亮的半人马像，看得我好眼馋哦，我想半人马说不定会和神庙很搭配呢。\n1月15日\n亲爱的日记：\n​\t决定了，我要给半人马一次机会。我觉得半人马真的可能挺适合的，他们姿态气宇轩昂又颇具细节，而且也不那么畸形可怕。至少得让神庙看上去稍微友好点呀！我可不想让冒险者以为这里是美杜莎层然后到处搞破坏。“据说矮人依法管好自己的事.”，但我不太敢相信每个人都有这么好心。\n1月16日\n亲爱的日记：\n​\t今天一直忙着刻石头。呼，累死了！没力气写东西了好困啊我需要睡觉。啊对了，第一个半人马快雕好了。今天日历也说了句非常有用的话呢：“据说一些石头比其他的石头重的多.”\n- - 更多 - -\n1月20日\n亲爱的日记：\n​\t\t最近真的好忙，都抽不出时间来给你写东西了！现在我已经做好了三座半人马像，还有一座正在雕刻中。另外，日历很委婉地提醒我需要找人来帮我把这些雕像摆对地方（“据说你能在轻松的时刻想出如何通过石头.”）不过那就不是今天要解决的问题了。\n- - 更多 - -\n1月25日\n亲爱的日记：\n​\t终于，所有半人马都完成了！我请一匹路过的小马帮忙把它们都拖到了适当的位置。“据说没人会注意巨石下面的垃圾.”，难以置信这附近已经脏成了什么样。我得好好来一次大扫除。\n1月26日\n亲爱的日记：\n​\t晚上狠狠吃了一顿大餐又睡了个好觉，心满意足。神庙现在看起来好可爱！！真的很开心我的努力能够有所回报。讲道理这简直太棒了，我甚至在考虑是不是应该办个正式的开幕晚会什么的然后邀请所有人来参观ww。以及，今天吹了一整天口哨，一个冒险者路过的时候我还告诉他“眼罩在你有感知能力的时候非常有用.”我真的想不出有什么比这更有用的建议了。愿他走得够远。\n1月27日\n亲爱的日记：\n​\t我之前和罗德尼提了一下关于聚会的想法，他也觉得很棒，所以我现在已经在好好做准备啦！今天去矿镇买了些幸运饼干和药水之酒，还DIY了一个好看的告示牌：“神庙：冒险者止步—私人聚会”。“据说上锁魔杖能锁的不仅仅是门.”，我在想我那根魔杖能不能把整个楼层都锁上呢？\n1月28日\n亲爱的日记：\n​\t聚会日！我希望大家能来参加……“据说德尔斐神谕知道蜥蜴尸体不会使人混乱”，但我真的不太知道今晚的聚会会有什么样的结果。\n1月29日\n亲爱的日记：\n​\t结果是聚会很顺利！所有人都来参加了，真不错，只是我有点担心自己吃太多了。“菠菜, 胡萝卜, 和果冻 –  一顿适合护士的菜单!”摩洛的高级祭司甚至都来看了看！我是知道他有多忙的。凯恩大师居然对我的作品赞不绝口，要知道他可并不是那种热情善良的人。弥诺陶洛斯也说，他都不知道我们地牢里已经有了一位初出茅庐的艺术家ww，还问我能不能有时间去看看他的迷宫、想想怎么能让它变得生动一些！另外，克罗伊斯还把我拉到一边，付了我一大笔zorkmid作为报酬呢。总之，一个非常成功的夜晚！\n1月30日\n亲爱的日记：\n​\t气吐了。下午来了个鼻涕精冒险家，他把我那些美丽的雕像全砸成了碎石子！这人还说自己想找什么法术书，我问他是不是想“鉴定他所有的物品”，因为他显然是在作死的道路上猪突猛进。“据说召唤的恶魔能提高你的游戏水平.”哼，要我说，这种自寻死路的无名小人才不会因此提高水平呢。我可希望恶魔能追着这家伙一路跑下楼，这样我连拖地的工夫都省了。\n1月31日\n亲爱的日记：\n​\t日历最开始的这个想法是正确的。我已经预定了8个雕像陷阱；接下来这堆雕像就都有能力还击了。“地牢深处有个幕后主谋.”——他们估计不太会怀疑那其实是我……至于明年呢？我会拜托弗拉德大公再送一本写着假传言的日历。\n\n作者后记：\n感谢“测试版”读者denisbloodnok和Ben。\n引语全部摘自rumor.tru文件。此文件是神谕所给出的小谕示的来源，因此这些语录都是她可能会在游戏里告诉你的话。文中大多数语录出自3.4.3版本（参见https://nethackwiki.com/wiki/Source:Rumors.tru ），但标题是3.6.0版本里的新增条目之一。\n（译者后记：\n​\t引语的译文全部摘自小鱼儿行者（SunnyYuer）的NetHack-cn repo中的rumor.tru文件（参见https://github.com/SunnyYuer/NetHack-cn/blob/NetHack-cn/NetHack/dat/rumors.tru ，部分语句为了通顺做了一定改动）。\n​\t感谢原作者molybdomantic授权我翻译这篇可爱的小同人，希望大家也喜欢它。\n​\t感谢狐萝卜催我发视频让我想起来还有别的坑没填（喂！）。\n​\t感谢Arlene Libitina姐鼓励我翻译更多感兴趣的内容。\n​\t感谢群主、狗佬、蜥蜴佬、咖喱棒、⑨、甘蔗和NetHack群里的大家陪伴我飞升——。\n​\t感谢论文让我不想写它（）\n​\t最后感谢你看到这里。）\n","plink":"http://www.ephesus.top/2022/just_readme/"},{"title":"","date":"2023-05-08T04:50:09.382Z","updated":"2023-03-30T14:53:54.000Z","content":"@font-face {\n  font-family: 'Baloo Bhaijaan';\n  font-style: normal;\n  font-weight: 400;\n  src: url(https://fonts.gstatic.com/s/baloobhaijaan/v16/RWmRoKCU5fcqq8fOWNzFLqSjx7EFLGE.ttf) format('truetype');\n}\n@font-face {\n  font-family: 'Consolas';\n  font-style: normal;\n  font-weight: 400;\n  src: url(https://fonts.gstatic.com/l/font?kit=X7nm4bA-A_-9jbjWaza9xMw&skey=3d1eb1871fcc58a1&v=v19) format('truetype');\n}\n@font-face {\n  font-family: 'Inconsolata';\n  font-style: normal;\n  font-weight: 400;\n  font-stretch: normal;\n  src: url(https://fonts.gstatic.com/s/inconsolata/v31/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8WR32kg.ttf) format('truetype');\n}\n@font-face {\n  font-family: 'Josefin Sans';\n  font-style: normal;\n  font-weight: 400;\n  src: url(https://fonts.gstatic.com/s/josefinsans/v25/Qw3PZQNVED7rKGKxtqIqX5E-AVSJrOCfjY46_DjQbMZhKg.ttf) format('truetype');\n}\n@font-face {\n  font-family: 'Montserrat';\n  font-style: normal;\n  font-weight: 400;\n  src: url(https://fonts.gstatic.com/s/montserrat/v25/JTUHjIg1_i6t8kCHKm4532VJOt5-QNFgpCtr6Hw5aX8.ttf) format('truetype');\n}\n@font-face{font-weight:400;font-style:normal;font-family:'方正像素12';src:url(\"fzxs12.ttf\") format(\"truetype\");}\n"},{"title":"Links","date":"2023-11-05T22:59:33.000Z","date_formatted":{"ll":"Nov 5, 2023","L":"11/05/2023","MM-DD":"11-05"},"updated":"2024-06-20T20:02:31.935Z","content":"一些友情链接！（有的是挂在我自己这里的\n\n黄小琥收集的大量席德梅尔文明系列资源，包含各个版本游戏文件、手册、地图、音乐、编辑器等\n飞鱼子的主页\nNLP八股1\n\n","plink":"http://www.ephesus.top/links/"},{"title":"","date":"2024-06-20T11:40:24.856Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:40:24.856Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/"},{"title":"Archive RL S","date":"2023-04-02T02:08:33.000Z","date_formatted":{"ll":"Apr 2, 2023","L":"04/02/2023","MM-DD":"04-02"},"updated":"2023-05-23T06:16:43.297Z","content":"这里存放着从2018年以来跨时空收集的许多传统roguelike游戏。但愿能帮到什么人吧。目前还没有搬完（还差得远呢），请等一等哦。\nMajor Roguelikes(“五大roguelike”)\nAngband\n沿承J.R.R.托尔金的中土大陆系列与古典rl MORIA的老牌rl，可能是有史以来分支版本最多的rl之一，其中不少分支版本都发展成了全新的游戏，如ToME和Sil。虽然在国内没什么名气，玩的人也不多，不过在霓虹发展相当好（也不知道为什么）。\n\nAngband 4.2.4, 2022-2-22\nAngband 官网\nAngband 开源repo\nAngband手册（epub版）\nAngband手册（镜像）\n\nNetHack\nThe Greatest Game You Will Ever Play!\n很难想象有什么比NetHack更加正统和传奇的roguelike游戏了，无数溢美之词在二十多年中被爱好者们传颂了一遍又一遍以至于我在这里重复都显得多余和言过其实。时至今日世界各地的玩家依然会参加属于它的赛事（TNNT,11月NetHack锦标赛），也有相当数量的分支版本和同人游戏不断涌现，其中包括我所见过最丧心病狂的游戏SlashEM Extended和相当适应现代世界的Pathos；NetHackwiki也是我见过最详细的游戏百科之一。祝愿这位老古董在新世界依然生生不息吧。\n\nNetHack 3.4.3 Amiga版\nNetHack 3.4.3 Windows版\nNetHack 3.6.1 Windows原版\nNetHack 3.6.1 Windows中文版\nNetHack 3.6.1 Android中文版\nNetHack 3.6.7 Windows原版\nNetHack 3.6.6 Android原版\nNetHack 3.6.7 Windows中文版\nNetHack 3.6.6 Android中文版\nNetHack 官网\nNetHack 开源repo\nNetHackWiki\nGridbug的NetHack助手（镜像）\nNetHack 3.6.x 音效包\nNetHack 桌游规则书\n\nADOM, Ancient Domains of Mystery\n作为在国内相当出名的JRPG rl——ELONA主要借鉴的原型，ADOM在许多方面都与五大rl的其他成员截然不同。是其中唯一的闭源单人开发游戏（当然图像版由HyperRogue作者ZenoRogue开发的NotEye引擎驱动）和唯一的多结局游戏，也是最剧情驱动和Role-Play感最强的游戏。\n在大部分老rl还满足于“主人公前往某些地牢打败某些怪物最终获得你也不知道有什么用的MacGuffin”的简单叙事时，Thomas Biskup却编织出一幅来自遥远大陆Ancardia的史诗画卷：这片雕琢自创世神Gaethra初梦后的第一滴泪水的世界里，在古老的群山Drakalor Chain中，诞生于创世神的噩梦、蛰伏了无数年的混沌力量卷土重来，而这一次它由不朽的远古混沌神Andor Drakon带领。没有人知道这只是又一场噩梦、还是创世神疯狂的前兆。在众多前赴后继的英雄中，你，作为诞生于和平宁静的第三纪的生灵，将会深入群山，从山中小镇Terinyo开始，寻找藏于深山中的混沌之穴底部那扇通往疯狂的混沌之门——在这一过程中，你可以感受到世界和你自己实实在在的变化，你的阵营会真的随着你对待世界的态度发生转变，而世界也会因你的选择而选择怎样对待你。你可以学习各种技术，从有益于战斗的近战远程魔法，到充满生活气息的烹饪、阅读、估价，甚至于不知道怎么评价的造桥、冶炼、腹语；你可以洁身自好乃至于鞭笞苦修、同时收获平凡民众与英雄们的善意，也可以大杀四方、将仇恨你的低贱生命生吞活剥；你可以挽救破碎的家庭、唤醒濒死的先驱，也可以溜门撬锁、雁过拔毛。甚至到了命运的大门前，你仍然可以选择，选择成为救世主，或是走上另一条未知的道路……ADOM的沉浸感很强，强到你也许真的认为自己在那个世界活着。\n故事之外，ADOM也有许多独创性的设计：它在传统的食物clock之外创造性地加入了混沌入侵的世界clock，在混沌环境下或是被混沌攻击命中累计到一定程度就会获得各种好坏参半的混沌变异，混沌到极致便会成为一团无定形的紫色黏液消失于世间；它的信仰系统独具特色，你可以给自族的神灵献上祂们最喜爱的礼品，甚至还可以呼唤神力活祭与你立场不同的敌人，而有智慧的敌人也不会放过任何一个活祭你的机会；它的武器复杂多样而又相对平衡，又有许多词条武器甚至神器混杂其中，其中不少神器甚至无法用肉眼辨别。与NetHack类似，ADOM是个喜欢藏信息、喜欢instakill的游戏，虽然深度不及NetHack但流程之长、内容之广阔远非前者可比，游戏体验绝对是独特而美妙的。\n(注意，ADOM和Ultimate ADOM是两个游戏，后者虽然称为前者的正统续作但仅有Caverns of Chaos一个地牢，内容大打折扣，Thomas更是看销量不佳直接结束EA圈钱跑路了，请不要混淆)\n\nADOM 3.3.3 Classic Windows字符版\nADOM 3.3.3 Classic Windows贴图版\nADOM 3.3.4 Deluxe版(Steam商店页)\nImproved ADOM Guidebook（镜像）\nADOM 官网\n\nDCSS, Dungeon Crawl Stone Soup\nDCSS与前几位相比要年轻不少，不过游戏性并不逊色几分，甚至某些意义上还犹有过之。作为NetHack的直系后辈之一（其前身Linlye’s Dungeon Crawl是Linley Henzell在沉迷NetHack后自学C语言开发而来），DCSS在许多方向上与NetHack走上了完全不同的道路：DCSS专注于每场遭遇中紧张而充满策略的战斗体验，摒弃了NetHack复杂累人的生存要素（甚至包括食物系统）与挫败感极强的Instakill；DCSS完全放弃了NetHack与ADOM那样重记忆、考验Spoiler的设计理念，堂堂正正地把所有信息交给玩家，同时也敢于不顾及玩家喜好大刀阔斧地做减法。如今在DevTeam和玩家们的努力下，DCSS即将迎来第30个正式版本/第一个官方中文版本，对国内的roguelike爱好者来说或许也是一个很好的体验机会。\nEdit: 2023年5月5日DCSS 0.30发布了！\n\nDCSS 0.28.0 Win32 字符版\nDCSS 0.28.0 Win32 贴图版\nDCSS 0.29.1 Win32 字符版\nDCSS 0.29.1 Win32 贴图版\nDCSS 0.30 Win32 贴图版（已配置汉化）\nDCSS 0.30 MacOSX 贴图版\nDCSS 0.30 Linux贴图版\nDCSS 0.30  Android贴图版\nDCSS 官网\nDCSS 开源repo\nCrawlWiki\n0.30如何配置汉化（贴吧） （豆瓣）\n\nToME\n严格来说ToME不是一个游戏，而是DarkGod主导开发的一系列游戏的统称：Troubles of Middle Earth(ToME1)，Tales of Middle Earth(ToME2)，T-engine 3(游戏未命名，ToME3)和Tales of Maj’Eyal(ToME4)。\nToME1, Troubles of Middle Earth\nToME1的前身是DarkGod早年基于ZAngband 2.2.0制作的Angband分支PernAngband，其原本世界观建立在安妮·麦卡芙瑞的奇幻小说《龙骑士：波恩年史》中的Pern星球上，夹杂了许多ZAngband中的《安珀志》和CthAngband中的克苏鲁神话内容。随着其在网络上知名度的提升，在PernAngband更新到5.x.x版本时，DarkGod收到了来自安妮本人和育碧公司（授权制作了许多Pern世界观的游戏）的版权警告——于是他删除了几乎所有与Pern相关的内容，改回了Angband的原始设定，即J.R.R.托尔金的中土大陆，并将游戏更名为Troubles of Middle Earth。ToME就这样诞生了。\n\nToME 1.0.0 DOS版\n\nToME2，Tales of Middle Earth\n我也不是很清楚ToME1和ToME2究竟有什么决定性的区别——DarkGod本人似乎对此也没有什么区分，在老版本的ToME官网和RogueBasin上这两者都有点混用的意思。比较确切可知的是ToME 1.0.0 大约发布于2002年5月5日，而仅仅两个多月后的2002年7月23日DarkGod就马不停蹄地发布了ToME 2.0.0，大概真的只是改了个名字吧hhhh。有个比较有意思的事情是这样的：我们都知道”五大Major Roguelikes“这个概念来自RogueBasin，但我们不知道的是RogueBasin的Major Roguelikes页面一开始只有ADOM、Angband、NetHack、Crawl这四款游戏。那ToME是怎么进来的呢？是DarkGod自己加进去的（笑死）。当然，在这之后并没有人试图将ToME从这个名单上删除，从侧面也说明了ToME确实有这样自称的实力。（后来ZAngband之类的游戏也短暂在这个名单上停驻过一段时间，但很快就有人反对并编辑掉了。）\n\nToME 2.2.0 DOS版\nToME 2.3.5 Windows版\nToME2 开源repo\n\nToME3，难产的T-engine 3\n关于ToME3的信息少之又少，RogueBasin上根本没有它的页面，ToMEwiki也只是将其一笔带过，称其还未开发完便为后来的T-engine 4让道了，甚至连正式名称都还未确定。如今能找到的ToME 3.0.0仅仅是一个半成品，它可以运行、创建人物和开始基本的游戏，甚至有背景音乐和简单的ASCII动画，但是绝大多数文档都还未开始动笔以至于你找不到多少有用的游戏帮助。但它的系统中已经能看到一些今天ToME4设定的雏形了，像是技能大系、技能点分配、职业细分之类的。或许在另一个世界线上这就是ToME的发展方向，谁知道呢。\n\nToME 3.0.0 alpha19 Windows版（半成品）\n\nToME4, Tales of Maj’Eyal\n如果说ToME3是ToME系列脱离Angband框架的一次失败尝试，那么ToME4则是它幻想中的另一个模样。T-engine 4抛弃了Angband笨重复杂的全C语言，仅仅将其作为坚实的地基，而选择拥抱灵活、轻量级的lua脚本语言将游戏做了最彻底的重构；ToME4也放弃了中土大陆的故事模板，取而代之的是DarkGod自己创作的、全新的Maj‘Eyal世界。ToME4的UI不再是古老的ASCII字符，而是SDL绘制、可以全鼠标操作的现代化GUI，玩法上也充分学习另一位商业上成功的Angband后代——暗黑破坏神（Diablo），加入了令人沉迷的技能树和词条装备。为了充分突出build-porn的快乐，ToME4甚至还删除了食物clock（与DCSS不同，它似乎没有添加任何其他clock）让玩家可以自由地刷掉落。这样的改进无疑是成功的，ToME4在2010~2012连续三年拿下Andrew Doull的ASCII Dreams举办的the Roguelike of the Year评选，也吸引了一大批原本并非核心Roguelike爱好者的玩家前来了解这个有趣的类型。\n（有点讽刺的是，2013年ToME4和ADOM疑似用bot刷票，在十几天之内各自获得了数千票数，这引起了许多玩家的不满，最终导致一个莫名其妙的成人游戏Noxico在三天之内刷出5000票终结了这次不正常竞争——也终结了the Roguelike of the Year这一延续了七年的爱好者评奖活动。）\n\nTales of Maj’Eyal 1.7.4 Windows免费版\nTales of Maj’Eyal(Steam商店页)\nTales of Maj’Eyal 官网\nTales of Maj’Eyal 开源repo\nTales of Maj’Eyal Wiki\n\nMajor Classic Roguelikes(五大古典roguelike)\n(学校里通常会把classic/classical按经典/古典区分，但一方面这两个翻译并不是特别一一对应，另一方面我想强调它们的古老，而且”经典roguelike“这个表述实在是太烂大街了)\nRogue\nRogue当然是roguelike，谁说不是呢？这款脱胎于DND、受益于curses库、在4.2 BSD系统的流行中风靡全美大学生，又在一代又一代游戏人的推陈出新中享誉全世界的游戏在四十多年中开枝散叶，将这种独具魅力的游戏类型带给我们。\n分享一个Rogue开发过程中的有趣冷知识（来自地球科学学者、Roguelike爱好者Cassie Stuurman，Roguelike Celebration 2017）：在Rogue中，玩家如果饮用了看见隐形药水会提示“这尝起来像是黏菌果汁”，黏菌从这里开始被许多rl反复致敬，成为玩家喜爱的食物（如NetHack）或是需要战胜的敌人。然而为什么是黏菌呢？在机缘巧合Cassie下采访了Rogue的作者之一Glenn Wichman，才解开了这个困扰她许久的疑问：在Rogue创作期间，两位原作者Michael Toy和Glenn Wichman还是加州大学圣克鲁兹分校的学生。当时校区食堂后的排水管上长了团黏菌，于是“将黏菌引入不相干的话题”成为学生们的流行笑话，而他们将这个笑话做进了自己手边的游戏，事情就这样成了。\n顺带一提，RogueBasin和Temple of the Roguelike站长Slashie在今年（2023）的Hidden Heroes Jam中提交了一个名为“Broken Connections”的小游戏，以这种形式描绘了Rogue诞生过程/两位作者创作历程的一个缩影。（为了方便网络不畅通的朋友，我在这里也部署了一份hhhh）\n\nRogue 1.0 TRS-80 CoCo3 Epyx发行版\nRogue 1.0 DOS版\nRogue 1.49 DOS Epyx发行版\nRogue 3.6.1 DOS版\nRogue 5.4.4 Windows版\n基于Rogue 5.3 Clone的Windows版\nRogue（Steam商店页，Epyx都倒闭了也不知道这个发行商从哪来的授权）\nRogue 5.4.4 开源repo（并不是活跃的开源项目）\nRogue Epyx版手册（镜像）\nRogue Epyx版手册（pdf版本）\nDonnie Russell II的JSRogue（镜像）\n神秘的Rogue网页重制版\n如何在虚拟机上玩4.2 BSD版 Rogue\nBroken Connections：因为硬件故障，Rogue将要胎死腹中了，快来帮助Michael和Glenn拯救它吧！\n\nMoria\n（我们所讨论的Moria是1983年首次发布在VMS操作系统上、由Koeneke最初开发的Roguelike游戏。尽管拥有相同的名字与相似的游戏背景设定，PLATO操作系统上1975年出现的MORIA并不是Roguelike游戏，和前者也没有任何开发上的联系。）\n尽管理论上最早符合roguelike特点的游戏是1978年Apple II上的Beneath Apple Manor，我们仍然可以说Moria是最早的“Rogue-like”游戏——第一个模仿Rogue制作的游戏。它同时也是最早引进可供补给修整的“城镇”概念的roguelike游戏和世界上第一个开源的roguelike游戏。\nMoria最早由Richard Alan Koeneke在俄克拉荷马大学（下称OU）的VAX 11/780机器上使用VMS BASIC语言开发。身为OU电子工程专业的学生和自中学起的资深游戏开发者，Koeneke时常泡在实验室研究当时流行的计算机游戏，在1981年左右游玩了Rogue并被深深吸引了。当年发生了两件“大事”促使了Moria的诞生：一是OU机房管理层认为学生过分沉迷游戏、影响学业，禁用了学生对电脑游戏的访问权限，导致Koeneke无法继续游玩机房PDP 11/70上的Rogue；二是Koeneke在OU计算机系找到一份助理工作，这使他得以管理数学系的微机、帮助学生使用VAX 11/780上的计算机应用。最终，在痛苦的Rogue戒断反应中，Koeneke盯上了当时还不支持任何一款游戏的VAX 11/780，决定从零开始开发一款“自己的Rogue”。1983年，Koeneke接触到更适合开发大型软件的VMS PASCAL语言，在学习过程中重写了Moria并于1983年发布了1.0版本，此后的数年又坚持开发到4.8版本，直到他被美航聘为程序员、全心忙于工作为止。Koeneke非常感激Rogue（以及文字冒险地牢游戏Colossal Cave Adventure）在他程序和游戏开发生涯中的启发与帮助，希望更多人能从他的代码中学到他所学到的知识和技能，因此决定将Moria的代码全部开源。\n继承Rogue的程序生成地牢、永久死亡、ASCII画面的基本特点之外，Moria加入了Koeneke独创的相当多特点（有些甚至是这些特点首次问世）：它建立在Koeneke热爱的J.R.R 托尔金的中土大陆背景上（Koeneke本人阅读LotR十遍以上，并作为dm带过Moria矿坑为背景的DND团），致力于还原书中Moria矿坑不祥的氛围与惊人的广阔地形；它将Rogue短暂的单向26层流程拓展为可来回的50层以上，每层占据了相当于五六个物理屏幕的巨大虚拟窗口，且任何一次上下楼都会重置之前的楼层；它在Rogue的26字母怪物基础上扩展为包含大小写的48个字符，每个字符还有不同颜色的变种以表示不同元素特征或职业、共计有上百种怪物；它将Rogue的简单回合制改进为计算速度的复杂回合制，让战术性变得更加复杂多样；它创造性地加入了初始城镇层并在其中开设商店、搭建了经济系统（甚至还可以讲价！），为Rogue中无用的金币赋予了新的价值。有趣的是，Koeneke致力于让自己的游戏变得富有挑战、难以通关，而当玩家显著超越同期测试的其他成员、甚至通关了游戏时，他会将玩家的角色名称和特点吸收为游戏中的独特新敌人作为纪念，同时进一步更新以削弱胜利玩法。他的友人William Ouchark曾利用最终Boss巴洛炎魔（Balrog）的避战机制、使用可穿墙的火球术击杀墙后的炎魔获得胜利，Koeneke便把Ouchark的角色——侏儒Iggy——变为游戏后期的强大怪物The Evil Iggy，并在下一个版本中禁止火球术穿墙释放。\nMoria的地位是毋庸置疑的。在Koeneke仍在OU就读期间，除了参与测试的OU校友同学们的支持外，他还收到了来自全美许多高校、乃至世界各地玩家（甚至有铁幕后的东欧国家，如罗马尼亚和匈牙利）的信件，希望能获得Moria的游戏副本。（在给德克萨斯大学的游戏副本里，Koeneke出于某种幽默感恶作剧地加入了城镇中名为OU Fans、可以快速复制自身的特殊怪物，并将游戏变为即时制，结果德克萨斯州的同学们启动游戏后惊恐地发现城镇中刷出了数之不尽的强大敌人，游戏完全无法进行。）Koeneke将Moria开源后，一些爱好者使用C语言重构了它（称为Unix Moria，也就是UMoria），并迁移到包括Unix、MS-DOS、Amiga、MacOS、雅达利ST等平台，一直开发到今天。在角色扮演游戏中，Moria早在80年代初期就实现了许多80年代末游戏都难以比肩的怪物设计、魔法和战斗系统。CRPG Addict为Moria打出了38分的高分，在1985年前的游戏中只有Ultima III（51分）高于这个评价。Moria启发了许多游戏，其中最广为人知的一部作品是北方暴雪开发的暗黑破坏神（Diablo）；由于其众所周知的影响力，我们甚至可以说今天任何一部存在战利品系统的游戏实际上拥有的都是源自Moria的战利品系统。在Roguelike品类中Moria的后继也相当多（如Angband和BOSS），一部分玩家甚至将Roguelike不严谨地分为Morialike和Hacklike两个子类型。连Michael Toy，Rogue的开发者之一，也在一次采访中坦陈“Moria就是他想象中Rogue理想的样子”。\n哦对了，我原以为Moria的50层杀巴洛炎魔在Angband的一百层杀魔苟斯面前相形见绌，但从CRPG Addict的战报中我第一次了解到Moria的50层不是矿底、只是巴洛炎魔开始可能出现的位置；在某个基于Moria 4.8的分支版本中，50层甚至只会刷新伪巴洛炎魔，而真正的关底boss将首次出现在第1200层（草）。大为震撼。\n\nMoria 4.8 VMS版（希望有人能运行）\nVMS Moria 4.8 源码\nVMS Moria 5.0 源码\nMoria 5.52 386Windows版\nUMoria 4.873 DOS版\nUMoria Amiga版，忘记版本号了\nUMoria 4.87 Atari ST版\nUMoria 5.2.2 Atari ST版\nUMoria 5.5 Atari 8 bit版\nUMoria 5.5.2 MacOS版\nUMoria 5.7.11 Windows版\nUMoria官网\nUMoria开源repo\nVMS Moria开源repo\n存活至今的Moria资源站\nDonnie Russell II的JSMoria（镜像）\nMoria手册（txt版）\nThe Moria Spoiler（pdf版）\nThe Moria Spoiler（镜像）\nMoria Monster Memory Lookups\n\nHack\nHack好难写……\n它不像其他老rl那样有一个稳定明确的“本体”、可以把其他分支都抛开到自己的章节去讲，而是分成好几个阶段，每个阶段都有各自的开发者、特性和影响，而你又没法说它们谁不是Hack。另一方面Hack和NetHack的关系实在太紧密了——NetHack之于Hack就像UMoria之于Moria，甚至可以说NetHack也是Hack的一个阶段，但NetHack的名气又太大了必须得单开一节。结果我现在想起Hack就觉得自己会写成NetHack发展史……\n总之还是写hack发展史吧。（原本只是想做下载站，为什么会变成这样呢）\nJay Fenlason’s Hack\nHack n.（尤指计算机方面）权宜之计；v. 砍\n这是Jay Fenlason为自己的Rogue复刻项目命名的两个寓意。前者是因为他当时玩不到Rogue，只能自己想办法；后者则是取自“hack-n-slash”（砍杀，≈踢门团）这一DND游戏风格，这也是Rogue本身所展现的风格。\n彼时Fenlason正在林肯萨德区域高中（LS）就读，是学校“电脑帮”的核心成员和小有经验的UNIX开发者。在高二这年暑假，Fenlason受邀前往旧金山州立大学担任暑校助教，期间在加州大学伯克利分校（UCB）第一次玩到Rogue，几乎立刻痴迷于它那前所未有的复杂系统和沉浸感（后来有一天又自己溜到UCB玩了好几个小时）。从加州回到麻省以后，无法游玩Rogue的Fenlason决心重造轮子，与电脑帮的伙伴们重现这款游戏。\n要纠正一个我之前疏忽产生的错误说法：Hack并不是Fenlason在USENIX 1982向Rogue作者讨要源码失败后才制作的。根据Fenlason的回忆，最早的Hack原型是在Apple II上用LOGO语言（就是小学电脑课上学的小海龟画图语言！）写成的，随后又用C语言移植到LS机房的PDP11/70上继续开发。不过Michael Toy和Ken Arnold的确在1982年的USENIX会议上发表了讲话（顺带一提，讲话主题是《Rogue：它已到何处，为何在此，为何从一开始就不该如此》），电脑帮的另一名成员Michael Thome也的确记得他们曾经请求其中一人提供源码、希望能帮忙修复游戏中的bug，而后者礼貌地拒绝了他们（Arnold说因为他们当时想看看能不能用Rogue赚点钱）。\n实际上Fenlason也并不需要Rogue源码的帮助。在Fenlason结束对Hack的开发工作时，他已经完成了自己所设想的“一个更好版本的Rogue”的绝大多数特性，如更多的怪物与更复杂的怪物行为、更大的背包空间、更复杂的地图生成；他也无私地邀请电脑帮的朋友们加入Hack的开发团队，为Hack提供了诸如可以变成其他怪物的变色龙（是的，在最早的Hack版本里它就存在了！）、地牢底层的迷宫层等灵感。只有少部分特性是身为高中生的Fenlason未能实现的，比如超过屏幕大小的可探索地图（Moria：）和非随机的预设地图。\n非常可惜，我们今天没法在互联网的任何地方找到JF’s Hack本体的代码或可执行文件，它的源代码被存放在Fenlason的一盘磁带中，与电脑帮的另一名成员Jonathon Payne的Emacs文本编辑器JOVE一同寄给了USENIX 1984会务组供参会者学习和改进。此外，在2011年一篇关于Hack的赛博考古博客下面，有一个自称Robert Grover的评论者（2013年评论）声称他曾在1985/86年与Jay Fenlason合租过，后者送给他一张装有JF’s Hack的5.25英寸软盘，但他现在无法读取它了。他在评论区留下了联系方式，希望有人能帮助他提取这份文件，可时隔十年也没有第二条评论……刚刚给他发了封邮件，希望不大但祝我好运（呃）。\nHack 1.21\n我很早就收了这个文件，但一直以为它就是Hack的某个早期版本，很晚才发现它是没有出处的神秘游戏！它没提到作者，没有版权声明，没有标题画面，互联网上也没有任何已知的它的源代码，最后修改时间是2000年4月26日但显然不会是这个时候制作的。在游戏中按Ctrl+V可以看到一行“Hack version 1.21 (Slak was here!)”，Slak是Jay Fenlason的网名但他在接受Dungeon Hack的采访时也没提到过这个东西。NetHackWiki将其描述为“16色CGA模式的Jay Fenlason’s Hack的DOS移植”（颜色很好看！比Eypx Rogue还好看），但不知道移植者是否对它做过什么内容上的改动（因为我们根本不知道Jay Fenlason’s Hack里到底有什么）。\n之所以猜测它是Jay Fenlason’s Hack的移植，也许是因为它具有这样一些特征：它启动时会进入一个仅有UI的商店，供玩家购买进入地牢的物资，Robert Grover说这一特点和JF’s Hack完全一致，而Andries Brouwer很早就把这个特性从他的Hack改版中删除了；它没有开局的角色选择和初始自带的宠物，这两点都是Andries Brouwer’s Hack一发布就存在的特性。这有点好笑和反直觉，Hack 1.21的出现时间或许还要早于Hack 1.0。\n在短暂的试玩过程中，我还体会到Hack 1.21中一点其他Hack版本不存在的有趣内容。主角的视力极大程度上受到光照的限制，比Rogue中走廊贴身才能看到怪物更加夸张，没有携带光源的话在黑暗中连被打了也不知道是谁在攻击自己，甚至还会看不见自己背包里原本装的卷轴药水法杖究竟是哪一种（连cdda都不敢轻易采用的设定.jpg），这使得光源提升到和食物一般的重要程度；而光源也真实到近乎折腾，你不仅需要携带火把或油灯（油灯还需要手动灌灯油），还需要打火石来点燃它们，没有经验的玩家大概率会在购买物资时便败局已定。NetHackWiki评价Hack 1.21可能是个很难的游戏，看来所言非虚。\n\nHack 1.21 DOS版\n\nAndries Brouwer’s Hack（Hack 1.0.x）\n当Jay Fenlason寄出那份包含他心血的磁带时，时年33岁的数学家、程序员Andries Brouwer正在荷兰数学中心（后称荷兰数学和计算机科学研究学会，CWI）任职，那是全欧洲最先连通计算机网络的设施之一。在翻阅机构收到的大量文档时，Brouwer无意间邂逅了那盘装有JF’s Hack的磁带。被这份前所未见的游戏所吸引，作为CWI夜生活的“游戏大师”的Brouwer立刻决定将其安装在CWI的许多台微型计算机上供同事游玩。Hack迅速成为CWI研究者间的热门话题，而出于其计算机学术背景，Brouwer个人对编辑和改进这款游戏的代码更感兴趣。他仔细聆听同事们茶余饭后的讨论，从中提取最创意十足的灵感并加以实现。许多如今NetHack广为人知的特性便诞生在这个时期，例如：\n\n宠物系统（开局宠物是一条狗，向宠物移动时不会尝试换位而是直接攻击宠物）\n开局角色选择（可选角色有游客、洞穴学者（后来的考古学家）、战士（后来的野蛮人）、骑士、穴居人和巫师，当时还不存在种族和职业的双重划分，且无法选择角色性别；比较特别的是，由于魔法系统尚未出现，巫师并不能释放法术，而是在开局时带着一大堆药水卷轴法杖）\n商店以外的特殊房间（金库和动物园（David’s Treasure Zoo）；其中David是Brouwer曾经的同事，他的学生在早期店主无限金钱、不会生气的版本偷商店刷了一大笔钱，说服Brouwer将店主改为财产有限且会生气的版本；在早期版本给自己取名为david可以显著增加动物园的刷新率）\n如今存在的大部分卷轴药水法杖和食物（蜥蜴尸体在这个版本已经存在且可以解除混乱，但当时鸡蛇的石化攻击会瞬间生效、直接结束游戏，蜥蜴尸体并不能挽救被石化的玩家；蜥蜴这种怪物并不存在）\n特殊的怪物机制（如蠕虫和紫蠕虫，玩家需要躲避头部的袭击并不断攻击其尾部，从中间攻击则会使虫子分裂成两条）\n神器！（只有Orcrist“杀兽剑”；任何武器命名为Orcrist都能对兽人附伤d10，可以同时存在好多把）\n\n1984年12月，Brouwer将他改进的Hack版本命名为Hack 1.0，通过当时还很缓慢、不稳定的Usenet网络连接发布到net.sources新闻组（因为网太差了所以分成14份发）供全网用户下载、自行编译和游玩。Hack的趣味和易用性在小小的Usenet中卷起了风暴，大量新玩家前来下载Hack的源代码、并在各个分组（尤其是net.games.rogue）发布Hack的讨论帖，这对其他社区的正常讨论和服务器的负载都带来了不小的压力（“灾难”）。次年（1985）一月，在Usenet管理员的紧急干预下，名为net.games.hack的新闻组正式建立起来，专供Hack的相关下载和讨论。\n当月，Brouwer发布了Hack 1.0.1（由于网络问题，该版本选择以.diff形式发布，只包含基于Hack 1.0文件的修改记录），该版本允许玩家主动拾取物品而不是走上物品自动拾取，形如“ad aerarium”的文本提示和对应的金库传送门也加入了游戏。当年四月，重量级的版本更新Hack 1.0.2也进入了公众的视野，它加入了更多我们耳熟能详的内容，如角色性别、月相（玩家平时带着蜥蜴尸体可以概率防止鸡蛇石化，只有新月时变为必然石化；特别地，狗和地狱犬在满月时攻击会造成额外伤害）、鹤嘴锄、更多地形（沼泽、蜂巢、金字塔（后来的墓地）；命名为david不再增加动物园的生成率，变成一个纯纪念性的文本）、低于30层以下的地狱（Hell，Gehennom的前身；如果进入地狱时没有火抗，玩家会被当场烧死）、岩德巫师（藏在地狱一个迷宫层中心的小房间里、周围被水围着，他身上带着岩德护符，身边有一条地狱犬）、神器（杀兽剑被限定为双手剑，但仍然可以同时存在多把）。Hack 1.0.2被分成10份发布，但不幸的是，第二份在后来的数年间被传丢了，直到2005年才重见天日。七月，最广为流传的版本Hack 1.0.3也公开发布（并且被收入了当时如日中天的BSD Unix系统）。该版本修复了Hack 1.0.2中的许多重大bug，并加入了诸如向导模式（wizard mode）、祈祷（当时还没有用，只会白白消耗一些回合）、冒烟药水召唤鬼魂（因为当时还不存在灯神）、蜥蜴尸体治疗鸡蛇石化（鸡蛇终于不是instakill了！然而蜥蜴尸体治疗混乱的特性在这个版本被移除了）等特性。\nAndries Brouwer’s Hack的热潮促使其被热情的玩家移植到大量平台。今天传播最广的DOS版Hack（PC-Hack）1.0.3归功于一位名为Don Kneller的玩家，他同时还移植了包括Amiga平台在内的一系列PC-Hack版本，我今天能找到的最早的Andries Brouwer’s Hack版本是他的Amiga Hack 1.0.1e……软盘中装的1.0.1a（笑死）。另一位名为R. Black的玩家用Lattice C重写了PC-Hack的3.51版本，并将其再次移植到雅达利ST上，命名为ST Hack 1.03。而又有一位名为Mike Stephenson的程序员惊异于Hack在80年代数个Unix平台的流行，看到这款游戏的进步空间，开始联系志同道合的网友一同开发新的功能。这些网友包括斯坦福和MIT的哲学系教授Izchak Miller和程序员Janet Walz，他们逐渐开始称自己为DevTeam；而不知是谁的提议，这款全新的游戏借来了他们维持联系的UseNet和把他们联系在一起的Hack，拥有了新的名字——NetHack。\n（NetHack的故事以后再补回NetHack一节吧。事情为什么会变成这样呢.jpg）\n\nHack 1.0 源码\nHack 1.0.1 源码\nHack 1.0.2 源码\nPC-Hack 1.0.1e Amiga版（包含PC-Hack 1.0.1a）\nPC-Hack 1.0.3 MS-DOS版\nUpdated Hack 1.0.3 Atari ST版，由NetHack DevTeam的Mike Stephenson改版（加入了武士和忍者），Andreas Bormann移植\nFlash Hack，基于PC-Hack 3.61，后者基于Hack 1.0.3\nDonnie Russell II的JSHack（镜像）\nAndries Brouwer’s Hack官网\nHack考古博客，共有三篇，此为第一篇\n另一个介绍Hack和NetHack的网站\n\nLarn\n在80年代的早期Roguelike中，Larn的设计相当不拘一格。先驱者们大多可以简单分到Morialike和Hacklike两个子类中，但Larn同时学习了两位前辈的特点：类似Moria，Larn拥有地面上的城镇层（甚至是最早拥有多地牢/多个可探索地点的Roguelike游戏）、注重角色的经验等级、使用单纯的菜单商店、物品生成同地牢层数密切相关、前往新层后落点并不在向上楼梯处；而对于Hack，Larn则借用了其地图不刷新、特殊地牢地形（喷泉、雕像等）以及合理利用规则提高角色强度的设计理念。继承之外，Larn也探索了一些早期Roguelike中前所未见的机制表达——例如其clock设计并未继承从Rogue开始一脉相承的食物-饥饿系统、驱动玩家不断下楼探索，而是加入了一个全局倒计时，玩家既可以在地牢中杀敌搜刮，也可以前往地面买入卖出、在银行存款获取利息、或是前往“Larn大学”花费宝贵的时间学习课程换取属性。\nLarn的背景故事也与主流的“冒险者前往地牢打倒宿敌/获得某件特别强大的MacGuffin”叙事完全不同，主角不是充满野心或身负重任的冒险者，只是一个绝望的普通人。\n\n欢迎回家XXX，医生的说法不太乐观。\n你的女儿已经确诊了Dianthroritis病，医生估计她只有300 mobuls可活了。只有靠你自己，XXX，才有可能找到拯救女儿的唯一希望，那种极为罕见的治疗Dianthroritis的魔药。传说这种药水只存在于洞穴的最深处。\n\n机制上的全局倒计时和剧情上女儿的生命流逝联系在一起，角色没有职业之分，背负的不是世界的存亡或神明的博弈，而是自己血肉至亲的健康和生死；从难度1开始主角开局会身无分文、赤手空拳，更加重了这种小人物无助的绝望感……\n与之形成强烈反差的是Larn角色的强度（草）。Larn可以说是五大古典Roguelike里入门难度最低的游戏了，维基百科甚至把它描述成“玩家有可能在第一局游戏就通关”（也没有那么简单吧）。举个例子，Larn城镇里的DND商店（武器装备道具店）货架上就摆着这游戏里最强大、甚至可能是Roguelike这一品类里空前绝后的武器Lance of Death死亡之枪，这把神器可以秒杀它命中的任何怪物；你也可以从搜集到的书本中随机获得诸如无敌术和恒定术之类的超级法术，前者可以让你在一定时间内免疫一切物理伤害，后者可以永久固化你身上的所有临时状态（包括无敌，嗯）。不过强大也是有代价的，一击必杀的死亡之枪售价165000（而地牢底层的Larn之眼项链可以卖160000，正常流程中你应当杀死地牢底层的守护恶魔才能攒够购买的本钱）；而法术的获得是完全随机的，恒定术之类的超级法术极为稀有且需要永久消耗智力施法，施放成功后还会立刻遗忘。另一方面，Larn的终极“地牢”——火山的怪物数量和强度也配得上这样的敌人，玩家一不小心可能会陷入与七八条巨龙的鏖战，被吐息喷得晕头转向，抑或是一失足落入无尽深渊，永远无法逃脱。（当然，在规则下你仍然有滥强的空子可钻，比如玩家圈子里著名的火山彩票玩法（提示：你其实完全没必要通过卖Larn之眼攒钱）；在死亡之枪的配合下，睡眠术、蛛网术这种看起来其貌不扬的控制法术后期的作用也是非常惊人的）\n相对于其他古典Roguelike，Larn有一个特殊的难度调整机制——玩家每次通关后游戏默认难度等级会上升1，这会带来一些机制上的变化（例如1以上难度开局背包里什么都没有，玩家必须冒着生命危险在地牢里攒够初始装备的启动资金，2以上难度无法用普通法术打破雕像）。此外，一个游戏中初见莫名其妙的机构“Larn税务局”会根据上次通关时携带的金币数对通关后的游戏收税，使得攒钱更加艰难。有个很有意思的设定是，在Unix/BSD/Linux系统中，Larn税务局会直接通过系统自带的邮件应用给玩家发邮件！\nLarn的影响力没有前面三个作品那么夸张，不过也在较小范围内拥有一批坚定拥趸。它被从pre-SysV版Unix和BSD上移植到DOS（移植者是我们的老朋友，PC-Hack的移植者Don Kneller），接着又迁移到VMS、雅达利ST和Amiga，还被爱好者们用Adobe Flash、JavaScript和C++等语言和工具各自重制过，其后有iLarn、NLarn、xLarn（这作上了steam）、Ularn（Ultra-Larn……怎么老RL都喜欢用U-XXX取分支/续作名字，用的词还都不一样）、LarnHD、FreeLarn等诸多分支版本。Facebook和Discord上都有Larn的群组，blogspot上还有一个专门写Larn战报的博客，Larn也是五大古典RL里唯一一个有仍然存活的较正式官网的游戏（虽然它是爱好者自己建的；可以在上面直接玩Larn/ULarn，还能看其他人的游戏录像和排行榜。Andries Brouwer的Hack“官网”虽然是本人建的但是太简陋了！），也是唯一一个仍然使用同样的名字维护和更新的游戏。ADOM的混沌入侵clock（以及elona的以太病clock）很难说没有受到Larn的全局倒计时的启发。（我印象里Larn有些版本可以踢楼梯，猜测和ADOM踢楼梯锻练力量的设定脱不了干系）\n在写这一段的准备过程中我找到了全网唯一存活的DOS版PC-Larn 12.0！发到Discord里了（官网站长Eye也在里面），相信不久之后其他人也可以下载了。但是现在在我这里就可以下载到！（叉腰）\n\nLarn 12.0 DOS版\nLarn 12.0 DOS版源码\nLarn 12.0 Atari ST版\nLarn 12.0 Atari ST版源码\nLarn 12.0b Amiga版 （以及另一个版本）\nLarn 12.3e DOS版\nLarn 12.4.0 DOS版\nLarn 12.4.4 Windows版\nLarn 14.1.3 Windows版\nLarn 14.1.5 Windows版\nDonnie Russell II的JSLarn（镜像）\nLarn第三方官网（可以玩在线版Larn/ULarn）\nLarn开源repo（仍在维护和更新！）\nLarn Facebook Group\nThe Larn Blog\n\nOmega\n假如你对Roguelike游戏中的动物说话，一般会发生什么？Moria、Angband和Larn不能和人或动物说话；ADOM的动物会发出叫声；NetHack的动物可能会根据血量和饥饿程度发出不同的叫声。Omega的动物……会递给你一篇由兽医杜立德医生撰写的学术论文，上面详细论述了动物们并不具有足够复杂的语言神经中枢，因此无法使用高级语言进行交流。\n我不知道这是否能让你对这部细致、幽默、疯狂、超前的作品有所认识。Omega拥有我从未在任何一部游戏中见过的“Play as yourself”的问卷式角色创建，拥有可能是Roguelike这一游戏品类中最大最复杂的主城镇The City of Rampart和第一个大世界LOCUS，可能是电子游戏史上最早的可以自己安装（还会不小心自己踩上去）的陷阱系统，可以自由设置以针对不同怪物的战斗动作序列，与你的一切行为息息相关的阵营系统，反馈方式异常多样的经验等级系统，与阵营相关且可以兼职的职业公会以及与经验等级并行的公会等级，极其广阔的多个胜利结局，可以永远留在启动页面上供其他玩家观看的游戏成就，以及十万甚至九万个埋藏在大地图中各种意想不到地方的小众科幻奇幻梗（上面提到的兽医杜立德是一个儿童文学系列的主角，他的特点是可以和动物对话……）。ATM机可以骇入，圣武士可以给怪物传教再杀怪（这是秩序行为！），公爵的城堡可以溜门撬锁进去扒窃，踩到陷阱可能会面见尤格索托斯，在我短暂的游玩和战报阅读中时刻充斥着各种“小小的Omega震撼”。\n而它诞生于1987年，由Laurence R. Brothers单人开发。这时Larn 12.0才度过它的第一个年头，NetHack 1.3d刚刚迈出它的第一步，电子游戏的丰碑Ultima V更是尚有一年才会问世。我完全无法想象究竟是怎样创造力和眼界驱动着作者创作出这样一部超越时代的作品，甚至有点觉得任何一个热爱Roguelike游戏、甚至是热爱回合制游戏的人没有玩过Omega都是一种损失了，这么多年来无人问津更是所有人的损失。（好夸张……）\n……怎么说呢，Omega的无人问津也不是毫无理由的，CRPG Addict在他前几篇写Omega的博文中开玩笑说Laurence这样的独立游戏开发者是“没有灵魂的、毫无底线的反社会人格”。举个例子——Omega的背包系统在我见过的游戏中折腾程度算是数一数二的，物品栏被分成随身携带的多个格子（包括装备栏）和一个先进后出（栈结构）的行囊；你捡起来的物品会先进入一个up-in-the-air区域，然后通过多达近十种不同的操作选择放入随身栏或行囊中。只有在随身区域的物品能被快速使用或出售给炼金术师换取魔法道具，行囊中的物品则会根据其到栈顶的距离决定其取出时间。草，我知道的下一个这么折磨的背包系统还是2020年的cdda！\n此外，你在Omega的任何时间任何地点都会遇到各种奇奇怪怪的随机事件，虽然有不少结果是好的，但坏结果很容易让人无法承受……比如在任何地方都可能会遭遇到“你被宇宙射线袭击！”，受到完全无法减免的10点伤害，如果你的角色还是0级（对，Omega的角色等级从0级开始）很容易吃一发直接被烤死。又比如战斗中有可能会武器挥空并出现笨拙的后果，例如武器打中自己或者直接被打碎——而且这些后果也完全无法规避，你的+5神装可能一不小心就爆成一地碎渣或者取了自己性命（然后看到一条嘲讽拉满的死亡消息“die of stupidity”，蠢死的）。在大地图旅行时也可能会遭遇到诸如“混沌风暴”之类可怕的随机事件，最好的结果是存活下来并获得经验，不太好的结果就是被扔到大地图的各个地方或者直接死了（如果被扔到混沌海里也会直接死了）；亦或者会进入随机遭遇战，见到从狼到灵魂到死亡杜宾犬（CRPG Addict：不知道Laurence和杜宾犬什么仇什么怨）之类的强力敌人，也可能一个敌人都没有但自己被困在一片没有出路的荆棘丛林中。陷阱召唤出的尤格索托斯根据阵营不同可能会给你知识也可能会让你当场去世，你也有可能从电梯掉落进无底的深渊直到超过256层把游戏整闪退……巨量难以防备的即死或重大负面随机事件+极其复杂、多样的长流程=？玩家砸键盘。\n无论如何我仍然希望大家至少可以试试，感受一下这款绝无仅有的游戏的独特魅力，对小众文化爱好者来说尤为如此。你可以在哥布林洞窟底部邂逅传奇的岩德护符（虽然在Omega里它好像什么用都没有），可以在途经荆棘丛时遇到三尖树并用某种特殊的方法将其驱散，可以不小心杀死一只鹦鹉并对随之而来的Monty Python梗会心一笑。如果你不介意的话，你甚至可以选择有限制地备份存档（帮助文档中甚至明文许可这样做），Omega即使允许sl依然是一个极富挑战性的游戏。关于它的影响，Thomas Biskop在创作ADOM的时候受到了Omega的很大启发（尤其是其大地图、多结局、复杂背景故事的设定）。巧合的是Laurence Brothers在CRPG Addict的评论区说ADOM是他最最喜欢的游戏，什么双向奔赴，吃一口（喂\n心血来潮搭建了一个mediawiki驱动的Omega Wiki，使用了一些NetHackwiki的模板，不过目前它还是空的，望可以渐渐填补上这份空白吧。Omega以前有一个官方发布站，2022年挂了，也不知道以后会不会续上……但愿我可以成为一个传递火种的人。\n\nOmega 0.60.2 源码\nOmega 0.71b DOS版\nOmega 0.71 源码\nOmega 0.75 OS/2版\nOmega 0.80.2 DOS版\nOmega 0.80.2 Windows版（含贴图）\nOmega 0.80.2 Windows版源码\nOmega 0.80.2 Windows版开源repo\nOmega 0.90.4 Windows版\nOmega 0.90.4 Windows版开源repo\nOmega 1.60??（非官方）Amiga版\nOmega FAQ （镜像） 0.75 txt版\nOmega Help File（镜像）\nOmega 0.75 Spoilers txt版\nOmega 0.75 hints txt版\n非常好笑的Omega 0.75 广告 txt版\nDonnie Russell II的JSOmega（镜像）\n已经挂掉的Omega官方发布站（时光机）\n我的OmegaWiki\n\n一些完成度比较高的Roguelike（和它们的前身）\nBrogue, BrogueCE, UnBrogue, RapidBrogue\nBrogue\nBrogue应该称得上最美丽的ASCII art Roguelike之一了！（其实应该是ANSI art，hhhhhh）一些玩家甚至会把Brogue称为Beautiful Rogue。美观层面上，我个人也觉得只有DF和Ultima Ratio Regum能与之相提并论，而它的光照特效和颜色变化在有史以来的所有Roguelike游戏中应该也是首屈一指的。用纯字符实现这样的画面效果确实很惊人。\nBrogue由Brian “Pender” Walker首次发布于2009年（和上面一批八九十年代的老游戏一比真是亲切），得名于作者本人的名字与灵感来源，Brogue=Brian+Rogue（因此这款游戏应当读作/brəʊɡ/或/broʊɡ/“不肉鸽”而不是/biːrəʊɡ/或/biːroʊɡ/“必肉鸽”。brogue这个词还表示爱尔兰/苏格兰口音或者一种男式低帮鞋，这和本游戏显然没有任何关系）。尽管年龄上相当现代，但Brogue并不像许多玩家传说的那样是“简化版的NetHack”。事实上，从小在车库里的奔腾286上玩DOS版Rogue、后来又转战Mac OS 9的Brian很早就在父亲和兄弟的帮助下开始修改Rogue的代码，加入自己的创作，可以说Brogue是Rogue的直接后继（早期版本甚至就是Rogue变体）。2008年暑期Brian前往华盛顿上学期间，有人闯入公寓偷走了他的笔记本电脑，Brogue的原型不幸丢失。不过这也给了Brian全新开始的机会，他将Rogue与自己玩过的另外两款Roguelike——NetHack与Angband比较，归纳了Rogue的优缺点并在rgrd上发帖讨论，将其作为自己开发新游戏的参考。在帖子发布后的第10个月，brogue 1.0正式发布了。（初代Brogue一开始仅有Mac版本和热心玩家移植的Linux版本，这当然是因为Brian的果粉身份（笑死））\nBrogue继承了Rogue极高的难度和简洁的游戏设计。与Rogue相仿，Brogue的角色从始至终很难有无敌乱杀的机会，怪物的挑战常常高出角色应对的能力，极其考验玩家的紧急逃生资源与策略；而固定的食物刷新又带来了紧迫的生存压力，角色不得不尽快下楼探索，避免饥饿而死的命运；较短的游戏流程也一定程度上缓解了类似NetHack长流程永久死亡带来的挫败感。Brogue不存在职业种族的区别，属性也和Rogue一样只有HP和力量，角色的build很大程度上取决于探索过程中获取的道具组合。操作行为上，Brogue的设计也与Rogue一样相当符合直觉，没有过多的按键映射，亦不依赖于长篇大论的剧透内容，所有道具的作用基本上都能通过简单的实践学会。\n在此基础上，Brogue还针对Rogue的一些缺点做了优化。首先是地图生成，3×3的房间和走廊在现代玩家的视角下实在是过于简单无聊，因此Brian设计了岩浆、峡谷、湖泊等大量多样化的地形，原有的矩形房间也被美化成更加现实的洞窟形状并加入各色地表植被，这极大增加了玩家探索的乐趣和沉浸感；独特的地图生成算法保证所有地区都是可达的，即使存在隐藏门也可以在视野内很快搜索到，免去了玩家贴着墙边苦苦搜索的困扰。Rogue的怪物设计过于单调，Brogue便加入了许多各具特色的怪物，从长矛突刺的哥布林、远程风筝的半人马、笨重却高伤的食人魔到使用各色限制法术的皮克精、强化友方的哥布林法师、暗精灵祭祀甚至是散发恶臭气体的僵尸和免疫物理攻击的亡魂，怪物之间还会相互配合，扩张了游戏的决策空间。Rogue的道具生成不稳定，可能在一局游戏中无法凑出任何一种build，Brogue便创造性地加入了可以多选一的宝库房间和类塞尔达/印第安纳琼斯的解密/机关钥匙房，修改了武器的生成方式保证各色武器的出现率，并把Rogue中多而杂乱的武器附魔/防具附魔卷轴合并为一种，使其可以强化包括戒指、法杖、魔杖在内的任何装备。武器和防具拥有各自的力量需求，力量不足会带来负面效果；绝大多数装备都可以通过不断穿戴或使用鉴定，武器防具还有概率拥有各自的符文效果。在1.7版本后，Brogue甚至删除了经验等级的设定，角色的能力提升完全依靠探索地图获得的力量药水、生命药水和附魔卷轴，这使得游戏重心更进一步落在Roguelike游戏最突出的地图探索上。当然，Brogue也吸收了前辈Roguelike游戏的经验，一方面制作了极为方便的鼠标和UI交互，清楚地讲明一切信息的同时使简单的纯鼠标游玩成为可能，甚至加入了自动探索功能；另一方面，取得岩德护符后玩家仍然可以向下探索、获取挑战性的符文石，并躲避一位完全无敌、行动缓慢的岩德守卫（Warden of Yendor）的追杀，不难猜出这些设定都是源自哪里（\n最后，Brogue对环境交互和气体的处理堪称一绝：腐蚀性气体可以削减生物敌人和自己的生命，麻痹气体可以让闯入其中的怪物任人宰割，混乱气体会让敌人如无头苍蝇一样四处乱转甚至误伤友军（有时候一群怪物还会反复踩踏同一个混乱气体陷阱导致气体不断被释放），这些气体又都会被火焰点燃并快速烧光；峡谷上的吊桥和洞窟中的植被也会被火焰点燃，使得玩家可以将怪物（或自己）活活烤死或者丢下深渊；有一种下落药水释放的气体会暂时穿透地板，让上面的怪物和物品也跌落到下一层，有一种特殊的自爆怪物也包含了这种气体；水流可以熄灭火焰，然而若是水上有可燃气体或液体被点燃就会产生大量高温蒸汽；沼泽会释放一种无害的绿色气体，但这种气体遇火不仅会燃烧还会发生爆炸，可以轻易带走任何不慎卷入其中的角色。各色气体配合Brogue独树一帜的光照与颜色效果，让整个地牢充满生机与活力。\nBrogue最新版本是2018年发布的1.7.5，有Windows、Mac、Linux版本，还被移植到安卓和iOS上，在北美、欧洲、新加坡各有一个活跃的在线游玩服务器。Roguelike社区的Oryx Design Club还制作了1.7.2版本的贴图版，这一版本2013年被jagttt汉化并发布到github上（这位是大名鼎鼎的《Roguelike 到底是啥——讲讲 Roguelike 相关知识》的作者，文中也着重介绍了Brogue）。在游戏本体之外，Brogue还产生了许多分支版本，并启发了俄罗斯开发者Oleg Dolya用Java Android SDK创作了一款操作更简单、交互更现代、更加简单易传播的游戏。Oleg Dolya网名watabou，而这款游戏就是万物起源、赫赫有名、年轻人的第一款Roguelike——Pixel Dungeon，像素地牢。\n\nBrogue 1.0.1 Mac版\nBrogue 1.5.1 Windows版\nBrogue 1.5.1 Mac版\nBrogue 1.5.1 Linux版\nBrogue 1.7.2 Windows汉化版\nBrogue 1.7.5 Windows版\nBrogue 1.7.5 Android修复版0.12.2\nBrogue官网\nBrogue reddit论坛\nBrogue Fandom Wiki\nBrogue在线服务器（北美） （欧洲） （新加坡）\nBrogue汉化版开源repo\nBrian Walker在rgrd上发表的关于Rogue的讨论\n\nBrogueCE\n1.7.5版本发布之后，Brogue再也没有更新过，社区传言Brian声称Brogue已经做完了，可能再也不会更新。在这一背景下，以tmewett为核心的社区活跃群体启动了一个名为Brogue: Community Edition的新项目，意图在Brogue的设计框架和哲学指导下修改Brogue存在的bug、进一步美化原有的UI并增加一些方便玩家游玩的新特性（例如将“不保存退出游戏”改为更无歧义的“放弃当前游戏”、处于冲突下的怪物不再会盯着玩家放法术、被复制魔杖复制的怪物不会血量减半，etc.）。去年（2022）BrogueCE的README删除了“如果Brogue更新了我们会怎么办”一节，似乎已经放弃幻想（雾），准备开始加入真正触及游戏玩法的、符合Brogue设计理念的新内容，让我们拭目以待。\n\nBrogueCE 1.8.2 Windows版\nBrogueCE 1.8.2 Mac版\nBrogueCE 1.9.2 Windows版\nBrogueCE 1.9.2 Mac版\nBrogueCE 1.9.2 Linux版\nBrogueCE 1.12 Windows版\nBrogueCE 1.12 Mac版\nBrogueCE 1.12 Linux版\nBrogueCE 1.12.2 Android版\nBrogueCE开源repo\nBrogueCE Wiki\n\nUnBrogue\nRoguelike Radio主持人之一、ASCII Dreams博客主Andrew Doull维护的Brogue分支版本，基于Brogue 1.7.1与部分1.7.2内容制作并加入了大量新内容，在2012年Roguelike年度发布聚会（Annual Roguelike Release Party）上首次发布。不过2013年就停更了……\n\nUnBrogue 1.1.6与1.1.7rc1 Windows版\nUnBrogue 1.1.6 Mac版\nUnBrogue 1.1.7rc1 Mac版\n\nRapid Brogue\nRapid Brogue是一个2021年新问世的Brogue分支版本，基于BrogueCE 1.9.4，定位是Coffeebreak Roguelike风格的分支，在致力于保留了Brogue体验的基础上做出了适合快节奏生活的修改：岩德护符所在地从地牢26层变成了第6层，附魔卷轴和力量药水带来的增幅变成原先的两倍，穿戴鉴定装备的时长也大大缩短了。目前Rapid Brogue已经更新到1.4版本，仍在持续更新中。（好玩！流程短很适合探索新build，以及我终于见到岩德护符了呜呜呜呜呜）\n\nRapid Brogue 1.0.0 Windows版\nRapid Brogue 1.0.0 Mac版\nRapid Brogue 1.0.0 Linux版\nRapid Brogue 1.4 Windows版\nRapid Brogue 1.4 Mac版\nRapid Brogue 1.4 Linux版\n\nPixel Dungeon, Shattered Pixel Dungeon, 以及十万个其他像素地牢\nPOWDER\nPathos: NetHack Codex, 帕索斯\nCataclysm, Cataclysm:DDA, 以及其他大灾变\nDwarf Fortress, 矮人要塞\nUnreal World\nCaves of Qud, 卡德洞窟\nELONA, Eternal League of Nefia\nCogmind\nInfra Arcana\nJupiter Hell，以及D**m, the Roguelike\nIVAN, Iter Vehemens ad Necem\nHyperRogue\nDungeons of Dredmor\nStoneshard\nRift Wizard\nZorbus\n","plink":"http://www.ephesus.top/roguelike/"},{"title":"","date":"2024-06-21T03:48:11.795Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:11.795Z","content":"Pytorch中mask是如何实现的代码版本1-阅读文本相似度模型\n代码参考链接: https://github.com/DA-southampton/TextMatch/tree/master/ESIM\n最近在用Pytorch重新ESIM代码，其中关于attention的细节我自己重新梳理了一下，附上代码解读。\n我先在有一个batch的数据。Sentence1 维度为[256,32,300],Sentence2的维度为[256,33,300]\n维度含义为[batch_size,batch中最大长度，词向量维度]\n数据流转ESIM第一个Bilstm之后，维度变化为：Sentence1 维度为[256,32,600],Sentence2的维度为[256,33,600]（假设hidden为300）\n此时我们需要计算两个句子输出的attention矩阵，以便计算每个句子的加权和。\n我这里主要是梳理矩阵的计算方式细节。\n核心代码是这个：\nhttps://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/model.py#L57\n1q1_aligned, q2_aligned = self.attention(q1_encoded, q1_mask, q2_encoded, q2_mask)\nself.attention 函数对应的是这个函数，如下：\nhttps://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/layers.py#L59\n12345678910class SoftmaxAttention(nn.Module):        similarity_matrix = premise_batch.bmm(hypothesis_batch.transpose(2, 1).contiguous())  ## 256*32 *33        # Softmax attention weights.        prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)        hyp_prem_attn = masked_softmax(similarity_matrix.transpose(1, 2).contiguous(), premise_mask)        # Weighted sums of the hypotheses for the the premises attention,        # and vice-versa for the attention of the hypotheses.        attended_premises = weighted_sum(hypothesis_batch, prem_hyp_attn, premise_mask)        attended_hypotheses = weighted_sum(premise_batch, hyp_prem_attn, hypothesis_mask)        return attended_premises, attended_hypotheses  \n首先我们看一下输入：\n1q1_encoded：256*32*600 q2_encoded：256*33*600  q1mask torch.Size([256, 32])  q2mask torch.Size([256, 33])\n然后对于这个函数，核心操作是这个：\n1prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)\nsimilarity_matrix 维度为256*32 33 hypothesis_mask 为25633\n我们去看一下masked_softmax这个函数：\nhttps://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/utils.py#L29\n12345678910111213def masked_softmax(tensor, mask):    tensor_shape = tensor.size()  ##torch.Size([256, 32, 33])    reshaped_tensor = tensor.view(-1, tensor_shape[-1]) ## torch.Size([7680, 33])    while mask.dim() &lt; tensor.dim():        mask = mask.unsqueeze(1)    mask = mask.expand_as(tensor).contiguous().float()    reshaped_mask = mask.view(-1, mask.size()[-1])  ## torch.Size([7680, 33])    result = nn.functional.softmax(reshaped_tensor * reshaped_mask, dim=-1)  ## 补长位置也就是置为零的位置之后进行softmax    result = result * reshaped_mask ## 再次置为零，因为上面这个对于补长位置还会有概率共现    # 1e-13 is added to avoid divisions by zero.    result = result / (result.sum(dim=-1, keepdim=True) + 1e-13) ## 普通的求概率公式    return result.view(*tensor_shape)\n简单总结一下：\n整个mask的代码其实我读起来感觉比较奇怪，我印象中mask的操作，应该是补长的部分直接为负无穷（代码里写一个-1000就可以），但是他这里的代码，是补长的部位置为0，所以\n在softmax的时候，虽然为1，但是也有贡献也有概率的输出，虽然很小。所以又把这些部分置为零，然后用每一行的值除以每一行的总和得到了新的概率值，这个概率和补长的部位就没有关系了。\n还有一个细节点需要注意的是，比如我的输入是2563233 batch为256，那么我在计算每一行的的时候，完全可以把batch中的数据并起来，也就是变成(256*32)*33\n所以我简单总结一下，在这里的mask的操作分为两个步骤：首先补长位置置为零然后计算softmax，随后对softmax的结构补长位置继续置为零，计算简单的分值（各自除以每一行的总和），得到最后的概率值。\n","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch中mask是如何实现的代码版本1-阅读文本相似度模型/"},{"title":"","date":"2024-06-21T03:48:11.825Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:11.825Z","content":"Pytorch修改ESIM代码中mask矩阵查看效果-效果一般\n我对ESIM中的mask矩阵有所怀疑，于是自己改写了一个mask的矩阵，不过效果确实没有原始的好，很奇怪\nhttps://github.com/DA-southampton/TextMatch/blob/master/ESIM/utils.py\n就是这个链接中，我改了主要是以下两个函数的部分地方：\n1234567891011121314151617181920def get_mask(sequences_batch, sequences_lengths):    batch_size = sequences_batch.size()[0]    max_length = torch.max(sequences_lengths)    mask = torch.ones(batch_size, max_length, dtype=torch.float)    mask[sequences_batch[:, :max_length] == 0] = -10000.0 ## 这里修改为-10000，印象中抱抱脸初始版本是这么实现的    return mask\tdef masked_softmax(tensor, mask):    tensor_shape = tensor.size()    reshaped_tensor = tensor.view(-1, tensor_shape[-1])    # Reshape the mask so it matches the size of the input tensor.    while mask.dim() &lt; tensor.dim():        mask = mask.unsqueeze(1)    mask = mask.expand_as(tensor).contiguous().float()    reshaped_mask = mask.view(-1, mask.size()[-1])    result = nn.functional.softmax(reshaped_tensor+reshaped_mask, dim=-1) ## 这里变为加    return result.view(*tensor_shape)\n改完之后效果不咋样，真的很奇怪\n","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch修改ESIM代码中mask矩阵查看效果-效果一般/"},{"title":"","date":"2024-06-21T03:48:12.025Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:12.025Z","content":"主要是积累关于Pytorch实战的一些经验和坑\nPytorch如何加载大数据：https://github.com/pytorch/text/issues/130\n\n\nPytorch技巧\n\n\n\n\n\npytorch对text数据的预处理-综述\n已经上传\n\n\npytorch处理文本数据代码版本1-处理文本相似度数据\n已经上传\n\n\npytorch处理文本数据代码版本2-处理文本相似度数据\n已经上传\n\n\nPytorch中mask attention是如何实现的代码版本1-阅读文本相似度模型的小总结\n\n\n\n\n\nPytorch调参总结\n\n\n\n\n\n验证集loss上升，准确率却上升该如何理解？\n\n\n\n\n\n\n\n\n\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/README/"},{"title":"","date":"2024-06-21T03:48:11.875Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:11.875Z","content":"pytorch处理文本数据代码版本1-处理文本相似度数据\n下面的代码，相比于版本2的代码，并没有使用gensim，而且处理的时候针对的是每一个样本，也就是每一行，也就是\nsentence1和sentence2并没有拆开来处理。\n整体代码是我自己完全整理出来的，比较整齐\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137&quot;&quot;&quot;@author: DASOU@time: 20200726&quot;&quot;&quot;import torchimport osimport pickle as pkl## 读取原始数据，生成对应的word2indexdef get_word_voc(config_base):    train_path=config_base.train_path    file=open(train_path,&#x27;r&#x27;)    lines=file.readlines()    min_freq,max_size,UNK,PAD=config_base.min_freq,config_base.max_size,config_base.UNK,config_base.PAD    vocab_dic=&#123;&#125;    for line in lines:        try:            line=line.strip().split(&#x27;\\t&#x27;)        except:            print(&#x27;The data formate is not correct,please correct it as example data&#x27;)            exit()        try:            if len(line)==3:                sen=line[0]+line[1]                tokenizer = lambda x: [y for y in x]                for word in tokenizer(sen):                    vocab_dic[word] = vocab_dic.get(word, 0) + 1 ## 为了计算出每个单词的词频，为之后过滤低频词汇做准备        except:            print(&#x27;The data formate is not correct,please correct it as example data&#x27;)            exit()    file.close()    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] &gt;= min_freq], key=lambda x: x[1], reverse=True)[:max_size]## 是为了计算每个单词的词频    vocab_dic = &#123;word_count[0]: idx for idx, word_count in enumerate(vocab_list)&#125;## 过滤掉低频词汇之后我们按照顺序来word-index的映射    vocab_dic.update(&#123;UNK: len(vocab_dic), PAD: len(vocab_dic) + 1&#125;) ## 补充unkonw和pad字符对应的数字    return vocab_dicdef load_data(cate,vocab_dic,config_base):    if cate==&#x27;train&#x27;:        data_path=config_base.train_path    elif cate==&#x27;dev&#x27;:        data_path = config_base.dev_path    else:        data_path = config_base.test_path    file=open(data_path,&#x27;r&#x27;)    contents=[]    for line in file.readlines():        words_line1=[]        words_line2=[]        line=line.strip().split(&#x27;\\t&#x27;)        sen1,sen2,label=line[0],line[1],line[2]        tokenizer = lambda x: [y for y in x]        token_sen1=tokenizer(sen1)        token_sen2 = tokenizer(sen2)        sen1_len = len(token_sen1)        sen2_len = len(token_sen2)        if config_base.pad_size:            if len(token_sen1) &lt; config_base.pad_size:                token_sen1.extend([config_base.PAD] * (config_base.pad_size - len(token_sen1)))            else:                token_sen1 = token_sen1[:config_base.pad_size]            if len(token_sen2) &lt; config_base.pad_size:                token_sen2.extend([config_base.PAD] * (config_base.pad_size - len(token_sen2)))            else:                token_sen2 = token_sen2[:config_base.pad_size]        for word1 in token_sen1:            words_line1.append(vocab_dic.get(word1, vocab_dic.get(config_base.UNK)))        for word2 in token_sen2:            words_line2.append(vocab_dic.get(word2, vocab_dic.get(config_base.UNK)))        contents.append((words_line1,words_line2,int(label)))    return contents# 导入/训练对应的word2indexdef get_w2i(config_base):    if not os.path.exists(config_base.w2i_path):        print(&#x27;There is not a pre word2index,now is to process data for geting word2index&#x27;)        vocab_dic = get_word_voc(config_base)        pkl.dump(vocab_dic, open(config_base.w2i_path, &#x27;wb&#x27;))        vord_size = len(vocab_dic)    else:        print(&#x27;There is pre word2index, now is to load the pre infomation&#x27;)        vocab_dic = pkl.load(open(config_base.w2i_path, &#x27;rb&#x27;), encoding=&#x27;utf-8&#x27;)        vord_size = len(vocab_dic)    return vocab_dic,vord_sizeclass DatasetIterater():    def __init__(self, batches, config_base):        self.batch_size = config_base.batch_size        self.batches = batches        self.n_batches = len(batches) // config_base.batch_size        self.residue = False  # 记录batch数量是否为整数        if len(batches) % self.n_batches != 0:            self.residue = True        self.index = 0        self.device = config_base.device    def _to_tensor(self, datas):        x1 = torch.LongTensor([_[0] for _ in datas]).to(self.device)        x2 = torch.LongTensor([_[1] for _ in datas]).to(self.device)        y = torch.LongTensor([_[2] for _ in datas]).to(self.device)        return (x1, x2), y    def __next__(self):        if self.residue and self.index == self.n_batches:            batches = self.batches[self.index * self.batch_size: len(self.batches)]            self.index += 1            batches = self._to_tensor(batches)            return batches        elif self.index &gt;= self.n_batches:            self.index = 0            raise StopIteration        else:            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]            self.index += 1            batches = self._to_tensor(batches)            return batches    def __iter__(self):        return self    def __len__(self):        if self.residue:            return self.n_batches + 1        else:            return self.n_batchesdef build_iterator(dataset,config_base):    iter = DatasetIterater(dataset,config_base)    return iter","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/pytorch处理文本数据代码版本1-处理文本相似度数据/"},{"title":"","date":"2024-06-21T03:48:11.965Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:11.965Z","content":"pytorch对text数据的预处理-综述\n我们需要把文本数据转化为向量从而可以被神经网络处理。在被喂给神经网络之前，我们需要对text文本数据进行预处理。\n关于这一块的预处理，其实有一个很高度抽象化的接口torchtext可以很高效的解决问题，但是有些时候不清楚里面怎么运作的心理总是没谱，所以我一般在写代码的时候都是使用人工自己处理代码。\n这个人工手动处理流程代码其实各式各样，我大概是写两个版本，之后如果看到不错的，可能还会整理，比如如何处理大数据，不过核心思想是一样的。\n大致流程是这样的：\n首先：对原始数据（一般是训练数据）进行预处理，进行分词，繁体字转化，半角符号转化\n随后：记录各个词汇的词频，过滤低词频词汇，简历Word2index的映射表保存起来，需要注意pad和unk符号\n随后：把数据（训练/测试/dev，使用参数进行控制）转化为对应的index，按照最大长度进行补全，并转化为tensor\n其次：制造自己的数据集类，改写关键部位，一般是get_item这里，以便被dataloder处理。\n","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/pytorch对text数据的预处理-综述/"},{"title":"","date":"2024-06-21T03:48:11.935Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:11.935Z","content":"pytorch处理文本数据代码版本2-处理文本相似度数据\n这里代码参考的是：https://github.com/DA-southampton/TextMatch/blob/master/SiaGRU/data.py\n感谢原作者\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Thu Mar 12 15:30:14 2020@author: zhaog&quot;&quot;&quot;import reimport gensimimport numpy as npimport pandas as pdimport torchfrom hanziconv import HanziConv  ##dasou:中文文本处理库from torch.utils.data import Datasetclass LCQMC_Dataset(Dataset):    def __init__(self, LCQMC_file, vocab_file, max_char_len):        p, h, self.label = load_sentences(LCQMC_file)        word2idx, _, _ = load_vocab(vocab_file)        self.p_list, self.p_lengths, self.h_list, self.h_lengths = word_index(p, h, word2idx, max_char_len)        self.p_list = torch.from_numpy(self.p_list).type(torch.long)        self.h_list = torch.from_numpy(self.h_list).type(torch.long)        self.max_length = max_char_len            def __len__(self):        return len(self.label)    def __getitem__(self, idx):        return self.p_list[idx], self.p_lengths[idx], self.h_list[idx], self.h_lengths[idx], self.label[idx]    # 加载word_index训练数据##dasou: 使用了pandas这个库，将文本相似度数据相同的列提取出来进行处理，而不是针对每一行一个样本进行处理，其实看到这里这个代码存在的一个问题就是如果将来##出来大的数据，也就是大的文件，pandas是没有办法直接全部读进来的，这是个缺点，不过对几个G的数据应该不存在这种问题def load_sentences(file, data_size=None):    df = pd.read_csv(file,sep=&#x27;\\t&#x27;,header=None)##dasou 为了适应我的数据格式    p = map(get_word_list, df[0].values[0:data_size]) ## p的每个元素类似这种 [&#x27;晚&#x27;, &#x27;上&#x27;, &#x27;尿&#x27;, &#x27;多&#x27;, &#x27;吃&#x27;, &#x27;什&#x27;, &#x27;么&#x27;, &#x27;药&#x27;]    h = map(get_word_list, df[1].values[0:data_size])    label = df[2].values[0:data_size]    #p_c_index, h_c_index = word_index(p, h)    return p, h, label# word-&gt;indexdef word_index(p_sentences, h_sentences, word2idx, max_char_len):    p_list, p_length, h_list, h_length = [], [], [], []    for p_sentence, h_sentence in zip(p_sentences, h_sentences):        p = [word2idx[word] for word in p_sentence if word in word2idx.keys()]        h = [word2idx[word] for word in h_sentence if word in word2idx.keys()]        p_list.append(p)        p_length.append(min(len(p), max_char_len))        h_list.append(h)        h_length.append(min(len(h), max_char_len))    p_list = pad_sequences(p_list, maxlen = max_char_len)    h_list = pad_sequences(h_list, maxlen = max_char_len)    return p_list, p_length, h_list, h_length# 加载字典def load_vocab(vocab_file):    vocab = [line.strip() for line in open(vocab_file, encoding=&#x27;utf-8&#x27;).readlines()]    word2idx = &#123;word: index for index, word in enumerate(vocab)&#125;    idx2word = &#123;index: word for index, word in enumerate(vocab)&#125;    return word2idx, idx2word, vocab&#x27;&#x27;&#x27; 把句子按字分开，中文按字分，英文数字按空格, 大写转小写，繁体转简体&#x27;&#x27;&#x27;def get_word_list(query):    query = HanziConv.toSimplified(query.strip())    regEx = re.compile(&#x27;[\\\\W]+&#x27;)#我们可以使用正则表达式来切分句子，切分的规则是除单词，数字外的任意字符串    res = re.compile(r&#x27;([\\u4e00-\\u9fa5])&#x27;)#[\\u4e00-\\u9fa5]中文范围    sentences = regEx.split(query.lower())    str_list = []    for sentence in sentences:        if res.split(sentence) == None:            str_list.append(sentence)        else:            ret = res.split(sentence)            str_list.extend(ret)    return [w for w in str_list if len(w.strip()) &gt; 0]def load_embeddings(embdding_path):    model = gensim.models.KeyedVectors.load_word2vec_format(embdding_path, binary=False)    embedding_matrix = np.zeros((len(model.index2word) + 1, model.vector_size))    #填充向量矩阵    for idx, word in enumerate(model.index2word):        embedding_matrix[idx + 1] = model[word]#词向量矩阵    return embedding_matrixdef pad_sequences(sequences, maxlen=None, dtype=&#x27;int32&#x27;, padding=&#x27;post&#x27;,                  truncating=&#x27;post&#x27;, value=0.):    &quot;&quot;&quot; pad_sequences    把序列长度转变为一样长的，如果设置了maxlen则长度统一为maxlen，如果没有设置则默认取    最大的长度。填充和截取包括两种方法，post与pre，post指从尾部开始处理，pre指从头部    开始处理，默认都是从尾部开始。    Arguments:        sequences: 序列        maxlen: int 最大长度        dtype: 转变后的数据类型        padding: 填充方法&#x27;pre&#x27; or &#x27;post&#x27;        truncating: 截取方法&#x27;pre&#x27; or &#x27;post&#x27;        value: float 填充的值    Returns:        x: numpy array 填充后的序列维度为 (number_of_sequences, maxlen)    &quot;&quot;&quot;    lengths = [len(s) for s in sequences]    nb_samples = len(sequences)    if maxlen is None:        maxlen = np.max(lengths)    x = (np.ones((nb_samples, maxlen)) * value).astype(dtype)    for idx, s in enumerate(sequences):        if len(s) == 0:            continue  # empty list was found        if truncating == &#x27;pre&#x27;:            trunc = s[-maxlen:]        elif truncating == &#x27;post&#x27;:            trunc = s[:maxlen]        else:            raise ValueError(&quot;Truncating type &#x27;%s&#x27; not understood&quot; % padding)        if padding == &#x27;post&#x27;:            x[idx, :len(trunc)] = trunc        elif padding == &#x27;pre&#x27;:            x[idx, -len(trunc):] = trunc        else:            raise ValueError(&quot;Padding type &#x27;%s&#x27; not understood&quot; % padding)    return x","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/pytorch处理文本数据代码版本2-处理文本相似度数据/"},{"title":"","date":"2024-06-21T03:48:12.265Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:12.265Z","content":"两个核心细节\n掌握FM，有两个细节需要注意：参数量级的变化和时间复杂度的变化。\n首先对于参数量级，由线性模型到多项式模型到FM模型参数量级变化为：\nn–&gt;n*n–&gt;kn (k&lt;&lt;n)\n其次是由原始FM公式到化简之后的FM公式复杂度的变化情况为：\nKn*n–&gt;kn\n线性模型\n回归问题我们一般使用的比较见得baseline就是线性回归，二元分类问题就是逻辑回归LR。\n线性模型公式如下（回归问题）：\n\n对于线性模型，我们的假设一般是认为特征之间是相互独立的，无法学习到特征之间的交叉影响。\n为了解决特征交叉的问题，我们一般可以人为的加入一些自己的先验信息，比如做一些特征之间的交互，不过这个很需要人们的经验。\nPOLY2模型–暴力组合特征交叉\n这个时候，POLY2模型成了可行的方案。POLY2 模型，对所有特征做了两两交叉，并对所有特征组合赋予了权重，在一定程度上解决了特征组合问题，本质仍然是线性模型，训练方法与逻辑回归没有区别。\n我们把POLY2（只是特征两两交叉的部分）加到线性模型中，从而模型可以过渡到多项式模型，公式如下：\n\n（ps：看到这里我自己有一个疑问，同一个特征onehot之后，会在自己里面做特征交叉吗）\n看这个公式，主要是看后面那个交叉的部分。看到这部分，其实很容联想到我们在LR中自己加入交叉特征的部分。\n但是需要注意的是，这里有点像暴力求解一样，忽视或者说省去了人工先验的部分，直接做到了所有特征之间的交叉，然后去求解对应的参数就可以。\nPOLY2模型两个问题\n但是这样暴力求解存在两个问题：参数量和参数稀疏导致学习困难的问题。\n先说参数量的问题，如果我自身特征（未交叉之前）就已经很多了，为n，那么交叉之后就是一个 n*n级别的参数量。极端情况会出现参数的量级比样本量级都大，训练起来及其的困难。\n再说参数稀疏的问题。互联网数据通常使用one-hot编码除了类别型数据，从而使特征向量极度稀疏，POLY2模型做无选择的特征交叉，使得特征向量更加的稀疏，导致大部分交叉特征的权重缺乏有效的数据进行训练，无法收敛。\n我自己理解的时候感觉这个很像是NLP中OOV情况。\nFM模型\n面对这两种问题，FM模型怎么解决呢？\nFM相比于POLY2模型，主要区别是用两个向量的内积代替了单一权重系数，具体来说，就是FM为每个特征学习了一个隐权重向量。在特征交叉的时候，使用两个特征向量的内积作为交叉特征的权重。\n这样其实就解决了上面两个问题。\n参数量的问题变为了 kn个参数，因为每个特征对应一个K维度的向量。\n其次是参数学习的问题。OOV问题很大缓解，即使当前特征交叉在训练样本中没出现过，但是每个特征已经学到了自己embedding，内积之后是有结果的。这也是为什么FM模型泛化能力强的根本原因。\nFM模型如下：\n\n其中涉及到的二阶部分可以通过公式的化简从Kn*n–&gt;kn：\n\n参考链接：\n文章：\nFM算法解析 - 王多鱼的文章 - 知乎 https://zhuanlan.zhihu.com/p/37963267\n推荐系统召回四模型之：全能的FM模型 - 张俊林的文章 - 知乎 https://zhuanlan.zhihu.com/p/58160982\n代码：\ndeepctr-torch 大概跑了一遍\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/FM/"},{"title":"","date":"2024-06-21T03:48:12.235Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:12.235Z","content":"在理解DeepFM的时候，我觉得有个细节点非常重要，就是FM中对应的特征组合前面的每个特征的隐向量在哪里？\n每个原始类别特征会进行one-hot，然后映射到embedding层，每个节点链接到embedding层的权重组合就是这个隐向量。直接看图：\n\n对于DeepFM，核心概括一下：\nDeepFM大致分为两个部门：DNN和FM部门，两者共享权值，分别提取特征，最终这输出。\nFM分为一阶特征组合和二阶特征交叉组合。特征一般分为类别型特征和连续性特征。\n我大致跑了一下DeepFM的代码，看了一下一阶特征和二阶特征的问题：\n一阶特征是类别型特征和连续特征都要，类别型特征直接embedding相加就可以，连续特征归一化之后乘以对应权重相加就可以，最终一起相加就可以。\n二阶特征组合是是使用到了离散特征的组合，直接embedding之后放入到FM模型中就可以。\n参考链接：\n深度推荐模型之DeepFM - 偶而君的文章 - 知乎 https://zhuanlan.zhihu.com/p/57873613\ndeepFM in pytorch----非常好\nhttps://blog.csdn.net/w55100/article/details/90295932\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/deepfm/"},{"title":"以弗所的阿尔忒弥斯神庙","date":"2023-05-08T12:50:09.362Z","date_formatted":{"ll":"May 8, 2023","L":"05/08/2023","MM-DD":"05-08"},"updated":"2023-05-07T12:07:22.000Z","content":"“朝圣者,你进入了一个神圣的地方!”--更多--\n大概是Silencess的个人小站和博客，也是折腾旧手机的副产物。也许会不定期更新一些小随笔、开发日志、翻译的同人什么的，也在考虑把收藏品放过来给大家看看，不过可能不太会自己做饭吃hhh\n关于我\nSilencess aka 言静，名字和个人资料都是凭直觉瞎编的。喜欢roguelike的Felis Venatus，囤积癖严重，擅长拖延画饼三分钟热度，梦想是成为赛博幽灵。后面没想好，不写了。\n哦记得看左边的CataclysmDDA综合攻略手册，it belongs to someone greater than me.\n","plink":"http://www.ephesus.top/about/"},{"title":"","date":"2024-06-21T03:48:13.295Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.295Z","content":"关于推荐的资源总结\n\n\n推荐系统资源总结\n主体内容\n进度\n\n\n\n\nEmbedding 技术的非端到端学习方法 - 腾讯技术工程的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/188569580\n本文主要介绍 Embedding 技术的非端到端学习方法在应用宝推荐场景的应用实践。\nno\n\n\nMulti-task多任务学习在推荐算法中应用(2） - 梦想做个翟老师的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/91285359\n多任务学习在推荐系统\nno\n\n\n知识蒸馏与推荐系统 - 凉爽的安迪的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/163477538\n知识蒸馏推荐系统\nno\n\n\nctr预估怎么构造时间相关的特征？ - 大博的回答 - 知乎\n\n\n\n\nhttps://www.zhihu.com/question/350863682/answer/860524396\n特征工程\n还没看\n\n\n深入理解推荐系统：排序 - 鱼遇雨欲语与余的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/138235048\n排序\nno\n\n\n负样本为王：评Facebook的向量化召回算法 - 石塔西的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/165064102\n很好\nno\n\n\n再评Airbnb的经典Embedding论文 - 石塔西的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/162163054\n\n\n\n\n最全推荐系统Embedding召回算法总结 - Garvin Li的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/156769032\n召回emebdding\n\n\n\n用户画像在携程商旅的实践 - 携程技术的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/161804005\n用户画像\n\n\n\n深度学习推荐系统中各类流行的Embedding方法（下）https://mp.weixin.qq.com/s/N76XuNJ7yGzdP6NHk2Rs-w\nembedding\nno\n\n\n一文梳理推荐系统的中 EMBEDDING 的应用实践\n\n\n\n\nhttps://mp.weixin.qq.com/s/7xTOCODlJQ42UkjRoRTE5A\nembedding\nno\n\n\n推荐系统主流召回方法综述\n\n\n\n\nhttps://mp.weixin.qq.com/s/Kxf_VX8cyN4vvveEPB1mcg\n召回\nno\n\n\n如何消除广告和推荐中的position bias\n\n\n\n\nhttps://mp.weixin.qq.com/s/rJ3pzxVEVZxCwKjXrNukXg\n广告bias\n\n\n\n浅谈电商搜索推荐中ID类特征的统一建模：Hema Embedding解读 - 力学渣的文章 - 知乎 https://zhuanlan.zhihu.com/p/104182282\n广告建模\nno\n\n\n推荐系统 embedding 技术实践总结 - 腾讯技术工程的文章\n\n\n\n\n揭开YouTube深度推荐系统模型Serving之谜 - 王喆的文章\n\n\n\n\nYouTube深度学习推荐系统的十大工程问题 - 王喆的文章 -\n\n\n\n\n重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文 - 王喆的文章\n\n\n\n\n看Youtube怎么利用深度学习做推荐 - 石塔西的文章\n\n\n\n\nFactorization Machine笔记及Pytorch 实现\n\n\n\n\n推荐系统遇上深度学习(三)–DeepFM模型理论和实践\n\n\n\n\nDeepFM全方面解析（附pytorch源码）\n\n\n\n\n详解 Wide &amp; Deep 结构背后的动机 - 刺猬的文章\n\n\n\n\nwide&amp;deep模型中为什么要将连续特征离散化？\n\n\n\n\n看Google如何实现Wide &amp; Deep模型(1) - 石塔西的文章\n\n\n\n\nCTR预估之Wide&amp;Deep和DeepFM - 张备的文章\n\n\n\n\n见微知著，你真的搞懂Google的Wide&amp;Deep模型了吗？\n\n\n\n\n用NumPy手工打造 Wide &amp; Deep - 石塔西的文章\n\n\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/推荐资源更新/"},{"title":"","date":"2024-06-21T03:48:13.335Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.335Z","content":"什么叫做倒排索引\n在搜索场景中，有一个名词非常的频繁叫做“倒排索引”，今天看了一篇参考资料，大致的了解了一下基本原理，记录下来，以备后用。\n首先我们来看，搜索场景是这样的：我有海量文本存储在数据库中，同时每次搜索请求，会有query。\n基于海量文本，一个比较直观的想法就是建立正排索引。就是我们的每一个文本（有着自己唯一的一个编号）对应着自己文档中的关键词。\n形式如下：\ndoc1: 关键词1，关键词2，关键词3…\ndoc2：关键词3，关键词4，关键词5…\ndoc3：关键词1，关键词9，关键词7…\n这样的数据存储结构很不利与搜索。假设我们有一关键词“苹果”，我要找到包含苹果的文档，那么我需要对每个文档进行遍历，才可以，这样下来搜索时长太大。\n倒排索引是这样的，使用关键词作为存储结构的key，每个关键词对应着包含这个关键词的doc，形式如下:\n关键词1: doc1,doc3…\n关键词2: doc1…\n关键词3: doc1,doc2…\n在这种情况下，如果我们去搜索包含“苹果”这个关键词的文档，只需要对key进行索引就可以。\n这就是倒排索引，简单讲就是关键词作为key，包含对应关键词的文档集合作为value。\n然后我们在讲一个比较细节的东西，整个倒排索引可以分为三个部分：词典，倒排列表，倒排文件\n词典：存储自身编号和指向倒排列表的指针\n倒排列表：存储包含某个关键词的所有文档列表以及对应关键词出现的次数位置等信息。\n倒排文件：倒排文件是存储倒排索引的物理文件\n参考文件链接：\nhttps://www.cnblogs.com/zlslch/p/6440114.html\n我觉得这个讲的很好\n","plink":"http://www.ephesus.top/links/NLP_ability/搜索/倒排索引基本概念/"},{"title":"","date":"2024-06-21T03:48:13.365Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.365Z","content":"搜索资源总结-持续更新\n最近看到一个不错的仓库，搜集搜索相关资源，地址在这里：\nhttps://github.com/frutik/awesome-search#types-of-search\n我直接fork了\n先列取我看到一些将来可能要看的文章，看到就更新，随时更新\n\n\n搜索相关资源总结\n\n\n\n\n\n搜索中的 Query 理解及应用1\n\n\n\n搜索中的Query扩展技术\n\n\n\n电商搜索是如何保证搜索结果不偏离搜索意图的？\n\n\n\nQuery意图方法（2）- 基于文本分类 - 星轨数据的文章 - 知乎\n\n\n\n浅谈Query理解和分析 - 机智的叉烧的文章 - 知乎\n\n\n\n搜索引擎的 Query 分析有哪些小技术点？\n\n\n\n大话搜索query理解 - 乔一的文章\n\n\n\n智能扩充机器人的“标准问”库之Query生成 - 刘聪NLP的文章\n\n\n\nBad Case方法论 - 姚凯飞的文章\n\n\n\nquery分析 - 知乎\nhttps://www.zhihu.com/topic/19611289\n机器学习（十）：损失函数 - DataScienceArt的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/136047113\n大话搜索query理解 - 乔一的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/111904993\n从算法理论到工程实践，AI学习路上你遇到了哪些阻碍、走过哪些弯路？ - copytang的回答 - 知乎\nhttps://www.zhihu.com/question/358290436/answer/921061372\n现阶段各家公司的广告算法使用的主流模型有哪些？ - copytang的回答 - 知乎\nhttps://www.zhihu.com/question/352306163/answer/905601365\n国内有哪些比较优秀的搜索引擎？ - copytang的回答 - 知乎\nhttps://www.zhihu.com/question/278288679/answer/402251102\n医疗搜索中的query词权重算法探索\nhttps://mp.weixin.qq.com/s/JCdzhd1wBKIzDkoqW87OAg\n搜索广告之自动化创意\nhttps://mp.weixin.qq.com/s/8CN6Ak9skzxXn_qZntJ0FQ\n教你如何动态配置词权重，检索系列文章之HDCT论文笔记 - 刘聪NLP的文章 - 知乎 https://zhuanlan.zhihu.com/p/148211196\n深度召回在招聘推荐中的挑战和实践\nhttps://mp.weixin.qq.com/s/_2pPa6v2wgb5ht1j_s4Plg\n说一说视频搜索\nhttps://mp.weixin.qq.com/s/Sxuv2H9zJLy04BGXeUiF-g\nEmbedding搜索能代替文本搜索吗？\nhttps://mp.weixin.qq.com/s/cbIqkGg8IwjnSKpEd54wZg\n[第9期] 如何识别用户搜索意图之 Query 扩展\nhttps://mp.weixin.qq.com/s/Zulh3iGXZwDJZ9nH4rbhXQ\n搜索引擎技术之Query意图分类\nhttps://mp.weixin.qq.com/s/JUjT1Z9yzyUgKA6cTA3VoQ\n在query理解中能ALL IN BERT吗？\nhttps://mp.weixin.qq.com/s/G3dr0toZjHH5hyzFBUU-aQ\nNLP技术在搜索中的应用–query理解在搜索中的应用\nhttps://mp.weixin.qq.com/s/ypZu9iO07mH5GskDEOuzfw\nQuery词权重方法（3） - 基于有监督学习\nhttps://mp.weixin.qq.com/s/1EUSz4_r8-j2wIfsHXq-IQ\nQuery分析三大法宝（2）- 百度结果\nhttps://mp.weixin.qq.com/s/PPBaBd1lgtTHmtXcB9Lg2A\nQuery分析三大法宝（3）- 片段粒度\nhttps://mp.weixin.qq.com/s/yL23MxTtAS5fF0fFBDDxWw\n搜索Query技术体系|方法论\nhttps://mp.weixin.qq.com/s/8wAJOfTV-BuOuVk9Ul5F4Q\nQuery 理解和语义召回在知乎搜索中的应用\nhttps://mp.weixin.qq.com/s/MAfK4B2F8sPXRLodXkwnmw\n","plink":"http://www.ephesus.top/links/NLP_ability/搜索/搜索资源总结-持续更新/"},{"title":"","date":"2024-06-19T22:15:51.856Z","date_formatted":{"ll":"Jun 19, 2024","L":"06/19/2024","MM-DD":"06-19"},"updated":"2024-06-19T14:15:51.846Z","content":"Avanor游记：从充满恶性bug的粪作中艰难通关\n故事要从一只没电又找不到充电线的鼠标说起，尽管我的旧ThinkPad有指点杆，但指望用那种东西玩即时制游戏还是太勉强了，毕竟在下不叫路明非。\n不过这也是玩老游戏难得的好机会，大概几个月前我因为别的缘故囤积了一批Roguelike——是那种画面由字符构成的回合制游戏，而不是有点随机要素就往自己身上贴标签的现代工业垃圾；在失去鼠标却充满无限可能的美好夜晚，我从存货里挑了款叫Avanor的游戏并毫无防备地开始了游玩，殊不知即将遭遇的……是堪称地狱般痛苦的游戏体验……\n\n进入游戏要做的第一件事必然是rtfm，也就是读说明书；但这玩意上来就给了我个下马威，提示“在目录下找不到帮助手册”，然而手动切出去一看文件夹，手册安安稳稳地躺在该待的位置，居然还是用html写成的，也不知道程序为什么死活不承认它的存在；识别不出路径倒也无妨，直接本地用浏览器看甚至更方便。\n\n\n\n依据手册内容简单介绍一下Avanor，这是一款奇幻风格的角色扮演游戏，有着类似于ADOM（传统roguelike佳作之一）的操作界面，除了UI设计上“借鉴”不少之外，一些机制也几乎照搬，比如同样的DV PV（闪避和防御值）设定以及从Coward到Berserker的几种战术风格；不知道该不该庆幸，Avanor里的时间观念倒没有那么重要，因为不会有腐化或者以太病什么的赶着玩家跑，但考虑到各种崩溃、闪退、还有成堆的bug，一个存档的游玩记录越长就越容易永久损坏，某种意义上这个世界也算是危在旦夕，正在逐步崩坏之中。\n剧情方面相当老套，也禁不太住推敲和考据：阿瓦诺王国在古代十分强盛，而今它已没落，突然隆起的山脉围困住了王都以及周边一带，许多可怕的生物趁着阿瓦诺孤立无援之时大肆破坏，您作为幸存下来的英雄，将毫不迟疑的拯救王国于水深火热之中，解决这一前所未有的危机……真的吗？\n\n创建角色的过程比较乏善可陈，选择种族、性别、职业，再输入姓名后就完成了；种族并不丰富，总共只有7种，都是奇幻世界观中常见的类型，除了会影响初始属性的区间和最大属性的上限之外，还会影响角色对食物的评价；比如一份普通人勉强能够下咽的食物，野蛮粗犷的半兽人尝了会觉得是美味珍馐，而对食物极度挑剔的半身人尝了可能会当场呕吐，导致饱食度不增反降；另外还有一项速度属性也是种族决定的，但作者并未对不同种族设置不同数值就停止了更新。\n性别在那个年代流行的还是简单的二元论，Avanor中不存在“沃尔玛塑料袋”之类让玩家眼花缭乱的性别选择，男性会增加一点力量，女性会增加一点体质，仅此而已；所选职业会提供相应属性的固定修正值，并与种族共同决定玩家的初始装备和习得技能，例如人类战士会拿到长剑并拥有烹饪技能，而矮人战士则会得到战斧以及一项挖掘技能。\n听起来不错是吗？但设定得多么饱满，表现出来的就多么贫乏，因为游戏中压根就没有对这些选项进行任何描述，手册同样没有，仅仅介绍了各种技能；可绝大多数技能无法后天习得，玩家要是对某一技能感兴趣，就必须控制变量并不断创建角色，挨个尝试不同选项再到游戏里摸索效果，尽量获得较为理想的技能与属性；然而就像前面提到的，不同类型的选项之间还会产生组合效果，这极大地提高了猜测难度；我是怎么能了解得这么清楚的？因为后来迫不得已把它的源码翻了个底朝天……往下看吧。\n\n玩家的初始位置处于一座村落当中，那些由#号构成的方框就是房子，东边有一条汇入湖泊的小溪，桥对面则能看见不少强盗；村子里有一家食品商店，货物种类不多，无非是大中小三种口粮（Ration）以及精灵的旅行面包（Waybread），这种食物的辛达语名称更广为人知，叫兰巴斯（Lembas），正是《指环王》里那种轻巧又抗饿的美味旅粮；食品的物价在Avanor中相当低廉，最贵的旅行面包才卖15金币，让人不禁怀疑隔壁萨鲁曼是否真的实现了工业化，不过刚开局玩家穷得叮当响，也只买得起一份就是了。\n手册里关于商店的介绍相当有意思：在邪恶降临这片土地之前那个更加幸福的时代，店主很乐意让人们捡起想要的全部东西再一并交钱，可某些流氓和恶棍利用了店主的善意，所以阿瓦诺的商人行会制定了条规则，您必须在捡起任何一件货物后为其付款，别想再装满购物篮后就逃离商店；换个角度理解其实等于允许零元购但是限购一件，只不过玩家没有给钱便离开商店的话，店主会计算玩家的债务利息并在下次要求一同支付。\n还有一种……姑且称之为特性吧，角色能够直接吃掉地上的可食用物品，不需要捡起来，而未拾起的物品就算消失也不会计入玩家的“购物篮”，这恰巧跳过了交易的检测环节，即使在店里吃到撑也不用付出任何代价，甚至把其他地方捡来的食物卖给店主后再吃掉也未尝不可；药剂卷轴等消耗品并不包含在食物当中，必须捡起来才能使用，对比下来这条特性似乎成了种防止玩家饿死的低保，然而考虑到获取食物的轻松程度，还有赶路过来的时间成本，这多半是项漏洞。\n本来我寻思着自己捏了个纯近战向的矮人战士，砍上几名新手村外的强盗那局势应该是一面倒才对，事实证明这完全正确，的确是一面倒的单方面爆杀，只不过倒下的人是我。\n\n\n出师未捷身先死，我就这样迎来了游戏中的首次死亡，愣是没想明白村口的强盗为何如此之强悍，幸好Avanor不像其他传统Roguelike那样会在死亡后强制删档，试错成本还处在尚可接受的范围内。\n介于跨过桥后强盗们便会主动攻击，走正路是行不通了，我只好在村里尝试与所见到的每位居民对话，试图获取有用的信息，但每次都仅能得到一句“你没有收到任何答复”的系统消息（后来发现是bug），直到我与村中长老交谈，他告诉我村子西边有个大家经常去采蘑菇的山洞，但前阵子一头恶魔占据了那里，湖泊南面说不定有人能帮上忙。\n对话系统做得稀烂也就罢了，最大的问题在于任务引导上，刚才这段指引内容在npc口中只会讲述一遍，并且因为可显示区域不多还是分段展示的，如果玩家没有看清或想再次确认，那么抱歉，长老仅会重复一句“那头邪恶的怪物仍然活着”；仔细思考前面的几句指引其实不难发现，长老是让我去湖泊对岸找人帮忙，但玩家要是读过手册会查看自己的任务面板，上面写着的却赫然是长老请求玩家杀死那头恶魔。\n\n\n初入这个世界我没有考虑太多，下意识认为新手任务应该不至于离谱过头，洞里没准是只imp那类的小恶魔，会让村民烦恼也很正常，随即拎着斧头便准备下去大展身手。\n山洞离村子非常近，就位于突然隆起的山脉前，第一层安静得吓人，空空荡荡没有任何生物，我警戒着走向第二层，刚拐过个弯就看见了一道蓝色的身影，正是任务当中提到的恶魔；从看见它的瞬间我便开始发怵，字符&amp;在大多数Roguelike中代表的都是血统纯正的恶魔系怪物，绝对不是imp那类亚种可以比拟的，况且这家伙有名有姓绝对是大恶魔级别以上的存在。\n通常情况下玩家应当立刻掉头就跑，但我仍怀有一丝侥幸，将战术模式切换成完全防御状态缓缓凑了过去，万一对方是魅魔之类可以交涉的种族呢；在靠近的过程中，它的确没有突然暴起并以惊人的速度冲过来将我撕成碎片，反而当我不存在似的，自顾自地在那闲庭信步，直到我接近它身旁时才随手一击清空了我的生命值，说是清空但其实不准确，从负数来看，这一击造成的伤害是我hp上限的十倍有余；杀我对它而言就轻松得如同捏死只虫子，难怪之前忽视我的行动，一只虫子的存在与否确实不值得它放在眼里。\n\n死亡接踵而至，一切又回到了原点，没有狗洞没有发疯的木匠，谁能料到新手任务要去杀死一头古代种的恶魔；也不知道这个世界中的蘑菇是不是什么硬通货，亚种就算了，连古代恶魔都来抢，更让人好奇的是这村子究竟有何神通，居然能在周围这群煞星手下平安待到现在。\n眼下最重要的是寻找出路，我四处转悠，很快在村子南边发现了一座通向湖心的小桥，走到尽头能够看见一扇传送门，我隐约回想起长老曾说过南面和湖泊，便义无反顾的走进了传送门中，转眼间我来到了一处充满建筑物的地方，后来才知道这是王城。\n出传送门往东走几步就能看见一座大理石建筑，里面有黄金砌成的地板与同样是黄金铸成的栅栏，在它们之间的，是一团被称为永恒烈焰的不灭之火，旁边站着的则是被称为火焰之主的魔法师；从介绍文本中得知他还是国王的顾问，然而这样一位实力高强又有权有势的大人物现在却满脸忧愁，与他对话后了解到有名邪恶的法师需要被消灭，目标就位于东南方向的地下城中，地表上方则有着邪法师塔楼的遗迹。\n\n能让皇家首席魔法师感到头疼，必然不会是什么省油的灯，事实上那名邪法师正是最终的反派头目，难以想象勇者刚出新手村就被安排去讨伐魔王，关键在于这魔王还不是一般的近，简直是刚出油锅又入火坑。\n王城当中有一家消耗品店，出售各种卷轴药剂还有魔法书，值得一提的是Avanor的法术系统与ADOM大相径庭，ADOM中阅读有次数限制的咒语书可以获得咒语知识，施法会消耗它，而知识一旦消耗完毕该咒语便无法再次使用，必须寻找新的咒语书来读；Avanor中的魔法书只能阅读一次，但习得的法术并不会随着使用而遗忘，魔力没有消耗殆尽即可不断使用，找到同样的魔法书再次阅读则能够提高法术等级。\n两款游戏最核心的差异在于阅读书籍的成功率，ADOM中读书需要考虑职业、等级、技能、以及书籍状态，一旦失败就得承担包括物品损坏或者属性永久降低在内的负面效果；但Avanor只要玩家角色不是文盲便能够阅读任何书籍，读写技能仅影响阅读所需要的时间，安全区域中唯一可能中断过程的大概只有饱腹度过低，如果阅读中断，书籍并不会被消耗，吃撑之后再读便是。\n这样的设定让Avanor中物理系与魔法系职业的同质化程度相当高，职业不提供加成搭配上属性培养也不困难，玩家到了后期基本都样样精通，远近攻守兼备；作者意识到了问题，从待办列表中不难看出他在考虑为不同书籍加入最低的属性与技能要求，可这毕竟是款停已久的古董，有生之年恐怕都没办法看见它们被实现了。\n\n自王城向东南方向行走几十余步便能看见一处建筑残骸，顺着正中央的楼梯向下前行就抵达了地下城；虽然是邪恶大反派所在的藏匿处，但里面的怪物却意外适合初出茅庐的玩家练手，浅层怪物主要为骷髅还有老鼠蝙蝠之类的小动物，都是很常见的种类。\n到第二层我还遇到了只ghoul，它的攻击有小概率造成麻痹，隔壁ADOM也有这种怪物但拼写方式为ghul，新手碰上的话还算比较致命；同样在不少游戏里出现另一位“常客”叫作灰色软泥（gray ooze），ADOM与Nethack中的这种怪物会腐蚀武器和盔甲，至于Avanor里这小东西则是攻击带毒，以及不像ADOM的亲戚那样会自我增殖。\n\n值得一提，Avanor的怪物间也有着仇恨关系，我曾在与一只老鼠搏斗时突然窜出来条褐蛇，但这条蛇不仅没有先攻击我反而一口咬向了同为怪物的老鼠，也算是意外之喜。\n\n打怪升级的过程整体上还算轻松愉快，逛上三四层地牢就够我升到7级，只不过随处可见的陷阱实在是折磨人；陷阱依据其类型能够造成物理或者元素伤害，可对于前期的玩家来说，不管踩到什么种类的伤害陷阱，下场通常都是被秒杀，“Killed by fooself”则会被记录为死因；更令人头疼的是这游戏并没有几乎每款Roguelike必备的搜索功能，甚至连等待一回合的功能都没有，我自己摸索了好一阵子才发现拾取物品的操作可以在不离开原位的情况下消耗回合，勉强有了替代品。\n\n由于不能主动搜索，被动探测就成了唯一的办法，具体就是在可疑位置附近来回走动或是不断按下拾取键，如果陷阱被发现便会立刻失效，能够安全地经过或是拾取战利品。\n发现陷阱的概率取决于技能等级，玩家升级后可以获得少量改进点数，先不提将宝贵的点数投入到非战斗技能上是否值得，每项技能所能使用的点数存在上限，而这个上限需要通过相应行为锻炼才会提高，直到最大值15；换句话说，想要增加探测概率以避开陷阱，就得先找出大量陷阱积累经验。\n\n与陷阱斗智斗勇已经不可避免，基于个人感受，它们主要集中在房间门后以及财宝室里，狭窄的走廊中通常不会设有陷阱，至少我的印象如此；最应当警惕的是平白无故躺在地上的宝物或装备，这些很可能是诱饵，或来自上一名踩中陷阱的倒霉蛋，必须要在周围仔细检查；由于怪物对陷阱没有豁免权，加上它们会主动捡起物品，我后来还有过在一格陷阱上捡到五六套装备的经历。\n当然，时刻记得在开门后不要急于冒进，说不定冲过来的怪物转眼就将旁边的陷阱踩了个正着。\n\n随着我四处磨蹭，一路疑神疑鬼地走走停停，饱食度很快见了底，更糟糕的是我并没有在出发前买足补给，这下不得不品味一番舌尖上的地下城了；实际上这种环节是大多数Roguelike中的保留节目，通过食用某些特殊生物还能获得额外加成与能力，不过眼下最重要的还是确保自己别饿死；很遗憾我不是一只会做饭的矮人，也不会有精灵队友在一旁吐槽食材，所以怪物尸体只能生吃，连调料都没得放。\n这并不意味着Avanor缺少烹饪功能，如果玩家在某个固定层级捡到了烹饪套件，便可以把尸体加工成更优质的食物，具体是指停止腐烂，让尸体的营养翻倍，且重量至多变为原来的十分之一；无法后天习得的烹饪技能不是必须的，但相当重要，它会影响成功概率与消耗时间，玩家没有该技能的情况下，保底成功率只有30%；考虑到烹饪失败不会返还原料，以及购买现成食品的轻松程度，就算提升这项技能也相当吃亏。\n虽然做不出花式菜肴和满汉全席，但至少聊胜于无，估计还有项缺点在于没法修改食物种类，如果东西本身难吃到吐，即使经过烹饪也于事无补；我自己只在机缘巧合的情况下得到过一次烹饪套件，可惜那个存档后来损坏了，而我读到相关源码的时间远比我首次完成游戏要晚，所以当时不知道刷新位置的我就这么错过了它，没能将全世界有头有脸的npc做成料理装进背包通关可太遗憾了（不是\n\n\n第一种被我品尝的是老鼠尸体，尽管描述为tasty但明显非常难吃，角色哇的一声就连带着前几顿全吐了出来；接着我尝试了灰色软泥的尸体，能吃，可不提供抗性就算了，关键是吃下去也有毒；本来只是饥饿状态，现在不仅因为呕吐而濒临饿死，还因为中毒导致生命值也在蹭蹭往下掉，附近似乎仍有具僵尸的遗骸，但那显然不是活人该碰的食物。\n天无绝人之路，背包中还有部分战利品可供食用，几条老鼠尾巴，几片蝙蝠翅膀，以及成堆的骨头；事到如今也管不了太多，只能闭着眼睛都往嘴里塞，至于结果嘛，我成功的活了下来，虽然每件物品都回复不了多少饱食度，也谈不上多好吃，但架不住骨头实在是量大……不管饱，勉强能让角色别饿死；返程路上还碰到了能增加元素抗性的甲虫，无需多言，三两下就进了我肚子。\n靠着这些难以言喻的食物，我挣扎着爬回城里并填饱了肚子，由衷希望店主没有被我饥不择食的模样吓到；不得不承认，在浅层地城里因食物而走投无路时，抓几根骨头啃居然是最为保险的方案。\n\n\n\n清理完一定数量的怪物后，我带回了不少武器装备，甚至有一把秘银制成的刺剑，可惜并不如我手中的斧头好用，我也没有相关武器的熟练度；Avanor并不像大多数Roguelike那样有着BUC状态，同时无需对武器及盔甲进行鉴定，也就是说玩家不必担心装备上一件未知盔甲后，发现它不仅属性烂得要命还因为诅咒而无法脱下。\n卷轴与饰品却并不遵循这一规则，使用或鉴定前无法得知其效果，我面临的正是这样的问题，虽然没有会勒死角色的护符，但当时知之甚少的我并不敢贸然戴上未知饰品，生怕捅出什么大娄子；另外与ADOM一样，商店里出售的物品都相当于被鉴定过，玩家浏览后即使在其他地方遇见同种物品仍然可以将其认出，这点真的要比前辈Nethack方便太多。\n当我在商店中买下一张鉴定卷轴后，才想起来这名角色是没有读写技能的文盲，即便拥有卷轴也无法使用；手册中提过可以花钱找人学习读写，但并没有说明详细位置，我只好寄希望于在后续的流程中碰到那位贵人，并揣着一背包未鉴定的戒指与护符重返探索之旅；介于店主只会收购与其出售物品类型相同的物品，无法卖掉又较为沉重的盔甲等杂物则被我留在了一间空置的小屋中，毕竟Avanor也有负重过高会降低速度的设定，npc不会打开关上的房门，掉落物品也不会自然消失，所以能安全地往屋里存放物资。\n\n尽管猜到了装备没有诅咒与祝福状态，但获得水瓶后我还是忍不住想尝试制作圣水；在王城传送门往北走几步便能看到一座大理石教堂，里面有几排教堂长椅和一块同样是大理石砌成的祭坛，阿瓦诺的大祭司则位于旁边祈祷，与她对话只能得到“祝福着你”的系统消息，并没有什么任务或交互选项。\n基于Nethack式的制作方法，我先将水瓶放在了祭坛上，它们并没有表现出任何异常，我随即站在相同的位置使用了祈祷功能，一道耀眼的白光一闪而过，成功了？不对……我水呢？变不出圣水在意料之中，没想到的是水瓶直接成为祭品被献祭掉了。\n\n既然说到了教堂与祈祷，那就不得不提Avanor中的两位神明，Marduk（马尔杜克）和Tiamat（提亚玛特），都源自于美索不达米亚神话，隔壁Nethack里要夺回的Yendor护符在设定上即是马尔杜克的宝物，当然，护符的传说来自后人虚构；游戏作者可能受到DND等流行文化影响较重，将祂们简单地设定为了死亡与生命之神，如果玩家在游戏中杀死有生命的活物便会引起生命神的反感，消灭骷髅亡灵等不死族则能够增加祂的好感，死亡神设定上大致相同，只不过喜好完全相反。\n通过献祭高价值物品的方式也可以增进与神明的关系，并且不会惹恼另外那一位，对神的信仰越虔诚，可以向神请求的帮助也就越多，小到治疗轻伤，大到直接对玩家的敌人降下神罚；由于信仰上限远比魔法上限要高，走神官系职业的角色不仅能对自己进行源源不断的治疗，还能够向敌人释放出狂风骤雨般的远程打击，单论持久能力远比纯粹的魔法师要高得多。\n听起来似乎很强，但上面提到的一切都建立在拥有足够的信仰之上，毕竟帮助不是没有代价的，可信仰这东西并不像魔力那样会自动回复，必须得让神满意才行；反映到游戏中就是这不能杀那不能杀，捡到点值钱的玩意还得先考虑献给神明，更痛苦的是就算玩家想要与怪物和平相处，也改变不了它们仍然会主动攻击玩家的事实，体验下来估计相当折磨。\n\n还有个问题，游戏内的信仰系统并不怎么完善，能祈求的神术种类少得可怜，所产生的攻击与辅助效果经常是复用普通法术充数；事实上这部分也确实只是个半成品，根据作者在定义中写下的注释来看，Avanor本来应当还有“地”“水”“风”“火”四方神明，然而随着游戏的停更，这一切也化作了泡影。\n直到通关我也没有想成为神职人员的意愿，绝大多数时间跟两位神明的关系都非常差，幸好祂们胸襟宽广，不会拿闪电劈我，也不会派遣神使把我彻底抹去。\n\n重新上路探索，我从王城离开后，在森林中一路向西穿行，本来只是想看看附近还有没有类似的地城，结果走了一阵子之后迎面撞上群强盗，虽然我的实力和装备都有所进步，但仍然寡不敌众被乱刀砍死；临终前我意识到了一件让我极度震惊的事情，这个世界，竟然是无缝连接的，而刚遇见的强盗也正是村口那伙人。\n解释一下为何我会如此震惊，像是同样拥有完整世界的ADOM及其后继者Elona，它们都有两套地图系统，一张是用于快速旅行但是操作有限的大地图，另一片是更加细致，可供正常游玩的区域；两种地图的核心差异在于比例尺不同，大地图上移动一格相当于游玩区域走上数百格，但后者有着空间限制，即使两处地点在大地图上相邻，玩家也无法在游玩区域直接行走过去，必须经历走到区域边缘、选择离开、在大地图上移动、进入新区域这几个步骤。\n而Avanor并没有大地图，玩家能在世界中无缝访问各个地点，包括在同类型游戏中很难见到的地下城表层建筑，即便时至如今，拥有这一特性的Roguelike仍然屈指可数；新手村的那扇传送门真的相当误导玩家，让人下意识地认为游戏切换了地图场景，最终发现真相的震撼，与近几年发售一部RogueLite作品《Noita》非常相似。\n无缝的开放世界也为玩法带来了更多的可能性，比如能将追逐玩家的怪物引到城内交给守卫解决，又或者是在大片的林地中边跑边攻击，把敌人当作风筝拉扯；利用不会被刷新的机制还可以将多余的战利品藏匿在野外，不必像其他游戏一样担心进出随机区域后导致里面的物品消失。\n\n读档后角色重新复活在了王城，还没从刚才的震撼中缓过神来的我决定先把自身的实力提上去，不然连探索世界的资格都没有，转头便带足补给到邪法师的地城里接着练级。\n向下深入的路途上基本没遇见什么麻烦，但在第5层走下楼梯后，周遭的环境突然一改画风，出现了极为广阔的空间；角色背后是面石墙，在昏暗的灯光下仿佛无边无际，眼前则是由黑曜石铺成的道路，似乎在引导着来访者前进，顺路走上几步便能听到潺潺的流……好吧，Avanor并没任何声音，音效全靠玩家脑补；忽略掉流水声，在前方静待玩家的，是条狂澜暗藏的地下河流，绵延的黑曜石凌空跨于两岸，通向一座规模庞大的地下宫殿。\n走过黑曜石构成的桥梁，推开地宫大门，一间地板同样是黑曜石铺成的前厅出现在了视线中；厅中没有任何陈设，空旷得让人感到不安，我在门口反复确认了许久，断定不可能有陷阱后才缓缓向里走去，打开第二扇门，藏在后面的是条狭窄且一眼望不到头的走廊，与外面的空旷形成了强烈的反差；我小心翼翼地继续前行，走到尽头隐约能看见大厅时才发觉到，有两名全副武装的黑骑士正守在出口，就如同等待猎物走进圈套一般。\n\n它们一手拿长剑，一手持塔盾，浑身上下都被漆黑的重甲所包裹着，这正是邪法师的精锐部队，没有感情只崇拜死亡的杀戮机器——死亡骑士。\n我顿时如临大敌，摆好架势做足了与它们决一死战的准备，回合制的优点在此体现的淋漓尽致，玩家即便身处险境也仍然拥有充足的时间来思考；首先它俩处于我的左右前方，如果我率先发起的攻击不能一击必杀的话，将遭到敌人的双重反击，从装备来看这当然不现实，必须另寻他法。\n根据先前与古代恶魔交手的经验，可以推测Avanor中的高阶敌人不一定会具有压倒性的速度优势，而面前这两位显然是注重防御的重装战士，自然就更不可能拥有远超于我的速度；那么后退就是此时最佳的决策方案，狭窄的过道只能容下一人并行，势必只有一名敌人能够紧跟着追上来，并刚好阻挡在我与另外那名敌人之间。\n无论如何，这至少能让局面从二对一变得更加有利，要是死亡骑士的速度与我接近，那我甚至可以撤退到楼梯处再逃之夭夭；虽然某些敌人似乎会追着玩家上下楼，但现在不想坐以待毙也只有放手一搏，确认完战术模式是全力防御后，我便操纵角色猛地向后一撤，是生是死就看这一步险棋……吗？\n奇怪的是，身后并没有任何东西跟上来，我愣了一下，随即小心翼翼地又等待了数回合，才确认了根本就没有敌人在追，毕竟它们速度再慢也不可能连玩家的几十分之一都不到；我壮着胆子再凑了上去，两名死亡骑士仍然一动不动，静静地矗立在走廊尽头的两旁，好像刚才不过是只苍蝇飞来罢了；结合它们的行动与之前疏漏的事实，我心中大致有了种猜想，便试探性的朝着一名骑士的位置移动，果然，系统弹出了“您是否真的要攻击……”的提示，这死亡骑士竟然是彻头彻尾的中立单位。\n\n也对，如果它们与角色敌对，那我走到尽头后再轮到其他生物行动的一瞬间，我便会身首异处，压根就没有继续思考的机会；捡回条命后总算是能把悬着的心放下来了，地宫的主人似乎并不打算加害于我，这番“盛情难却”之下也不免让人好奇后面究竟还有什么，眼前即使是龙潭虎穴也得闯上一闯。\n大厅中央是座黑曜石祭坛，跟周遭环境几乎融为一体，其供奉对象不用想都能猜到是死亡神马尔杜克；后边不远处有六名死亡骑士整齐的列于过道两侧，装备类型与之前那俩位别无二致，它们簇拥着游戏的反派头目，邪法师Ahk-Ulan；遇见大魔王也倒是意料之中，毕竟任务目标就是除掉这家伙，但整整八名精锐战士配上一名深不可测的施法者，我这样羽毛未丰的冒险者只有脑子不开窍才会动手。\n敌我实力悬殊，可试着交谈过后，这位大魔王居然也有求于我，他声称一帮法师摧毁了他的塔楼，他现在正为了复仇而积攒力量，需要玩家为他取得三块古代机器的部件，并答应事成之后给予丰厚的报酬。\n怀着复杂又矛盾的心情我原路返回了城中，苦思冥想也没搞懂正邪两方咋都喜欢给我这样一位无名小卒派任务，还件件都涉及到世界的危急存亡般重大；不过目前想太多也没用，邪法师我现在弄不死，古代部件找起来也没有一点头绪，到头来貌似依旧停留在原点。\n\n转念一想，邪法师完全不提部件位置，这说明任务设计者在潜意识里就笃定玩家能轻易将其找到，或者后续会有内容引导玩家；另外既然是邪法师委托我，那就表示部件所处位置对于他而言难以进入，而对我来说能够轻易抵达；结合这两种因素，嫌疑就重点指向了戒备森严的皇宫，虽然村子西面那座蘑菇洞也很可疑，但邪法师一人估计就能把那村里村外屠干净，不太可能将我当做战力派过去。\n所以眼下的首要目标是摸清楚王城，在教堂与永恒之火的东边就是皇宫，相较于邪法师朴素的藏匿处，这里明显更加奢华且富有生活气息；花园，餐厅，起居室等日常设施一应俱全，还能见到不少皇家守卫在各处站岗，但他们并没有对我在皇宫里到处晃悠提出任何异议，说不定是因为里面确实没啥好偷的。\n唯一让我在意的是皇宫花园的西南角附近，那里有处被守卫严密把守的下行楼梯，与他们交谈没法得到任何反馈，想下去的话战斗似乎不可避免，只好暂且作罢。\n\n到王座厅找国王对话可以领取到清理墓穴的任务，虽然这位君主身边同样有不少守卫护驾，不过这种奇幻世界里能位居一国之君，其自身实力必然不可小觑，本人很可能是前代勇者之类的人物，据我后来的交手经历，即便城内所有卫兵加上大魔法师和祭司也不会是国王的一合之敌，守卫更多是作为权力象征顺便起装饰作用；邪法师就算带齐部下估计也很难从国王手底全身而退，这可能解释了为什么他不敢主动出击而需要积攒力量。\n令人费解之处在于国王为什么不亲自去墓穴把入侵的不死族处理干净，这对他来说应当是举手之劳，或许是懒得动以及不想跟死亡神搞坏关系吧，另有隐情也说不定。\n\n墓穴位于王城的西南方，离邪法师塔的遗迹很近，地表建筑像是一座石丘；穿过甬道和几扇墓门就进入了大约是前厅的地方，此处现在充满了骷髅，它们察觉到活物存在后便一窝蜂地涌了上来，我起初应付得还比较谨慎，站在甬道里等它们挨个冲到跟前再逐一解决；可这堆没有武器盔甲且不知道腐烂了多久的骷髅实在是构不成什么威胁，攻击迟缓容易避开不说，即使命中了被防具阻挡后也产生不了多少伤害，我后来干脆换成狂战模式边往前冲锋边砍瓜切菜，结果hp都没怎么动弹。\n清理完表层我便顺着楼梯来到了地下，整个墓穴除去表层建筑后只有一层，里面看似空无一人但实际上却充满了各种幽灵系怪物，正常情况下玩家无法用肉眼看见它们，必须佩戴足够强力的see invisible戒指或护符才行；然而正如前文提到的，我这名角色是文盲，鉴定不出饰品，这导致隐形怪物对我来说相当棘手，只能将它们引到狭窄的墓道里，同时不断向空气挥斧以便“凑巧”击中这些鬼魂。\n然而事情并不总会按照预期那般一番风顺，地下的清理过程堪称胆颤心惊，各种鬼魂不仅行动诡谲，伤害还奇高无比；有时候拉扯不到位，砍了半天空气发现没鬼过来，有时候又不小心走得太深，直接被数只怨魂围殴，要是运气不好碰上几只特别厉害的惧魔（Dread），生命值三两下就能见底。\n\n光是看不见敌人倒也还能接受，更大的问题在于出现了极为严重的bug，本来只是十几级再升一级，可游戏在升级完成后依然反复弹出升级界面导致角色的等级一路飞窜至了近百级；刚开始我以为是技能加满后剩余的改进点数无法使用而造成的，毕竟这游戏前中期的等级提升速度高于技能上限的增加速度，但后续流程中该bug不定期触发，以及在新建的低技能角色上同样出现的情况否决了我这一猜想。\n结合触发bug的大致情形，似乎是输入过快以及移动键的问题，因为角色仅仅连升几十级就停了下来，差值范围远比类型错误以及死循环等情况要小；游戏会在升级过程中读取玩家输入用于技能加点，而升级前我正巧在不断按下方向键以砍杀怪物，Avanor推荐使用数字键进行全向移动并且不支持键位映射，但这台笔记本没有NumPad，所以大部分时间我都用方向键进行移动，那么很可能是我的输入队列卡到缓冲区影响了调用或表达式，让升级函数多执行了几十次；考虑到我和游戏之间隔着一层虚拟机和几套不同的字符编码，这一切也不是说不通。\n后来翻阅源码时倒没找到符合我这一猜测的相关片段，反而发现程序有块部分一直在监测等级最高的生物，仿佛有种惊天大阴谋的感觉。\n\n\n嘛，就结果来看，我轻而易举地超越了等级的设计上限，这项数值大于50级再查看经验界面就只会显示一片漆黑；虽然技能全满后升级仅能依照属性增加些许生命与魔法的上限，但这样的bug还是极为影响游戏体验，我后续游玩中因为此漏洞在内的各种运行错误而重开过不下几十趟，不过为了叙述方便，本文还是尽量将其串成一场完整的旅程。\n杜绝类似的问题不太可能，最好还是经常保存（尤其在升级前），并对记录文件进行定期备份，尽管类似行为就Roguelike这种常是永久死亡的游戏而言较为“可耻”，但现在为了减少程序问题产生的损失也顾不上那么多了。\n\n幽灵系怪物无比难缠，好在角色可以随时退回地表恢复状态，毕竟任务没有限时，不断消耗之下还是艰难地完成了清理工作，可扫兴的是这些鬼魂没有形体，自然也不会掉落任何战利品；大失所望的我将目光投向墓中陈设，整座墓穴当中没有任何陪葬品能够拾取，甚至连装饰物都没有，除了建筑结构之外就只剩四丘坟墓，墓碑上刻有死者的姓名与生卒年份。\n意外发生于我脑子一抽后，鬼使神差般地对一间耳室内的坟墓按下了使用键，结果哐当一下掉出把长剑，这把剑在黑暗中闪烁着微光，一看就不是凡品；捡起剑后我整个人都懵了，因为这把长剑实在是让人难以置信的强悍，手中战斧骰满得到的最大总和，还不如这剑的伤害加成，再一对比命中修正，我的斧头仿佛成了根极为笨重的棒槌，另外长剑的属性增幅还能让我的力量和体质翻上近一倍，这些数值就算到后期都不太可能有其他武器超过。\n此剑名为阿瓦诺之防卫者，乃是该游戏中不可多得的unique item，这些独特物品大多都性能优异，固定生成且仅有一件；我拿到它之后脑子里没有开心或激动，想的净是得赶紧去把其他几座坟给掘了，说不定有整套盔甲和盾牌呢，结果是除了几根骨头之外一无所获。\n\n说实话，我一周目里清完怪物就走了，并没有拿到这把神兵利器，后来重开的时候虽然在读源码，但也没有翻到与之相关的部分，能从四座坟墓中一次就选对这座靠后的，纯属机缘巧合。\n\n当作是同一场旅程吧，我兴冲冲的跑回皇宫交任务，没想到国王居然一眼认出了角色手上的长剑，并以打扰先祖安息为由，直接痛下杀手；作者整这一出也不知道该夸人心细还是说人恶毒，无奈，只好再次读档。\n将武器卸下藏回包里才能正常领取报酬，国王会感谢玩家并给予1000金币，再次对话还可以接到一项后续任务，他说几年前有名非常受信任的仆人偷走了一件强大的神器，Eye of Raa，那名仆人后来企图将其藏到南方的一处地洞里，但人们声称他在藏匿途中就被杀害了；国王希望玩家能够找到这件神器并带回给他，不过一件神器丢了几年才派人找，究竟是心眼大还是暗藏玄机，莫非这件神器就是邪法师要找的部件？当时的我不得而知。\n\n路过塔楼遗迹，再向南穿过一片密林便可来到地穴上方，如果再往前走一段的话可以看到环形山脉的南部；地穴没有什么显眼的地标，看到下行入口直接走过去就行，然而，即将迎来的是游戏中最为折磨的部分。\n地穴主要有两种类型，一种是房间式的地下城，另一种是不规则状的洞窟，但里面的怪物类型倒没有什么差异，主要威胁来自于妖精（goblin），他们的装备良莠不齐，有全身上下都被精金秘银所包裹的，也有装备栏位还空着的；危险之处在于这些家伙拥有智慧，不仅会从地上拾取武器盔甲并替换穿戴物品，还会主动喝下隐形等药剂为己方在战斗中增加优势，甚至能向玩家一样对神明祈祷和献祭。\n\n\n怪物的ai和背包系统并不是问题的元凶，这种合理的强度反而能为游戏增添趣味；真正值得诟病的是随机地城的设计，我初见时幸苦杀穿整座洞窟后并没有找到什么所谓的神器，对着截图回忆了一番任务文本才猛然领悟到了“one of the caves”和“was killed while hiding it”的含义；一座地穴约有十层，而这样的地穴有整整三座，那名仆人可能死在其中的任何一层，并且怪物会拾取装备，这使得玩家必须地毯式清空每个层级的每个角落，一只怪物都不能放过。\n从todo list不难看出，搜索和暗门属于未实现的内容，然而玩家却可以在地城中遇到完全密闭的房间，直接被困死；算上初见清了两趟，地图生成错误又再找了几趟，通关又是一趟，累计下来我估摸着扫荡了百余层地牢，这个战绩放到隔壁Moria（以流程拖沓而著名）都够通关了；我也是从这一阶段开始被迫审查源码以继续游戏。\n\n无独有偶，Avanor也有gelatinous cube（凝胶方块）这种怪物，且在随机地城中大量分布，它不像在其他游戏的表亲那样人畜无害，而是能麻痹玩家后继续攻击并叠加麻痹效果，直到死亡，在生命值因为升级bug而来到四位数时，它仍然可以一套连招让角色直接去世，丝毫没有还手的机会。\n处于麻痹状态时，玩家无法进行任何操作，甚至无法键入退出游戏的指令，想主动读档都做不到，只能拍空格以继续显示战斗消息，主打一个无助，过高的hp又使得该过程变得更加漫长，我后来都养成了在被麻痹时干脆重启整个虚拟机的习惯。\n\n应对方法也有，那便是佩戴free action类的饰品，先不考虑当前角色是名文盲，无法识别出道具功效，即使正确佩戴了相关饰品的情况下玩家依然可能会被麻痹，因为饰品提供的抗性强度是一个随机值，而该数值不会直接展示，想判断强弱只能通过在店里试图出售的方式查看价格，价高者的效果自然更好。\n但Avanor并没有类似于Nethack那样能对装备起别名的功能，如果玩家移动高属性饰品时没有清空同类物品，那么它们会直接混到一起，得手动再次鉴别；我后期嫌麻烦，把两个戒指位和一个护符位全装上了free action，以量取胜且干脆不换了，毕竟其他攻击的致死率远远无法相提并论。\n\n头几回我都无功而返，不是杀到最底下啥也没找着，就是走一半到了死胡同，所以便打起了其他地方的主意；从已知的情报来看，被山脉围住的区域里西北角是村庄，东北角是王城，东南角则是森林地穴，那么西南方向有什么呢？于是我转头一路向西，途中除了密林之外别无他物，相当单调，最终在横跨了大半个区域后遇上了一群orc（半兽人），他们看到孤单的旅行者路过自然是不分青红皂白的攻了过来。\n开放世界的好处体现出来了，尽管这群半兽人在前中期都相当致命，但只需稍微往北跑上一段路就会见到一座军营，而这里的士兵与半兽人有仇恨关系，玩家可以利用他们御敌或者脱身，运气好还能渔翁得利，趁乱捡些死者的遗物。\n这里有家武器店，允许出售和购买武器，如果与守卫队长交谈将得知他一直在担心半兽人们会随时攻打过来，玩家消灭掉所有敌人则可以得到一柄独特的匕首，名为Death Hack，除了基础属性不错之外，对恶魔还有整整三倍的额外伤害加成，难怪新手村长老让玩家到湖对岸寻求帮助。\n拿到它的过程也挺有意思，当时的军营被半兽人们杀了个片甲不留，而我再经过此地时已是一身神兵利器，非常随意的就报了曾经的追杀之仇，找了几圈没有看见队长的遗物，还以为任务失败了武器不生成，没想到临近通关回王城的时候，在护城河外见到了这名踌躇不前的逃兵队长，他交出匕首的那一刻仿佛放下了千斤重担，也不知道是什么支撑着他横穿整片地图，从最西跑到最东边，希望他之后能以普通人的身份，回到城内与家人团圆吧；纵使我能直面源码，知道这只是npc随机移动的结果，但程序生成的情节加上脑补，依然是这类游戏最为迷人的特点之一。\n\n\n\n营地上方的一排房屋中，最左边那间有条下去的路，通向一层似乎是牢房的地方，在源码中被标记为“老鼠地窖：1”，虽然有个数字但并不存在第二层或者其他的地窖（与墓穴相仿），也许是本打算在未来进行拓展的内容；里面确实有非常多的老鼠，还固定生成两只鬼魂，错综复杂的小道后面能找到一箱武器盔甲，旁边还有口箱子则装了一大堆魔法书和卷轴，唉，只可惜吾乃文盲，目不识丁。\n四角的大房间内还固定能找到一件森林兄弟……或者说绿林好汉的披风，很可能来自被处极刑后遭到老鼠啃食的受害者，它的防护能力约等于没有，我头回见到的时候还差点当垃圾给扔了，但它却是件定义在unique item当中的独特物品，关于它的用途与故事，则暂时先留个悬念。\n\n离开军营往南，略过那群半兽人之后将看到一座残丘，上面本该有个入口能直通半兽人洞窟，但它只存在于作者的todo list，并未实装至游戏；\n弄错了，后来读源码发现那个入口通往独眼巨人巢穴，翻山进去击杀boss可以得到全游戏最重的武器：黑曜石大棒，质量为8000游戏内单位。\n往西是环形山脉的一部分，无法逾越，但如果顺着山脉的走势往北前进，小心翼翼的穿过湖泊与山体夹缝之间的窄路，就又回到了新手村，没错，虽然看似不可通行，但湖泊旁边确实有条小路可供行走，并且这条路靠山的一侧还有处通往矮人王国的密道。\n\n\nL:PDC3于源码中的完整描述是&quot;Path to the Dwarven City Level 3&quot;，需要穿过6层这样的地方以抵达矮人王国，并完成那里的任务以获得pickaxe（十字镐）用于挖掘墙壁；但眼下在接任务的半路上遇到了完全闭塞的房间，陷入了拿不到镐子无法前进，无法前进就拿不到镐子的死循环。\n不死心的我拾取了半天空气也只搜出了一个无关紧要的陷阱，周围的墙壁依然纹丝不动，这种地图生成问题曾在之前的随机地城出现过，获取镐子的主要原因之一便是需要挖开那边的洞穴；现在的情况约等于宣告死档，只能重开，缘由可能是作者打算添加隐藏门，改完生成机制后忘记相关功能还没做就直接发布了游戏。\n除了十字镐这件工具之外，还有一项挖掘技能，该技能无法后天习得，只可通过开局创建角色时选择矮人或地精时自带，我一度以为缺少技能便无法使用相关工具，还庆幸自己刚开始就选对了种族，不过后来阅读源码才了解到，技能仅会加快挖掘速度，并不影响能否进行挖掘。\n\n\n\n从叙事角度考虑，还是当作一切正常继续把故事讲下去，路上都是些常见的怪物与陷阱，随着深度而逐渐变得更加危险，快到目的地时还能捡到烹饪套件，没有看见的话多半是被妖精什么的捡走了，需要满地图追杀。\n矮人之城相较于人类的王城显得十分朴素，大体上都是灰白色的岩石基调，规模也小不少，路过居民区和中央广场的生命祭坛就抵达了矮人王的居所；另外城里有位铁匠特别容易错过，通常他会呆在商店旁边的屋子里，但他也可能和广场上的人群混到一起，我通关后翻阅npc的对话文本才发现有这么号人，花钱可以让他为武器添加元素伤害，以及对半兽人的额外攻击倍率。\n\n与矮人王对话将接到收复矿坑的任务，不过这家伙的脾气非常暴躁，没完成任务再与其对话时他会让玩家滚出去；矿坑里倒没有半兽人和炎魔，全是从岩石中渗透出来的神秘毒气，玩家需要前往最底层启动一台气泵把毒气抽干。\n设计比较蠢的地方在于它纯粹是个“数值检测器”，任务未被标记完成前每走一步都有概率会强制扣上2到7hp，不存在任何豁免，尽管游戏中有着毒素抗性和魔法戒指等各类道具，玩家能做的却只有走上一段再喝口治疗药剂让生命值别见底，要是恢复能力足够高甚至连喝药都免了；毒气矿坑因为可以稳定掉血且没有怪物，刚好还非常适合刷恢复技能的等级上限，代价只是几份食物和一点微不足道的时间，就可以将上限刷到最高值，突出一个无脑粗暴。\n\n完成任务获得十字镐之后也没什么留在矮人之城的必要了，本来打算立即动身，但矮人王右手边紧锁的门扉勾起了我的好奇，门旁一左一右两名精壮的矮人战士更凸显了它的不凡；靠近后玩家会立即遭到他们驱逐，可我现在不仅神剑在手，数值也经历了相当的积累，警告只当作耳旁风，毕竟此处不光没有了利用价值，矮人王的态度还十分恶劣，早就看不顺眼了。\n随着我一脚将门踹开，两名门卫意识到劝说无果，挥舞着兵器就和其他士兵一起攻了过来，可这些攻击要么被闪开，要么被防具阻挡了绝大部分伤害；我三下五除二解决了他们，顺着前方的阶梯走下，展现在我眼前的，正是矮人王国的宝库，但遍地的黄金与宝箱之间，潜藏着大量的传送及其他陷阱，不幸触发便会被转移到没有出路的幽暗洞穴中，万一没有挖掘工具，将会活活困死在洞里，不得不说很有矮人作风。\n抛开那些司空见惯的财物装备，宝库中最为让人惊喜的是一块古代机器部件，正是邪法师苦苦寻找的三件之一，结合门外的状况，几乎可以断定皇宫花园那处被严加看守的地方就是王城宝库了，里面多半也有古代部件。\n\n简单的置换一下身上的盔甲，又挑了点暂时还看不懂的魔法书带走，这座宝库便搜刮完毕，返回刚才的事故现场，门卫尸骸仍倒在血泊之中，虽然逻辑上是进门时他们对我先动的手，但在我“不正当防卫”后，角色依然与整个矮人王国产生了敌对关系，场外的其余士兵甚至市民都在蜂拥而来；令人啼笑皆非的是矮人王非但没有率先发难，反倒逃去了墙角，可惜游戏没有更详尽的姿态描述，是否在角落瑟瑟发抖我们无从得知，姑且当作他是担心腹背受敌吧，哈哈。\n战斗过程没什么可说的，几乎一面倒的屠杀，唯一一点小插曲大概是有名士兵喝下隐形药剂，让我不得不与空气搏斗一番；矮人王虽然躲挺远，但其战斗能力依然大于所有卫兵之和，交战过后我的hp定格在了164/267，损失的近一半生命值中，估计90%都是由他造成；npc的战斗ai要是懂得合围，重伤后撤，利用远程击破走廊战术，那鹿死谁手还不一定，眼下这帮乌合之众可改变不了我完胜的局面。\n不出所料，堂堂矮人王者果然有不少独特装备，最为引人注目的是一柄重1200单位的巨斧，该重量相当于10件钢铁鳞甲，其威力自然不言而喻，除此之外还有一顶王冠与一面Torin之盾，它们穿戴后都能够增加属性，尤其是体质方面。\n\n\n\n遗憾在于我最不缺的便是体质属性，因为初始种族选择了矮人，不但体质基础值高，还可以安全地通过食用怪物尸体继续增加，更重要的是在我随机地城的战利品中有一双“至高王之靴”，它能提供整整25点体质，此数据比穿戴矮人王全套装备的加值还高，可惜这双靴子仅是带有高级词条的随机物品，而非独特的unique item。\n\n\n离开了矮人王国，准备利用传送门快速回到王城，虽然西侧小路可以直接抵达村庄，可湖泊东面的那片未知着实引人好奇，于是我向东从军营与湖泊之间穿过，打算绕湖一周从村子正门回去。\n没走出去多远，一座模样怪异的建筑出现在了面前，似乎有几分眼熟，凑近之后猛地一拍脑门想起来了，这不就是邪法师塔楼的完好无损版吗；顺楼梯上到塔顶，见到位名字非常日系的精灵巫师叫Yohjishiro，与她对话可以花500金币学习识字，没错，我几乎把整个世界绕了一圈才终于在回到原点的时候摆脱了文盲身份，而此时背包里的金币已是数以万计。\n很大程度上要怪角色视野不足，巫师塔就位于军营东北方向不远处，甚至开局都可以非常安全地从小路绕过来，但我却三番五次与它失之交臂，并因为不识字而在冒险之旅上吃了许多暗亏；除了能教识字以外，这位巫师还可以为玩家鉴定携带物品，而代价只是一条老鼠尾巴或者一片蝙蝠翅膀，还记得我曾在初出茅庐没有食物时而吃过它们吗？这相当于我在把珍贵的全背包鉴定卷轴当饭吃。\n\n\n\n那两样东西倒不是说有多难得，顾名思义只要杀老鼠和蝙蝠就有可能掉落，而且概率高达整整四十分之一（捧读），问题是这两类生物几乎不会在中高等级的地城中出现，如果专门为了刷它们而前往没什么经验的低级地城又十分吃亏，显然是种仅对新人友好的机制，使得我如鲠在喉般难受。\n与之类似，军营中还有条叫Gekta的大狗，给予骨头后它会从地里刨出一些东西作为回报，可由于军营被半兽人血洗，我直到通关后翻阅源码时才留意到这一隐藏功能。\n\n\n识字后终于可以让积压的卷轴与魔法书派上用场了，虽然看书学了很多乱七八糟的的法术，不过真正常用的却只有鉴定术与无属性的魔法箭，前者用于探索时辨别高价值战利品，后者适合在绝大多数情况下作为远程辅助手段。\n绕着湖泊来到村庄正门，作为新人杀手的强盗们依然是二话不说就杀了过来，可对如今的角色来说犹如以卵击石；将他们当靶子试验完新法术后，在其掉落物里居然发现了绿林好汉的披风，与我之前在军营地下室捡到的unique item一模一样。\n结合他们在村口却不攻击村庄的情况，这些所谓的“强盗”应该是劫富济贫的绿林好汉才对，那军营地下室里的鬼魂是他们的同胞吗？为何只留下了一件披风，有人成功逃脱了吗？两口宝箱是原有的储备物资还是哪次行动被缴获的赃物？当地正规军与他们还发生过哪些摩擦？悬念太多让人一头雾水，但完全有理由怀疑这是什么依托于物品的碎片化叙事。\n看着眼前的披风，我突然冒出了个大胆的猜想，随即将角色的服饰卸下，换上捡来的绿林披风，并试着朝着角落残余的“强盗”走去，结果如图设想一般，他们居然真的没有再主动发起攻击，伪装成功了；虽然一周目我不小心杀了他们不少弟兄，但后来我专门戴上隐形戒指，不伤一人的情况下将他们同伴的遗物送还了回去，意想不到的是他们竟然对我放下的披风进行祈祷，然后把它献祭给了死亡之神马尔杜克；虽然我知道这是npc处理多余物品的常规行为，但我更倾向将其认作是送还失物，悼念亡者的一种仪式。\n\n\n山洞里的恶魔老朋友也当然不能忘记，经过数座地城洗礼，现在的角色早已今非昔比，不费吹灰之力便可手刃仇敌，考虑到手上盗墓弄来的先王剑对它也有额外伤害，最终斩杀恶魔只用了两刀。\n这头邪恶生物的尸体似乎化为了飞灰，掉落物仅有一枚戒指，名为“伟大元素之戒”，游戏内没有途径显示物品描述，但从源码中可以了解到它提供几种元素抗性与看破隐形，具体数值以毕业装的角度还算过得去，不过为了预防麻痹至死，大多数时候我并不戴它。\n\n恶魔所在的一层有两处向下的道路，一处通往钴鬼（kobold）洞穴，另一处通往蘑菇山洞，先不提我没有草药学技能，摘了蘑菇也没法加以利用，光是站在这鬼地方就有概率永久降低角色的属性，也不知道恶魔和那些村民怎么会看上这种凶险之处。\n至于隔壁自然是有成群的钴鬼，细分下来还存在萨满和弓箭手等远程杂兵，对初期玩家应该算不小的威胁，然而它们的掉落物基本上都是些破烂，源码里也表明此处仅会生成一只没有特殊装备的头目，但玩家都有能力击杀恶魔了，实在是想不出有什么来这探索的理由，或许让下去的玩家倒霉就是它唯一的意义。\n\n村里的待办事项全部解决，向长老报告完成情况，好，如我所料没有任何奖励，毕竟恶魔已经“给”过戒指了，然后……然后游戏就又崩溃了。\n不过这次崩溃与以往的任何一次都不同，它非常稳定的出现在与长老交谈的瞬间，如果不交任务那么大家相安无事，但一旦对话游戏程序便会炸成一团灿烂的乱码，找不着解决办法，报错信息也看不出个所以然，倒是视觉冲击力相当震撼；翻了翻源码，发现既没奖励也没后续任务，那少听一句感谢也损失不大，其他任务别出问题就行，可墨菲定律告诉我们，越担心什么它就越有可能发生，等着瞧吧。\n\n既然已经提到了程序崩溃，就顺便再说说存档损坏的情况，问题出在保存进度是通过直接覆写存档文件完成的，没有创建副本；本来无伤大雅，但这游戏有概率在储存时崩溃，而文件覆写中断会导致格式失效并无法再被读取，意味着永远丢失进度只能从头再来。\n储存时坏档的概率大致上取决于数据复杂程度，刚开局那会儿一般遇不上，但到了后期，即使在森林里跑一跑都有可能乱码，这时候存个档堪称心惊胆战，还好我曾因为升级bug而备份了巨量存档，坏档问题虽然也碰上了不少次，幸亏都没造成太大损失。\n长话短说，重返王城沿着熟悉的路线再次下到随机地城底部，依次挖开封堵的岩壁以寻找道路，功夫不负有心人，三座随机地城里有两座都存在隐藏洞穴，一路往下地毯式搜刮，最后我成功于一只惧魔附近找到了Eye of Raa这件神器。\n\n历经艰难险阻，我自然希望这件神器有个惊天动地泣鬼神的外观，最好在拿起来的时候再出现一道直冲天际的金色闪光，遗憾的是在这种字符世界里它的长相就是个‘*’，甚至连物品描述都没得看；从源码中得知它能提高精神相关的属性，还能施放一种不耗mp的远程攻击，但最大的优点是它属于工具类别，不用跟其他饰品抢装备栏位。\n与随机地城永别，到皇宫提交任务，时刻记得在这里要将盗墓弄来的先王剑藏好，不然一对话国王会翻脸；给出神器之后国王戴上它，深刻地感谢了玩家两句话，接着呢？接着就没下文了，不光把东西拿走连奖励也不给，感觉就差一句“最珍贵的回报正是你一路上的成长”。\n发送消息，标记任务完成，给出物品，结束操作；四行代码让我起了杀心，之前清理墓穴好歹给一千让人尝个甜头，这次忙活这么久居然一毛不拔，那就不要怪我心狠手辣了；拔剑斩击一气呵成，但是手感有些不对，臆想中血溅五步的场景也并未出现，查看战斗记录，国王居然躲开了突如其来的一剑，还反手几招将我击伤，其速度之快，破甲力道之强，着实令人汗流浃背，雪上加霜的是周围一队皇家守卫也围了上来，虽然几乎无法对我造成有效伤害，但致命之处在于他们堵住了皇宫唯一的逃脱路线，最终害我饮恨当场。\n\n几番读档与国王“切磋”后，我大致掌握了他的信息，首先他是一名纯粹的物理战士，拥有非常高的速度与闪避率，尽管我防护能力也不低，但我一剑的功夫够他反击三四次，实在吃亏，其次是虽然有相当程度的元素抗性，但他并不免疫射弹伤害与恐惧状态，不会使用刚刚交出的神器，也造成不了麻痹等状态。\n综上所述，我的策略转变成了穿戴增加智慧的饰品，然后在极限射程发射无属性的魔法箭进行消耗，必定命中但是伤害较低，肯定不足以直接击杀国王，所以等他冲过来后势必需要近身作战，好在这个世界持剑穿重甲也不影响施法，可以无缝切换近战模式；接下来我在死亡前大概率能将他的hp削减至恐惧范围，他会逃跑，但此时千万不能追，角色状态较差容易被他回身几棍子送走，保持使用魔法压制，运气好可以将国王的hp消耗完，运气差让其脱离恐惧状态则可以在他冲回来之前喝药，这下再面对重伤国王应该能轻松取胜。\n战前也当然要给自己叠好状态，力量药水英雄卷轴什么的有就都用上，多少增加一些容错，另外不太建议在国王未恐惧之前喝那些轻伤治疗药水，按照对方的速度，hp恢复量可能还没一回合吐出去的多；国王身上也会固定生成不少healing药水，最好提前偷走，不然会增加翻车风险。\n\n最终击杀国王时角色hp还剩四分之一不到，再吃两下重击便会暴毙，mp也空了大半，堪称惊险万分，这离谱的强度除了他是前代勇者外，我想不出还能有什么合理的解释，至于那些烦人的皇家守卫，国王一死早就跑到角落里逃命去了，打扫战场时直接挨个手起刀落。\n除去Eye of Raa，掉落物中还有两件unique item，一顶阿瓦诺的王冠与一柄阿瓦诺的权杖，王冠和矮人王那顶几乎是同款，除了外观不同，所提供的加成都完全一致；权杖则没想到是近战武器，骰面比先王剑大概要低一半，但是能额外增加16点力量，可以考虑拿在副手，不过国王拿根棍子都这么强，要是我这剑给他拿去了那强度想都不敢想。\n从花园旁边的神秘入口下去，底下果然是王城宝库，也如同预料般躺着古代部件，而且还是两块，先前在矮人王国“捡”来的那件在去随机地城的时候就顺路给邪法师送去了，既然三件都已浮出水面，那么激动人心的时刻马上就要到了……吗？在我捡起第二块部件时，游戏程序又崩溃了，这次崩溃得也非常稳定，固定在捡起两块部件时发生。\n\n\n起初我以为这是不让玩家一次性将所有部件带上逃走，必须击败国王能再来一趟拿剩下的那件，但转念一想谁会用进程崩溃当作阻止手段，这肯定又是什么恶性bug；当时还乐观的我想着一件件拿也无所谓，就多跑一趟的事，可等我找到邪法师给出部件时，游戏再次崩溃。\n一部分原因得怪所有生物都有背包设定，包括敌人和npc，这表示我给出的第一块部件真真切切的躺在邪法师包里，而并非交完任务便消失，即使我将第二块部件扔到地上尝试让邪法师自己拾取也无济于事，因为任何物品栏只要含有复数块的部件就会导致崩溃，别无例外。\n也不知道这玩意是不是铀235做的，两坨凑一起就发生临界反应；在反复读档以及试验失败后我决定将目光投向广大的互联网，首先是Roguelike的大书库RogueBasin，其中记载了Avanor在dosbox上存在缺乏长文件名支持的问题，也就是无法在游戏内打开帮助手册的元凶，我尝试切换了模拟器，但文件名与崩溃的问题依旧未能解决，仅能调整出过于清晰的字体。\n\n\n根据官网的更新日志与irc上的求助信息，可以推断出游戏的原生平台应该是win32，但即使我连后续移植的linux版都找到了，也没能发现任何windows版本的踪影。\n曾经还有些人在游玩过Avanor后也留下了自己的观点，从Ben Power在大书库中的评价来看，原生环境下程序运行得堪称完美，他甚至请了病假在凌晨玩到了第二天晚上；个人角度更好奇Jeff那篇充满负面言论的评论在哪里，猜测他和我肯定遇到了不少一样的问题。\n\n\n正当我考虑是否要用源码自己编译个破解版出来的时候，我突然想起了曾经看到过的一篇俄语冒险记录，帖主似乎没有遇到相关问题，只是随口提到过一块古代部件也能完成任务，给出去再偷回来就行。\n再看到这句话我顿时犹如醍醐灌顶，简单阐明一下工作原理，虽然所有生物都拥有背包设定，可任务不会检测npc持有的物品数量以判断完成情况，而是在玩家执行“给予”这一操作时更新状态；换个说法，该任务的完成条件其实可以看作是将部件交给邪法师三次。\n即使物品存在bug无法堆叠，但这一方法自始至终都不会出现两块古代部件，依然是由于背包空间，只需要利用偷窃将部件盗回，对方物品栏中的对应物品也就真的不翼而飞，如此重复至计数器归零，任务即可完成。\n一切由bug引起又以bug终结，理解到背后的运作逻辑时我不由得拍案叫绝，利用漏洞欺骗游戏系统以绕过故障，酷毙了。\n\n不出意外的话又该出意外了，查完源码发现能教玩家偷窃技能的大师在村口那群绿林好汉里……已经不小心横尸村头了，只得被迫回档，本文姑且当作无事发生吧。\n随着一次次给出刚偷回来的物品，任务进度变成了3/3，与邪法师交谈他会称赞玩家并提出最后一个请求，杀死阿瓦诺之王Roderick，这样就没人能阻止他的计划了；慢着，国王不已经是具尸体了吗，我突然有股不详的预感，难道要领完任务之后击杀才能检测到？\n我赶忙再次对话，试图说明情况，结果表示不过是虚惊一场，游戏成功弹出了统计信息，通关！\n\n很难说明当时是什么感受，毫无疑问，游戏体验相当之“粪”，但里面确实又有可取之处，不然我也不会坚持到结束。\n如果让我总结，我会认为这是一款相当“遗憾”的作品，它有很多基础框架都十分扎实，像是显示方式、物品生成、射击寻路、各种怪物的特质与风味文本等等，UI虽说是仿的，但也确实做得有模有样；每个生物都有背包，都有技能，甚至都可以学习法术，同时期的ADOM都未能做到这一点。\n\n事实上，2002年ADOM停更在1.1.1之后的那些日子里，irc上甚至有人认为Avanor能够取代它的地位；Avanor于2001年发布首个版本，持续开发至2003年后公布了源代码，接着要是能吸收开源和社区的力量，那它也许会成为一个“CddADoM”式的玩意，可没想到它跟前辈有样学样，同样陷入了停更。\n不知道是幸运还是不幸，制作组回光返照得比较快，两三年后突然又更了次新，一副要恢复长期开发的架势，较为重大的一项改动是使用LUA脚本，以便让游戏世界更容易扩展，这本有可能让它吸引elona系里喜欢omake mma的受众。\n能保持住这个势头的话，或许它真会成为新一代中流砥柱，因为忙于JADE的Thomas要到2012年才恢复ADOM更新，顺便赚上一笔巨款；但说不定是这次诈尸依旧反响平平，Avanor再后来就彻底没了消息。\n它的基础设定充满潜力，无缝开放世界，所有生物同时移动，如果作者再想着加入z轴并3d化，没准我们今天流行的就是“AvanorCraft”之类的东西，毕竟《矮人要塞》在2006年才发布，而受此启发的notch要在2009年才创作出《Minecraft》的原始版本。\n\n\n抛开未来的发展形态不谈，光是现有的内容就还有很多疑问和可拓展之处：古代恶魔与蘑菇洞有什么秘密？国王为何不肯离开王城一步？丢失数年的神器怎么才开始找？与现在的异变是否存在关系？\n能在灾难之中存活下来，玩家这名角色的真实身份是什么？散尽力量又怎会被正邪双方一起看上？\n外面的世界多半早已天翻地覆，围住这一带的环形山脉有没有可能不是牢笼，而是最后的壁障？\n邪法师与国王莫非是更高位存在的棋子？元凶真有能力造成山脉隆起这样的天地异象吗？从未现身的神明对世界动荡又持怎样的态度？\n另外Avanor的游戏流程太短，对整个世界犹如管中窥豹，凑齐部件后完全可以来段山崩地裂的演出，然后从山体缺口展开一副更为广阔的外界画卷；曾经那个dos与软盘的时代可能会受限于机能，但现如今这种字符世界能有多大几乎完全取决于创作者的想象能力。\n\n该项目的主要作者是Vadim Gaidukevich，在详情页面上还有另外两位程序，其中一位的github主页至今仍然活跃，除此之外应该还有许多其他成员也参与了开发，像是撰写了生物描述的Uriah Otting与许多汇报了bug的玩家。\n翻源码的过程中也能感受到些许码风差异，几位程序里肯定有一名狂热的OOP爱好者，比如对于游戏内的物品，多数情况下定义个item class或者struct就差不多了，剩下的数据打表或者从本地文件读取；尽管部分药水列表啥的让人神清气爽，但Avanor中有非常多的物品与生物都单独以一个派生类的形式出现，如果将继承关系用树状图画出来应该是朵蒲公英，由衷希望当事人有配置良好的自动补充模板，否则我会非常佩服ta的毅力。\n\n那些人共同创作了这部作品，他们曾有过一座论坛，一张响应迅速的bug收集表，肯定还度过了几段欢乐的时光；纵使时间流逝，绝大部分存在过的痕迹都抹平，也总会有些美好的东西留下来，就像irc上那些发自内心的赞扬与冒险分享。\n\n\n游戏实际上还有另外一种隐藏结局，将国王和邪法师全部击杀后，回到初入王城的地方找火焰之主对话，他便会为玩家加冕，封角色为新一任的Avanor之王，看样子这家伙精明的很，懂得将自己隐藏在幕后；我后来专门再去完成了这一结局，通关的统计数据放到了本文末尾，如果感兴趣的话可以下载看看。\n可惜Avanor这部作品的结局则早已注定，未来也几乎不可能有机会“复活”，至少希望本篇短文能够成为它的一份Memory File，记录曾有过这么一款游戏存在。\n非常感谢读到这里的各位，能撑着看下来可真不容易。\nPS：多亏了Simon Swerwer的Danger Room才让我在枯燥的rng地牢里没有当场Bo毙，如果没听过这首歌请一定要抽空听听\n👉本文首发于言静rl网，点我前往以弗所查看角色统计👈\n","plink":"http://www.ephesus.top/files/Avanor/Avanor/"},{"title":"","date":"2024-06-21T03:48:12.105Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:12.105Z","content":"背景介绍\nNLP日常工作经验和论文解析，包含：预训练模型，文本表征，文本相似度，文本分类，多模态，知识蒸馏，词向量。\n我觉得NLP是一个值得深耕的领域，所以希望可以不停的提升自己核心竞争力和自己的段位！\n微信公众号：DASOU\n深度学习自然语言处理\nTransformer\n\n史上最全Transformer面试题\n答案解析(1)-史上最全Transformer面试题\nPytorch代码分析–如何让Bert在finetune小数据集时更“稳”一点\n解决老大难问题-如何一行代码带你随心所欲重新初始化bert的某些参数(附Pytorch代码详细解读)\n3分钟从零解读Transformer的Encoder\n原版Transformer的位置编码究竟有没有包含相对位置信息\nBN踩坑记–谈一下Batch Normalization的优缺点和适用场景\n谈一下相对位置编码\nNLP任务中-layer-norm比BatchNorm好在哪里\n谈一谈Decoder模块\nTransformer的并行化\nTransformer全部文章合辑\nRNN的梯度消失有什么与众不同的地方\nVIT-如何将Transformer更好的应用到CV领域\n\nBert-基本知识\n\nFastBERT-CPU推理加速10倍\nRoBERTa：更多更大更强\n为什么Bert做不好无监督语义匹配\nUniLM:为Bert插上文本生成的翅膀\ntBERT-BERT融合主题模型做文本匹配\nXLNET模型从零解读.\n如何在脱敏数据中使用BERT等预训练模型\n\nBert-知识蒸馏\n\n什么是知识蒸馏\n如何让 TextCNN 逼近 Bert\nBert蒸馏到简单网络lstm\nPKD-Bert基于多层的知识蒸馏方式\nBERT-of-Theseus-模块压缩交替训练\ntinybert-全方位蒸馏\nALBERT：更小更少但并不快\nBERT知识蒸馏代码解析-如何写好损失函数\n知识蒸馏综述万字长文\n\n词向量-word embedding\n\n史上最全词向量面试题-Word2vec/fasttext/glove/Elmo\n\n\nWord2vec\n\n\nWord2vec两种训练模型详细解读-一个词经过模型训练可以获得几个词向量\nWord2vec两种优化方式细节详细解读\nWord2vec-负采样和层序softmax与原模型是否等价\nWord2vec为何需要二次采样以及相关细节详细解读\nWord2vec的负采样\nWord2vec模型究竟是如何获得词向量的\nWord2vec训练参数的选定\nCBOW和skip-gram相较而言，彼此相对适合哪些场景\n\n\nFasttext/Glove\n\n\nFasttext详解解读(1)-文本分类\nFasttext详解解读(2)-训练词向量\nGLove细节详细解读\n\n多模态\n\n多模态之ViLBERT：双流网络，各自为王\n复盘多模态任务落地的六大问题\n如何将多模态数据融入到BERT架构中-多模态BERT的两类预训练任务\n层次分类体系的必要性-多模态讲解系列(1)\n文本和图像特征表示模块详解-多模态讲解系列(2)\n多模态中各种Fusion方式汇总\n\n句向量-sentence embedding\n\n句向量模型综述\n\n文本相似度\n\n五千字全面梳理文本相似度/文本匹配模型\n如何又好又快的做文本匹配-ESIM模型\n阿里RE2-将残差连接和文本匹配模型融合\n聊一下孪生网络和DSSM的混淆点以及向量召回的一个细节\nDSSM论文-公司实战文章\nbert白化简单的梳理:公式推导+PCA&amp;SVD+代码解读\nSIMCSE论文解析\n\n关键词提取\n\n基于词典的正向/逆向最大匹配\n实体库构建：大规模离线新词实体挖掘\n聊一聊NLPer如何做关键词抽取\n\n命名体识别\n\n\n命名体识别资源梳理(代码+博客讲解)\n\n\nHMM/CRF 详细解读\n\n\n工业级命名体识别的做法\n\n\n词典匹配+模型预测-实体识别两大法宝\n\n\nautoner+fuzzy-CRF-使用领域词典做命名体识别\n\n\nFLAT-Transformer-词典+Transformer融合词汇信息–公众号\n\n\nTENER-复旦为什么TRM在NER上效果差\n\n\n文本分类\n\nTextCNN论文详细解读\n只使用标签名称就可以文本分类 \n半监督入门思想之伪标签\nACL2020-多任务负监督方式增加CLS表达差异性\nBert在文本分类任务上微调\nUDA-Unsupervised Data Augmentation for Consistency Training-半监督集大成\nLCM-缓解标签不独立以及标注错误的问题\n关键词信息如何融入到文本分类任务中\n\n对比学习\n\nMoco论文解析\n\n","plink":"http://www.ephesus.top/links/NLP_ability/"},{"title":"","date":"2024-06-20T11:36:58.621Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:36:58.621Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Cheat');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Cheat/"},{"title":"","date":"2024-06-20T11:39:16.166Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:39:16.166Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Other%20Platform');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Other Platform/"},{"title":"","date":"2024-06-20T11:39:16.506Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:39:16.506Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Picture');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Picture/"},{"title":"","date":"2024-06-20T11:39:17.126Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:39:17.126Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Scenario/');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Scenario/"},{"title":"","date":"2024-06-20T11:39:26.626Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:39:26.626Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Sound/');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Sound/"},{"title":"","date":"2024-06-20T11:39:27.006Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:39:27.006Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Update');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Update/"},{"title":"","date":"2024-06-20T11:39:29.016Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:39:29.016Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Win/');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Win/"},{"title":"","date":"2024-06-21T03:20:40.923Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:40.923Z","content":"Pytorch中mask是如何实现的代码版本1-阅读文本相似度模型\n代码参考链接: https://github.com/DA-southampton/TextMatch/tree/master/ESIM\n最近在用Pytorch重新ESIM代码，其中关于attention的细节我自己重新梳理了一下，附上代码解读。\n我先在有一个batch的数据。Sentence1 维度为[256,32,300],Sentence2的维度为[256,33,300]\n维度含义为[batch_size,batch中最大长度，词向量维度]\n数据流转ESIM第一个Bilstm之后，维度变化为：Sentence1 维度为[256,32,600],Sentence2的维度为[256,33,600]（假设hidden为300）\n此时我们需要计算两个句子输出的attention矩阵，以便计算每个句子的加权和。\n我这里主要是梳理矩阵的计算方式细节。\n核心代码是这个：\nhttps://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/model.py#L57\n1q1_aligned, q2_aligned = self.attention(q1_encoded, q1_mask, q2_encoded, q2_mask)\nself.attention 函数对应的是这个函数，如下：\nhttps://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/layers.py#L59\n12345678910class SoftmaxAttention(nn.Module):        similarity_matrix = premise_batch.bmm(hypothesis_batch.transpose(2, 1).contiguous())  ## 256*32 *33        # Softmax attention weights.        prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)        hyp_prem_attn = masked_softmax(similarity_matrix.transpose(1, 2).contiguous(), premise_mask)        # Weighted sums of the hypotheses for the the premises attention,        # and vice-versa for the attention of the hypotheses.        attended_premises = weighted_sum(hypothesis_batch, prem_hyp_attn, premise_mask)        attended_hypotheses = weighted_sum(premise_batch, hyp_prem_attn, hypothesis_mask)        return attended_premises, attended_hypotheses  \n首先我们看一下输入：\n1q1_encoded：256*32*600 q2_encoded：256*33*600  q1mask torch.Size([256, 32])  q2mask torch.Size([256, 33])\n然后对于这个函数，核心操作是这个：\n1prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)\nsimilarity_matrix 维度为256*32 33 hypothesis_mask 为25633\n我们去看一下masked_softmax这个函数：\nhttps://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/utils.py#L29\n12345678910111213def masked_softmax(tensor, mask):    tensor_shape = tensor.size()  ##torch.Size([256, 32, 33])    reshaped_tensor = tensor.view(-1, tensor_shape[-1]) ## torch.Size([7680, 33])    while mask.dim() &lt; tensor.dim():        mask = mask.unsqueeze(1)    mask = mask.expand_as(tensor).contiguous().float()    reshaped_mask = mask.view(-1, mask.size()[-1])  ## torch.Size([7680, 33])    result = nn.functional.softmax(reshaped_tensor * reshaped_mask, dim=-1)  ## 补长位置也就是置为零的位置之后进行softmax    result = result * reshaped_mask ## 再次置为零，因为上面这个对于补长位置还会有概率共现    # 1e-13 is added to avoid divisions by zero.    result = result / (result.sum(dim=-1, keepdim=True) + 1e-13) ## 普通的求概率公式    return result.view(*tensor_shape)\n简单总结一下：\n整个mask的代码其实我读起来感觉比较奇怪，我印象中mask的操作，应该是补长的部分直接为负无穷（代码里写一个-1000就可以），但是他这里的代码，是补长的部位置为0，所以\n在softmax的时候，虽然为1，但是也有贡献也有概率的输出，虽然很小。所以又把这些部分置为零，然后用每一行的值除以每一行的总和得到了新的概率值，这个概率和补长的部位就没有关系了。\n还有一个细节点需要注意的是，比如我的输入是2563233 batch为256，那么我在计算每一行的的时候，完全可以把batch中的数据并起来，也就是变成(256*32)*33\n所以我简单总结一下，在这里的mask的操作分为两个步骤：首先补长位置置为零然后计算softmax，随后对softmax的结构补长位置继续置为零，计算简单的分值（各自除以每一行的总和），得到最后的概率值。\n","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch/Pytorch中mask是如何实现的代码版本1-阅读文本相似度模型/"},{"title":"","date":"2024-06-21T03:20:40.953Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:40.953Z","content":"Pytorch修改ESIM代码中mask矩阵查看效果-效果一般\n我对ESIM中的mask矩阵有所怀疑，于是自己改写了一个mask的矩阵，不过效果确实没有原始的好，很奇怪\nhttps://github.com/DA-southampton/TextMatch/blob/master/ESIM/utils.py\n就是这个链接中，我改了主要是以下两个函数的部分地方：\n1234567891011121314151617181920def get_mask(sequences_batch, sequences_lengths):    batch_size = sequences_batch.size()[0]    max_length = torch.max(sequences_lengths)    mask = torch.ones(batch_size, max_length, dtype=torch.float)    mask[sequences_batch[:, :max_length] == 0] = -10000.0 ## 这里修改为-10000，印象中抱抱脸初始版本是这么实现的    return mask\tdef masked_softmax(tensor, mask):    tensor_shape = tensor.size()    reshaped_tensor = tensor.view(-1, tensor_shape[-1])    # Reshape the mask so it matches the size of the input tensor.    while mask.dim() &lt; tensor.dim():        mask = mask.unsqueeze(1)    mask = mask.expand_as(tensor).contiguous().float()    reshaped_mask = mask.view(-1, mask.size()[-1])    result = nn.functional.softmax(reshaped_tensor+reshaped_mask, dim=-1) ## 这里变为加    return result.view(*tensor_shape)\n改完之后效果不咋样，真的很奇怪\n","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch/Pytorch修改ESIM代码中mask矩阵查看效果-效果一般/"},{"title":"","date":"2024-06-21T03:20:41.063Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:41.063Z","content":"主要是积累关于Pytorch实战的一些经验和坑\nPytorch如何加载大数据：https://github.com/pytorch/text/issues/130\n\n\nPytorch技巧\n\n\n\n\n\npytorch对text数据的预处理-综述\n已经上传\n\n\npytorch处理文本数据代码版本1-处理文本相似度数据\n已经上传\n\n\npytorch处理文本数据代码版本2-处理文本相似度数据\n已经上传\n\n\nPytorch中mask attention是如何实现的代码版本1-阅读文本相似度模型的小总结\n\n\n\n\n\nPytorch调参总结\n\n\n\n\n\n验证集loss上升，准确率却上升该如何理解？\n\n\n\n\n\n\n\n\n\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch/README/"},{"title":"","date":"2024-06-21T03:20:40.973Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:40.973Z","content":"pytorch处理文本数据代码版本1-处理文本相似度数据\n下面的代码，相比于版本2的代码，并没有使用gensim，而且处理的时候针对的是每一个样本，也就是每一行，也就是\nsentence1和sentence2并没有拆开来处理。\n整体代码是我自己完全整理出来的，比较整齐\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137&quot;&quot;&quot;@author: DASOU@time: 20200726&quot;&quot;&quot;import torchimport osimport pickle as pkl## 读取原始数据，生成对应的word2indexdef get_word_voc(config_base):    train_path=config_base.train_path    file=open(train_path,&#x27;r&#x27;)    lines=file.readlines()    min_freq,max_size,UNK,PAD=config_base.min_freq,config_base.max_size,config_base.UNK,config_base.PAD    vocab_dic=&#123;&#125;    for line in lines:        try:            line=line.strip().split(&#x27;\\t&#x27;)        except:            print(&#x27;The data formate is not correct,please correct it as example data&#x27;)            exit()        try:            if len(line)==3:                sen=line[0]+line[1]                tokenizer = lambda x: [y for y in x]                for word in tokenizer(sen):                    vocab_dic[word] = vocab_dic.get(word, 0) + 1 ## 为了计算出每个单词的词频，为之后过滤低频词汇做准备        except:            print(&#x27;The data formate is not correct,please correct it as example data&#x27;)            exit()    file.close()    vocab_list = sorted([_ for _ in vocab_dic.items() if _[1] &gt;= min_freq], key=lambda x: x[1], reverse=True)[:max_size]## 是为了计算每个单词的词频    vocab_dic = &#123;word_count[0]: idx for idx, word_count in enumerate(vocab_list)&#125;## 过滤掉低频词汇之后我们按照顺序来word-index的映射    vocab_dic.update(&#123;UNK: len(vocab_dic), PAD: len(vocab_dic) + 1&#125;) ## 补充unkonw和pad字符对应的数字    return vocab_dicdef load_data(cate,vocab_dic,config_base):    if cate==&#x27;train&#x27;:        data_path=config_base.train_path    elif cate==&#x27;dev&#x27;:        data_path = config_base.dev_path    else:        data_path = config_base.test_path    file=open(data_path,&#x27;r&#x27;)    contents=[]    for line in file.readlines():        words_line1=[]        words_line2=[]        line=line.strip().split(&#x27;\\t&#x27;)        sen1,sen2,label=line[0],line[1],line[2]        tokenizer = lambda x: [y for y in x]        token_sen1=tokenizer(sen1)        token_sen2 = tokenizer(sen2)        sen1_len = len(token_sen1)        sen2_len = len(token_sen2)        if config_base.pad_size:            if len(token_sen1) &lt; config_base.pad_size:                token_sen1.extend([config_base.PAD] * (config_base.pad_size - len(token_sen1)))            else:                token_sen1 = token_sen1[:config_base.pad_size]            if len(token_sen2) &lt; config_base.pad_size:                token_sen2.extend([config_base.PAD] * (config_base.pad_size - len(token_sen2)))            else:                token_sen2 = token_sen2[:config_base.pad_size]        for word1 in token_sen1:            words_line1.append(vocab_dic.get(word1, vocab_dic.get(config_base.UNK)))        for word2 in token_sen2:            words_line2.append(vocab_dic.get(word2, vocab_dic.get(config_base.UNK)))        contents.append((words_line1,words_line2,int(label)))    return contents# 导入/训练对应的word2indexdef get_w2i(config_base):    if not os.path.exists(config_base.w2i_path):        print(&#x27;There is not a pre word2index,now is to process data for geting word2index&#x27;)        vocab_dic = get_word_voc(config_base)        pkl.dump(vocab_dic, open(config_base.w2i_path, &#x27;wb&#x27;))        vord_size = len(vocab_dic)    else:        print(&#x27;There is pre word2index, now is to load the pre infomation&#x27;)        vocab_dic = pkl.load(open(config_base.w2i_path, &#x27;rb&#x27;), encoding=&#x27;utf-8&#x27;)        vord_size = len(vocab_dic)    return vocab_dic,vord_sizeclass DatasetIterater():    def __init__(self, batches, config_base):        self.batch_size = config_base.batch_size        self.batches = batches        self.n_batches = len(batches) // config_base.batch_size        self.residue = False  # 记录batch数量是否为整数        if len(batches) % self.n_batches != 0:            self.residue = True        self.index = 0        self.device = config_base.device    def _to_tensor(self, datas):        x1 = torch.LongTensor([_[0] for _ in datas]).to(self.device)        x2 = torch.LongTensor([_[1] for _ in datas]).to(self.device)        y = torch.LongTensor([_[2] for _ in datas]).to(self.device)        return (x1, x2), y    def __next__(self):        if self.residue and self.index == self.n_batches:            batches = self.batches[self.index * self.batch_size: len(self.batches)]            self.index += 1            batches = self._to_tensor(batches)            return batches        elif self.index &gt;= self.n_batches:            self.index = 0            raise StopIteration        else:            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]            self.index += 1            batches = self._to_tensor(batches)            return batches    def __iter__(self):        return self    def __len__(self):        if self.residue:            return self.n_batches + 1        else:            return self.n_batchesdef build_iterator(dataset,config_base):    iter = DatasetIterater(dataset,config_base)    return iter","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch/pytorch处理文本数据代码版本1-处理文本相似度数据/"},{"title":"","date":"2024-06-21T03:20:40.993Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:40.993Z","content":"pytorch处理文本数据代码版本2-处理文本相似度数据\n这里代码参考的是：https://github.com/DA-southampton/TextMatch/blob/master/SiaGRU/data.py\n感谢原作者\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Thu Mar 12 15:30:14 2020@author: zhaog&quot;&quot;&quot;import reimport gensimimport numpy as npimport pandas as pdimport torchfrom hanziconv import HanziConv  ##dasou:中文文本处理库from torch.utils.data import Datasetclass LCQMC_Dataset(Dataset):    def __init__(self, LCQMC_file, vocab_file, max_char_len):        p, h, self.label = load_sentences(LCQMC_file)        word2idx, _, _ = load_vocab(vocab_file)        self.p_list, self.p_lengths, self.h_list, self.h_lengths = word_index(p, h, word2idx, max_char_len)        self.p_list = torch.from_numpy(self.p_list).type(torch.long)        self.h_list = torch.from_numpy(self.h_list).type(torch.long)        self.max_length = max_char_len            def __len__(self):        return len(self.label)    def __getitem__(self, idx):        return self.p_list[idx], self.p_lengths[idx], self.h_list[idx], self.h_lengths[idx], self.label[idx]    # 加载word_index训练数据##dasou: 使用了pandas这个库，将文本相似度数据相同的列提取出来进行处理，而不是针对每一行一个样本进行处理，其实看到这里这个代码存在的一个问题就是如果将来##出来大的数据，也就是大的文件，pandas是没有办法直接全部读进来的，这是个缺点，不过对几个G的数据应该不存在这种问题def load_sentences(file, data_size=None):    df = pd.read_csv(file,sep=&#x27;\\t&#x27;,header=None)##dasou 为了适应我的数据格式    p = map(get_word_list, df[0].values[0:data_size]) ## p的每个元素类似这种 [&#x27;晚&#x27;, &#x27;上&#x27;, &#x27;尿&#x27;, &#x27;多&#x27;, &#x27;吃&#x27;, &#x27;什&#x27;, &#x27;么&#x27;, &#x27;药&#x27;]    h = map(get_word_list, df[1].values[0:data_size])    label = df[2].values[0:data_size]    #p_c_index, h_c_index = word_index(p, h)    return p, h, label# word-&gt;indexdef word_index(p_sentences, h_sentences, word2idx, max_char_len):    p_list, p_length, h_list, h_length = [], [], [], []    for p_sentence, h_sentence in zip(p_sentences, h_sentences):        p = [word2idx[word] for word in p_sentence if word in word2idx.keys()]        h = [word2idx[word] for word in h_sentence if word in word2idx.keys()]        p_list.append(p)        p_length.append(min(len(p), max_char_len))        h_list.append(h)        h_length.append(min(len(h), max_char_len))    p_list = pad_sequences(p_list, maxlen = max_char_len)    h_list = pad_sequences(h_list, maxlen = max_char_len)    return p_list, p_length, h_list, h_length# 加载字典def load_vocab(vocab_file):    vocab = [line.strip() for line in open(vocab_file, encoding=&#x27;utf-8&#x27;).readlines()]    word2idx = &#123;word: index for index, word in enumerate(vocab)&#125;    idx2word = &#123;index: word for index, word in enumerate(vocab)&#125;    return word2idx, idx2word, vocab&#x27;&#x27;&#x27; 把句子按字分开，中文按字分，英文数字按空格, 大写转小写，繁体转简体&#x27;&#x27;&#x27;def get_word_list(query):    query = HanziConv.toSimplified(query.strip())    regEx = re.compile(&#x27;[\\\\W]+&#x27;)#我们可以使用正则表达式来切分句子，切分的规则是除单词，数字外的任意字符串    res = re.compile(r&#x27;([\\u4e00-\\u9fa5])&#x27;)#[\\u4e00-\\u9fa5]中文范围    sentences = regEx.split(query.lower())    str_list = []    for sentence in sentences:        if res.split(sentence) == None:            str_list.append(sentence)        else:            ret = res.split(sentence)            str_list.extend(ret)    return [w for w in str_list if len(w.strip()) &gt; 0]def load_embeddings(embdding_path):    model = gensim.models.KeyedVectors.load_word2vec_format(embdding_path, binary=False)    embedding_matrix = np.zeros((len(model.index2word) + 1, model.vector_size))    #填充向量矩阵    for idx, word in enumerate(model.index2word):        embedding_matrix[idx + 1] = model[word]#词向量矩阵    return embedding_matrixdef pad_sequences(sequences, maxlen=None, dtype=&#x27;int32&#x27;, padding=&#x27;post&#x27;,                  truncating=&#x27;post&#x27;, value=0.):    &quot;&quot;&quot; pad_sequences    把序列长度转变为一样长的，如果设置了maxlen则长度统一为maxlen，如果没有设置则默认取    最大的长度。填充和截取包括两种方法，post与pre，post指从尾部开始处理，pre指从头部    开始处理，默认都是从尾部开始。    Arguments:        sequences: 序列        maxlen: int 最大长度        dtype: 转变后的数据类型        padding: 填充方法&#x27;pre&#x27; or &#x27;post&#x27;        truncating: 截取方法&#x27;pre&#x27; or &#x27;post&#x27;        value: float 填充的值    Returns:        x: numpy array 填充后的序列维度为 (number_of_sequences, maxlen)    &quot;&quot;&quot;    lengths = [len(s) for s in sequences]    nb_samples = len(sequences)    if maxlen is None:        maxlen = np.max(lengths)    x = (np.ones((nb_samples, maxlen)) * value).astype(dtype)    for idx, s in enumerate(sequences):        if len(s) == 0:            continue  # empty list was found        if truncating == &#x27;pre&#x27;:            trunc = s[-maxlen:]        elif truncating == &#x27;post&#x27;:            trunc = s[:maxlen]        else:            raise ValueError(&quot;Truncating type &#x27;%s&#x27; not understood&quot; % padding)        if padding == &#x27;post&#x27;:            x[idx, :len(trunc)] = trunc        elif padding == &#x27;pre&#x27;:            x[idx, -len(trunc):] = trunc        else:            raise ValueError(&quot;Padding type &#x27;%s&#x27; not understood&quot; % padding)    return x","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch/pytorch处理文本数据代码版本2-处理文本相似度数据/"},{"title":"","date":"2024-06-21T03:20:41.023Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:41.023Z","content":"pytorch对text数据的预处理-综述\n我们需要把文本数据转化为向量从而可以被神经网络处理。在被喂给神经网络之前，我们需要对text文本数据进行预处理。\n关于这一块的预处理，其实有一个很高度抽象化的接口torchtext可以很高效的解决问题，但是有些时候不清楚里面怎么运作的心理总是没谱，所以我一般在写代码的时候都是使用人工自己处理代码。\n这个人工手动处理流程代码其实各式各样，我大概是写两个版本，之后如果看到不错的，可能还会整理，比如如何处理大数据，不过核心思想是一样的。\n大致流程是这样的：\n首先：对原始数据（一般是训练数据）进行预处理，进行分词，繁体字转化，半角符号转化\n随后：记录各个词汇的词频，过滤低词频词汇，简历Word2index的映射表保存起来，需要注意pad和unk符号\n随后：把数据（训练/测试/dev，使用参数进行控制）转化为对应的index，按照最大长度进行补全，并转化为tensor\n其次：制造自己的数据集类，改写关键部位，一般是get_item这里，以便被dataloder处理。\n","plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch/pytorch对text数据的预处理-综述/"},{"title":"","date":"2024-06-21T03:48:13.255Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.255Z","content":"0.背景\nwide&amp;deep 理论的介绍可以参考我之前写的那个文章。\nWDL在实际应用的时候，有很多细节需要注意。\n在我自己的应用中，对WDL模型做了一个简单的修改，加入了多模态（图片加标题）的特征，效果比单纯的xgboost要提升不少。\n因为涉及到具体业务，所以不能详细展开。\n不过我之前有读一个很不错的文章：，顺着这个文章的脉络，我们可以来看看WDL需要注意的地方。\n全文思维导图：\n1. wide &amp; deep 在贝壳推荐场景的实践\nWDL应用场景是预测用户是否点击推荐的房源。\nhttps://mp.weixin.qq.com/s/rp6H_HydTbKiSanijDZwBQ\n1.1 如何构建正负样本\n首先，模型离不开样本，样本一般从日志中获取。一般是通过埋点，记录用户行为到日志，然后清洗日志，获得用户行为。\n贝壳这里样本格式是三元组：userid，itemid和label；\n至于对应的特征，一般是需要根据userid，itemid到对应hive表格获取整合。\n\n正样本：用户点击的房源\n负样本：用户点击最大位置以上曝光未点击的房源；从未点击的用户部分曝光未点击的房源。\n\n样本构建细节整理\n在这里，想要详细说一下正负样本构建的细节。\n首先是对于日志的处理，需要区分web端和app端。不要增加无效的负样本\n其次，用户点击最大位置以上曝光未点击的房源，这个方法其实叫做Skip Above，也就是过滤掉最后一次的点击。这样做我们是基于用户对之后的item是没有观测到的。\n其次为了避免高度活跃用户的对loss的影响，在训练集中对每个用户提取相同数量的样本。\n然后我们来想一下这个问题：从未点击的用户部分曝光未点击的房源。\n首先，去除了这部分用户，会出现什么问题？\n模型学习到的只有活跃用户和有意向用户的行为习惯，这样线上和线下的数据分布会不一致。我们在线上的遇到的数据，肯定会出现那种不活跃用户。\n如果不去除这部分用户，会出现什么情况？\n这样的用户在本质上是无效用户。为什么这么说呢？我们模型的作用是为了提升用户点击。\n如果频繁给这个用户推荐物品，他一直不点击，也就是说没有正反馈。两种情况，一种是我们推荐的是有很大问题的，但是这种概率极低。还有一种情况，就是这个用户是有问题的。\n所以简单来说，我们需不需要对这样的用户做为样本？\n很简单，做A/B测试，看是否需要这部分用户以及需要多少这部分用户作为样本。\n还有一定需要注意的是，特征需要控制在样本时间之前，防止特征穿越。\n1.2 如何控制样本不平衡\n一般来说，负样本，也就是未点击的房源肯定是更多的。所以在训练模型的时候，肯定是存在样本不平衡的问题。\n贝壳这里采用的是下采样负样本和对样本进行加权。\n之前写个一个简单的文章，来讲述了一下如何缓解样本不平衡，可以参考这里：\n文章总结的结论就是，无论你使用什么技巧缓解类别不平衡，其实都只能让模型有略微的提升。最本质的操作还是增加标注数据。\n就拿贝壳的操作来说，下采样和样本加权，本质上都修改了样本的分布情况。\n就会出现训练样本分布和线上真实分布不一致的情况，那么你现在训练的模型究竟在线上真实环境能不能有好的提升，就看模型在真实数据上的评估情况了。\n1.3 解决噪声样本\n贝壳指的噪声样本指的是：\n\n在我们的业务场景下，用户在不同时间对同一房源可能会存在不同的行为，导致采集的样本中有正有负。\n\n我自己的感受是很奇怪的是，只是猜测而已哈，样本特征中没有加入时间特征吗？加入时间特征应该可以学到用户短期兴趣变化。\n1.4 特征处理：\n\n\n缺失值与异常值处理：常规操作；不同特征使用不同缺失值填补方法；异常值使用四分位；\n\n\n等频分桶处理：常规操作；比如价格，是一个长尾分布，这就导致大部分样本的特征值都集中在一个小的取值范围内，使得样本特征的区分度减小。\n不过有意思的是，贝壳使用的是不同地区的等频分布，保证每个城市下特征分布均匀。\n\n\n归一化：常规操作；效果得到显著提升；\n\n\n低频过滤：常规操作；对于离散特征，过于低频的归为一类；\n\n\nembedding：常规操作；小区，商圈id做embedding；\n\n\n1.5 特征工程\n预测目标是用户是否点击itme，所以特征就是从三方面：用户，item，交互特征；\n\n用户：\n\n\n注册时长、上一次访问距今时长等基础特征，最近3/7/15/30/90天活跃/浏览/关注/im数量等行为特征，以及画像偏好特征和转化率特征。\n\n\n房源：\n\n\n价格、面积、居室、楼层等基础特征，是否地铁房/学区房/有电梯/近医院等二值特征，以及热度值/点击率等连续特征。\n\n\n交叉：\n\n\n将画像偏好和房源的特征进行交叉，主要包含这几维度：价格、面积、居室、城区、商圈、小区、楼层级别的交叉。交叉值为用户对房源在该维度下的偏好值。\n\n1.6 模型离线训练\n\n数据切分：采用7天的数据作为训练集，1天的作为测试集\nembedding：尝试加入，没有获得很好的效果\n模型调优：\n\n防止过拟合：加入dropOut 与 L2正则\n加快收敛：引入了Batch Normalization\n保证训练稳定和收敛：尝试不同的learning rate（wide侧0.001，deep侧0.01效果较好）和batch_size（目前设置的2048）\n优化器：我们对比了SGD、Adam、Adagrad等学习器，最终选择了效果最好的Adagrad。\n\n\n\n1.7 模型上线\n\n模型部署：使用TensorFlow Serving，10ms解决120个请求\n解决线上线下特征不一致：将离线特征处理的参数存储在redis中\n效果提升：\n\nCTR：提升6.08%\nCVR:：提升15.65%\n\n\n\n2. WDL代码实现\nGithub上有太多了，TF也有官方的实现，我就不多说了\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/WDL/WDL在贝壳中的应用实践总结/"},{"title":"","date":"2024-06-21T03:48:13.135Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.135Z","content":"更多NLP文章在这里：\nhttps://github.com/DA-southampton/NLP_ability\n谈到WDL，一个经常看到的总结是：Wide and Deep 模型融合 wide 模型的记忆能力和 Deep 模型的泛化能力，进行两个模型的联合训练，从而兼顾推荐的准确性和多样性。\n理解上面这句话，还是要先弄清楚：什么是记忆能力，什么是泛化能力？\n1. 什么是记忆能力与泛化能力\n1.1记忆能力\n我们先说记忆能力，从中文的角度理解，记忆能力就是之前做过的事情，在后面做同样的事的时候会利用到之前的经验和教训。\n进一步，记忆能力就是对之前学习到的经验或者说规律的遵守。\n原论文是这么说的：\n\nMemorization can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data.\n\n从这段话可以看出来记忆能力分为两个部分：\n\n从历史数据中学习共现的物体/特征组合—&gt;这就对应到上面谈到的经验规律\n在预测的时候利用到这种学习到的这种相关性—&gt;这就对应到上面谈到的对经验的遵守。\n\n在这里，我想提一下，在第一点中提到的 “学习共现的物体/特征组合” 的主体是谁？\n最开始我认为是模型，后来认为不是。\n因为LR模型属于广义线性模型，本身不具备对特征之间非线性关系进行建模。\n所以需要我们从历史数据中找到有用的特征组合（当然我们也可以使用一些工具来找到哪些特征组合是有效的），人为的加入到模型中，给LR模型增添非线性建模能力。\n简单来说，记忆能力是一种共现规律，表现方式为特征交叉，它需要人为或者通过工具从历史数据中找到，并放入到模型中作为新的特征，从而增加非线性建模能力。\n记忆能力过强会出现一个问题，就是推荐物体的单一化。\n原文是这么说的：\n\nRecommendations based on memorization are usually more topical and directly relevant to the items on which users have already performed actions.\n\n1.2泛化能力\n对于泛化能力，原论文是这么说的：\n\nGeneralization, on the other hand, is based on transitivity of correlation and explores new feature combinations that have never or rarely occurred in the past.\n\n关键词是：从未或者很少出现的特征组合\n神经网络无需人为构建组合特征，有着自动做特征组合的方式。可以通过对类别特征做embedding，这样就是在测试集中出现在训练集中没有出现过的特征组合方式，也可以使用embedding进行计算得到对应的值。\n综合来说，LR模型有着更强的记忆能力，Deep模型有着更强的泛化能力。\n2.模型架构图\n\n整个模型分为三个部分，左边的Wide模型，右边的Deep模型，最后输出的Softmax/sigmoid函数。\nWide使用的是LR模型，这里需要注意的点是LR的输入特征包含两部分：\n\n原始特征\n特征交叉之后的特征（特征交叉之前各自特征需要one-hot）\n\nDeep模型使用的是前馈神经网络，会对类别特征做embedding，连续特征不动直接输入就好（需要提前做好特征工程）。\n联合训练，Wide使用FTRL算法进行优化，Deep模型使用AdaGrad进行优化。\n在实际中，Wide和Deep部分直接使用一个优化器就可以。\n3.实践\n3.1 实践架构\n\n这个是原论文中的架构图，我们自己在实践的时候不一定完全遵守。比如架构图中Wide部分只是使用了交叉特征，我们在使用的时候可以把原始的离散特征或者打散后的连续特征加过来。\n3.2 多模态特征的加入\n有些时候对于用户或者待推荐的物体会有Text和Image，为了增加效果，可能会使用到多模态特征。\n（是否需要加入多模态特征需要大家亲自试，很有可能吭哧吭哧写了好久代码调试通了，最后发现效果提升不多甚至还会降低，哭了）\n我这里给几个简单的思路。\n\nText 和 Image 的 embedding 向量，采用 和Wide模型一样的方式加入到整体模型中就可以了。至于 两者的Embedding向量如何获取，就看你自己了。\nText和Image之间使用attention之后再加入\nText和Image 和Deep 模型的输出拼接之后再做一次处理\n多看 Paper-给个关键词：Multimodal Fusion\n\n3.3 特征工程小记录\n在详细写一个特征工程的文章，写完之后会放出来。\n后记\n读完整个论文，让我去回顾整个模型，给我这样一个感觉：\n对于隐藏在历史数据中的共现特征关系，Deep模型是可以学习到的。但是WDL模型做的是，把其中的一部分（容易观察出来或者通过其他工具找出来的特征组合）放到LR这边，从而显式的加入到模型中。\n往极端的方面想一下，LR模型这边更像是一种规则，是对Deep模型输出的补充。\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/WDL/WDl/"},{"title":"","date":"2024-06-21T03:20:41.183Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:41.183Z","content":"两个核心细节\n掌握FM，有两个细节需要注意：参数量级的变化和时间复杂度的变化。\n首先对于参数量级，由线性模型到多项式模型到FM模型参数量级变化为：\nn–&gt;n*n–&gt;kn (k&lt;&lt;n)\n其次是由原始FM公式到化简之后的FM公式复杂度的变化情况为：\nKn*n–&gt;kn\n线性模型\n回归问题我们一般使用的比较见得baseline就是线性回归，二元分类问题就是逻辑回归LR。\n线性模型公式如下（回归问题）：\n\n对于线性模型，我们的假设一般是认为特征之间是相互独立的，无法学习到特征之间的交叉影响。\n为了解决特征交叉的问题，我们一般可以人为的加入一些自己的先验信息，比如做一些特征之间的交互，不过这个很需要人们的经验。\nPOLY2模型–暴力组合特征交叉\n这个时候，POLY2模型成了可行的方案。POLY2 模型，对所有特征做了两两交叉，并对所有特征组合赋予了权重，在一定程度上解决了特征组合问题，本质仍然是线性模型，训练方法与逻辑回归没有区别。\n我们把POLY2（只是特征两两交叉的部分）加到线性模型中，从而模型可以过渡到多项式模型，公式如下：\n\n（ps：看到这里我自己有一个疑问，同一个特征onehot之后，会在自己里面做特征交叉吗）\n看这个公式，主要是看后面那个交叉的部分。看到这部分，其实很容联想到我们在LR中自己加入交叉特征的部分。\n但是需要注意的是，这里有点像暴力求解一样，忽视或者说省去了人工先验的部分，直接做到了所有特征之间的交叉，然后去求解对应的参数就可以。\nPOLY2模型两个问题\n但是这样暴力求解存在两个问题：参数量和参数稀疏导致学习困难的问题。\n先说参数量的问题，如果我自身特征（未交叉之前）就已经很多了，为n，那么交叉之后就是一个 n*n级别的参数量。极端情况会出现参数的量级比样本量级都大，训练起来及其的困难。\n再说参数稀疏的问题。互联网数据通常使用one-hot编码除了类别型数据，从而使特征向量极度稀疏，POLY2模型做无选择的特征交叉，使得特征向量更加的稀疏，导致大部分交叉特征的权重缺乏有效的数据进行训练，无法收敛。\n我自己理解的时候感觉这个很像是NLP中OOV情况。\nFM模型\n面对这两种问题，FM模型怎么解决呢？\nFM相比于POLY2模型，主要区别是用两个向量的内积代替了单一权重系数，具体来说，就是FM为每个特征学习了一个隐权重向量。在特征交叉的时候，使用两个特征向量的内积作为交叉特征的权重。\n这样其实就解决了上面两个问题。\n参数量的问题变为了 kn个参数，因为每个特征对应一个K维度的向量。\n其次是参数学习的问题。OOV问题很大缓解，即使当前特征交叉在训练样本中没出现过，但是每个特征已经学到了自己embedding，内积之后是有结果的。这也是为什么FM模型泛化能力强的根本原因。\nFM模型如下：\n\n其中涉及到的二阶部分可以通过公式的化简从Kn*n–&gt;kn：\n\n参考链接：\n文章：\nFM算法解析 - 王多鱼的文章 - 知乎 https://zhuanlan.zhihu.com/p/37963267\n推荐系统召回四模型之：全能的FM模型 - 张俊林的文章 - 知乎 https://zhuanlan.zhihu.com/p/58160982\n代码：\ndeepctr-torch 大概跑了一遍\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/推荐/FM/"},{"title":"","date":"2024-06-21T03:20:41.143Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:41.143Z","content":"在理解DeepFM的时候，我觉得有个细节点非常重要，就是FM中对应的特征组合前面的每个特征的隐向量在哪里？\n每个原始类别特征会进行one-hot，然后映射到embedding层，每个节点链接到embedding层的权重组合就是这个隐向量。直接看图：\n\n对于DeepFM，核心概括一下：\nDeepFM大致分为两个部门：DNN和FM部门，两者共享权值，分别提取特征，最终这输出。\nFM分为一阶特征组合和二阶特征交叉组合。特征一般分为类别型特征和连续性特征。\n我大致跑了一下DeepFM的代码，看了一下一阶特征和二阶特征的问题：\n一阶特征是类别型特征和连续特征都要，类别型特征直接embedding相加就可以，连续特征归一化之后乘以对应权重相加就可以，最终一起相加就可以。\n二阶特征组合是是使用到了离散特征的组合，直接embedding之后放入到FM模型中就可以。\n参考链接：\n深度推荐模型之DeepFM - 偶而君的文章 - 知乎 https://zhuanlan.zhihu.com/p/57873613\ndeepFM in pytorch----非常好\nhttps://blog.csdn.net/w55100/article/details/90295932\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/推荐/deepfm/"},{"title":"","date":"2024-06-21T03:20:41.883Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:41.883Z","content":"关于推荐的资源总结\n\n\n推荐系统资源总结\n主体内容\n进度\n\n\n\n\nEmbedding 技术的非端到端学习方法 - 腾讯技术工程的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/188569580\n本文主要介绍 Embedding 技术的非端到端学习方法在应用宝推荐场景的应用实践。\nno\n\n\nMulti-task多任务学习在推荐算法中应用(2） - 梦想做个翟老师的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/91285359\n多任务学习在推荐系统\nno\n\n\n知识蒸馏与推荐系统 - 凉爽的安迪的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/163477538\n知识蒸馏推荐系统\nno\n\n\nctr预估怎么构造时间相关的特征？ - 大博的回答 - 知乎\n\n\n\n\nhttps://www.zhihu.com/question/350863682/answer/860524396\n特征工程\n还没看\n\n\n深入理解推荐系统：排序 - 鱼遇雨欲语与余的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/138235048\n排序\nno\n\n\n负样本为王：评Facebook的向量化召回算法 - 石塔西的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/165064102\n很好\nno\n\n\n再评Airbnb的经典Embedding论文 - 石塔西的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/162163054\n\n\n\n\n最全推荐系统Embedding召回算法总结 - Garvin Li的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/156769032\n召回emebdding\n\n\n\n用户画像在携程商旅的实践 - 携程技术的文章 - 知乎\n\n\n\n\nhttps://zhuanlan.zhihu.com/p/161804005\n用户画像\n\n\n\n深度学习推荐系统中各类流行的Embedding方法（下）https://mp.weixin.qq.com/s/N76XuNJ7yGzdP6NHk2Rs-w\nembedding\nno\n\n\n一文梳理推荐系统的中 EMBEDDING 的应用实践\n\n\n\n\nhttps://mp.weixin.qq.com/s/7xTOCODlJQ42UkjRoRTE5A\nembedding\nno\n\n\n推荐系统主流召回方法综述\n\n\n\n\nhttps://mp.weixin.qq.com/s/Kxf_VX8cyN4vvveEPB1mcg\n召回\nno\n\n\n如何消除广告和推荐中的position bias\n\n\n\n\nhttps://mp.weixin.qq.com/s/rJ3pzxVEVZxCwKjXrNukXg\n广告bias\n\n\n\n浅谈电商搜索推荐中ID类特征的统一建模：Hema Embedding解读 - 力学渣的文章 - 知乎 https://zhuanlan.zhihu.com/p/104182282\n广告建模\nno\n\n\n推荐系统 embedding 技术实践总结 - 腾讯技术工程的文章\n\n\n\n\n揭开YouTube深度推荐系统模型Serving之谜 - 王喆的文章\n\n\n\n\nYouTube深度学习推荐系统的十大工程问题 - 王喆的文章 -\n\n\n\n\n重读Youtube深度学习推荐系统论文，字字珠玑，惊为神文 - 王喆的文章\n\n\n\n\n看Youtube怎么利用深度学习做推荐 - 石塔西的文章\n\n\n\n\nFactorization Machine笔记及Pytorch 实现\n\n\n\n\n推荐系统遇上深度学习(三)–DeepFM模型理论和实践\n\n\n\n\nDeepFM全方面解析（附pytorch源码）\n\n\n\n\n详解 Wide &amp; Deep 结构背后的动机 - 刺猬的文章\n\n\n\n\nwide&amp;deep模型中为什么要将连续特征离散化？\n\n\n\n\n看Google如何实现Wide &amp; Deep模型(1) - 石塔西的文章\n\n\n\n\nCTR预估之Wide&amp;Deep和DeepFM - 张备的文章\n\n\n\n\n见微知著，你真的搞懂Google的Wide&amp;Deep模型了吗？\n\n\n\n\n用NumPy手工打造 Wide &amp; Deep - 石塔西的文章\n\n\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/推荐/推荐资源更新/"},{"title":"","date":"2024-06-21T03:20:41.903Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:41.903Z","content":"什么叫做倒排索引\n在搜索场景中，有一个名词非常的频繁叫做“倒排索引”，今天看了一篇参考资料，大致的了解了一下基本原理，记录下来，以备后用。\n首先我们来看，搜索场景是这样的：我有海量文本存储在数据库中，同时每次搜索请求，会有query。\n基于海量文本，一个比较直观的想法就是建立正排索引。就是我们的每一个文本（有着自己唯一的一个编号）对应着自己文档中的关键词。\n形式如下：\ndoc1: 关键词1，关键词2，关键词3…\ndoc2：关键词3，关键词4，关键词5…\ndoc3：关键词1，关键词9，关键词7…\n这样的数据存储结构很不利与搜索。假设我们有一关键词“苹果”，我要找到包含苹果的文档，那么我需要对每个文档进行遍历，才可以，这样下来搜索时长太大。\n倒排索引是这样的，使用关键词作为存储结构的key，每个关键词对应着包含这个关键词的doc，形式如下:\n关键词1: doc1,doc3…\n关键词2: doc1…\n关键词3: doc1,doc2…\n在这种情况下，如果我们去搜索包含“苹果”这个关键词的文档，只需要对key进行索引就可以。\n这就是倒排索引，简单讲就是关键词作为key，包含对应关键词的文档集合作为value。\n然后我们在讲一个比较细节的东西，整个倒排索引可以分为三个部分：词典，倒排列表，倒排文件\n词典：存储自身编号和指向倒排列表的指针\n倒排列表：存储包含某个关键词的所有文档列表以及对应关键词出现的次数位置等信息。\n倒排文件：倒排文件是存储倒排索引的物理文件\n参考文件链接：\nhttps://www.cnblogs.com/zlslch/p/6440114.html\n我觉得这个讲的很好\n","plink":"http://www.ephesus.top/links/NLP_ability/搜索/搜索/倒排索引基本概念/"},{"title":"","date":"2024-06-21T03:20:41.923Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:41.923Z","content":"搜索资源总结-持续更新\n最近看到一个不错的仓库，搜集搜索相关资源，地址在这里：\nhttps://github.com/frutik/awesome-search#types-of-search\n我直接fork了\n先列取我看到一些将来可能要看的文章，看到就更新，随时更新\n\n\n搜索相关资源总结\n\n\n\n\n\n搜索中的 Query 理解及应用1\n\n\n\n搜索中的Query扩展技术\n\n\n\n电商搜索是如何保证搜索结果不偏离搜索意图的？\n\n\n\nQuery意图方法（2）- 基于文本分类 - 星轨数据的文章 - 知乎\n\n\n\n浅谈Query理解和分析 - 机智的叉烧的文章 - 知乎\n\n\n\n搜索引擎的 Query 分析有哪些小技术点？\n\n\n\n大话搜索query理解 - 乔一的文章\n\n\n\n智能扩充机器人的“标准问”库之Query生成 - 刘聪NLP的文章\n\n\n\nBad Case方法论 - 姚凯飞的文章\n\n\n\nquery分析 - 知乎\nhttps://www.zhihu.com/topic/19611289\n机器学习（十）：损失函数 - DataScienceArt的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/136047113\n大话搜索query理解 - 乔一的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/111904993\n从算法理论到工程实践，AI学习路上你遇到了哪些阻碍、走过哪些弯路？ - copytang的回答 - 知乎\nhttps://www.zhihu.com/question/358290436/answer/921061372\n现阶段各家公司的广告算法使用的主流模型有哪些？ - copytang的回答 - 知乎\nhttps://www.zhihu.com/question/352306163/answer/905601365\n国内有哪些比较优秀的搜索引擎？ - copytang的回答 - 知乎\nhttps://www.zhihu.com/question/278288679/answer/402251102\n医疗搜索中的query词权重算法探索\nhttps://mp.weixin.qq.com/s/JCdzhd1wBKIzDkoqW87OAg\n搜索广告之自动化创意\nhttps://mp.weixin.qq.com/s/8CN6Ak9skzxXn_qZntJ0FQ\n教你如何动态配置词权重，检索系列文章之HDCT论文笔记 - 刘聪NLP的文章 - 知乎 https://zhuanlan.zhihu.com/p/148211196\n深度召回在招聘推荐中的挑战和实践\nhttps://mp.weixin.qq.com/s/_2pPa6v2wgb5ht1j_s4Plg\n说一说视频搜索\nhttps://mp.weixin.qq.com/s/Sxuv2H9zJLy04BGXeUiF-g\nEmbedding搜索能代替文本搜索吗？\nhttps://mp.weixin.qq.com/s/cbIqkGg8IwjnSKpEd54wZg\n[第9期] 如何识别用户搜索意图之 Query 扩展\nhttps://mp.weixin.qq.com/s/Zulh3iGXZwDJZ9nH4rbhXQ\n搜索引擎技术之Query意图分类\nhttps://mp.weixin.qq.com/s/JUjT1Z9yzyUgKA6cTA3VoQ\n在query理解中能ALL IN BERT吗？\nhttps://mp.weixin.qq.com/s/G3dr0toZjHH5hyzFBUU-aQ\nNLP技术在搜索中的应用–query理解在搜索中的应用\nhttps://mp.weixin.qq.com/s/ypZu9iO07mH5GskDEOuzfw\nQuery词权重方法（3） - 基于有监督学习\nhttps://mp.weixin.qq.com/s/1EUSz4_r8-j2wIfsHXq-IQ\nQuery分析三大法宝（2）- 百度结果\nhttps://mp.weixin.qq.com/s/PPBaBd1lgtTHmtXcB9Lg2A\nQuery分析三大法宝（3）- 片段粒度\nhttps://mp.weixin.qq.com/s/yL23MxTtAS5fF0fFBDDxWw\n搜索Query技术体系|方法论\nhttps://mp.weixin.qq.com/s/8wAJOfTV-BuOuVk9Ul5F4Q\nQuery 理解和语义召回在知乎搜索中的应用\nhttps://mp.weixin.qq.com/s/MAfK4B2F8sPXRLodXkwnmw\n","plink":"http://www.ephesus.top/links/NLP_ability/搜索/搜索/搜索资源总结-持续更新/"},{"title":"","date":"2024-06-21T03:48:13.525Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.525Z","content":"BERT模型的压缩大致可以分为：1. 参数剪枝；2. 知识蒸馏；3. 参数共享；4. 低秩分解。\n其中，对于剪枝，比较简单，但是容易误操作降低精读；\n对于知识蒸馏，之前我写个一系列的文章，重点可以看一下这里：\n对于参数共享和低秩分解，就和今天分享的ALBERT息息相关；\n它减少了BERT的参数，但是需要注意的一个细节点是，同等规格下，ALBERT速度确实变快，但是并不明显（和大量自媒体文章解读给大家的印象差距很大）；\n举个形象的例子就是，（这个例子并不严谨，只是帮助理解）参数共享让它训练的时候把多层压缩为一层去训练，但是在预测的时候，我们需要再展开多层去进行预测。\n主要掌握以下的几个知识点：\n\n词向量嵌入参数分解\n跨层参数分享\n取消NSP，使用SOP\n预训练的时候采用更满的数据/n-gram mask方式\n\n1.词向量嵌入分解\n词向量嵌入参数分解，简单说就是将词向量矩阵分解为了两个小矩阵，将隐藏层的大小和词汇矩阵的大小分离开。\n在Bert中，词汇表embedding大小是$V*H$；\nAlbert 的参数分解是这样的，将这个矩阵分解为两个小矩阵：$VE$和$EH$\n这样做有什么好处呢？\n如果说，我觉得我的模型表达能力不够，我想要通过增大隐层H的大小来提升我们模型能力的表达能力，那么在提升H的时候，不仅仅隐层参数增多，词汇表的embedding矩阵维度也在增多，参数量也在增大。\n矩阵分解之后，我们可以只是做到提升隐层大小，而不去改变表词汇表的大小。\n2.跨层参数分享\n跨层参数分享，这个操作可以防止参数随着网络层数的增大而增加。\n\n分为三种形式，只是共享attentions，只是共享FFN，全部共享。\n共享的意思就是我这部分结构只使用同样的参数，在训练的时候只需要训练这一部分的参数就可以了。\n看表格我们可以发现一个细节，就是只是共享FFN比只是共享attention的参数，模型效果要降低的多。\n小声嘀咕一下，这是不是说明FFN比attention在信息表达上要重要啊。或者说attention在学习信息表达的时候。attention层学习共性比较多。FFN学习到的差异性比较多。这只是我自己的猜测哈。\n3. SOP\n作者认为，NSP不必要。与MLM相比，NSP失效的主要原因是其缺乏任务难度。\nNSP样本如下:\n\n从训练语料库中取出两个连续的段落作为正样本\n从不同的文档中随机创建一对段落作为负样本\n\nNSP将主题预测和连贯性预测合并为一个单项任务；\n但是，与连贯性预测相比，主题预测更容易学习，并且与使用MLM损失学习的内容相比，重叠性更大。\n对于ALBERT，作者使用了句子顺序预测（SOP）损失，它避免了主题预测，而是着重于句间建模。\n其实就是预测句子顺序，正样本是顺着，负样本是颠倒过来。都是来自同一个文档。\n\n其他细节\n\n数据格式：Segments-Pair\n\n这个在RoBERTa中也有谈到，更长的序列长度可以提升性能。\n\nMasked-ngram-LM\n\n\n这就有点类似百度的ERINE和SpanBERT了\n\n推测速度\n\n\n从图中知道，同一规模ALBERT和BERT，比如同为Base：\nBERT base: 4.7x；ALBERT base：5.6x；速度确实变快，但是确实加速并不明显；\n同等效果的情况下，比如BERT base（Avg=82.3）和ALBERT large（Avg=82.4）：\nBERT base：4.7x；ALBERT large：1.7x；速度变慢了\n总结\n总结一下可以学习的思路：\n\n预训练的时候，数据填充的更满，到512这种，有利于提升模型效果，这点在RoBERTa有谈到\nmask n-gram有利于提升效果，这点类似百度的ERINE和SpanBERT了\n词向量矩阵分解能减少参数，但是也会降低性能\n跨层参数分享可以降低参数，也会降低性能，通过实验图知道，attention共享效果还好，FFN共享效果降低有点多\n取消NSP，使用SOP，正负样本来自同一个文档，但是顺序不同。\n推理速度来看，同等规格，ALBERT速度确实变快，但是并不明显，同等效果，速度变慢；https://kexue.fm/archives/7846)\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/ALBERT-更小更少但并不快/"},{"title":"","date":"2024-06-21T03:48:13.705Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.705Z","content":"Bert各种后续预训练模型-预训练模型的改进\n参考资料：\n站在BERT肩膀上的NLP新秀们（PART II） - kaiyuan的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/68362016\n√ XLMs from Facebook\n√ LASER from Facebook\n√ MASS from Microsoft\n√ UNILM from Microsoft\n\n邱锡鹏老师发表了关于NLP预训练模型的综述《Pre-trained Models for Natural Language Processing: A Survey》\n\n这里有一个对这个的解读，写的非常好，在这个文章中，这个作者也列出来了自己的另外另个文章，可以看一看\nNLP算法面试必备！史上最全！PTMs：NLP预训练模型的全面总结 - JayLou娄杰的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/115014536\n当然在这里文章里面，有一个链接，非常重要，就是对预训练模型单模型的精度，注意这里是精度，都是链接到了知乎文章\n写的都是非常好！！！！！非常好，链接地址在这里https://github.com/loujie0822/Pre-trained-Models\n这里链接一定要看\n这里还有一个关于邱老师综述的解读，也很好\n论文笔记 - NLP 预训练模型综述 - 徐阿衡的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/139015428\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/Bert各种后续预训练模型-预训练模型的改进/"},{"title":"","date":"2024-06-21T03:48:13.735Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.735Z","content":"Bert如何融入知识(一)-百度和清华ERINE\n首先想一下Bert是如何训练的？首先我获取无监督语料，随机mask掉一部分数据，去预测这部分信息。\n这个过程其实和W2C很类似，上下文相似的情况下，mask掉的单词的词向量很可能非常相近。\n比如说”今天米饭真好吃“和”今天苹果真好吃“，很有可能”米饭“和”苹果“学出来的向量就很相似。\n我在李如有一篇文章中《BERT句子表示的可视化》有这样一句话，contextual dependent词向量的一个缺点，就是上下文相似的情况下词向量也很接近。\n从这里，我觉得很容易就可以发现一个问题，就是Bert确实抽取能力非常的强，但是他也是在死记硬背的学这些知识。\n想一下，为什么我们需要在Bert中融入一些知识呢？\n我们考虑这么一个例子，比如我要对一个文本进行分类：”库克今日来北京进行商务洽谈活动“\n单从bert做一个文本分类，可能模型很难从语义角度进行决断。\n但是，我现在的知识图谱中有这样一个三元组：库克-CEO-苹果公司\n我把这个三元组的信息融入到我的模型之中，也就是我在文本分类的时候不仅仅使用了你的原始文本，还是使用了知识图谱中的三元组信息，相当于一种信息的增强，这个时候我的模型就可以文本分类为”IT公司“这个类别。\n一般来说，涉及到Bert中融入知识，大家都会涉及到两个文章：百度的 ERNIE from Baidu 和清华的ERNIE from THU\n我先从整体的思路说一下两者：\nERNIE from Baidu 出发点是这样的，Bert 的mask只是 mask掉单字，放在中文中，一般来说词汇会带有比字更多的信息。\n比如说\n哈[mask]滨真冷啊 是Bert基础操作\n[mask][mask][mask]真冷啊 是ERNIE from Baidu的操作\n也就是，我预测的不仅仅是一个单字，而是一个实体词组。\n对于这个操作，我是这么想的，首先从难度来讲，去预测一个词组会比预测一个单字难，而且这个词组是一个实体，所以在学习的时候回学习到实体信息\nERNIE from THU\n对于这个模型，我是这么想的，百度利用的是预测句子中的实体信息。而清华这边的操作是加入了外部的知识信息。\n就像最开始我们的例子，”库克-CEO-苹果公司“，这是外部知识，这个不是我文本中的信息，相当于显示的加入了外部信息。\n当然清华这边应该也只是使用到了实体信息（做了实体对齐）\n我们需要考虑两个问题：\n\n\n如何抽取并且更好的表达知识图谱的信息：知识嵌入算法（如TransE）\n\n\n实体向量和Bert的向量在不同的空间，如何缓解两者之间的Gap：\n\n\n对于这个问题，从模型架构上来解决，使用两种：\ntextual encoder (T-Encoder)：类别Bert\nknowledgeable encoder (K-Encoder)：用于将外部的知识图谱的信息融入到模型中；\n对于Bert融入知识信息，主要是参考以下文章：\n站在BERT肩膀上的NLP新秀们（PART I） - kaiyuan的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/68295881\n写的还不错，介绍了百度和清华的ERINE\nBert 改进： 如何融入知识 - 老宋的茶书会的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/69941989\n写的还不错，介绍了百度和清华的ERINE\nBERT与知识图谱的结合——ERNIE模型浅析 - 段易通的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/75466388\n写的还不错，介绍了百度和清华的ERINE\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/Bert如何融入知识一-百度和清华ERINE/"},{"title":"","date":"2024-06-21T03:48:13.755Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.755Z","content":"Bert如何融入知识(二)-Bert融合知识图谱\nBert如何融入知识(一)中主要是百度和清华ERINE，其实还有很多的Bert结合知识图谱的文章内容，这里我先列出来一些参考：\n当BERT遇上知识图谱 - kaiyuan的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/91052495\n√ KG-BERT from NWU\n√ K-BERT from PKU\n√ KnowBERT from AI2\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/Bert如何融入知识二-Bert融合知识图谱/"},{"title":"","date":"2024-06-21T03:48:13.775Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.775Z","content":"Bert的可视化-Bert每一层都学到了什么\n首先我罗列一下重要的知识点：\n主要是参考\nBERT句子表示的可视化 - 李如的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/87942922\n理解BERT每一层都学到了什么 - xijiz的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/74515580\n这个是对ACL论文What does BERT learn about the structure of language? 的解读，很好\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/Bert的可视化-Bert每一层都学到了什么/"},{"title":"","date":"2024-06-21T03:48:13.805Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.805Z","content":"Bert\nNLPCC：预训练在小米的推理优化落地\nhttps://mp.weixin.qq.com/s/itOyETgKBoRHOrIfKuphrw\nElasticsearch遇上BERT：使用Elasticsearch和BERT构建搜索引擎\nhttps://mp.weixin.qq.com/s/NV0SR7YveXwkhG3lrhMMQQ\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/Bert资源总结/"},{"title":"","date":"2024-06-21T03:48:13.835Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.835Z","content":"论文标题《FastBERT: a Self-distilling BERT with Adaptive Inference Time》。\n关于这个论文已经有不错的解读了，所以我写的侧重点可能和别人的不太一样，具体的往下看吧，欢迎讨论。\n这个论文从两个方面去掌握：\n\n样本自适应推断：使用不确定性指标（a normalized entropy）去过滤样本；\n模型自蒸馏：分支网络分类器学习主干网络分类器，并使用KL散度进行度量；\n\n然后我们聊一聊 FastBert 究竟在做什么事情？\nBert本身有12层，模型在进行推理的时候， 每一个样本都会完整的走过12层。而 FastBert 做到了让简单的样本的不必走过12层，只需要走过3层或者4层（这个数字并不确定）就可以。\n仔细想一下这个过程，简单来讲，就是杀鸡不要用牛刀，一件很简单的样本，不值得我们调用12层去搞定它，3层transformer学的参数就可以消灭掉这个样本。\n1. 整体过程\n整个过程分为三步：\n\n微调主干网络：使用自己下游数据微调主干网络。\n自蒸馏：可以使用无监督数据，用KL散度度量每一层和最后一层分布距离，并使用和作为损失。交叉熵也没差，H(p)不变，两者等价。\n自适应推理：自己设定速度(阈值)，大于这个的往后走，小于这个的输出结果。\n\n2. 需不需要自蒸馏\n但是，看到这里，我们肯定会有一个疑问。\n基于“杀鸡不用牛刀”的想法，我可以简单改造 Bert 模型，在每一层都加一个分类器，使用我自己的下游数据训练模型就可以了，损失函数直接使用每一层的交叉熵损失之和就可以了，为什么需要用到自蒸馏呢？\n简单来说，就是我完全可以不需要使用自蒸馏就可以达到同样的目的。\n基于这一点，作者有做一个实验对比，得出的结论就是没有自蒸馏，会导致精读的降低。\n具体看这个图：\n\n其实从这个图可以看到，如果没有自蒸馏，确实会有精度的下降。\n论文中在自蒸馏的时候，使用的是无监督的数据。\n我们一般可以会有大量的无监督数据，所以这个方法真的很适合少样本的情况的冷启动问题。\n不过，如果有监督数据，使用 labeled data应该会取得更好的效果，也就是不用去学习soft，而是去学习Hard。\n3. LUHA假设证明\n这个论文其实我自己更感兴趣的是文中针对假设：“不确定性越低，分类的准确度越高”的证明。\n作者分析了一个 FastBert 模型的三层：Student-Classifier0, StudentClassifier5, Teacher-Classifier 的分类结果，并在不同的不确定度区间评判分类准确度。\n\n我自己的理解哈，在这里是没有进行speed的筛选的，而是计算所有的样本，不然对于第一层，大于speed的地方准确度应该是0（因为直接就往后走了，并没有使用它的分类结果），不应该出现在图中。\n从这个图中，我们可以得到一个结论，就是不论是是针对分支网络的哪一层，还是针对主干网络，不确定性越高，分类的准确度越低。\n换过来讲，不确定性越低（图中横坐标越靠左），分类的准确度越高。\n当然作者是使用画图的方式来证明的，我自己当时理解的时候是这么想的：\n作者对不确定性的定义就是熵（在类别维度上做了normalized）。\n熵越低，说明分布的越集中，也就是说在做类别判定的时候，出现在某个类别的地方的概率越大，这样当然可以说明分类的准确度越高。\n我这个思路可能并不严谨，不过我觉得还是挺好理解的。\n4. 每层样本分布问题\n从这个图，还有一个问题需要注意：\n就是学生网络的分类层在每个不确定区间内的准确度都是高于主干网络分类层的（看最下面一个和最上面一个对比）。\n这一点真是非常的奇怪，如果按照这个理解，那么完全可以使用第一层替代主干网络。\n当然作者这里也给出了解释，就是每层的样本分布其实是不一样的，越靠近后面的层，样本的不确定度越低，越靠近左边，所以样本走完12层之后使用主干网络整体准确度肯定是比直接使用第一层的要好。\n具体可以看下面这个图（a）:\n\n对于上面这个图，我们还可以仔细去看一看里面的内容。\n需要明确一点，同一个样本，每经过一层计算一次它的不确定度（熵），都是在变化的，而且会逐渐靠近不确定度低的地方。\n也就是这个样本被分出类别的可能性越来越大。\n这一点其实很容易理解，就是Bert抽取能力随着层数越来越强，那么文本被正确分类的可能性当然越来越大，不确定度当然越来越大。\n你的Speed越高，筛选出来的样本越多，留给后面的就越是不确定高的样本，那么分布越高近图中的右侧。\n其他细节李如（她原来是个女生…）有个文章讲的挺好的，在这里：\nFastBERT：又快又稳的推理提速方法 - 李rumor的文章 - 知乎 https://zhuanlan.zhihu.com/p/127869267\n代码地址在这里：\nhttps://github.com/autoliuweijie/FastBERT\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/FastBert/"},{"title":"","date":"2024-06-21T03:48:14.245Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:14.245Z","content":"Pytorch代码分析–如何让Bert在finetune小数据集时更“稳”一点\n前几天在知乎刷到邱震宇同学的一个文章，如何让 Bert 在 finetune 小数据集时更“稳”一点，主要是分析了一篇论文，感觉很有意思。\n小数据集的时候，很容易就训练震荡。作者主要是分析了两个点进行优化。一个是adam偏移修正的问题，一个是权重初始化的问题。\n作者通过实验给出的结论是：1.使用误差修正，训练效率提高（收敛变快），开发集效果变好。2.使用权重初始化（后6层），训练效率提高（收敛变快），开发集效果变化不大。\n我做了简单的测试，有一个小点和作者结论不太一样，adam的修正并没有加速模型收敛，具体的看正文分析吧。\n我基于 huggingface 的 Bert，做了个两个简单的实验，详细情况如下。\n1. adam偏移修正\n对于adam偏移修正的实验，我这边得到的结果是：使用误差修正，收敛并未变快（反而变慢），训练更加稳定，开发集效果变好。\n对于收敛速度这块和作者结论不太一样，多跑了几个种子，都是收敛速度并未变快，希望有关大佬解惑。\n1.1 代码情况讲解\n首先说一下，在翻看源代码的时候，发现对于抱抱脸的 Bert 的实现，默认偏移修正是开启的，具体代码的位置在这里：\nhttps://github.com/huggingface/transformers/blob/84be482f6698fac822a5113735f2242c6d3abc76/src/transformers/optimization.py#L107\n抱抱脸使用的是 AdamW:  Adam algorithm with weight decay fix\n代码如下\n12345class AdamW(Optimizer):    def __init__(self,...,correct_bias=True):    ...    ...\n所以在测试偏移修正的对比的实验的时候，一个是保持默认不变得出一个结果；一个是修改这个参数，你可以在调用函数的时候修改参数传入值，也可以修改源代码，如果是修改的源代码，记得做完实验把代码改回来，别对之后工作造成影响。\n1.2 任务基本情况\n任务基本情况是这样的：\n\n任务类别：文本分类/15个类别\n数据来源: 今日头条新闻数据\n数据量级：训练集1K/开发集50k\n训练参数：\n\nBert : chinese_l-12_h-768_a-12，使用原始版本未做修改\nbatchsize：16\nmax_seq_length ：128\nlearning_rate：2e-5\nEpoches: 10\n\n\n\n因为数据量较小，并且想要观察变化，没使用 earlly stopping，完整的跑完了10个epoch，一共是 600 steps 左右，文中所示图以每隔 10 steps 打点显示，所以最大显示为 60/30。\n1.3 结果展示\n结果展示分为两个，一个是 Loss 变化图，一个是在开发集 Acc 展示图。\nLoss 变化如下图：\n\n可以看到，没有使用偏移纠正的 loss 收敛的更加的迅速一点，反而使用了修正的模型收敛的慢一点。但是同时可以观测到，修正之后，模型的收敛更加的稳定，相比较而言，并没有特别大的震荡。\nAcc变化如下图（后期没怎么变化，所以截取了前300steps）：\n\n对于在开发集来说，经过修正的收敛速度慢，但是比较稳定，没有大幅度的震荡，准确度相比，有可观收益（图中看不不太明显，无修正最好效果:0.80，加入修正最好效果: 0.82）\n2. 权重初始化\n权重初始化比较简单，平常任务也试过，因为是文本分类任务，所以在这里只是简单的测试了一下重新初始化 Pooler 层。\nLoss结果如下图：\n\n从图中可以看出，重新初始化，收敛速度变快了，但是不明显。\nAcc没什么变化，就不上图了，没什么变化（主要是被我无意删了，懒得再重跑一次了，不影响大局）\n3. 简单总结\n简单总结一下：\n与没有修正的adam之后，修正之后，模型收敛速度变慢，收敛过程变得稳定，效果提升比较明显。\n与没有重新初始化的模型相比，初始化最后一层pooler之后，模型收敛速度有所变快，但是不明显，效果也没有明显变化。\n上面这两个实验只是基于邱震宇同学的文章做的，在这里感谢作者。关于收敛速度这里，结果有一点不一样，希望有大佬可以解惑，我也会抽空去看看原论文，仔细研读一下，看论文还有没有值得挖的东西，有任何进展，我再和大家说。\n打完收工，看完我这么辛苦画图（真是累死了）的份上，点个赞再撤吧，鞠躬感谢。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/Pytorch代码分析-如何让Bert在finetune小数据集时更“稳”一点/"},{"title":"","date":"2024-06-21T03:48:14.275Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:14.275Z","content":"RoBERTa：更大更多更强\n今天分享一个Bert的改进工作RoBERTa。RoBERTa是训练充分的Bert。\n主要掌握以下几点，与Bert相比较，RoBERTa预训练的时候：\n\n动态掩码：comparable or slightly better\n去掉NSP任务并且更改数据输入格式为全部填充可以跨越多个文档\n更多数据，更大bsz，更多的步数，更长训练时间\n\n1. 动态掩码\n首先明确Bert使用的是静态掩码。但是这样会存在一个现象，比如我训练40个epoches，那么每次epoches都是使用同一批数据。\n这其实不是什么大问题，我们在深度学习训练模型的时候，每个epoches基本都没咋变过。\n不过对于Bert，其实本质是一个自监督模型。每次的训练输入如果是不同的，对于模型肯定是更好的。\n比如我们句子为：今天去哪里吃饭啊？\nmask之后为：今天去哪里[mask]饭啊？\n每次训练使用同一个mask样本，那么模型见得就少。\n如果换一个mask：[mask]天去哪里吃饭啊？\n模型对于同一个句子，在预测不同的单词，那么模型对句子的表达能力直觉上肯定是会上升的。\n所以为了缓解这种静态掩码的问题，Bert的操作是这样的：\n复制原始样本10份，每份都做不同的静态mask，然后进行训练。\n我们想一下这个过程：比如我仍然是训练40个epoches，复制了十份样本，相当于每4个epoches使用的是同一个mask的样本。\n这个操作确实缓解了静态掩码的问题，但是毕竟还有重复mask的情况出现。\n这个时候其实有个朴素的思想，为啥不直接复制40份，然后分在40个epoches中进行训练，这个到时候写Bert的时候再说。\nRoBERTa 是咋做的呢？\n动态掩码，也就是不是在最开始的时候的数据处理的过程中就生成mask样本，而是在送入到模型之前才进行mask，这样同一个句子，在40epoches中，每次mask都不同。\n效果直接看图\n\n2. NSP和模型数据输入格式\n这一点其实很有意思。\n我们先说RoBERTa 的四种输入形式和实验效果，然后再详细分析：\n\nSEGMENT-PAIR+NSP：就是Bert的输入形式\nSENTENCE-PAIR+NSP：输入的是一对句子，即前后是单个句子\nFULL-SENTENCES：输入为全量的句子，填满512的长度，采集样本的时候可以跨越文章的界限，去除了NSP loss\nDOC-SENTENCES：输入和FULL-SENTENCE类似，但是一个样本不能跨越两个document\n\n然后看一下实验效果：\n\n对上面这个图一个最简单的总结就是NSP没啥用。然后我们来详细说一下这个事情。\n首先Bert的消融实验证明，NSP是应该有的，如果没有NSP，在部分任务上效果有损失。\n但是上图RoBERTa实验证明，NSP没啥效果，可以没有。\n一个直观的解释，或者说猜测是因为，可能是Bert在消融实验去除NSP的时候，仍然保持的是原始的输入，即有NSP任务的时候的输入形式。\n这就相当于，构造了好了符合NSP任务的数据，但是你删去了针对这个任务的损失函数，所以模型并没有学的很好，在部分任务精读下降。\n但是RoBERTa这里不是的，它删除NSP任务的时候，同时改变了输入格式，并不是使用上下两句的输入格式，而是类似文档中的句子全部填满这512个字符的格式进行输入。\n简单说就是，去掉了NSP任务的同时，去掉了构造数据中NSP数据格式。\n比较SEGMENT-PAIR和DOC-SENTENCES两个模式的时候，证明没有NSP效果更好。其实看起来好像并没有控制变量，因为变了两个地方，一个是是否有NSP，一个是输入格式。\n这种情况下，就只能去看在下游任务中的效果了。\n3. 数据+bsz+steps\n\n数据：Bert：16G；RoBERTa：160G；十倍\nbsz：Bert：256；RoBERTa：8K\nsteps：Bert：1M；RoBERTa：300K/500K\n\n4. 总结：\n简单总结一下学到的东西：\n\n动态掩码：comparable or slightly better\n去掉NSP任务并且更改数据输入格式为全部填充可以跨越多个文档\n更多数据，更大bsz，更多的步数，更长训练时间\n动态掩码那里，说到一个复制10份的细节，那里是针对的Bert，RoBERTa是每次输入之前才mask，注意区分，不要搞混\n\n参考资料：RoBERTa: 捍卫BERT的尊严 - yangDDD的文章 - 知乎 https://zhuanlan.zhihu.com/p/149249619\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/RoBERTa/"},{"title":"","date":"2024-06-21T03:48:14.605Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:14.605Z","content":"UniLM：给Bert插上文本生成的翅膀\n今天分享一个论文UniLM，核心点是掌握三种LM任务形式：单向LM，双向LM，序列到序列LM；\n1. 生成任务\nNLP任务大致可以分为NLU和NLG两种；Bert在NLU任务上效果很好，但是天生不适合处理生成任务。\n原因在于Bert的预训练过程是使用的MLM，和生成任务的目标并不一致。\n生成任务目标是每次蹦出来一个词，只能看到当前位置之前的词汇。\n而Bert采用的是双向的语言模型，除了mask的单词，两个方向的词汇都可以被看到。\n所以对Bert的一个改进思路就是让它在具有NLU能力的时候，同时兼备NLG能力。\n2. 三种LM任务\nUniLM做的就是这样一个事情。\n具体的实现方式是设计了一系列的完形填空任务，这些完形填空任务的不同之处在于对上下文的定义。\n\n从左到右的LM：使用mask单词的左侧单词来预测被遮掩的单词\n从右到左的LM：和上面第一个相比就是方向的变化，使用mask单词的右侧单词来预测遮掩的单词\n双向LM：就是当前mask的左右词汇都可以看到\nsequence-to-sequence LM：这个就是UniLM能够具有生成能力的关键。我们的输入是source句子和target句子，mask单词在target上，那么当前mask的上下文就是source句子的所有单词和target句子中mask单词左侧的词汇可以被看到\n\n我们把从左到右LM和从右到左LM我们归为一种任务叫单向LM；\n有个点需要注意，三个任务是一起优化的，具体来讲是这样做的：\n在训练的时候，1/3的时候使用双向LM，1/3的时候使用序列到序列 LM，1/6的时候使用从左到右的LM，1/6的时间使用从右到做的LM。\n我们是使用不同的Mask矩阵来对应不同任务输入数据形式。\n文中使用的是这样一张图来展示：\n\n3. 其他细枝末节\n\nGelu 激励函数\n24层TRM，最大长度512，1024Hidden Size，16Heads，340M参数量\n初始化使用Bert Large\n15%被mask，其中80%真正替换mask，10%随机替换，10%不动。替换的时候，80% 的时候替换单个token，20%的时候替换bigram 或者 trigram\n\n第四个步骤类似中文实体词的mask，也算是一点改进。\n有个细节点需要注意的是，作者强调，不同的segment embedding用来区分不同LM任务。\nBert的时候，区分上下句子，我们使用0和1，在这里，我们使用这个segment embedding用来区分任务：\n比如说，双向对应0和1；单向left-right对应2；单向right-left对应3；序列对应4和5；\n4. 总结\n掌握以下几个细节点就可以：\n\n联合训练三种任务：单向LM，双向LM，序列LM\n使用不同的attention矩阵控制三种任务形式的参与\nsegment embedding可以区分不同的任务形式\nmask的时候15% 的有被替换的概率，其中80% 被真正替换。在这80%真正替换的里面有80%单个token被替换，20%的二元或者三元tokens被替换\n\n5. 加我微信，点赞之交\n\n参考链接：\nUniLM论文阅读笔记 - 刘聪NLP的文章 - 知乎 https://zhuanlan.zhihu.com/p/113380840\nBERT生成式之UNILM解读 - rumor的文章 - 知乎 https://zhuanlan.zhihu.com/p/68327602\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/UniLM/"},{"title":"","date":"2024-06-21T03:48:14.765Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:14.765Z","content":"XLNET里面的细节点有很多，重点掌握以下两点：\n\nAR和AE两种无监督预训练的优化目标\n双流自注意力机制：为什么需要把位置信息和内容信息拆分\n\n1. 无监督目标函数\n在NLP中，无监督表示学习已经获得长足发展。一般的流程是先将模型在大量无标签数据上进行预训练，然后在具体的下游任务上进行微调。\n一般来说，无监督预训练有两种目标函数很受重视：AR和AE。\n\nAR，也就是autoregressive，我们称之为自回归模型；用$x_{0},x_{1},x_{2}…$去预测$x_{t}$，可以分为正向和反向，只能考虑单侧的信息，典型的就是GPT\nAE，也就是autoencoding，我们称之为自编码模型；从损坏的输入数据中预测重建原始数据。可以使用上下文的信息，Bert就是使用的AE；\n\nAE模型能够看到句子中的更多的信息，这也是BERT在下游任务中表现很好的原因。\n先来看一下优化目标，对于AR语言模型来说，它是基于概率的一个链式法则，优化如下：\n\n而AE模型，拿BERT举例，它的优化目标是是从损坏的输入数据中重建原始未被破坏的输入，优化目标如下：\n\n举个简单的例子，原始输入为【我爱吃饭】，那么AR模型在做的时候，它的优化是P(我爱吃饭) = P(我)P(爱|我)P(吃|我爱)P(饭|我爱吃)；\n阈值对应的，假设我们mask之后为【我爱mask mask】，那么BERT的优化目标是：P(我爱吃饭|我爱maskmask)=P(吃|我爱)P(饭|我爱)；\n根绝这两个例子，我们首先知道对于AR模型，它在优化的时候，只能看到单向信息。\n对于AE模型，它可以看到双向信息，但是有一个问题就是，它认为mask之间相互独立，也就是上面例子中【吃】和【饭】是相互独立的。\n很显然，这一点是错误的，【吃】和【饭】之间肯定是有联系的，但是BERT在预训练的时候并没有考虑这一点。\nBERT还有一个缺点就是，在预训练的时候，是存在mask字符的，但是在微调的时候，也就是在我们的在具体任务上训练的时候，我们的输入是不存在mask字符的，造成了预训练和微调之间的gap；\nAR（单向缺点）和BERT（mask缺点）都存在缺点，XLNET想办法解决了这两个问题。\n2. Permutation Language Modeling\n为了解决AR模型不能关联上下文信息，提出这个策略。\n如果我们的序列x的长度为T，那么对于这个序列，我们有T!种排列方式。\n比如说，原始排列为1，2，3，4；那么对它进行重排列，就有24中排列方式。\n我们挑两种来看：1，2，3，4和1，4，3，2；\n在这两种排列中，假设我们都处于预测第三个位置3的时刻，那么对于第一种，它能够看到的内容信息来自1和2，对于第二种排列方式，它能够看到的内容信息来自1和4。\n这样一来，对于3 来说，它在训练之后，既看到了前面的信息1和2，又看到了后面的信息4。\n通过这种方式，AR模型可以联系到上下文的信息。\n但是如何做到输入序列进行重排序呢，使用TSSA；这样输入序列顺序不会发生变化，顺序的变化只是发生在内部；\n3. Two-Stream Self-Attention\n假设我们先把某个点的信息分为内容信息和位置信息，为啥这么分，看完例子就知道了。\n先来一个简单的例子，句子序列：1，2，3，4；\n如果要是预测3，那么需要做什么：\n首先，我需要1和2的全部信息（包括它们内容信息和位置信息）；其次需要看到3当前这个点的位置信息，确保知道预测的是哪一个位置，但是我不能看到3这点的内容信息，因为我要预测这个单词，不能做到标签信息的泄露。\n如果要是预测4，那么需要做什么：\n首先，我需要1，2，3的全部信息（包括它们的内容信息和位置信息），其次，我需要看到4当前这个点的位置信息；\n好了，两个例子联合起来看，对于3这个点，有的时候我需要向4提供全部信息（包括内容信息和位置信息），有的时候我需要向自己提供位置信息（不能包含内容信息，防止造成标签的泄露）；\n这就是为什么需要将信息分为内容信息和位置信息，如果不分开，那么对外提供信息的时候就不能有效的隔离。\n仔细琢磨这个例子，对照着这个例子，可以看一下下面这个图：\n\n这个图，细节一点要注意，在计算位置信息的时候，QKV分别代表着什么？\n这点需要大家仔细去看。\n然后看c，在最后预测的时候我们使用的query stream，并没有使用content stream；这点需要注意；\n4.其他细节\n\n使用部分预测：句子预测起始阶段，上文信息较少，担心误差较大，所以只对句子后1/K的tokens被预测\n使用Transformer-XL，用于处理长文本\n\n5. 总结\n说一下值得注意的点，主要就是双流自注意力机制这里很有意思，在初看图的时候很容易看混。\n这么理解会更加的方便，对于同一个token，在预测自身的时候，它需要向外提供自己的位置信息，在预测别的单词的时候，它需要对外提供全部信息。\n所以一个好办法就是把内容信息和位置信息分隔开对外提供。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/XLNET/"},{"title":"","date":"2024-06-21T03:48:14.375Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:14.375Z","content":"今天分享一个论文ACL2020-tBERT，论文主要融合主题模型和BERT去做语义相似度判定，在特定领域使用这个模型，效果更明显。\n掌握以下几点：\n\n【CLS】向量拼接两个句子各自的主题模型，效果有提升\n尤其是在特定领域的数据集合会有更好的表现。\n\n1. 架构图\n先看架构图：\n\n模型架构比较简单，BERT这边使用的【CLS】输出向量：$C=BERT(S_{1},S_{2})$\n主题模型使用两种，LDA和GSDMM，主要是因为LDA在长文本效果更好；GSDMM在短文本效果更好。\n获取主题模型如下所示：\n$$D_{1} = TopicModel([T_{1},…,T_{N}]) \\in R^{t}$$\n$$D_{2} = TopicModel([T^{,}{1},…,T^{,}{M}]) \\in R^{t}$$\n$t$ 代表的是主题数量，N是$S_{1}$的字数量，M是$S_{2}$的字数量\n进而我们可以得到单词的主题分布:\n$w_{i} = TopicModel(T_{i})$\n$$W_{1} = \\frac{\\sum^{N}{i=1}w{i}}{N} \\in R^{t}$$\n$$W_{2} = \\frac{\\sum^{M}{i=1}w^{,}{i}}{M} \\in R^{t}$$\n所以在最后和【CLS】连接的时候，可以使用文档主题$D_{1}和D_{2}$，也可以使用单词主题$W_{1}和W_{2}$。\n2.实验效果\n\n看实验效果，LDA效果会比GSDMM更好一点。\n其实有一个比较有意思的点是，BERT的建模能力已经足够强了，为啥加上主题模型还会有提升。\n换句话说，主题模型在基于BERT的方向上，能够在哪些方面提升。\n作者是这么做的实验，他选了和主题模型相关的三个属性：实体，特定领域词和不规范拼写。根据三个属性抽取样本，总共500个， 然后让BERT和tBERT做预测。\n\n看实验效果是这样的，发现在特定领域tBERT效果更明显一点。\n作者认为在预训练的时候，可能是BERT碰到特定领域词汇的机会比较少，没有很好的学习到这些信息，所以主题模型很好的补充了这部分信息。\n不过，感觉这个实验并不充分，一个属性这块挑选感觉有点不太充分，还有一个是样本数量感觉太少了，500个…\n总结\n说一下掌握的知识点：\n\n【CLS】向量拼接两个句子各自的主题模型，效果有提升\n尤其是在特定领域的数据集合会有更好的表现。\n\n说一下我自己的思考，关于特定领域这块。一般来说，微调是可以解决这个问题的。\n不过看作者的实验，即使是微调之后的BERT，在特定领域这块，效果也没有tBERT好，说明主题模型在这块还是很有用的。\n进一步思考，可不可以这么推论，如果说我们的任务输入越是特定领域，那么假如tBERT越有明显的提升呢？\n这个感兴趣的大家可以去试一试，比如医疗领域，比如金融领域之类的。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/tBERT-BERT融合主题模型/"},{"title":"","date":"2024-06-21T03:48:14.985Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:14.985Z","content":"今天分享的论文解决了我很久以来的一个疑惑，名字叫做：On the Sentence Embeddings from Pre-trained Language Models：\n这个论文主要探讨两个问题：\n\n为什么Bert做无监督文本匹配效果不好？是因为bert的输出携带的语义信息太少还是我们没有正确的利用挖掘这些语义信息\n如果说是因为没有正确挖掘利用这些语义信息，那么我们怎么使用无监督的方式，让这种语义信息很容易的被利用\n\n本文主要聊第一点，先说一下结论：\nBert向量空间并不平滑，有些区域不好被定义，也就是说是存在问题的，导致简单的相似性度量不能很好起到度量作用。\n简单说就是Bert输出向量语义信息是足够的，但是consine这种简单东西不能很好的度量出来而已。\n那么就有大致两个解决办法：转化Bert的输出空间或者使用其他的有用的相似性度量函数。\n论文使用的是第一种，对Bert的输出空间做了一个转化，这个不细说，大家去看原论文。\n1. 之前遇到的小疑惑\n我简单说一下我之前遇到的问题：\nBert出来之后，在下游任务上，横扫各大榜单。\n一个最基础的应用是使用【cls】的输出向量微调做文本分类任务；\n这样做的前提我们认为因为由于自注意力的存在，【cls】可以看到全局的信息。\n但是需要注意的是，上面这句话绝对不代表没有在具体任务微调过的【CLS】向量可以代表整个句子的语义信息。\n为什么这么说呢？\n我们使用【CLS】向量做无监督的文本匹配，会发现一个非常奇怪的现象。\n一般直觉上认为，由于Bert强大的编码能力，应该可以很好的表示语义信息，所以做无监督语义匹配，准确度应该还不错。\n但是结果会发现相似度都比较大，没有什么区分度。\n更进一步，我们可以对Bert输出，不仅仅是【CLS】向量，可以使用所有Tokens的输出向量做各种操作：max/mean/min Pooling等等吧。\n甚至我们可以使用倒数第1/2/3/4或者把最后几层输出联合起来使用。\n总之各种骚操作之后，会发现，无监督文本匹配效果依然不好。\n究其原因就是👇谈到的词向量的各向异性。\n2. 词向量的各向异性\n我们来详细说说Bert的向量空间存在的问题。\n之前的论文发现Bert的词向量存在各向异性（anisotropic），两个现象\n\n词频影响了词向量空间分布\n\n根据词频来区分不同的单词，然后计算不同词频的单词距离原点的距离，发现词频越低，距离原点越远。\n我们希望句子的embedding表达可以用来度量句子之间的相似性；但是如果单词的embedding的分布是受词频影响的，那么这种相似性是不能让人信服的。\n我举个简单的例子，两句话，A和B：\n\nA：你喜欢吃苹果吗？\nB：你爱吃苹果吗？\n\n不同的单词是【喜欢】和【爱】。我们假设哈，假设【喜欢】出现的频次足够的高，那么它离原点就足够的近。【爱】出现的频率足够的低，那么它离原点就足够的远。\n这样的话，词频信息就误导了我最终句子向量的表达，那么embdding度量相似性就不靠谱了。\n\n词频影响词向量空间稀疏性\n\n基于上面的1，论文观察到，词频高的词汇分布的密集，词频低的词汇分布的稀疏。\n这一点问题很大，因为如果存在稀疏性，那么在词频低的词汇周围，就会存在很多意义不明的地方。\n我们可以直观的想一下，如果分布的很稀疏，那么也就是词与词之前的空间存在大量的不确定性，这些地方是没有明确的语义信息表达的。\n如果我们最终所有单词的加和正合适落在这种语义不明确的地方，那么相似性的度量准确度自然很低。\n为什么sen-emb相似度很大\n其实还有一个小点我想说一下，就是自己做实验的时候，会发现语义匹配效果不好的体现是因为consine计算出来的相似度基本都大于0.9。\n文中解释了为啥效果不好，但是并没有详细说为啥相似度计算出来都很大。（或者是有提到但是我粗心错过了）\n针对这一点，我想说一下我自己的感受。\n我觉得这两点的结合是根本原因：高频离原点近+高频分布紧密。\n怎么说呢？\n这句话其实可以这么理解：高频词基本都挤在一块了。\n句子大部分还是高频词占主要部分，在计算句子向量sen-emb的的时候，我们的操作是直接相加。\n那么计算出来的和当然相似度就很高。\n我这个解释不严谨，没啥严格证明，但是我觉得比较容易理解。\n简单总结\n简单来说，就是Bert的向量空间存在各向异性（anisotropic）：\n\n高频离原点近，低频离原点远\n高频分布紧密，低频分布稀疏\n\n这两个现象的存在导致Bert的语义信息不能很好的表达出来，所以做语义相似度不好。\n一个解决办法，就是把Bert的向量空间转换到另一个更加合适的空间，然后再做相似性度量，这就是论文的另一部分，感兴趣的可以去看原论文吧。\n为啥相似度都很大，主要是因为高频词都挤在一块了。\n加我微信，做个点赞之交\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/为什么Bert做不好无监督语义匹配/"},{"title":"","date":"2024-06-21T03:48:15.005Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.005Z","content":"前几天有朋友问了一下【小布助手短文本语义匹配竞赛】的问题，主要是两个；\n\n如何在脱敏数据中使用BERT；\n基于此语料如何使用NSP任务；\n\n比赛我没咋做，因为我感觉即使认真做也打不过前排大佬[囧]，太菜了；不过我可以分享一下我自己的经验；\n对于脱敏语料使用BERT，一般可以分为两种：\n第一种就是直接从零开始基于语料训练一个新的BERT出来使用；\n第二种就是按照词频，把脱敏数字对照到中文或者其他语言【假如我们使用中文】，使用中文BERT做初始化，然后基于新的中文语料训练BERT；\n大家可以先看一下当时我的回复：\n\n\n然后我发现很多朋友对于预训练模型其实理解的还是不深刻，很疑惑为什么在脱敏数据中也可以训练BERT等预训练模型；\n其实这一点很容易理解，就像我截图中说到的：\n最开始BERT是用英文语料训练出来的，然后有朋友基于中文语料开源了中文的BERT；\n那么我的脱敏数字就是类似于中文的一种另外的语言，你可以看成是【X】语言，我们当然可以基于【X】语言的语料去训练一个新的BERT或者其他的预训练模型了；\n有的朋友谈到了NSP任务如何去使用的问题；\n很明显，在当前这个任务中是一个文本匹配的形式；\n语料不是我们自己有主动的去获取的能力，所以构造一个NSP任务的格式比较困难；\n但是NSP任务仅仅是一种任务形式，我们完全可以基于训练语料构造一个是否匹配的任务，可以称之为类NSP任务；\n基于此，测试数据是使用不了的，因为测试数据没有label；\n不过，我自己认为可以测试数据使用MLM任务，训练数据使用MLM+类NSP任务；\n更加具体大家可以看我当时的回复：\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/如何在脱敏数据中使用BERT等预训练模型/"},{"title":"","date":"2024-06-21T03:48:15.055Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.055Z","content":"Bert我们一般使用方法是，加载预训练模型，在我们自己的任务上进行微调。但是我们有些时候会遇到这种情况，比如说，之前文章提到的，\n我不想要你预训练模型中最后三层参数，而是使用我自己的方法重新初始化。\n首先解释一下为什么需要这么做？有的论文发现，bert越靠后面（越靠近顶层，也就是输出层），学到的知识越是笔记抽象高级的知识，越靠近预训练模型的任务情况，和我们自己的任务就不太相符，所以想要重新初始化，基于我们自己的任务从零学习。\n好了，代码是怎么实现？\n一般pytorch的初始化方法我就不说了，这个比较简单，之后可能有时间写一下，这里专门介绍一下bert里面如何去做。\n首先，我们看一下源代码，加载模型的时候是怎么加载的：\n1model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool(&#x27;.ckpt&#x27; in args.model_name_or_path), config=config)\n链接在这里：https://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/run_classifier.py#L462\n再执行到这里之后，会进入并执行这个函数：\n1def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n代码链接在这里看：\nhttps://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/transformers/modeling_utils.py#L224\n这个函数就是我们要修改的函数，核心操作是这个操作：\n1module._load_from_state_dict(state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n代码位置在这里：\nhttps://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/transformers/modeling_utils.py#L404\n主要是两个参数最重要：\nmissing_keys：就是我们自己定义的模型有哪些没在预训练模型中，比如我们的模型现在是 BertForSequenceClassification ，那么这里结果就是 [‘classifier.weight’, ‘classifier.bias’]\nunexpected_keys:预训练模型的参数有很多，这里的结果是定义的我们对哪些参数忽视，并不采用，这里的正常结果是这样的：[‘cls.predictions.bias’, ‘cls.predictions.transform.dense.weight’, ‘cls.predictions.transform.dense.bias’, ‘cls.predictions.transform.LayerNorm.weight’, ‘cls.predictions.transform.LayerNorm.bias’, ‘cls.predictions.decoder.weight’, ‘cls.seq_relationship.weight’, ‘cls.seq_relationship.bias’]\n重点来了，如果我们想要对第一层的query的进行重新初始化，怎么做？分两个步骤，第一步，定义你想要重新初始化哪些参数，第二步代入进去。看代码：\n1unexpected_keys =[&#x27;bert.encoder.layer.0.attention.self.query.weight&#x27;,&#x27;bert.encoder.layer.0.attention.self.query.bias&#x27;]\n就这么简单，这里定义了就可以\n代码位置在这里\nhttps://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/transformers/modeling_utils.py#L364\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/解决老大难问题-如何一行代码带你随心所欲重新初始化bert的某些参数附Pytorch代码/"},{"title":"","date":"2024-06-21T03:48:15.135Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.135Z","content":"大概会花一到两周的时间，把 transformer 系统的讲一遍，可能会涉及到到 Bert/GPT 的一些基本知识，每天只讲一个知识点。\n所有的关于NLP知识的文章都会放在下面这个仓库，大家快去看。\nhttps://github.com/DA-southampton/NLP_ability\n预告一下明天内容，是关于transformer位置编码的讲解，很多同学对位置编码这个概念很模糊，只是知道是正余弦函数，别的就不太清楚，我们之后花几篇文章好好聊一聊这个概念。这个已经更新在github，想看的朋友可以提前去看一哈。\n正文\nTransformer 分为两个部分，encoder 侧 和 decoder 侧。今天，我们聊一下 encoder 侧。这部分由 N 个完全相同的大模块堆叠而成（原论文N=6）。\n这个结构怎么理解？这个构造就需要我们确保每一个模块的输入和输出维度是相同的，在实现代码的时候，我们只需要完成一个模块的代码的构造就可以。\n注解：你可以把这个过程想象成 RNN 竖过来的一个流程，是不是就很好理解（当然这样想只是帮助你理解）。\n其次对于这每一个大的模块，又分为两个模块，分别是多头注意力层和前馈神经网络层。进一步拆分，多头注意力层可以分为注意力层和 Add&amp;Norm 层。前馈神经网络可以分为 Linear 层和 Add&amp;Norm 层。\n多头注意力层，核心点在于 Q/K/V 三个矩阵，其中 Q/K 矩阵生成权重矩阵(经由softmax)，随后和V矩阵得到加权和。\n这个过程重复了 n_heads 次，这个 n_heads 代表的就是头的数目，这里需要注意的是我们需要确保 hidden_size/n_heads 需要为一个整数，不然代码会报错。\nAdd 代表一个残差结构。对于残差结构，可以使得信息前后向传播更加顺畅，缓解了梯度破碎问题。在 NLP 角度来看，残差结构一定程度上促进了 NLP 网络结构向窄而深的方向发展。\n我们可以把 Transformer 和之前的模型对比一下，比如 RNN 模型，一般来说，我们会选择 单层RNN 或者 一个 Bilstm，对于这些比较传统的模型，只是在时间长度上进行了延展，并没有在深度上做的太深。\n所以说，残差结构是有助于网路变深的。\n顺便联想一下 Elmo，使用的是 双层双向lstm，训练起来已经非常慢了，所以对于RNN这种比较传统的模型，做深太难了，GNMT也是用了很多的 tricks 进行加速训练。\nNorm 代表的是 Layer Normalization。为什么这里使用 Layer Normalization，而不是BN，这个后面有文章说，这里直白的回答就是，BN的效果差，所以不用。\n随后多头注意力层的输出经过前馈神经网络。对前馈神经网络，比较简单，我们需要注意的是它分为两个 Linear 层，第一层的激活函数为 Relu，第二层没有使用激活函数。\n最后我们谈一下整个encoder的输入和输出。\n先说输入，分为两个部分：word embedding 和 position encoding\nword embedding 没什么可说的，初始化后跟着训练或者使用word2vec这种已经有的看具体任务的效果。\nposition encoding 这里 transformer 使用的是 正余弦函数进行表达。其实这里进行初始化然后进行训练也是可以的，论文原作者的实验表明效果基本没区别。\n对于 position encoding 表示的绝对位置，这点大家都没异议，那么 position encoding 究竟有没有表达相对位置信息，之后会有个文章专门讲讲这个知识点。\n然后说一下 encoder的输出，感觉很少有人谈到这里。\nencoder 的输出需要注意的细节点在于它需要和 decoder做交互，所以它的输出为 K/V 矩阵，记住这个细节点，Q 矩阵来自decoder模块，K/V矩阵来自encoder。\n写到这里，我估摸这三分钟差不多能看完，现在没有留言功能，有问题大家在公众号对话框发送，我后台能看见。\n能点个在看，老铁们 ！！鞠躬感谢！！\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/3分钟从零解读Transformer的Encoder/"},{"title":"","date":"2024-06-21T03:48:15.175Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.175Z","content":"BN踩坑记–谈一下Batch Normalization的优缺点和适用场景\n这个问题没有定论，很多人都在探索，所以只是聊一下我自己的理解，顺便为讲 layer-norm做个引子。\nBN的理解重点在于它是针对整个Batch中的样本在同一维度特征在做处理。\n在MLP中，比如我们有10行5列数据。5列代表特征，10行代表10个样本。是对第一个特征这一列（对应10个样本）做一次处理，第二个特征（同样是一列）做一次处理，依次类推。\n在CNN中扩展，我们的数据是N·C·H·W。其中N为样本数量也就是batch_size，C为通道数，H为高，W为宽，BN保留C通道数，在N,H,W上做操作。比如说把第一个样本的第一个通道的数据，第二个样本第一个通道的数据…第N个样本第一个通道的数据作为原始数据，处理得到相应的均值和方差。\nBN有两个优点。\n第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使损失平面更加的平滑，从而加快的收敛速度。\n第二个优点就是缓解了梯度饱和问题（如果使用sigmoid激活函数的话），加快收敛。\nBN的缺点：\n第一个，batch_size较小的时候，效果差。这一点很容易理解。BN的过程，使用 整个batch中样本的均值和方差来模拟全部数据的均值和方差，在batch_size 较小的时候，效果肯定不好。\n第二个缺点就是 BN 在RNN中效果比较差。这一点和第一点原因很类似，不过我单挑出来说。\n首先我们要意识到一点，就是RNN的输入是长度是动态的，就是说每个样本的长度是不一样的。\n举个最简单的例子，比如 batch_size 为10，也就是我有10个样本，其中9个样本长度为5，第10个样本长度为20。\n那么问题来了，前五个单词的均值和方差都可以在这个batch中求出来从而模型真实均值和方差。但是第6个单词到底20个单词怎么办？\n只用这一个样本进行模型的话，不就是回到了第一点，batch太小，导致效果很差。\n第三个缺点就是在测试阶段的问题，分三部分说。\n首先测试的时候，我们可以在队列里拉一个batch进去进行计算，但是也有情况是来一个必须尽快出来一个，也就是batch为1，这个时候均值和方差怎么办？\n这个一般是在训练的时候就把均值和方差保存下来，测试的时候直接用就可以。那么选取效果好的均值和方差就是个问题。\n其次在测试的时候，遇到一个样本长度为1000的样本，在训练的时候最大长度为600，那么后面400个单词的均值和方差在训练数据没碰到过，这个时候怎么办？\n这个问题我们一般是在数据处理的时候就会做截断。\n还有一个问题就是就是训练集和测试集的均值和方差相差比较大，那么训练集的均值和方差就不能很好的反应你测试数据特性，效果就回差。这个时候就和你的数据处理有关系了。\nBN使用场景\n对于使用场景来说，BN在MLP和CNN上使用的效果都比较好，在RNN这种动态文本模型上使用的比较差。至于为啥NLP领域BN效果会差，Layer norm 效果会好，下一个文章会详细聊聊我的理解。\n列一下参考资料：\n模型优化之Batch Normalization - 大师兄的文章 - 知乎 https://zhuanlan.zhihu.com/p/54171297\n这个文章写的很好，推荐，从BN的特点（ICS/梯度饱和），训练，测试以及损失函数平滑都讲了一下。\n李宏毅- Batch Normalization  https://www.bilibili.com/video/av16540598/\n大佬的讲解视频，不解释，推荐\n各种Normalization - Mr.Y的文章 - 知乎 https://zhuanlan.zhihu.com/p/86765356\n这个文章关于BN在CNN中使用的讲解很好，推荐一下。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/BN踩坑记--谈一下Batch Normalization的优缺点和适用场景/"},{"title":"","date":"2024-06-20T11:38:12.971Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:38:12.971Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Dos');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Dos/"},{"title":"","date":"2024-06-21T03:48:15.825Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.825Z","content":"NLP任务中，layer-norm比BatchNorm好在哪里\n本文主要是讲一下，为什么NLP任务中，比如Transformer，使用LayerNorm而不是使用BatchNorm\n这个问题其实很有意思，理解的最核心的点在于：为什么LayerNorm单独对一个样本的所有单词做缩放可以起到效果。\n大家往下慢慢看，我说一下我自己的理解，欢迎大佬拍砖，如果觉得我说的还行，点个在看鼓励一下。\n为啥BN在NLP中效果差\n上一个文章有说 BN的使用场景，不适合 RNN这种动态文本模型，有一个原因是因为batch中的长度不一致，导致有的靠后面的特征的均值和方差不能估算。\n这个问题其实不是个大问题，可以缓解。我们可以在数据处理的时候，使句子长度相近的在一个batch，就可以了。所以这不是为啥NLP不用BN的核心原因。\n回忆一下上个文章中，BN在MLP中的应用。 BN是对每个特征在batch_size上求的均值和方差。记住，是每个特征。比如说身高，比如说体重等等。这些特征都有明确的含义。\n但是我们想象一下，如果BN应用到NLP任务中，对应的是对什么做处理？\n是对每一个单词！\n也就是说，我现在的每一个单词是对应到了MLP中的每一个特征。\n也就是默认了在同一个位置的单词对应的是同一种特征，比如:“我/爱/中国/共产党”和“今天/天气/真/不错”\n如何使用BN，代表着认为 &quot;我&quot;和“今天”是对应的同一个维度特征，这样才可以去做BN。\n大家想一下，这样做BN，会有效果吗？\n不会有效果的，每个单词表达的特征是不一样的，所以按照位置对单词特征进行缩放，是违背直觉的。\nlayner-norm 的特点\nlayner-norm 的特点是什么？layner-norm 做的是针对每一个样本，做特征的缩放。换句话讲，保留了N维度，在C/H/W维度上做缩放。\n也就是，它认为“我/爱/中国/共产党”这四个词在同一个特征之下，所以基于此而做归一化。\n这样做，和BN的区别在于，一句话中的每个单词都可以归到一个名字叫做“语义信息”的一个特征中（我自己瞎起的名字，大家懂就好），也就是说，layner-norm也是在对同一个特征下的元素做归一化，只不过这里不再是对应N（或者说batch size），而是对应的文本长度。\n上面这个解释，有一个细节点，就是，为什么每个单词都可以归到“语义信息”这个特征中。大家这么想，如果让你表达一个句子的语义信息，你怎么做？\n最简单的方法就是词语向量的加权求和来表示句子向量，这一点没问题吧。（当然你也可以自己基于自己的任务去训练语义向量，这里只是说最直觉的办法）\n上面这个方法就是出于每个单词都是语义信息的一部分这个insight。\n引申-为啥BN在CNN可以而在NLP不可以\n但是，我还想问一个问题，CNN中证明BN效果是很好的，NLP中的文本可以类比为图像，为什么BN在图像中效果好，在文本上效果差。\n我是这样理解的。还是回到刚才，BN是对单词做缩放，在NLP中，单词由词向量来表达，本质上是对词向量进行缩放。词向量是什么？是我们学习出来的参数来表示词语语义的参数，不是真实存在的。\n这就是NLP和图像的一个区别，图像的像素是真实存在的，像素中包含固有的信息。比如说，一张图像，最上面的一行像素，可以归为背景这个特征（这里只是为了理解，CNN做BN是基于整个feature map，而不是单独某一行像素）。\n这个理解不确保正确，只是我自己的理解（记得是从一个知乎答案看到的，改天好好找一找）\n简答说一下\n写到这里，我写文章不是为了推导公式，因为这种推导文章太多了，而是想让大家看了我的文章之后再去看这些推导公式能够更加容易理解。\n然后大家有问题的话，私信和我说，我也知道我自己写的哪里有问题，好改进。\n点个在看再走呗，老弟\n列一下参考资料：\n各种Normalization - Mr.Y的文章 - 知乎 https://zhuanlan.zhihu.com/p/86765356\n这个文章关于BN和LN如何应用讲解的比较好，就是CNHW\nNLP中 batch normalization与 layer normalization - 秩法策士的文章 - 知乎 https://zhuanlan.zhihu.com/p/74516930\n这个文章也还行，我在看的时候，看到中间那个图给了我点启发，就是在理解BN的时候，仅仅是在这个时候啊，我们的C，在CNN中是通道数，在理解BN的时候，理解为句子长度，这样”，每个样本通道数为 C，高为 H，宽为 W。对其求均值和方差时，将在 N、H、W上操作，而保留通道 C 的维度。具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道 …… 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值“这句话才比较好理解。\n一般NLP来说，C为1吧。\n模型优化之Layer Normalization - 大师兄的文章 - 知乎 https://zhuanlan.zhihu.com/p/54530247\n推荐一下这个文章，总结了对比实验：”这里我们设置了一组对照试验来对比普通网络，BN以及LN在MLP和RNN上的表现“，我还没细看，之后看。\ntransformer 为什么使用 layer normalization，而不是其他的归一化方法？ - pymars的回答 - 知乎 https://www.zhihu.com/question/395811291/answer/1260290120\n推荐这个答案，很好\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/NLP任务中-layer-norm比BatchNorm好在哪里/"},{"title":"","date":"2024-06-21T03:48:15.915Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.915Z","content":"Transformer的并行化\n正文\n本文主要谈一下关于 Transformer的并行化。文章比较短，适合大家碎片化阅读。\nDecoder不用多说，没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。\n对于Encoder侧：\n首先，6个大的模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。\n从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。\n当然对于注意力层在做attention的时候会依赖别的时刻的输入，不过这个只需要在计算之前就可以提供。\n然后注意力层和前馈神经层之间是串行，必须先完成注意力层计算再做前馈神经层。\n有点绕，不知道有没有讲清楚。\n简单讲，就是6个encoder之间是串行，每个encoder中的两个子模块之间是串行，子模块自身是可以并行的。\n系列总结\n整个Transformer这一块基本就是讲完了，基本上可以解决之前那个关于transformer面试题百分之八十的题目。\n至于剩下的题目会放在之后别的模块去讲，比如 wordpiece model 会在总结机器翻译知识点的时候写一下，然后 GPT 会在总结词向量知识点的时候写一下。\n写这个系列过程中，很多朋友也有私信我一些问题，交流过程中，对我自己帮助也很大，能回答的问题我都尽力回答了，也感谢大家的关注。平时工作挺忙的，尽量输出干货，也欢迎大家和我交流问题。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/Transformer的并行化/"},{"title":"","date":"2024-06-21T03:48:15.965Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.965Z","content":"Transformer面试题全部答案解析\n202007更新–如果对您没有帮助，你点个关闭页面就可以。如果您觉得需要这个东西，不需要您说谢谢，不过至少不太希望换来您的嘲讽。我之前在知乎上分享的是微信文章的界面，但是被知乎认定为含有垃圾广告营销，因为是个小白，不知道问题出在了哪里，改了好几次没改对被禁言了一天。后来想了这个折中的办法，跳转到了这个页面。\n公众号：NLP从入门到放弃\n大家去公众号后台回复 “答案解析” 四个字，获取对应的网盘链接。\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/Transformer面试题全部答案解析合辑/"},{"title":"","date":"2024-06-21T03:48:15.875Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.875Z","content":"\n\ntransformer/bert资源总结\n\n\n\n\n\nTransformer改进之相对位置编码(RPE)\n\n\n\n\n\n\n\n\n\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/transformer-bert资源总结/"},{"title":"","date":"2024-06-21T03:48:15.945Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.945Z","content":"transformer 资源总结\ntransformer中的positional encoding(位置编码)\nhttps://blog.csdn.net/Flying_sfeng/article/details/100996524\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/transformer资源总结/"},{"title":"","date":"2024-06-21T03:48:15.985Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:15.985Z","content":"VIT：如何将Transformer更好的应用到CV领域\n大家好，我是DASOU；\n最近因为在做TRM在多模态视频的分类，会写一些TRM在CV中的应用，今天先来讲一下VIT；\n论文名称是：AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n这个论文看下来，有这么几个重点需要去掌握：\n\n将整张图片转化为多个patches，作为trm的序列输入\n输入的时候需要加入位置编码，三种位置编码：一维，二维，相对位置编码没有太大区别\nTRM可以接受CNN的输出作为输入，作为一种TRM的混合结构，区别于VIT这种无卷积结构\n可能是由于缺乏inductive biases，数据集上直接训练的VIT效果一般，需要先在大数据及上做预训练然后在任务数据上做微调才可以达到不错的效果\nVIT的【CLS】可有可无\npatches重叠与否区别不是特别大；\n\n1. 简单背景介绍\n在CV领域，CNN一直是主流模型；\nTRM的最核心的一点就是自注意力机制，把这点借鉴到CV来说，一个最简单的想法就是我把每个像素当做是一个token，然后作为序列输入；\n那么就是对每个token之间都做了多头注意力机制；假设我们的图像大小是2242241，那么序列长度就是50176，相当于BERT最大长度的512的100倍左右，这个参数量肯定是不能承受的；\n针对这种情况，我们怎么处理呢？这个问题，本质上是去解决随着像素增加，复杂度平方级增长的问题；\n一个改进就是将全局的这种注意力机制改为局部的注意力机制，也就是做token周围几个领域tokens之间的注意力机制；\n还有一种改进是做稀疏注意力，是对注意力做了改进，本质在缓解TRM模型随着长度的增加，Attention部分所占用的内存和计算呈平方比增加的问题。\n这几种改进思路可行，但是实施复杂；\n所以一个比较简单的方法，就是将整个图像化整为零，从一整张图片转化为一个个的patch，也就是一个个的小方块；\n直接看下面这个图：\n\n其实在这里我想插一句，我之前在对图片元素做自注意力机制的时候，是对CNN提取图片的特征图，然后做attention；只不过，VIT这个模型在追求的一个特点就是完全抛弃掉卷积这个操作~~\n2. 具体细节\n2.1 模型架构图\n论文中自带的模型架构图已经足够清晰，我直接搬过来，然后一点点去讲一下：\n\n我们通过形状来了解一下数据的流动情况：\n首先我们有一张图片，形状为:$$HWC$$ ;其中H是高度，W是宽度，C是通道数量；\n然后我们把这个图片转化为一个个的pathes，其中每一个patch的形状是$$P^{2}*C$$ ; P是每个patch正方形的边长；\n然后可以将多个通道压扁，转化为一个一维形状：$$P^{2}.C$$ ;注意，这里就类似变成了一维数组；\n总共有N个patches；\n我们TRM的输入定为维度为$$D$$大小的token，那么我们就需要对每个一维数组$$P^{2}.C$$ 做一个linear映射到$$D$$大小；\n在TRM的输入中，处理token的embedding，其实还有一个是位置编码，VIT使用的就是简单的一维位置嵌入，映射到D维度就够了；这里论文提了一下，因为原始信息是图片，所以尝试了二维编码，但是没有明显提升；\nVIT学习BERT，在最开始加入了CLS符号；\n看到这点我其实疑惑了一下，BERT中加入CLS的一个原因是它预训练的时候使用了NSP，CLS的输出可以作为二分类的任务；但是图片这里显然没有第二张图片，所以加入CLS的解释就变成了想使用元素之间的注意力学习到所有tokens的信息；\n2.2 位置编码消融实验\n还有一点就是位置编码这块，作者做了消融，有四组实验，分别是：没有位置编码，一维位置编码，二维位置编码，以及相对位置编码；\n在位置编码这里，作者还做了另外三组实验，就是只在最开始输入的时候加入位置编码，在每一层加入位置编码同时各自学习，在每一层加入位置编码同时共享这个位置编码；\n实验结果看下面这个图：\n\n结论就是没看出太大的区别，直接用一维的位置编码，同时只是在最开始的时候加入位置编码就可以【从结果看每一层位置编码共享效果会更好一点】\n2.3 是否需要加入【CLS】token\nVIT模型为了最小限度的改变trm架构，依旧沿用了bert中的【CLS】中的这个token；但是就像我在最上面说到的，BERT中加入CLS的一个原因是它预训练的时候使用了NSP，CLS的输出可以作为二分类的任务；但是图片这里显然没有第二张图片，所以可以不加如【CLS】token；\n在整合图片信息的时候，两种方式，一种是使用【CLS】token，另一种就是对所有tokens的输出做一个平均，简称GAP；实验结果证明，两者可以达到的同样的效果，只不过要控制好学习率；\n结果看下面这个图：\n\n3. 预训练以及下游微调\n在上面谈到，VIT在小数据上效果一般，需要做一个预训练再来一个微调，效果还不错；\n这个其实不陌生，TRM在文本上也是，小数据不如lstm，bert在大量文本上预训练，学习到了大量的知识，才横扫了NLP；\n直接看图吧，绝大部分效果还是不错的：\n\n参考：&quot;未来&quot;的经典之作ViT：transformer is all you need! - 小小将的文章 - 知乎 https://zhuanlan.zhihu.com/p/356155277\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/VIT-如何将Transformer更好的应用到CV领域/"},{"title":"","date":"2024-06-21T03:48:16.005Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.005Z","content":"原版Transformer的位置编码究竟有没有包含相对位置信息。\n不涉及到公式推导，面试的时候能大致说出来就可以，很少会让推导，尽最大可能让大家明白\n简单概述\nTransformer 原版的位置编码也就是正余弦函数编码，表达的是绝对位置信息，同时包含相对位置信息。但是经过线性变化，相对位置信息消失。基于此，需要对位置编码进行优化。\n正文\n原版位置编码使用的是正余弦函数，通过三角函数，可以得出一个结论就是：$PE_{pos+k}$可以被$PE_{pos}$线性表示。\n从这一点来说，原版位置编码可以反应一定的相对位置信息。\n接下来，我们来看，经过注意力层，这个相对位置信息还在不在？\n很简单，把词向量和位置向量作为输入，经过注意力层，然后因式分解，得到四个部分，我们重点关注包含两个不同位置编码的公式部分，形式如下：\n$PE_{pos}^{T}W_{q}^{T}W_{k}PE_{pos+k} \\tag{1}$\n我们想要证明，这个公式能不能反应相对位置信息。\n为了解决这个问题，我们化繁为简，先从下面这个公式入手：\n$PE_{pos}^{T}PE_{pos+k} \\tag{2}$\n注意看公式(1)和公式(2)的区别，在中间多了两个矩阵相乘，这两个矩阵，是我们的Q/K矩阵，可以看做是一个线性变化，记住这个细节点。\n经过公式推导，我们很容易知道公式(2)最后的结果只和两个位置的相对位置 $k$ 相关，这个结果是包含相对位置信息。也就是说两个不同位置$PE$的点积的结果可以反映相对距离。\n通过实验我们知道这个结果大小随着相对距离的增大而减小，值得注意的是它并不能反映相对位置的方向，因为他是一个对称的。\n具体的我们可以看下面这个图：\n\n很好，接下来，我们就是证明本来可以反映相对位置信息的公式(2)，在加上中间这个线性变化之后，相对位置信息还在不在。\n直接看效果图：\n\n这个图需要重点看的下面两个，也就是加了线性变化之后，变化趋势从最上面蓝色图标的线变成了下面两条线，也就是趋势已经完全没有了。\n也就是说，实验结果显示，公式(1)的结果随着 k 的变化没有明显的趋势变化，也就是说相对位置信息消失了。\n上面这些内容，估计5分钟左右吧，本来想加上相对位置编码，不过内容也挺多的，下回再发吧。\n同学们，如果觉写的还行，给个在看。\n参考链接：\n一文读懂Transformer模型的位置编码\n这个文章写的不错，主要是给出来正余弦函数表达相对信息的公式推导\n浅谈Transformer模型中的位置表示\n哈工大的SCIR写的文章，不错，从正余弦函数位置信息和相对位置信息和transformerx-l都讲出来了\nTransformer改进之相对位置编码RPE\n这个文章很好，讲了位置编码的几种优化，值得好好看看推导一下公式\n如何优雅地编码文本中的位置信息？三种positioanl encoding方法简述\n夕小瑶的文章，讲了三种位置编码，还可以，没事的时候可以看看\n[相对位置编码一)Relative Position Representatitons RPR - Transformer\n大佬讲了一下相对位置编码，很好，推荐\n相对位置编码(二) Relative Positional Encodings - Transformer-XL\n大佬讲的transformer-xl，推荐\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/原版Transformer的位置编码究竟有没有包含相对位置信息/"},{"title":"","date":"2024-06-21T03:48:16.045Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.045Z","content":"史上最全Transformer面试题\n\nTransformer为何使用多头注意力机制？（为什么不使用一个头）\nTransformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？\n（注意和第一个问题的区别）\nTransformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n在计算attention score的时候如何对padding做mask操作？\n为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）\n大概讲一下Transformer的Encoder模块？\n为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？\n简单介绍一下Transformer的位置编码？有什么意义和优缺点？\n你还了解哪些关于位置编码的技术，各自的优缺点是什么？\n简单讲一下Transformer中的残差结构以及意义。\n为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？\n简答讲一下BatchNorm技术，以及它的优缺点。\n简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？\nEncoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）\nDecoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)\nTransformer的并行化提现在哪个地方？Decoder端可以做并行化吗？\n简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？\nTransformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？\n引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/史上最全Transformer面试题/"},{"title":"","date":"2024-06-21T03:48:16.085Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.085Z","content":"答案解析(1)—史上最全Transformer面试题：灵魂20问帮你彻底搞定Transformer\n简单介绍\n之前的20个问题的文章在这里：\nhttps://zhuanlan.zhihu.com/p/148656446\n其实这20个问题不是让大家背答案，而是为了帮助大家梳理 transformer的相关知识点，所以你注意看会发现我的问题也是有某种顺序的。\n本文涉及到的代码可以在这里找到：\nhttps://github.com/DA-southampton/NLP_ability\n问题基本上都可以在网上找到答案，所以大家可以先去搜一搜，自己理解一下，我也不会重新把答案回答一遍，而是列出来我看到的比较好的回答，然后加上点自己的注解帮助大家理解，在这里感谢那些大佬回答者，今天整理了其中的五个，剩下的我抽空在整理一下。\n这里我先小声说一下，写这些笔记有两个目的。\n一个是方便大家，好多题目都太散了，没有人归纳一下。\n二个就是方便自己重新复习一遍，所以我也不可能是直接把答案一粘就完事，这对我自己就没啥帮助了。所以没啥别的目的，不是为了博关注粉丝之类的，因为这些如果做不到大V基本没啥用，我也没那时间去经营成为大V，工作忙的要死，就是想要有个一起沟通的渠道而已。\n公众号/知乎/github基本同步更新，大家关注哪一个都可以，不过可能微信链接跳转不方便，知乎编辑不方便，github对有些同学不太方便打开。大家看自己情况关注吧。\n正文\n1.Transformer为何使用多头注意力机制？（为什么不使用一个头）\n答案解析参考这里：为什么Transformer 需要进行 Multi-head Attention？\nhttps://www.zhihu.com/question/341222779\n注解：简单回答就是，多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。其实本质上是论文原作者发现这样效果确实好，我把作者的实验图发在下面：\n\n2.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？\n答案解析参考这里：transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？ - 知乎\nhttps://www.zhihu.com/question/319339652\n注解：简单回答就是，使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。\n3.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n答案解析：为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的效果越显著。更具体的结果，大家可以看一下实验图(从莲子同学那里看到的，专门去看了一下论文)：\n\n4.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n答案解析参考这里：transformer中的attention为什么scaled? - LinT的回答 - 知乎\nhttps://www.zhihu.com/question/339723385/answer/782509914\n注解：针对大佬回答的第二个问题，也就是方差的问题，我简单的写了一个代码验证了一下，不愿意看公式推导的同学直接看代码结果就可以。代码如下:\n123456import numpy as np arr1=np.random.normal(size=(3,1000))arr2=np.random.normal(size=(3,1000))result=np.dot(arr1.T,arr2)arr_var=np.var(result)print(arr_var) #result: 2.9 (基本上就是3，和就是我们设定的维度)\n5.在计算attention score的时候如何对padding做mask操作？\n答案解析：padding位置置为负无穷(一般来说-1000就可以)。对于这一点，涉及到batch_size之类的，具体的大家可以看一下抱抱脸实现的源代码，位置在这里：\nhttps://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720\n这个是最新版，比较老版本的实现地址我也罗列一下，应该没啥区别，我没细看，一直用的老版本的：\nhttps://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/transformers/modeling_bert.py#L607\n参考链接：\n关于Transformer，面试官们都怎么问？\n写的很好，面试题总结的很好，把整体梳理了一遍。\n关于Transformer的若干问题整理记录 - Adherer的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/82391768\n关于Transformer的若干问题整理记录 - Adherer的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/82391768 和上面是一个文章，在知乎\nTransformer的细节与技巧 - 沧海一栗的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/69697467\n讲了几个代码上的小细节\nNLP预训练模型：从transformer到albert - Serendipity的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/85221503\n大佬主要是大白话讲了一下代码的实现，包括维度的变化\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/答案解析—史上最全Transformer面试题：灵魂20问帮你彻底搞定Transformer/"},{"title":"","date":"2024-06-21T03:48:16.135Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.135Z","content":"谈一谈Decoder模块\n本文主要是谈一些比较容易误解的细节点，说实话，把自己的理解用文字表达出来真是个细致活。\n如果觉得对您有点帮助，帮忙点个在看或者赞。\n一个小小的问题\n我先说一个自己花了点时间才琢磨出来的东西，其实不难，就是当时没转过弯来。\n我们都知道，decoder的交互层，Q矩阵来自本身，K/V矩阵来自整个Encoder层输出。\n但是对于每个单词都会有一个encoder的输出，那么K/V矩阵是用的其中哪个输出计算过来的？\n我这个问题的问法其实是错误的。\n我当时的理解背景是认为这个交互的过程很类似seq2seq的attention，它一般是使用最后一个时刻的隐层输出作为context vector。\n我基于此产生了上面这个问题，这个K/V矩阵应该由哪个位置单词（对比RNN就是哪个时刻的单词）的输出生成。\n后来看了一下代码，才明白自己错在哪里？\nK/V矩阵的计算不是来自于某一个单词的输出，而是所有单词的输出汇总计算K/V矩阵。这个过程和在Encoder中计算K/V矩阵是一样的，只不过放在了交互层，一时没想明白。\n正文\n与Encoder很类似，Decoder同样由完全相同的N个大模块堆叠而成，原论文中N为6。\n每个大的模块分为三部分：多头注意力层，交互层，前馈神经层；每个层内部尾端都含有 Add&amp;Norm。\n和Encoder重复的内容我就跳过了，之前讲过，没看过的同学可以去看那个文章。\n多头自注意力层\n首先谈一下多头自注意力层，这里需要注意的细节点是，需要对当前单词和之后的单词做mask。\n为什么需要mask？\n最常规的解释就是在预测阶段，你的模型看不见当前时刻的输出以及未来时刻单词。\n这句话其实有点绕，如果读的不仔细会让人误解为mask的时候需要把当前时刻的单词也mask掉…(拖出去斩了吧)。\n从代码角度讲，你只需要把当前时刻之后所有单词mask掉就好了。\n我自己对这句话的理解是我们需要确保模型在训练和测试的时候没有GAP。\n举个简单的例子来理解，如果做机器翻译，你需要翻译出来的句子是 “我/爱/吃/苹果”。\n当前时刻是”爱“这个单词作为输入的一部分，另一部分是上一个时刻”我“作为输入的时候的输出值。\n当然在机器翻译中，我们一般使用 teacher forcing加速收敛，所以这里就使用”我“作为当前时刻输入的另一个部分。\n所以这个时候，输入就是”我“的编码信息和”爱“的编码信息（当然还有位置编码）。\n我要预测的是”吃“这个单词。\n如果我们没有mask，模型也是可以运行的，也就说此时”吃“和”苹果“两个词对”爱“这个时刻的输出是有贡献的。\n那么问题来了，测试数据中你根本没有ground truth，你怎么办？\n也就说，训练的时候，你的模型是基于知道这个时刻后面的单词进行的训练，但是测试的时候，做机器翻译，你不知道自己应该翻译出来什么东西。\n这就是问题的核心。\n你训练模型的时候，一部分精力花在了”吃“和”苹果“两个词上，这不就是无用功吗？\n所以，确保模型在训练和测试的时候没有GAP，我们需要mask掉”吃“和”苹果“两个词。\n交互模块\n这一块需要注意的就是之前文章提到的，Q矩阵来自本身，K/V矩阵来自encoder的输出。\n还有一个细节点是，K/V矩阵对应的是来自整个encoder的输出。\n如果看transformer那个经典图的话，初期很容易理解为encoder和decode对应的每一层互相交互，这是不对的。\n是整个输出与decoder做交互。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/谈一谈Decoder模块/"},{"title":"","date":"2024-06-21T03:48:16.105Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.105Z","content":"谈一下相对位置编码RPR\n经过线性变化之后，正余弦函数表示的相对位置信息消失，所以需要优化。\n一般来讲，谈到优化，三种比较有名：RPR； Transformer-XL；complex embeddings；\n我在这个文章简单讲一下RPR。\n老样子，不涉及到公式推导，尽量把我的理解讲出来。\nRPR思路\nRPR思路很简单，原始正余弦函数，是在输入的的时候与词向量相加作为输入，在attention丢失相对位置信息.\n改进的话就是不在输入的时候进行位置编码，而是在attention中显示把相对位置信息加入进去。\n如何理解相对位置\n绝对位置编码是在每个位置都对应一个唯一的位置编码信息，RPR把这一部分去掉去学习一个相对位置编码。\n首先我们需要知道相对位置是有方向的的。\n举个例子：”我/爱/中国/共产党“\n”我“对”爱“的相对位置就是 -1， ”中国“对”爱“的相对位置就是 1。\n所以方向不同，对应两个不同的相对位置，在学习的时候，一个距离，也就需要学习两个相对位置编码。\nRPR修改思想\n作者认为在相对位置小于4的时候，attention对相对位置比较敏感，在大于4之后，相对位置不敏感。所以窗口设置为4。\n需要注意的是，窗口设为4，代表的当前位置左边4个，右边也有4个，再加上自己，就是一共9个位置，也就是：\n$[i-4,i-3,i-2,i-1,i,i+1,i+2,i+3,i+4]$\n注解：有方向\n当你的attention进行到哪个单词的时候，你的 $i$ 就对应的是哪个位置。\n还是上面那句话举例子。\n如果此时的输入是“我”，那么用到的相对位置编码就是$[i,i+1,i+2,i+3]$\n如果此时输入的是“爱”，那么这个时候用到的相对位置编码就是$[i-1,i,i+1,i+2]$\n了解了这个，我们再谈一下这个相对位置信息是怎么显示加入进去的。\n这个显示的加入分为两个部分。\n第一个部分是在计算$e_{ij}$的时候，涉及到RPR的一个表征:$a_{ij}^{K}$，表示对 Q/K/V三者中的K做了修改。\n第二个部分就是在计算$z_{i}$的时候，涉及到另一个RPR的表征:$a_{ij}^{V}$，表示对Q/K/V三者中的V做了修改。\n两个部分的修改都是使用加法。\n关于RPR大概就讲这么多吧。其实思路还是比较简单的，总结来说，就是把相对位置信息在attention之中，显势的加入进去，而不是在输入的时候与词向量相加。\n如果觉得对您有点帮助，点个赞再走吧。\n参考资料\ndasou\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/谈一下相对位置编码/"},{"title":"","date":"2024-06-21T03:48:16.205Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.205Z","content":"关键词提取\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/关键词提取/README/"},{"title":"","date":"2024-06-21T03:48:16.295Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.295Z","content":"关键词的提取，也可以称之为文本标签提取。\n比如说，”今天这顿烧烤是真不错啊“，在这句话中，”烧烤“这个词就可以被认为是一个关键词，或者说这个句子的一个标签。\n这个标签在一定程度上能够表现出这个句子的含义，比如这个”烧烤“，如果用在文本分类任务中，可以隐含带有”美食“这个类别的信息。\n这些标签有些时候也可以用在推荐系统的召回，比如直接按照”烧烤“这个标签做一路召回。\n对于关键词的提取一般来说分为抽取式和生成式。其实类比到摘要，其实也是分为抽取式和生成式。\n生成式有一个缺点就是有些结果不可控，这其实还挺要命的。\n对于抽取式，就是从现有的数据中拿出来词组。最差的结果也就是拿出的单词并不重要，不是我们想要的。\n我们的重点是在抽取式提取关键词。\n关键词的提取可以分为两个步骤：召回+排序\n1.召回\n召回就是得到文本中的候选关键词，也就是得到这个句子中有可能是关键词的词汇。\n这一步，可以做的方法有很多，比如\n我们有积累的关键词词库，在这里直接匹配出来。\n一些符合的词性的候选词，比如我挑选出名词作为候选词\n还可以基于一些统计特征提出候选词，比如TF-IDF（有些时候统计特征也会用在排序中作为特征）\n基于一些规则，比如一个句子出现了人名地名，书名号中词，这些很有可能就是关键词\n召回其实是一个很重要的部分，在这一步骤，尽可能的召回有用的词汇。我自己的标准是宁可多不能少。如果多了，无非就是增加了资源消耗，但是少了，可能在排序阶段就是无米之炊了。\n2.排序\n排序阶段，我们可以将方法大致的分为有监督和无监督的方法\n2.1无监督抽取关键词\n对于无监督，我们分为基于统计和基于图。基于统计就是TF-IDF和各种变种。基于图最常见的就是TextRank。\n关键词提取的一个baseline就是 TF-IDF 提取，这种方法效果已经很好。投入产出比很高，我们一般需要去掉常用的停用词，保留重要的词语。\nTF-IDF基于统计，易于实现，但是缺点就是没有考虑词与词，词与文档之间的关系。是割裂的。\n另一个baseline就是基于图的TextRank, TextRank 由 PageRank 演变而来。\n相比于TF-IDF，TextRank考虑了词与词之间的关系（提取思想就是从窗口之间的词汇关系而来），但是缺点是它针对的是单个文本，而不是整个语料，在词汇量比较少的文本中，也就是短文中，效果会比较差。\n随着数据量的积累，我们需要把模型更换到有监督模型加上。一般来说，有监督分为两种，一种是看做序列标注，一种是看做二分类的问题。\n2.2有监督之二分类\n先说二分类问题，比较简单，就是找到词汇的各种特征，去判断这个词汇是不是这个文本的关键词。\n我大概罗列一些可能会用到的特征。\n位置特征：\n使用位置特征是我们基于文本关键词出现的位置是在大量数据的情况下是有规律可言的，比如微博文本中出现在##符号中部分词汇有很大概率就是文本的一个关键词。\n是否出现在开头，是否出现在中间部分，是否出现在末尾，出现的位置（具体是第几个单词）；相对于整个文本的位置；是否出现在##符号中…\n统计特征：\n共现矩阵信息；词频；逆词频；词性；词跨度；关键词所在句子的最大长度/最小长度/平均长度;\n向量特征：\n关键词词向量和文档向量的相似性\n2.3有监督之序列标注\n关键词的提取，就是一个典型的序列标注的问题。判断句子中关键词的开头中间结尾的位置。\n序列标注最基础的就是HMM和CRF方法，但是特征工程比较复杂。\n为了解决特征工程复杂的问题，我们使用深度学习模型序列标注。\n关于序列标注，大家可以参考我这个文章内容：\n工业级命名体识别经验+代码总结\n3.新词发现\n还会出现一个问题，如果我们使用二分类判定关键词，上述的过程我们都是基于我们的分词器来做的。有可能会出现一些新词，由于分词错误，不能及时的出现在你的候选词库中，比如”爷青结“。\n这个时候，我们需要一个新词发现系统，持续不断的补充到词库中，在召回阶段可以提升召回率。\n对于新词发现来说，基操就是从文本的自由程度和凝固程度来判断是否是新词，这样的问题就是阈值不好调整从而导致召回和精准不好平衡。\n我们还可以通过别的方法离线挖掘实体词补充道词库中，之前有借鉴美团ner的文章实现了一下，效果还不错，在这里，大家可以参考我这个文章：实体库构建：离线大规模新词实体挖掘\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/关键词提取/关键词提取方法综述/"},{"title":"","date":"2024-06-21T03:48:16.335Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.335Z","content":"关键词提取资源总结\nNLP关键词提取方法总结及实现 https://blog.csdn.net/asialee_bird/article/details/96454544\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/关键词提取/关键词提取资源总结/"},{"title":"","date":"2024-06-21T03:48:16.065Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.065Z","content":"1. 史上最全Transformer面试题\n\nTransformer为何使用多头注意力机制？（为什么不使用一个头）\nTransformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？\n（注意和第一个问题的区别）\nTransformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n在计算attention score的时候如何对padding做mask操作？\n为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）\n大概讲一下Transformer的Encoder模块？\n为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？\n简单介绍一下Transformer的位置编码？有什么意义和优缺点？\n你还了解哪些关于位置编码的技术，各自的优缺点是什么？\n简单讲一下Transformer中的残差结构以及意义。\n为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？\n简答讲一下BatchNorm技术，以及它的优缺点。\n简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？\nEncoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）\nDecoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)\nTransformer的并行化提现在哪个地方？Decoder端可以做并行化吗？\n简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？\nTransformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？\n引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？\n\n\n2. 3分钟从零解读Transformer的Encoder\nTransformer 分为两个部分，encoder 侧 和 decoder 侧。今天，我们聊一下 encoder 侧。这部分由 N 个完全相同的大模块堆叠而成（原论文N=6）。\n这个结构怎么理解？这个构造就需要我们确保每一个模块的输入和输出维度是相同的，在实现代码的时候，我们只需要完成一个模块的代码的构造就可以。\n注解：你可以把这个过程想象成 RNN 竖过来的一个流程，是不是就很好理解（当然这样想只是帮助你理解）。\n其次对于这每一个大的模块，又分为两个模块，分别是多头注意力层和前馈神经网络层。进一步拆分，多头注意力层可以分为注意力层和 Add&amp;Norm 层。前馈神经网络可以分为 Linear 层和 Add&amp;Norm 层。\n多头注意力层，核心点在于 Q/K/V 三个矩阵，其中 Q/K 矩阵生成权重矩阵(经由softmax)，随后和V矩阵得到加权和。\n这个过程重复了 n_heads 次，这个 n_heads 代表的就是头的数目，这里需要注意的是我们需要确保 hidden_size/n_heads 需要为一个整数，不然代码会报错。\nAdd 代表一个残差结构。对于残差结构，可以使得信息前后向传播更加顺畅，缓解了梯度破碎问题。在 NLP 角度来看，残差结构一定程度上促进了 NLP 网络结构向窄而深的方向发展。\n我们可以把 Transformer 和之前的模型对比一下，比如 RNN 模型，一般来说，我们会选择 单层RNN 或者 一个 Bilstm，对于这些比较传统的模型，只是在时间长度上进行了延展，并没有在深度上做的太深。\n所以说，残差结构是有助于网路变深的。\n顺便联想一下 Elmo，使用的是 双层双向lstm，训练起来已经非常慢了，所以对于RNN这种比较传统的模型，做深太难了，GNMT也是用了很多的 tricks 进行加速训练。\nNorm 代表的是 Layer Normalization。为什么这里使用 Layer Normalization，而不是BN，这个后面有文章说，这里直白的回答就是，BN的效果差，所以不用。\n随后多头注意力层的输出经过前馈神经网络。对前馈神经网络，比较简单，我们需要注意的是它分为两个 Linear 层，第一层的激活函数为 Relu，第二层没有使用激活函数。\n最后我们谈一下整个encoder的输入和输出。\n先说输入，分为两个部分：word embedding 和 position encoding\nword embedding 没什么可说的，初始化后跟着训练或者使用word2vec这种已经有的看具体任务的效果。\nposition encoding 这里 transformer 使用的是 正余弦函数进行表达。其实这里进行初始化然后进行训练也是可以的，论文原作者的实验表明效果基本没区别。\n对于 position encoding 表示的绝对位置，这点大家都没异议，那么 position encoding 究竟有没有表达相对位置信息，之后会有个文章专门讲讲这个知识点。\n然后说一下 encoder的输出，感觉很少有人谈到这里。\nencoder 的输出需要注意的细节点在于它需要和 decoder做交互，所以它的输出为 K/V 矩阵，记住这个细节点，Q 矩阵来自decoder模块，K/V矩阵来自encoder。\n写到这里，我估摸这三分钟差不多能看完，现在没有留言功能，有问题大家在公众号对话框发送，我后台能看见。\n能点个在看，老铁们 ！！鞠躬感谢！！\n\n3. 原版Transformer的位置编码究竟有没有包含相对位置信息\n简单概述\nTransformer 原版的位置编码也就是正余弦函数编码，表达的是绝对位置信息，同时包含相对位置信息。但是经过线性变化，相对位置信息消失。基于此，需要对位置编码进行优化。\n正文\n原版位置编码使用的是正余弦函数，通过三角函数，可以得出一个结论就是：$PE_{pos+k}$可以被$PE_{pos}$线性表示。\n从这一点来说，原版位置编码可以反应一定的相对位置信息。\n接下来，我们来看，经过注意力层，这个相对位置信息还在不在？\n很简单，把词向量和位置向量作为输入，经过注意力层，然后因式分解，得到四个部分，我们重点关注包含两个不同位置编码的公式部分，形式如下：\n$PE_{pos}^{T}W_{q}^{T}W_{k}PE_{pos+k} \\tag{1}$\n我们想要证明，这个公式能不能反应相对位置信息。\n为了解决这个问题，我们化繁为简，先从下面这个公式入手：\n$PE_{pos}^{T}PE_{pos+k} \\tag{2}$\n注意看公式(1)和公式(2)的区别，在中间多了两个矩阵相乘，这两个矩阵，是我们的Q/K矩阵，可以看做是一个线性变化，记住这个细节点。\n经过公式推导，我们很容易知道公式(2)最后的结果只和两个位置的相对位置 $k$ 相关，这个结果是包含相对位置信息。也就是说两个不同位置$PE$的点积的结果可以反映相对距离。\n通过实验我们知道这个结果大小随着相对距离的增大而减小，值得注意的是它并不能反映相对位置的方向，因为他是一个对称的。\n具体的我们可以看下面这个图：\n![rela_posi](/Users/zida/Documents/GitHub/NLP_ability/深度学习自然语言处理/T](/links/NLP_ability/深度学习自然语言处理/Transformer/images/rela_posi.png)\n很好，接下来，我们就是证明本来可以反映相对位置信息的公式(2)，在加上中间这个线性变化之后，相对位置信息还在不在。\n直接看效果图：\n![rela_po_none](/Users/zida/Documents/GitHub/NLP_ability/深度学习自然语言处理/T](/links/NLP_ability/深度学习自然语言处理/Transformer/images/rela_po_none.png)\n这个图需要重点看的下面两个，也就是加了线性变化之后，变化趋势从最上面蓝色图标的线变成了下面两条线，也就是趋势已经完全没有了。\n也就是说，实验结果显示，公式(1)的结果随着 k 的变化没有明显的趋势变化，也就是说相对位置信息消失了。\n上面这些内容，估计5分钟左右吧，本来想加上相对位置编码，不过内容也挺多的，下回再发吧。\n同学们，如果觉写的还行，给个在看。\n\n4. 谈一下相对位置编码\n经过线性变化之后，正余弦函数表示的相对位置信息消失，所以需要优化。\n一般来讲，谈到优化，三种比较有名：RPR； Transformer-XL；complex embeddings；\n我在这个文章简单讲一下RPR。\n老样子，不涉及到公式推导，尽量把我的理解讲出来。\nRPR思路\nRPR思路很简单，原始正余弦函数，是在输入的的时候与词向量相加作为输入，在attention丢失相对位置信息，改进的话就是不在输入的时候进行位置编码，而是在attention中显示把相对位置信息加入进去。\n如何理解相对位置\n绝对位置编码是在每个位置都对应一个唯一的位置编码信息，RPR把这一部分去掉去学习一个相对位置编码。\n首先我们需要知道相对位置是有方向的的。\n举个例子：”我/爱/中国/共产党“\n”我“对”爱“的相对位置就是 -1， ”中国“对”爱“的相对位置就是 1。\nRPR修改思想\n作者认为在相对位置小于4的时候，attention对相对位置比较敏感，在大于4之后，相对位置不敏感。所以窗口设置为4。\n需要注意的是，窗口设为4，代表的当前位置左边4个，右边也有4个，再加上自己，就是一共9个位置，也就是：\n$[i-4,i-3,i-2,i-1,i,i+1,i+2,i+3,i+4]$\n当你的attention进行到哪个单词的时候，你的 $i$ 就对应的是哪个位置。\n还是上面那句话举例子。\n如果此时的输入是“我”，那么用到的相对位置编码就是$[i,i+1,i+2,i+3]$\n如果此时输入的是“爱”，那么这个时候用到的相对位置编码就是$[i-1,i,i+1,i+2]$\n了解了这个，我们再谈一下这个相对位置信息是怎么显示加入进去的。\n这个显示的加入分为两个部分。\n第一个部分是在计算$e_{ij}$的时候，涉及到RPR的一个表征:$a_{ij}^{K}$，表示对 Q/K/V三者中的K做了修改。\n第二个部分就是在计算$z_{i}$的时候，涉及到另一个RPR的表征:$a_{ij}^{V}$，表示对Q/K/V三者中的V做了修改。\n两个部分的修改都是使用加法。\n\n5. BN踩坑记–谈一下Batch Normalization的优缺点和适用场景\n这个问题没有定论，很多人都在探索，所以只是聊一下我自己的理解，顺便为讲 layer-norm做个引子。\nBN的理解重点在于它是针对整个Batch中的样本在同一维度特征在做处理。\n在MLP中，比如我们有10行5列数据。5列代表特征，10行代表10个样本。是对第一个特征这一列（对应10个样本）做一次处理，第二个特征（同样是一列）做一次处理，依次类推。\n在CNN中扩展，我们的数据是N·C·H·W。其中N为样本数量也就是batch_size，C为通道数，H为高，W为宽，BN保留C通道数，在N,H,W上做操作。比如说把第一个样本的第一个通道的数据，第二个样本第一个通道的数据…第N个样本第一个通道的数据作为原始数据，处理得到相应的均值和方差。\nBN有两个优点。\n第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使损失平面更加的平滑，从而加快的收敛速度。\n第二个优点就是缓解了梯度饱和问题（如果使用sigmoid激活函数的话），加快收敛。\nBN的缺点：\n第一个，batch_size较小的时候，效果差。这一点很容易理解。BN的过程，使用 整个batch中样本的均值和方差来模拟全部数据的均值和方差，在batch_size 较小的时候，效果肯定不好。\n第二个缺点就是 BN 在RNN中效果比较差。这一点和第一点原因很类似，不过我单挑出来说。\n首先我们要意识到一点，就是RNN的输入是长度是动态的，就是说每个样本的长度是不一样的。\n举个最简单的例子，比如 batch_size 为10，也就是我有10个样本，其中9个样本长度为5，第10个样本长度为20。\n那么问题来了，前五个单词的均值和方差都可以在这个batch中求出来从而模型真实均值和方差。但是第6个单词到底20个单词怎么办？\n只用这一个样本进行模型的话，不就是回到了第一点，batch太小，导致效果很差。\n第三个缺点就是在测试阶段的问题，分三部分说。\n首先测试的时候，我们可以在队列里拉一个batch进去进行计算，但是也有情况是来一个必须尽快出来一个，也就是batch为1，这个时候均值和方差怎么办？\n这个一般是在训练的时候就把均值和方差保存下来，测试的时候直接用就可以。那么选取效果好的均值和方差就是个问题。\n其次在测试的时候，遇到一个样本长度为1000的样本，在训练的时候最大长度为600，那么后面400个单词的均值和方差在训练数据没碰到过，这个时候怎么办？\n这个问题我们一般是在数据处理的时候就会做截断。\n还有一个问题就是就是训练集和测试集的均值和方差相差比较大，那么训练集的均值和方差就不能很好的反应你测试数据特性，效果就回差。这个时候就和你的数据处理有关系了。\nBN使用场景\n对于使用场景来说，BN在MLP和CNN上使用的效果都比较好，在RNN这种动态文本模型上使用的比较差。至于为啥NLP领域BN效果会差，Layer norm 效果会好，下一个文章会详细聊聊我的理解。\n\n6. NLP任务中-layer-norm比BatchNorm好在哪里\n本文主要是讲一下，为什么NLP任务中，比如Transformer，使用LayerNorm而不是使用BatchNorm\n这个问题其实很有意思，理解的最核心的点在于：为什么LayerNorm单独对一个样本的所有单词做缩放可以起到效果。\n大家往下慢慢看，我说一下我自己的理解，欢迎大佬拍砖，如果觉得我说的还行，点个在看鼓励一下。\n为啥BN在NLP中效果差\n上一个文章有说 BN的使用场景，不适合 RNN这种动态文本模型，有一个原因是因为batch中的长度不一致，导致有的靠后面的特征的均值和方差不能估算。\n这个问题其实不是个大问题，可以缓解。我们可以在数据处理的时候，使句子长度相近的在一个batch，就可以了。所以这不是为啥NLP不用BN的核心原因。\n回忆一下上个文章中，BN在MLP中的应用。 BN是对每个特征在batch_size上求的均值和方差。记住，是每个特征。比如说身高，比如说体重等等。这些特征都有明确的含义。\n但是我们想象一下，如果BN应用到NLP任务中，对应的是对什么做处理？\n是对每一个单词！\n也就是说，我现在的每一个单词是对应到了MLP中的每一个特征。\n也就是默认了在同一个位置的单词对应的是同一种特征，比如:“我/爱/中国/共产党”和“今天/天气/真/不错”\n如何使用BN，代表着认为 &quot;我&quot;和“今天”是对应的同一个维度特征，这样才可以去做BN。\n大家想一下，这样做BN，会有效果吗？\n不会有效果的，每个单词表达的特征是不一样的，所以按照位置对单词特征进行缩放，是违背直觉的。\nlayner-norm 的特点\nlayner-norm 的特点是什么？layner-norm 做的是针对每一个样本，做特征的缩放。换句话讲，保留了N维度，在C/H/W维度上做缩放。\n也就是，它认为“我/爱/中国/共产党”这四个词在同一个特征之下，所以基于此而做归一化。\n这样做，和BN的区别在于，一句话中的每个单词都可以归到一个名字叫做“语义信息”的一个特征中（我自己瞎起的名字，大家懂就好），也就是说，layner-norm也是在对同一个特征下的元素做归一化，只不过这里不再是对应N（或者说batch size），而是对应的文本长度。\n上面这个解释，有一个细节点，就是，为什么每个单词都可以归到“语义信息”这个特征中。大家这么想，如果让你表达一个句子的语义信息，你怎么做？\n最简单的方法就是词语向量的加权求和来表示句子向量，这一点没问题吧。（当然你也可以自己基于自己的任务去训练语义向量，这里只是说最直觉的办法）\n上面这个方法就是出于每个单词都是语义信息的一部分这个insight。\n引申-为啥BN在CNN可以而在NLP不可以\n但是，我还想问一个问题，CNN中证明BN效果是很好的，NLP中的文本可以类比为图像，为什么BN在图像中效果好，在文本上效果差。\n我是这样理解的。还是回到刚才，BN是对单词做缩放，在NLP中，单词由词向量来表达，本质上是对词向量进行缩放。词向量是什么？是我们学习出来的参数来表示词语语义的参数，不是真实存在的。\n这就是NLP和图像的一个区别，图像的像素是真实存在的，像素中包含固有的信息。比如说，一张图像，最上面的一行像素，可以归为背景这个特征（这里只是为了理解，CNN做BN是基于整个feature map，而不是单独某一行像素）。\n这个理解不确保正确，只是我自己的理解（记得是从一个知乎答案看到的，改天好好找一找）\n简答说一下\n写到这里，我写文章不是为了推导公式，因为这种推导文章太多了，而是想让大家看了我的文章之后再去看这些推导公式能够更加容易理解。\n然后大家有问题的话，私信和我说，我也知道我自己写的哪里有问题，好改进。\n点个在看再走呗，老弟\n\n7. 谈一谈Decoder模块\n本文主要是谈一些比较容易误解的细节点，说实话，把自己的理解用文字表达出来真是个细致活。\n如果觉得对您有点帮助，帮忙点个在看或者赞。\n一个小小的问题\n我先说一个自己花了点时间才琢磨出来的东西，其实不难，就是当时没转过弯来。\n我们都知道，decoder的交互层，Q矩阵来自本身，K/V矩阵来自整个Encoder层输出。\n但是对于每个单词都会有一个encoder的输出，那么K/V矩阵是用的其中哪个输出计算过来的？\n我这个问题的问法其实是错误的。\n我当时的理解背景是认为这个交互的过程很类似seq2seq的attention，它一般是使用最后一个时刻的隐层输出作为context vector。\n我基于此产生了上面这个问题，这个K/V矩阵应该由哪个位置单词（对比RNN就是哪个时刻的单词）的输出生成。\n后来看了一下代码，才明白自己错在哪里？\nK/V矩阵的计算不是来自于某一个单词的输出，而是所有单词的输出汇总计算K/V矩阵。这个过程和在Encoder中计算K/V矩阵是一样的，只不过放在了交互层，一时没想明白。\n正文\n与Encoder很类似，Decoder同样由完全相同的N个大模块堆叠而成，原论文中N为6。\n每个大的模块分为三部分：多头注意力层，交互层，前馈神经层；每个层内部尾端都含有 Add&amp;Norm。\n和Encoder重复的内容我就跳过了，之前讲过，没看过的同学可以去看那个文章。\n多头自注意力层\n首先谈一下多头自注意力层，这里需要注意的细节点是，需要对当前单词和之后的单词做mask。\n为什么需要mask？\n最常规的解释就是在预测阶段，你的模型看不见当前时刻的输出以及未来时刻单词。\n这句话其实有点绕，如果读的不仔细会让人误解为mask的时候需要把当前时刻的单词也mask掉…(拖出去斩了吧)。\n从代码角度讲，你只需要把当前时刻之后所有单词mask掉就好了。\n我自己对这句话的理解是我们需要确保模型在训练和测试的时候没有GAP。\n举个简单的例子来理解，如果做机器翻译，你需要翻译出来的句子是 “我/爱/吃/苹果”。\n当前时刻是”爱“这个单词作为输入的一部分，另一部分是上一个时刻”我“作为输入的时候的输出值。\n当然在机器翻译中，我们一般使用 teacher forcing加速收敛，所以这里就使用”我“作为当前时刻输入的另一个部分。\n所以这个时候，输入就是”我“的编码信息和”爱“的编码信息（当然还有位置编码）。\n我要预测的是”吃“这个单词。\n如果我们没有mask，模型也是可以运行的，也就说此时”吃“和”苹果“两个词对”爱“这个时刻的输出是有贡献的。\n那么问题来了，测试数据中你根本没有ground truth，你怎么办？\n也就说，训练的时候，你的模型是基于知道这个时刻后面的单词进行的训练，但是测试的时候，做机器翻译，你不知道自己应该翻译出来什么东西。\n这就是问题的核心。\n你训练模型的时候，一部分精力花在了”吃“和”苹果“两个词上，这不就是无用功吗？\n所以，确保模型在训练和测试的时候没有GAP，我们需要mask掉”吃“和”苹果“两个词。\n交互模块\n这一块需要注意的就是之前文章提到的，Q矩阵来自本身，K/V矩阵来自encoder的输出。\n还有一个细节点是，K/V矩阵对应的是来自整个encoder的输出。\n如果看transformer那个经典图的话，初期很容易理解为encoder和decode对应的每一层互相交互，这是不对的。\n是整个输出与decoder做交互。\n\n8. Transformer的并行化\n本文主要谈一下关于 Transformer的并行化。文章比较短，适合大家碎片化阅读。\nDecoder不用多说，没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。\n对于Encoder侧：\n首先，6个大的模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。\n从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。\n当然对于注意力层在做attention的时候会依赖别的时刻的输入，不过这个只需要在计算之前就可以提供。\n然后注意力层和前馈神经层之间是串行，必须先完成注意力层计算再做前馈神经层。\n有点绕，不知道有没有讲清楚。\n简单讲，就是6个encoder之间是串行，每个encoder中的两个子模块之间是串行，子模块自身是可以并行的。\n\n9. 谈一下 Transformer为何使用多头注意力机制？\n答案解析参考这里：为什么Transformer 需要进行 Multi-head Attention？\nhttps://www.zhihu.com/question/341222779\n注解：简单回答就是，多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。其实本质上是论文原作者发现这样效果确实好，我把作者的实验图发在下面：\n![attention_head](/Users/zida/Documents/GitHub/NLP_ability/深度学习自然语言处理/T](/links/NLP_ability/深度学习自然语言处理/Transformer/images/attention_heads.png)\n\n10. Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？\n答案解析参考这里：transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？ - 知乎\nhttps://www.zhihu.com/question/319339652\n注解：简单回答就是，使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。\n\n11. Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n答案解析：为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的效果越显著。更具体的结果，大家可以看一下实验图(从莲子同学那里看到的，专门去看了一下论文)：\n![attention_methods](/Users/zida/Documents/GitHub/NLP_ability/深度学习自然语言处理/T](/links/NLP_ability/深度学习自然语言处理/Transformer/images/attention_methods.png)\n\n12. 为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n答案解析参考这里：transformer中的attention为什么scaled? - LinT的回答 - 知乎\nhttps://www.zhihu.com/question/339723385/answer/782509914\n注解：针对大佬回答的第二个问题，也就是方差的问题，我简单的写了一个代码验证了一下，不愿意看公式推导的同学直接看代码结果就可以。代码如下:\n123456import numpy as np arr1=np.random.normal(size=(3,1000))arr2=np.random.normal(size=(3,1000))result=np.dot(arr1.T,arr2)arr_var=np.var(result)print(arr_var) #result: 2.9 (基本上就是3，和就是我们设定的维度)\n\n13. 计算attention score的时候如何对padding做mask操作？\n答案解析：padding位置置为负无穷(一般来说-1000就可以)。对于这一点，涉及到batch_size之类的，具体的大家可以看一下抱抱脸实现的源代码，点击在这里\n这个是最新版，比较老版本的实现地址我也罗列一下，应该没啥区别，我没细看，一直用的老版本的，点击这里\n\n系列总结\n整个Transformer这一块基本就是讲完了，基本上可以解决之前那个关于transformer面试题百分之八十的题目。\n至于剩下的题目会放在之后别的模块去讲，比如 wordpiece model 会在总结机器翻译知识点的时候写一下，然后 GPT 会在总结词向量知识点的时候写一下。\n欢迎大家关注微信公众号: NLP从入门到放弃\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/答案合辑/"},{"title":"","date":"2024-06-21T03:48:16.505Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.505Z","content":"\n大家最重要的可以看一下第五个问题，一个读者朋友根据我之前分享论文，在无标注语料上做中文文本分类的实践，我觉得比较有借鉴意义。\n\n主要梳理的7问题，梳理如下，大家可以看下有没有自己感兴趣的问题，希望对大家有帮助：\n\n有没有比较好的NLP开源项目\n如何融合BERT所有Tokens输出语义信息\n英文BERT如何加载中文参数-这个待后续更新\n没有机器学习基础是否可以学习NLP深度学习知识\n只用标签无需标注语料就可以进行文本分类在中文语料的效果\n有没有关于Transformer的面试题\n如何在NER的时候加入词汇信息\n\n涉及到敏感信息，比如大家私人idea之类的，以及个人信息，我不会公开的，大家可以放心。有不便公开的，也可以直接和我说的。\n所以大家有问题可以在最后面扫码加我私人微信，我一般都会在我力所能及的范围内，比较详细的回答的。\n1. 有没有比较好的NLP开源项目\n\n2. 如何融合BERT所有Tokens输出语义信息\n\n3. 英文BERT如何加载中文参数-这个待后续更新\n\n4. 没有机器学习基础是否可以学习NLP深度学习知识\n\n5. 只用标签无需标注语料就可以进行文本分类在中文语料的效果\n之前写的这个文章，有读者朋友在中文语料上做了测试，效果还不错，感兴趣的可以看看。\n\n\n\n6. 有没有关于Transformer的面试题\n\n\n7. 如何在NER的时候加入词汇信息\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/其他/20201210一周技术问题答疑汇总/"},{"title":"","date":"2024-06-21T03:48:16.385Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.385Z","content":"实体库构建：离线新词发现流程\n命名体识别我们一般有两个操作：词典匹配+模型预测。\n对于词典匹配来说，速度快，准确度高。但是有一个问题是由于不同人对同一个东西有不同的表达，所以OOV问题比较严重。\n缓解OOV，我们可以使用模型预测增加泛化，还可以离线挖掘实体进行补充实体库。\n美团在这个文章中提到了一种新词离线挖掘补充实体库的方法，我借鉴了其中的思路，并且用到了自己工作中，效果还不错。在这个文章，我主要是详细解读一下整个过程。\n我们聊一下为什么需要做新词发现？\n新词是什么？按照最普通的定义就是我词典中不存在的词汇都属于新词。如果按照这个思路去挖掘新词，我们一般使用两种方法：有监督和无监督。\n无监督一般来说就是使用紧密度加自由度调整阈值就可以提取新词。但是这种方法有一个问题，就是你这个阈值的调整到哪里才可以，这个取决于你的召回和精确的一个平衡。\n有监督的话，一个简单的思路就是序列标注做中文分词，出来的词汇不在字典中的我们就可以作为新词。\n但是我们想一下这样新词出现的是什么情况？\n举个最简单的例子，可能你挖掘出来的就是“爷青结”这样的词汇，确实是新词，不在我们已经有词典中，但是对于我们的实体库有没有帮助呢？\n有没有帮助要看我们的目的。如果说我们的目的是为了分词的准确，那么这个新词完全可以用，直接放到txt文件中，保证下回分类的准确。\n但是在这里，我们是做的事情是为了补充实体库，也就是需要有意义的词汇，比如说“外滩十八号”这种词汇。\n所以，普通的新词发现的有监督和无监督方法只能挖掘词汇，不能保证挖掘的是实体。\n基于此目的，可以借鉴新词挖掘的思路，对词汇做二元分类判断是不是实体的有监督方法就很容易想到。\n总结下来步骤就是这样：\n\n\n挖掘频繁项\n\n\n提取频繁项的各种统计特征\n\n\n频繁项和已经有的实体交集作为正样本，负采样得到负样本。使用多个分类器进行集成，训练多个二元分类器。\n\n\n采用负样本的时候，美团有提到一个论文，大家可以去看一下。\n\n搜索日志中搜索次数比较高的词条和正样本的交集作为高质量短语，负样本减去词条作为低质量短语，使用Bert训练质量打分器。\n\n整个流程通读下来，其实很好理解。\n一般来讲，如果实践过程，第四个步骤其实很难做。\n我是这样想的，首先这个美团搜索很垂直，一般搜索属于短query，你很难去在美团搜索框去搜一个很长的句子。\n这种情况下，就会出顾客的搜索记录本身就是高质量的短语或者实体。想一下是不是这样，你去搜“来杯啤酒烧烤”，这本身就是个商户名称，就是个实体。所以交集才可以作为高质量短语。\n如果你是个大搜的搜索日志，这种情况基本不存在的，有长短语，有短的词汇，你找交集的阈值都无从下手。\n第二个难点就是Bert打分器这个东西的可靠性。一般来说实体的字数都比较少，比如五六个字，字数这么少，这个打分究竟可靠不可靠我没有实践过，只是有这个疑惑。\n整个做完，还有一个问题，实体库是分类别的，比如美食有一个词典，景点有一个词典等等吧。我们上面挖掘出来的是全部的实体，不分类别的，那么怎么分类呢？\n美团提到他们使用的AutoNER，大家可以去看一下相关论文。针对这一块，其实能做的思路还挺多的，由于工作原因，这块我就不说了。大家可以发散思路。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/关键词提取/实体库构建：大规模离线新词实体挖掘/"},{"title":"","date":"2024-06-21T03:48:16.535Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.535Z","content":"如果面试官问【聊一下RNN中的梯度消失】\n盲猜很多同学的回答可以简化成这样形式【由于网络太深，梯度反向传播会出现连乘效应，从而出现梯度消失】\n这样的回答，如果用在普通网络，类似MLP，是没有什么问题的，但是放在RNN中，是错误的。\nRNN的梯度是一个和，是近距离梯度和远距离梯度的和；\nRNN中的梯度消失的含义是远距离的梯度消失，而近距离梯度不会消失，从而导致总的梯度被近的梯度主导，同时总的梯度不会消失。\n这也是为什么RNN模型能以学到远距离依赖关系。\n简单的解释一下原因。\n首先，我们要明白一点，RNN是共享一套参数的（输入参数，输出参数，隐层参数），这一点非常的重要。\n当然，我们在理解RNN的时候，会把RNN按照时间序列展开多个模块，可能会认为是多套参数，这个是不对的哈。\n如下所示：\n\n然后，假设我们现在的时间序列为3，有如下公式存在：\n\n现在假设我们只是使用t=3时刻的输出去训练模型，同时使用MSE作为损失函数，那么我们在t=3时刻，损失函数就是:\n$$L_{3}=\\frac{1}{2}(Y_{3}-O_{3})^{2}$$\n求偏导的时候，就是这样的情况：\n\n其实看到这里，答案已经出来了。\n我们以第二个公式为例，也就是对$w_{x}$ 求偏导，如果时间序列程度为t，我们简化一下成下面这个公式：\n$$W_{x}=a_{1}+a_{2}+…+a_{t}$$\n时间序列越长，出现连乘的部分越集中出现在靠后面的公式上，比如$a_{t}$，但是前面的公式是不受影响的，比如$a_{1}$，也就是梯度是肯定存在的。\n总结一下：RNN中的梯度消失和普通网络梯度消失含义不同，它的真实含义是远距离的梯度消失，而近距离梯度不会消失，同时总的梯度不会消失，从而导致总的梯度被近的梯度主导。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/其他/RNN的梯度消失有什么与众不同的地方/"},{"title":"","date":"2024-06-21T03:48:16.705Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.705Z","content":"句向量\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/句向量/README/"},{"title":"","date":"2024-06-21T03:48:17.065Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.065Z","content":"在做中文NER的时候，我们的常规操作是以字为单词。这是因为如果以词为单位，\n很容易造成切分错误，导致误差的积累。\n我举个简单的例子，比如我现在有一句话，【你去北京老哥】\n但是以字为单词，有一个问题就是会忽视词的信息。\n所以，大家很自然就想仍然以字为单词做NER，但是把词的信息补充进来。\n这个时候，一个很朴素的想法就是，我输入的时候过一遍分词，然后把词向量和字向量拼接或者相加或者做别的操作来融合起来。\n这个方法一般来说能够提升准确度，但是不会太多。\n后来还有一种思想就是使用 lattice structure，这种确实做到了词汇信息的增强，但是存在并行化困难以及推理速度慢的缺点，换句话说，方法是好方法，但是落地困难。\n这个论文做了一个什么事情呢？把栅栏式结构通过相对位置编码展平。\n我们知道transformer为了保持位置信息，对于每个token，是使用了位置编码的。在这里，为了这个晶格结构设计了一个巧妙的位置编码，来把复杂结构展开展平：\n如图所示：\n\n看这个图需要注意的是，【重】这个字对应到英文代表的是character-字符，【重庆】这个词组对应到英文代表的是word-单词，这一点，大家在读论文的时候需要注意。\n为每一个token（包含char和word）分配两个位置索引：头位置和尾位置；\n在原来的晶格结构中，比如【店】只能和【人和药店】以及【药店】产生关系，但是在TRM中，由于self-attention的存在，【店】是可以和序列中的每个token都发生关系，不仅仅是和self-matched的词汇。这算是一个意外之喜。\nself-matched的词汇，就是包含当前char的\n谈一下为什么这么转化：\n一般来说，我们有语料，和词典，通过词典，我们可以得到一个晶格\n为什么要把晶格结构压平\n头部的索引就是第一个单词的位置，尾部就是最后一个单词所在的位置，如果是一个char，头尾就是相同的。\n通过这个巧妙的设置，我们是可以把展平的东西再重建到晶格模式的，所以认为是可行的。\n相对位置编码\n通过头尾索引，我们可以把晶格结构压平。\n现在还面临一个问题，就是对于【人和药店】头尾索引是【3】【6】，但是这并不包含位置信息。\n对于NER来说，位置信息是很重要的。\n对于普通的TRM，使用绝对位置编码保持位置信息，但是有研究表示，这种位置信息在self-attention中使用向量内积的时候，会减弱。\n具体的大家可以看我这个文章：原版Transformer的位置编码究竟有没有包含相对位置信息；\n所以，我们现在就要考虑使用相对位置信息来表达位置，同时还要把我们头尾索引融合进来。\n对于句子中的两个spans（包含char和words）$x_{i},x_{j}$，它们可能有三种关系：相交，包含，和分离。\n比如上面那个例子，【药店】和【人和药店】就是包含的关系；【重庆】和【人和药店】就是分离的关系。\n我们使用一个向量来描述两个spans之间的关系。\n先说两个spans之间存在的距离关系可以用如下公式去表达：\n\n上角标的$(hh)$代表的就是两个spans之间的头部索引差值，其他上角标类似的意思。\n具体的实际是什么样子，大家可以看上面的图c；\n然后我们使用如下的公式去生成相对位置编码：\n\n接下来的问题就是利用这个相对位置编码融入到TRM之中。\n\n简单来说，就是利用相对位置编码，生成了一个包含相对位置编码信息的新的attention矩阵，不再使用原始的attention矩阵\n看到这里，其实有注意到一个很有意思的点就是FLAT使用的是一层encoder。\n实验\n实验比较感兴趣的是\n一个是和其他词汇增强的网络结果相比，效果如何。\n还有一个就是使用transformer之后，TRM长距离依赖的优点和每个token之间都可以交互的优点有没有在提升效果上发挥作用\n还有一个其实很自然的会想到能不能使用将FLAT和BERT融合起来。也就是如何将动态的字向量和FLAT这种词向量结合起来。\n先看第一二点\n\n再看第三点\n\n有意思的是，使用了FLAT之后，在Resume和Weibo效果有提升，但是不明显，作者认为可能是因为数据集有点小。在大数据集Ontonotes和MSRA上，效果提升比较明显。\n推理速度的话，和Lattice LSTM相比，BSZ为16的情况下，基本是8倍左右。\n总结\n梳理一下怎么把词汇信息加入进去的：\n\n首先我们知道NER融合词汇信息能提升最终效果，但是一般的Lattice结构落地困难\n然后受TRM位置信息的启发，将Lattice结构展开\n然后由于普通TRM绝对位置信息在self-attention中会被削弱，所以想要使用相对位置信息。\n从头尾索引，我们可以知道tokens之间有三种关系：相交，包含，隔离；从这三种关系，我们可以得到两个tokens的四种距离公式，并且把这个四种距离公式融入到了相对位置信息。\n得到最终的相对位置信息，将相对位置信息融合进入attention矩阵，参与Encoder计算\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/FLAT-Transformer/"},{"title":"","date":"2024-06-21T03:48:16.735Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.735Z","content":"问：如何判断”今天你吃饭了“和”今天去哪里吃饭“这两个句子的相似度？\n上面这个问题，就是我们为什么需要句子向量的原因。神经网络或者其他的机器学习方法很难直接对中文进行处理，我们需要对句子使用一定的方法进行数字化或者向量化。\n我在讲词向量的时候，说了一个很好的比喻，向量化的过程就非常的类似计算机把我们的输入转为二进制以便处理的过程。\n只不过二进制的转化我们是可以规定，而向量化的过程根据我们任务不同目标不同，有着多种方法。\n我简单花了一个概述图，大家可以看一下：\n\n1.基线模型\n1.1 基于统计的词袋模型\nOne-hot 模型简单来说就是单词出现的位置为1，不出现的位置为0，形如[1,1,1,0,1],来将句子向量化。\nTF-IDF 使用到了单词在句子中出现的词频和在所有文档中出现的频率。相比于One-hot，增加了单词重要性这个维度的特征，所以效果一般来说比One-hot要好。\n1.2 基于词向量的词袋模型\n为什么使用词向量这个特征？相较于One-hot和TF-IDF，词向量能够提取语义信息。\n对词向量最简单的操作就是求平均获取句子的表征。对于词向量，一般可以使用Word2vec/Fasttext/Glove。后期Bert出现之后，我们也可以使用Bert的最后一层（或者某一层）的输出作为词向量。但是效果有待商榷。\n简单求均值简单粗暴，优化方法就是使用各种方法进行加权求均值。\n我们可以使用TF-IDF对词向量做加权求和获得句子的表征。为了简便，我们也可以去掉TF，只是使用IDF做加权求和。\n对于SIF模型，它分为两个步骤。首先使用平滑倒词频为权重求和，随后减去所有句子的共有信息，获得的结果作为句子表征。\n对于Power Mean 均值模型，它引入了幂均值改进加权求均值，通过修改不同的P值拼接不同的句子向量得到最后的句子表征。\n以上都属于我们词袋模型求得句子向量。词袋模型存在一个最大问题，就是忽略了或者没有那么重视句子的语序问题，不管你是不是有用到词向量。\n1.3基于任务\n我们来看一下基于任务的，分为RNN和CNN。举个简单的例子，我们使用RNN和CNN做文本分类任务，然后使用最后一个时刻或者最后一层（或者你使用其他方式）作为句子的向量。\n这种方式很好，但是存在的问题就是句子向量的表达严重依赖任务形式。\n我们用文本分类训练出现的句子向量如果还是用在文本分类任务，效果可能还不错，但是如果用在情感分析任务上，可能就一塌糊涂。\n这是因为我们的模型是依赖于任务的，文本分类模型侧重点和情感分类的侧重点是不同的，导致模型参数也应该是不相同的。\n所以基于任务的句子向量模型迁移性比较差。\n2. 无监督模型\n无监督模型最大的好处就是可以不使用标签数据。这一点真的很重要。\n当然我想提一点就是我这里说的无监督模型是做的是端到端。其实本质上，我们使用基于词袋的模型，也属于无监督模型。仔细想一下是不是这个道理，词袋模型同样没有使用到标签数据。\n拉回来，我们说端到端的无监督模型。主要谈两个：Skip-Thought Vectors 和 Quick-Thought Vectors。\nSkip-Thought Vectors 模型输入为三个连续的句子，然后使用Encoder-Decoder模型，输入中间的句子，分别生成上一个句子和下一个句子。这个过程非常类似于Word2vec。\nQuick-Thought Vectors 是对Skip-Thought Vectors 的改进。首先说为啥需要改进，最大的原因就是 Skip-Thought Vectors 太慢了。首先它是一个生成任务。生成任务在预测阶段很难并行。其次他是做了两个生成任务，一个是上一个句子的生成，一个是下一个句子的生成。\nQuick-Thought Vectors把生成任务改为了分类任务，Decoder从一组句子中选择出正确的上/下一个句子。\n3. 有监督模型\nInferSent模型注意两个细节点就可以。首先就是使用的是自然语言推理（NLI）数据集上训练 Sentence Embedding。\n这一点其实很重要，作者是认为从这个数据集上训练出来的词向量是可以很好的被迁移到别的任务上的。\n其次使用的是LSTM或者其他模型对句子进行编码，作者在论文中对不同编码模型有详细比较。\nUniversal Sentence Encoder 使用多任务，通过在不同的数据集和不同的任务上同时训练，动态地适应各种的 NLP 任务。Encoder使用两种模型，一个是Transformer，是为了获取更高的精度，另一个事DAN (Deep Averaging Network）为了获得更快的速度\n已经尽了最大努力缩减内容，提取重点了，接下来会用几篇文章详细的谈一谈其中的部分模型。\n写文不易，点个在看或者赞让更多人看到吧，谢谢。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/句向量/句向量模型综述/"},{"title":"","date":"2024-06-21T03:48:17.315Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.315Z","content":"命名体识别\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/README/"},{"title":"","date":"2024-06-21T03:48:17.265Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.265Z","content":"HMM\n三个参数\n我们这个HMM模型，含有三种参数，定义如下:\n$$\n\\lambda =（\\pi,A,B）\n$$\n注解:\n首先\n$$\n\\pi： 这里不是我们的圆周率那个符号，而是代表的是初始概率矩阵，具体看下面的讲解\n$$\n其次\n$$\nA：代表的是状态转移概率矩阵，具体看下面的讲解\n$$\n最后\n$$\nB：代表的是发射概率矩阵，具体看下面的讲解\n$$\n现在引入数学符号，首先定义观测变量为符号:\n$$\no: o_{1},o_{2},o_{3},o_{4}…o_{t}…\n$$\n观测变量的值域，也就是观测变量的取值范围：\n$$\nV={v_{1},v_{2},…v_{M}}\n$$\n也就是观测变量我们有M个取值结果。\n同理我们可以得到状态变量为:\n$$\ni: i_{1},i_{2},i_{3}…i_{t}…\n$$\n状态变量的值域，也就是状态变量的取值范围：\n$$\nQ={q_{1},q_{2}…g_{N}}\n$$\n也就是说，我们的状态变量有N个不同的取值。\n我们定义A为状态转移概率矩阵，公式定义为:\n$$\nA=[a_{ij}]，其中a_{ij}=P(i_{t+1}=q_{j}|i_{t}=q_{i})\n$$\n注解：这里我们的状态转移概率矩阵很容易理解，就是说我们上面不是定义了状态变量为符号\n$$\ni\n$$\n，其中状态有N种取值范围。我们以词向标注为例，这里我们假设我们的N有四种方式，分别为[名词，动词，谓词，形容词]。那么A这个矩阵中的每个元素就是其中一个状态转移到另一个状态的概率，比如名词之后接动词（也就是名词转移为动词）的概率，比如动词之后接谓词（也就是动词转移为谓词）的概率，依次类推。\n我们定义B为发射矩阵\n$$\nB=[ b_{j}(k)], 其中b_{j}(k)=P(o_{t}=v_{k}|i_{t}=q_{j})\n$$\n注解：这里简单记住，发射矩阵就是上面状态发射到下面的概率，注意看箭头的方向。\n这个时候，我们再去看\n$$\n\\pi ：这个符号代表的就是 i_{1}={q_{1},q_{2}…q_{n}}时的状态概率{q_{1},q{2}…q_{n}}\n$$\n两个假设：\n\n\n马尔科夫假设：当前时刻的状态变量只与t-1时刻有关，而和别的变量无关。\n$$\np(i_{t+1}|i_{t},i_{t-1}…i_{1},o_{t},o_{t-1}…o_{1})=p(i_{t+1}|i_{t})\n$$\n\n\n齐次性假设，可以理解为时间平移不变\n\n\n![image-20201221164438044](/Users/zida/Library/Application%20Support/typora-u](…/image-20201221164438044.png)\n\n观测独立假设：当前的观测变量只与当前时刻的状态变量有关，而和其他无关\n\n$$\np(o_{t}|i_{t},i_{t-1},…i_{1},o_{t-1},…o_{1})=p(o_{t}|i_{t})\n$$\n三个需要解决的问题\nHMM 需要解决的问题。\n首先求值问题：已经知道三种参数的情况下，那么我一句话出现的概率多大：我爱中共产党\n简单讲就是已知\n$$\n\\lambda\n$$\n求\n$$\no_{1},o_{2}…o{n}\n$$\n这句话出现的概率有多大。\n我们常用的算法是前向后向算法。前向算法后向算法解决的问题是求在给定三个参数的情况下求观测序列出现的概率  注意一定是求得观测序列，也就是放在序列标注中，是求我们本身文字序列出现的概率。\n第二个问题，就是参数如何求？\n$$\n也就是如何求得：\\lambda\n$$\n我们使用EM算法求得这个参数\nEM算法是在估计HMM三个参数的办法。当然之前有谈到如果我们有观测序列和对应的隐藏序列，那么我们直接从数据中去统计就可以了。但是现实情况是我们很难获取标注序列，也就是隐藏序列。这个时候我们就需要使用到EM算法去预估。\n也就是，如果没有标注序列，我们使用EM算法，如果有了标注序列我们直接从语料中统计出来就可以了。\n第三个问题就是解码问题，也就是要找到一个状态序列，可以使得\n$$\nI=argmaxP(I|O)\n$$\n也就是解决当前这个句子最有可能的序列标注结果是什么样子的。\nHMM最可能的额隐藏状态序列求解使用维特比算法。\n使用一句话话可以很精辟的总结出来维特比的过程：\n在每一时刻，计算当前时刻落在每种隐状态的最大概率，并记录这个最大概率是从其哪一个时刻那个隐状态转移过来的，然后再从结尾达到最大概率的那个隐状态回溯，就有可能得到最优路径。\n维特比使用动态规划，解决寻找全局最优路径的问题。\nCRF\n全局归一化避免偏置\n对于CRF我们的目标函数是让正确的标注序列出现的概率在所有路径汇总是最大的。所以分母我们是针对的所有路径。而不是在每一个时刻去计算最优值。因为在某一个时刻计算的最优可能在整体路径上并不是最优。\n分数并不是概率\n在bilstm-crf中，我们包括转移分数，发射分数，我们都是分数而不是概率。并且我们是做了log操作的，所以在计算某个路径的分数的时候我们并不是概率相乘而是分数相加。\n损失函数\n损失函数其实本质很简单，就是正确路径概率最大，拆分之后我们会对应两个部分一个是一元分值，就是在某个时刻成为某个实体标签的分数。一个是二元分值，就是标签之间的转移分数。\n解码-维特比\n在每一时刻，计算当前时刻落在每种隐状态的最大概率，并记录这个最大概率是从其哪一个时刻那个隐状态转移过来的，然后再从结尾达到最大概率的那个隐状态回溯，就有可能得到最优路径。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/HMM_CRF/"},{"title":"","date":"2024-06-21T03:48:17.595Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.595Z","content":"今天介绍复旦的一个论文TENER ；普通的TRM在其他NLP任务中效果很不错，但是在NER中表现不佳。为了解决性能不佳，论文做了几点改进。\n主要掌握以下三点改进：\n\n方向\n距离\n无缩放的注意力\n\n1. 架构图\n先看TENER架构图：\n\n2. 距离和方向信息\n对于NER任务来说，距离和方向都很重要；\n举个简单的例子：【李华住在北京】；李华是人名，北京是地名，如果忽视了方向，那么【北京住在李华】，这个肯定是说不通的。\n换句话说，每类NER实体在哪种位置是有着某种关系或者规则的。所以方向很重要。\n简单概述普通TRM位置编码的问题，如下：\n普通TRM中的正弦位置编码能够捕捉到距离信息，但是不能捕捉到方向信息。而且这种基本性质（distance-awareness）会在sefl-attention消失；\n为了改进这种问题，使用了经过改进的相对位置编码，弃用了绝对位置编码；\n2.1 为什么没有方向信息：\n位置编码的点积可以看做在度量两者之间的距离:$PE^{T}{t}PE{t+k}$\n点积结果画图表示如下：\n\n从这个图，我们可以很清楚的看到，是对称的，也就是说在k=20和k=-20的时候，点击结果相同，换句话说，方向信息没有体现出来。\n公式上体现就是：$PE^{T}{t}PE{t+k}=PE^{T}{t-k}PE{t}$\n2.2 distance-awareness 消失\n再进一步，在self-attention中，distance-awareness 也在消失，这一点，我之前的文章有写，可以看原版Transformer的位置编码究竟有没有包含相对位置信息。\n改进之后的相对位置编码以及attention计算为：\n\n3. attention缩放\n传统TRM的attention分布被缩放了，从而变得平滑。但是对于NER来说，一个更加尖锐或者说稀疏的矩阵是更合适的，因为并不是所有的单词都需要被关注；一个当前的单词的类别，足够被周围几个单词确定出来。\n矩阵越平滑，关注的单词越多，可能会引入更多的噪声信息。\n4. 总结\n\n原始TRM绝对位置编码不含有方向信息，Self-attention之后相对位置信息也会消失；故使用改进的相对位置编码和新的attention计算方式\nattention计算不使用缩放系数，减少了噪声信息\n使用TRM进行char编码，结合预训练的词向量拼接输入TENER\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/TNER-复旦为什么TRM在NER上效果差/"},{"title":"","date":"2024-06-21T03:48:16.805Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.805Z","content":"今天介绍一个论文autoner，主要是为了探索如何在只有词典的情况下，提升NER实际落地效果；\n首先，如果手中含有词典，常规操作就是远程监督打标数据，然后做NER；\n远程监督一个比较常见的操作就是使用我们手中的字典，通过字符匹配的形式对文本中可能存在的实体打标。\n但是对于这种远程监督的形式，存在比较多的问题，这个论文主要探讨两种： 多标签(multi-label tokens) 和标签不完善的问题；\n针对multi-label tokens，论文提出的是Fuzzy-LSTM-CRF，简单讲就是讲LSTM后面的CRF层变为了Fuzzy CRF层，可以在处理tokens对应多标签的情况下，不牺牲计算效率；\n第二个问题标签不完善，是因为字典毕竟是有限的，不可能把所有的实体都覆盖到，那么句子中没有被字典打标成功的词组很有可能也是某种实体，但是远程监督并没有对此做处理。\n针对这个问题，本文提出了一种比较新的标注框架，简单来讲就是在这新的框架中，不去预测单个的token的类别，而是去判断两个相邻的tokens是不是在同一个实体中被tied；\n上面只是我自己简单的分类，其实存在的两个问题和两种解决架构是相互融合在一起的，具体的我们下面谈。\n0. 词典形式简单介绍\n首先定义一下词典形式，包含两个部分，第一部分是实体的表面名称，这个包括规范名称和对应的同义词列表；第二个部分就是实体的类型；\n其次，词典的标注肯定是有限的，肯定存在不在词典中的某些词组但是也属于某种类型的实体；\n对于这部分实体，我自己的理解大体可以包含两个大部分；第一个大部分就是比如说【科技】这个领域覆盖的【科技】实体有有限的，所以有漏网之鱼；第二部分就是词典的实体类型是有限的，比如词典总共包含2个实体类型，但是你真实的文本包含更多的实体类型，存在漏网之鱼。\n对于这些漏网之鱼的实体，我们的策略是这样的。\n首先通过AutoPhrase从文中挖掘出来高质量短语，然后统一赋值为unknown type，也就是未知类型。\n1. Fuzzy-LSTM-CRF\n1.1 标注策略\n梳理一下，我们现在手上有词典；\n词典包含两个部分，一部分是已知实体类型（假设是2个，当然可能更多或者更少）；另一个部分就是我们通过某种方式挖掘出来的高质量实体对应的未知类型；\n然后我们通过手中的词典对原始无标注文本进行打标；\n那么现在对于句子中的某个token，它存在三种可能性；第一它可能是已知实体类型中的一种或者多种；第二它属于未知类型；第三是属于O这种情况，就是non-entity；\n基于传统架构BIlstm-CRF如何解决多标签的问题？\n其实本质解决的思路很简单。对于原来的每个token，只是预测一个类别，现在是预测多个类别就可以了。\n详细点讲就是，首先对于远程监督标注的过程，我们会使用三种策略。\n我们先假设我们使用{I；O；B；E；S}的标注形式；\n第一，对于某个token，如果它对应到了已知类型中的某一个或者多个实体，那么按照对应的位置直接标记上，不要漏掉；也就是说{I；B；E；S}和对应的一个或者多个实体类型对上标；\n第二对于对于某个token，如果属于未知类型，那么对应的这个token就需要把所有已知实体类型（区别于上面的一个或者多个已知实体类型）和 {I, O, B, E, S}对应的打标上；\n注意，这里并没有使用未知实体类型，而是使用的所有的已知实体类型；\n第三个对于既不属于已知类型的，也不属于未知类型的，全部打上O；\n1.2 Fuzzy-LSTM-CRF 模型架构\n其实很好理解，传统的CRF最大化唯一一条有效的标注序列。在这里，我们最大化所有有可能的标注序列。\n公式如下：\n\n看架构图：\n\n2. AutoNER\n区别于Fuzzy-LSTM-CRF 模型沿用传统架构，在这里论文提出一种新的标注架构-Tie or Break；\n这个标注框架更加关注的是当前token和上一个token是否在同一个实体里面；如果在同一个实体里面，那么就标注为Tie；\n如果当前单词和上一个单词至少有一个在unkonw类型的高质量短语，那么标注为unkonw，其他情况标注为Break；\n优化过程：把实体识别和实体类型判定分离开。\n原论文中描述的是先做实体识别，两个Break之间作为一个span，然后做实体类型判定；\n实体识别中，对于当前单词和上一个单词之间类别的的输出，对Tie和Break做二分类损失，如果类别是unkown类别，直接跳过，不计算损失。\n概率公式如下：\n\n\n第二步预测实体类型，包含None实体类型\nunkonw这种，知道这属于实体，在高质量短语词典中，但是不知道短语类型，所在这里我们会标注为None实体类型。\n其他的不在词典中的，当然也就会被标注为None实体类型。\n为了应对多标签，也就是同一个实体对应不同的类别，这里修改了最后的CE损失函数：\n\n\n使用的是软标签的进行的CE的计算，并没有使用硬标签。\n$L_{i}$对应的是在远程监督中，当前实体真实类型标签集合。从公式我们可以知道，尤其是看分母，在不属于这个集合的标签概率我们并没有计算在内。\n总结\n多提一个小细节，就是高质量短语的挖掘使用的是AutoPhrase，大家可以去试一下；\n论文提出两种结构解决多标签和标签不完善的问题。\n首先对于标签不完善，使用上面提到的AutoPhrase去挖掘文本中的高质量短语，作为词典中的未知类型。\n在Fuzzy-LSTM-CRF，需要注意的细节是，对于未知类型的标注，我们使用的策略是标注所有已知类型；\n对于AutoNER，有两个细节需要注意，一个是新的标注框架tie or break，重点在于去看两个相邻单词是否属于同一个实体；第二个细节就是为了解决多标签问题，修改了损失函数，使用的软标签；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/autoner/"},{"title":"","date":"2024-06-21T03:48:17.615Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.615Z","content":"最近在梳理命名体识别和关键词提取的东西，之前有建一个仓库，专门梳理相关内容。新关注的人比较多，分享给大家，地址在这里:\nhttps://github.com/DA-southampton/ner\n点star 不迷路，相关文章在github上更新的会更频繁一点QAQ。\n微信没有外链跳转，涉及到代码的部分，大家去仓库去看。\n之前做过一段时间的命名体识别，项目背景其实也很简单，就是我要做一个关键词匹配的功能，第一步我需要挖掘关键词。数据调研之后发现对于一部分领域文本，比如说娱乐领域，明星领域，财经领域等等吧，这些领域的文本很有特色，一般人名/地名/公司名称/书名/电影名称都可以很好的表示文本关键信息。\n在这种项目背景之下，很自然的就会想到使用命名体识别。我把在做这个项目的过程中，积累的一些资料总结了一下，希望对大家有所帮助。\n关于命名体识别，这是个很大的领域，要是做好，有很多工作要做。标题完全是为了能增加曝光，自己还是知道只是一个小学生，我会把自己看过的有用的东西都列出来，给大家提供一些先验信息。\n之后看到的关于nert的文章会在此基础继续更新（最近存了好多新文章还没看/苦逼码农/QAQ），不过建议大家star一下Github，不迷路，我给自己的计划是精读一些论文和博客，做一些思维导图，复现一些代码，我会努力的。\n经验介绍\n对于命名体识别的代码这一块，我大概的经验就是，工作中很少直接就上复杂模型，一般都是先来简单模型，然后在优化迭代。我给个大概的方向（大家视情况而定）：\n词典匹配–&gt;HMM/CRF–&gt;BiLSTM-CRF–&gt;Bert系列\n一般来说词典匹配是最简单的，也是最快的。不过很依赖于你的词典情况。一般来说，词典的补充需要你自己搞定，比如找相关的运营人员/产品人员，因为他们比较靠近一线工作，手上会积累一些相关的词典。或者使用合法爬虫手段（至于如何合法就自己考虑吧）去专业的垂直领域网站获取数据补充词典。\n我大概分为两个个模块，第一个是各种模型的代码实现相关资源，第二个就是关于命名体识别基础知识之类的相关资源\n代码实现\n代码不再多，把一个反复看，看懂了，自己能写出来做二次开发就可以，不要今天看一个代码明天换一个代码看（小声嘟囔）\n左侧是有链接的，微信点不开，大家去仓库看！！！\nBert系列 (Bert/Albert-softmax/CRF/Span/Span+focal_loss/Span+label_smoothing)做命名体识别\t仓库下面有Bert系列完成命名体识别的效果对比（一般来说看F1就可以）以及训练时间之类的比较，很推荐大家去看一看\nBiLSTM-CRF实现命名体识别(Pytorch版本)\tBiLSTM-CRF我就推荐这一个吧，其他的都是大同小异，大家可以一步步去调试，做二次开发就可以，比如换个损失函数之类的。\nNLP实战-中文命名实体识别-HMM/CRF 代码的实现\t(引用原文)本文章将通过pytorch作为主要工具实现不同的模型（包括HMM，CRF，Bi-LSTM，Bi-LSTM+CRF）来解决中文命名实体识别问题，文章不会涉及过多的数学推导，但会从直观上简单解释模型的原理，主要从零的内容会集中在代码部分。\n隐马尔可夫模型命名实体识别NER-HMM-1  [隐马尔可夫模型命名实体识别NER-HMM-2\t不愿意看书想看视频的同学可以看一下这个，B站首页偶然推荐给我的（推荐算法精准石锤了），讲的确实好\n双向最大匹配和实体标注：你以为我只能分词？------这个是词典方法命名体识别\t这个作者总结了自己实体词典+jieba词性标注进行实体自动打标，有Python代码实现，大家可以关注一下这个博主，名字叫“叫我NLPer”，行文很有意思\n基本上代码，我觉的看上面几个就够了吧，反复咂摸一下。\n博客讲解\n有些时候看到有人说，要想对某个概念真正有所了解，一定要看原论文。这句话肯定没错，但是不是有些时候没时间看论文吗（哭了苦逼码农）。而且有些博客讲的是真的好啊。我大概罗列一些我局的真心不错的文章，主要就是HMM/CRF/Bilstm-CRF\n左侧是有链接的，微信点不开，大家去仓库看！！！\n概率图模型体系：HMM、MEMM、CRF\t这个文章传播的比较广，讲的确实比较详细，不过大佬写的有些地方还是有些小问题，大家自己去挖掘吧。。。\n最通俗易懂的BiLSTM-CRF模型中的CRF层介绍-孙孙的文章\t这个文章讲的是对CRF模型的讲解，翻译的外文，原文很精彩，看译文也可以。大概讲一下，在看的过程中，要多琢磨。比如CRF有个特点全局归一化，这是区别于MEMM模型的；比如在代码实现的时候，我们一般都是使用log，所以乘法会对应加法，这样你在看源代码时候就不会懵逼；比如CRF损失函数有两个部分组成，分别有啥作用。\nner自动化打标方法\t叉烧大佬讲了一下如何用词典+最大逆向匹配做命名体识别整体思路，代码的实现可以参考第一部分那个Python代码\n中文NER任务实验小结报告——深入模型实现细节\t作者写了一下自己在做命名体识别的时候针对Bert的优化:BERT+CE_loss;BERT+lstmcrf;尝试用更少的标签列表;对损失函数进行了优化尝试(解决类别不平衡，因为O类别太多了);BERT+MRC;(改天我可能要精读一下，大佬写了很多内容，感觉有很多细节可以挖)\n如何通俗地讲解 viterbi 算法？\t讲解了维特比算法，维特比一般是用于解码\n小标注数据量下自然语言处理实战经验\t小标注数据如何处理\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/命名体识别资源梳理（代码+博客讲解）/"},{"title":"","date":"2024-06-21T03:48:17.635Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.635Z","content":"背景介绍\n建了仓库，地址在这里:\nhttps://github.com/DA-southampton/ner\n点star 不迷路，相关文章在github上更新的会更频繁一点QAQ\n之前做过一段时间的命名体识别，项目背景其实也很简单，就是我要做一个关键词匹配的功能，第一步我需要挖掘关键词。数据调研之后发现对于一部分领域文本，比如说娱乐领域，明星领域，财经领域等等吧，这些领域的文本很有特色，一般人名/地名/公司名称/书名/电影名称都可以很好的表示文本关键信息。\n在这种项目背景之下，很自然的就会想到使用命名体识别。我把在做这个项目的过程中，积累的一些资料总结了一下，希望对大家有所帮助。\n关于命名体识别，这是个很大的领域，要是做好，有很多工作要做。标题完全是为了能增加曝光，自己还是知道只是一个小学生，我会把自己看过的有用的东西都列出来，给大家提供一些先验信息。\n之后看到的关于nert的文章会在此基础继续更新（最近存了好多新文章还没看/苦逼码农/QAQ），不过建议大家star一下Github，不迷路，我给自己的计划是精读一些论文和博客，做一些思维导图，复现一些代码，我会努力的。\n微信公众号: NLP从入门到放弃\n（我果然是个渣渣，公众号名字都这么渣…欢迎关注）\n经验介绍\n对于命名体识别的代码这一块，我大概的经验就是，工作中很少直接就上复杂模型，一般都是先来简单模型，然后在优化迭代。我给个大概的方向（大家视情况而定）：\n词典匹配–&gt;HMM/CRF–&gt;BiLSTM-CRF–&gt;Bert系列\n一般来说词典匹配是最简单的，也是最快的。不过很依赖于你的词典情况。一般来说，词典的补充需要你自己搞定，比如找相关的运营人员/产品人员，因为他们比较靠近一线工作，手上会积累一些相关的词典。或者使用合法爬虫手段（至于如何合法就自己考虑吧）去专业的垂直领域网站获取数据补充词典。\n我大概分为两个个模块，第一个是各种模型的代码实现相关资源，第二个就是关于命名体识别基础知识之类的相关资源\n代码实现\n代码不再多，把一个反复看，看懂了，自己能写出来做二次开发就可以，不要今天看一个代码明天换一个代码看（小声嘟囔）\n\n\nBert系列 (Bert/Albert-softmax/CRF/Span/Span+focal_loss/Span+label_smoothing)做命名体识别\n仓库下面有Bert系列完成命名体识别的效果对比（一般来说看F1就可以）以及训练时间之类的比较，很推荐大家去看一看\n\n\n\n\nBiLSTM-CRF实现命名体识别(Pytorch版本)\nBiLSTM-CRF我就推荐这一个吧，其他的都是大同小异，大家可以一步步去调试，做二次开发就可以，比如换个损失函数之类的。\n\n\nNLP实战-中文命名实体识别-HMM/CRF 代码的实现\n(引用原文)本文章将通过pytorch作为主要工具实现不同的模型（包括HMM，CRF，Bi-LSTM，Bi-LSTM+CRF）来解决中文命名实体识别问题，文章不会涉及过多的数学推导，但会从直观上简单解释模型的原理，主要的内容会集中在代码部分。\n\n\n隐马尔可夫模型命名实体识别NER-HMM-1  [隐马尔可夫模型命名实体识别NER-HMM-2\n不愿意看书想看视频的同学可以看一下这个，B站首页偶然推荐给我的（推荐算法精准石锤了），讲的确实好\n\n\n双向最大匹配和实体标注：你以为我只能分词？------这个是词典方法命名体识别\n这个作者总结了自己实体词典+jieba词性标注进行实体自动打标，有Python代码实现，大家可以关注一下这个博主，名字叫“叫我NLPer”，行文很有意思\n\n\n基本上代码，我觉的看上面几个就够了吧，反复咂摸一下。\n博客讲解\n有些时候看到有人说，要想对某个概念真正有所了解，一定要看原论文。这句话肯定没错，但是不是有些时候没时间看论文吗（哭了苦逼码农）。\n而且有些博客讲的是真的好啊。我大概罗列一些我局的真心不错的文章，主要就是HMM/CRF/Bilstm-CRF\n\n\n概率图模型体系：HMM、MEMM、CRF\n这个文章传播的比较广，讲的确实比较详细，不过大佬写的有些地方还是有些小问题，大家自己去挖掘吧。。。\n\n\n\n\n最通俗易懂的BiLSTM-CRF模型中的CRF层介绍-孙孙的文章\n这个文章讲的是对CRF模型的讲解，翻译的外文，原文很精彩，看译文也可以。大概讲一下，在看的过程中，要多琢磨。比如CRF有个特点全局归一化，这是区别于MEMM模型的；比如在代码实现的时候，我们一般都是使用log，所以乘法会对应加法，这样你在看源代码时候就不会懵逼；比如CRF损失函数有两个部分组成，分别有啥作用。\n\n\nner自动化打标方法\n叉烧大佬讲了一下如何用词典+最大逆向匹配做命名体识别整体思路，代码的实现可以参考第一部分那个Python代码\n\n\n中文NER任务实验小结报告——深入模型实现细节\n作者写了一下自己在做命名体识别的时候针对Bert的优化:BERT+CE_loss;BERT+lstmcrf;尝试用更少的标签列表;对损失函数进行了优化尝试(解决类别不平衡，因为O类别太多了);BERT+MRC;(改天我可能要精读一下，大佬写了很多内容，感觉有很多细节可以挖)\n\n\n如何通俗地讲解 viterbi 算法？\n讲解了维特比算法，维特比一般是用于解码\n\n\n小标注数据量下自然语言处理实战经验\n小标注数据如何处理\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/工业级命名体识别的做法/"},{"title":"","date":"2024-06-21T03:48:17.775Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.775Z","content":"今天主要聊我在做多模态任务中的六个方面的介绍，如下：\n\n多模态业务简单介绍；\n多模态数据问题；\n如何确保多模态任务的预测速度；\n如何确定多模态任务确实起到了作用；\n多模态中多张图片如何处理；\n交互的时候哪种attention方式更好；\n训练的时候需要注意什么；\n\n1.多模态业务简单介绍；\n之前花了不少时间在多模态这块的落地工作，取得了一定的效果，今天分享一下我的经验；\n首先在调研多模态任务的时候大家可以看一下最近的论文，这两年的多模态任务基本上都在往Transformer上去靠，基本可以分为两种：单流网络和双流网络；\n双流网络就是文本过一个编码器，图片过一个编码器，然后两个编码器的输出进行一个交互；\n单流网络就是文本和图片先concat，然后直接输入到Transformer编码器中，然后输出；\n一般来说，这里的编码器使用的都是TRM结构；\n文本这块，输出的时候得到的是embedding就可以；图片这里，一般来说使用的是Faster-RCNN模型识别出图片中包含的多个物体及其对应的矩形位置信息，把这个作为TRM的输入；\n但是我在真正去做的时候，并没有按照这个思路去做，我是先按照自己的思路做了个baseline，然后有效果，之后再去看论文架构提升模型效果；\n我简单分享一下我的主体思路，文本过的BERT，图像过的Resnet,然后输出的两个表征向量之间做多头注意力，然后接全连接输出logits；\n按照分类，我这个架构应该属于双流网络；\n架构其实很简单，但是在真正去做的时候，真的是比较复杂，有很多细节，我在这里简单的梳理一下，一起探讨；\n2.多模态数据问题；\n多模态一般来说就是双模态数据，我主要接触的是文本+图片；很幸运，我有标注数据~~ 如果没有基于自己场景下的标注数据，还是不太建议强行上多模态任务；\n3.如何确保多模态任务的预测速度；\n为了保证我的预测速度，我不可能所有的case都过多模态网络；所以我做的策略很简答，就是单从文本输出结果置信度不高的而且含有图片信息的case走多模态任务；\n4.如何确定多模态任务确实起到了作用；\n这个问题其实很关键，首先我们当然可以做测试集，验证一下单走文本或者单走图片得到的f1以及做多模态得到的f1，两者一个比较就可以；\n当时确实也这么做了，但是我纠结点在于能不能使用一种可见的方式，告诉大家多模态度确实起到了作用？\n那么一个很有用的方法就是使用attention的可视化；这个方法可以可视化出文本和图片之间确实是有交互的，而且交互的部分是有意义的，比如有的单词就是对图片中的某个部分更加关注；\n5.多张图片如何处理；\n因为我图片过的是Resnet网络，所以输入是多张图片的数量是动态的，这是个问题；\n我们退一步说，按照现在bert多模态预训练中的方法，多张图片完全可以作为transformer中的输入tokens部分；或者把多张图片合并在一起生成一个图片再走正常流程；\n我这边处理的时候需要注意的细节就是resnet输出池化的时候k是个动态的池化就可以；\n6.哪种attention方式更好；\n一般来说做互相之间的交互更好，就是文本对图片做一次attention，图片对文本做一次attention，两者结合来做；\n7.训练的时候需要注意什么；\nbert和resnet网络架构不太一样，训练的时候容易不收敛，需要控制一下不同部分的学习率；\n如上，因为业务的原因，很多东西不能细说，所以我只是大体的介绍了一些自己的经验，希望能对大家有帮助；\n之后我会写一些BERT多模态预训练论文的解读文章，大体是LXMERT，ViLBERT，Unicoder-VL、VisualBERT、VL-VERT、UNITER等等；\n求点赞，求在看，求转发，求一切，爱你们哦~ ~\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/多模态/复盘多模态需要解决的6个问题/"},{"title":"","date":"2024-06-21T03:48:17.685Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.685Z","content":"命名体识别\n关于这一块，主要是参考了美团ner的文章，写的非常的好：\n美团搜索中NER技术的探索与实践\nhttps://tech.meituan.com/2020/07/23/ner-in-meituan-nlp.html\n中文NER的正确打开方式: 词汇增强方法总结 (从Lattice LSTM到FLAT) - JayLou娄杰的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/142615620\n实际工作中做实体识别，分为两个方向：词典匹配和模型预测。一般情况下，两者会被同时使用，相辅相成。\n方法：词典匹配+模型预测\n首先聊一下为什么使用词典匹配。对于词典匹配来说，很简单就是，来一个句子，看句子中有没有含有我的字典中的词汇，如果有，直接输出就可以，这个过程匹配速度很快，不存在性能上的瓶颈。\n这个时候，我们要思考一个问题，词典匹配上线不存在性能上的瓶颈，那么它的瓶颈在哪里？\n首先，语义消歧问题：我的词典是分类别的，就是说针对不同的垂直领域我会有不同的词典列表，即美食含有一个词典列表，景点含有一个列表，以此类推，类别越多，你的垂直度就会越好。美团文章有这么一个例子，query是”黄鹤楼美食“，那么在词典匹配的时候会出现这样一种情况，就是景点词典匹配上了”黄鹤楼“，美食词典也匹配上了”黄鹤楼“（可能是北京一个美食商家名称）。\n那么，我们选择哪一个作为输出？这个例子就显示了一个问题，词典匹配不能解决语义消歧功能。而模型预测能够有泛化能力。\n其次，词典数量有限，泛化能力比较差：词典效果再好，但是它的数量是有限的，这就会出现OOV情况。缓解这个问题，有两种办法，一个是来做实体挖掘的方式不同的补充实体库，第二个就是使用深度学习的方式进行模型预测实体。\n这个时候，会思考一个问题，就是词典匹配和模型预测两路的结果，如何合并输出？\n美团是训练了要给CRF打分器，这一步我猜测是这么做的：\n对于我来说，两者完全可以这么做：词典匹配全部输出，模型预测提高阈值，至于这个阈值输出的阈值就看你自己去调了。\n词典匹配\n离线挖掘补充实体库\n词典匹配的一个瓶颈问题就是数量有限，OOV问题会比较明显。也就是说，有些词语不是那么正常，比如”爷青结“，但是这些词有需要补充到实体库。这个时候就需要我们离线的从数据中对实体进行挖掘。\n离线挖掘首先面临的一个问题是数据问题。数据如果是结构化数据，就很好办了。比如说直接从电影榜单获取到电影实体，从电视剧榜单获取电视剧的实体等等吧，这个没有什么难度。\n如果数据是非结构的数据怎么办？什么叫做非结构化数据呢？比说微博的博文文本，这个是UGC内容，就是我们平常说的话。\n从非结构化数据中提取出实体才是我们想要的东西。这个过程应该是分为两个步骤的，首先第一步，提取实体，第二步我们需要对实体分类。也就是我们的实体是需要对应到不同的类别词典中。\n美团在这一点说自己使用的是新词发现的一个流程来做实体识别。其实我仔细思考了他的这个流程，它和常规的新词发现还不太一样。\n首先，我们知道新词发现一般来说分为有监督和无监督。有监督就是序列标注，进行中文分词，结果中不再词库的就是我们的新词。无监督就是凝固度和自由度来评判词汇是不是新词。\n这个是新词发现的流程，但是我们要做的是找到新的实体，如果仅仅是做新词发现，肯定不能保证你挖掘出来的新词就是一个实体类别，也有可能是一些不是实体的那种网络新词。\n所以美团这边只是借鉴了新词发现的一部分。\n它的具体流程是这样的：\n\n\n挖掘频繁集合作为候选\n\n\n候选集中的词语和已有积累的实体交集作为正样本。比如”烧烤“在频繁集合中有，在已有的实体词典中也有，就是一个正样本。基于负采样生成负样本。\n\n\n提取正负样本四个维度特征训练二分类判断是不是一个实体。\n\n\n注意看到这第三点，从这个点，我发现一个问题。学习的目标是什么？二元分类判断词汇是不是实体。如果是实体，这个实体的数据来源于哪里？是交集，所以从本质上是已积累的实体。所以，我们相当于在挖掘UGC内容中，和我已经积累的实体\n有相同特性的实体，至于这个实体是不是一个新词，不是我们考虑的。\n负样本中也有部分是高质量实体，也就是说我挖掘出来的高频繁集合有些也是比较好的实体，但是由于没存在交集中，所以被认为是负样本了，所以这个时候可以使用集成多个弱分类器的方式减少误差。\n接下来，我们使用的是Bert做了一个短语质量评分。对于这一个部分，其实很有意思，经过上面这个步骤，我们获取到了大量的正负实体。我们可以这样想一下，美团搜索其实有这样一个特点，就是说，我们基本上的搜索和大搜很不一样，我们的搜索都很垂直，\n而且很短，都是很有意义的，比如我直接就是找某个商家名称，某个地方，这就是一个实体。\n所以，我们完全可以把搜索次数大于一定阈值的词条作为一个实体，而且这个实体天然就具有高质量，因为是人搜出来的。美团做了这样一件事情，把这个搜索记录和正正实体的交集作为正样本，把负实体中减去搜索记录作为负样本，做一个短语质量评分。\n这里的短语质量评分在我看来更像是一种判断实体是不是符合语言模型的标准。\n在预测的时候，我是这么想的，我们首先筛选出来正实体，然后短语质量打分挑选出高质量实体。\n在得到实体之后，我们要做的一个事情就是对实体进行分类，放到不同类别的词典中去。 这一块美团使用的autoner，这个我待定更新\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/命名体识别/词典匹配+模型预测-实体识别两大法宝/"},{"title":"","date":"2024-06-21T03:48:17.795Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.795Z","content":"多模态中各种Fusion骚操作\n大噶好，我是DASOU；\n今天继续写多模态系列文章，对多模态感兴趣的可以看我之前的文章：\n其实对于多模态来说，主要可以从三个部分去掌握它：\n\n如何获取多模态的表示【learning multimodal representations】\n如何做各个模态的融合【fusing multimodal signals at various\nlevels】\n多模态的应用【multimodal applications】\n\n今天我主要放在第二个部分，也就是各个模态的Fusion方式汇总；\nFusion做的事情简单来说就是把不同模态的信息整合为一个信息，得到一个特征向量，然后利用这个特征向量再去做下游任务；\n所以它的任务就是更深的挖掘不同模态信息同时更好的融合进最终的representation;\n我们可以把Fusion分为三种融合方式：\n\n基于简单操作的融合\nAttention-based Fusion\n双线性池化融合\n\n1. 基于简单操作的融合\nSimple Operation-based Fusion 就是说来自不同模态的特征向量可以使用很简单的方式进行整合，比如多个模态的特征向量的拼接，加权和；\n举个简单的例子，比如我们现在做一个图文双模态的分类任务，我们获取了文本特征向量和图片特征向量，那么我们可以把两个特征向量直接拼接，就当做是融合后的向量了；\n如果我认为文本的包含的信息更加的重要，图片包含的信息不是那么重要，我完全可以自定义文本特征向量权重为0.7，图片特征向量权重为0.3，然后两者的向量再concat或者做加权的和；\n其实如果我们自己最开始做一个多模态任务，最先想到的方式就应该是这种基于简单操作的方式；\n但是这个方式存在一个问题，就是两个模态之后没有做足够的交互，两者之间的联系比较弱一点；\n针对这个，我们一般会在得到concat features之后，不会直接去做分类任务，而是再接一个或者几个全连接层，让模型自动的去学习两个模态之间的关系，这样效果会更好；\n这里还有一点需要注意的是，对于concat方式，我们最好是确保文本特征向量和图片特征向量维度是固定的，这样后面接全连接层维度不会出错；\n但是有些时候我们输入的图片数量不固定，那么图片特征向量维度不一定，这个时候操作比较多，举个简单例子可以先做一个max pooling到固定维度再去和文本拼接；\n如果做加权和，我们需要确保文本和图片特征维度是相同的，这个就不多说，很好理解；\n以我自己个人经验来说，在图文多模态分类这个，使用concat这种方式，能比单一的使用文本效果提升不到2个点左右，当然case by case；\n2. Attention-based Fusion\n第一种方式我一般是在任务中作为基线，简单粗暴有提升；之后任务迭代的时候，一般都会往attention上靠一靠；\n因为concat虽然后面加上了全连接层学习两者之间关系，但是在两者的交互上来说还是有点弱的；\n对于attention的操作可以简单分为：1.Image attention；2. Symmetric attention for images and text；3. Attention in a bimodal transformer； 4. Other attention-like mechanisms；\n我详细说一下第三点，就是基于TRM的attention，因为TRM太火了；\n基于TRM的attention这块，从两个类别去理解它，一个是基于TRM的多模态预训练模型，一个是基于TRM的微调模型；\n基于TRM的多模态预训练模型，就是所借助TRM，输入是图片和文本信息，然后做预训练任务，从大量数据中学习到信息，然后得到多模态预训练模型，然后放入到下游任务中去；\n但是这些有个问题，很多人都没有大量的图文平行无监督数据，相反大家一般都有图文平行的标注数据；\n所以我们可以直接借助TRM的结构，直接做下游任务的微调就可以，这一块有个论文是facebook的MMBT；\nMMMBT其实很简单，直接看这个图：\n就是借助bert做初始化，然后图片从resent得到向量输出，一般是三个，然后拼接文本，输入到bert，直接在下游任务做微调；\n在这里我想多说几句，其实还可以直接对文本和图片之间做attention，多头或者单头都可以，其实单头就够了；\n在写代码的时候，我在遇到一个问题，就是文本和图片之间attention的矩阵化，我踩了下坑~~~；\n3. 基于双线性池化的融合办法\n双线性池化也是一个比较受重视的融合方法，不过它的问题就是在于会把n为变成n的平方，复杂度大大提升，后续的改进一般都是在降低复杂度这一块；\n双线性池化最初的操作，就是做向量的外积，获得一个矩阵，然后对矩阵做sum池化，得到特征向量，然后再去做分类；\n如果是在实际业务，大家还是优先考虑前两种吧，双线性池化这个放后一点；\n先写这么多，后续会写一个MMBT论文的解读；\n参考论文：Multimodal Intelligence: Representation Learning, Information Fusion, and Applications\nhttps://arxiv.org/pdf/1911.03977.pdf\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/多模态/多模态中各种Fusion方式汇总/"},{"title":"","date":"2024-06-21T03:48:17.825Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.825Z","content":"通篇读完这个论文，需要解决如下问题：\n\nViLBERT架构是什么样子的？\nViLBERT预训练任务是什么？\nViLBERT实现细节有哪些？\n\n我之前写了两个多模态基础的文章，没看过的同学可以先看看这两个文章：\n分别是 在解决多模态任务的时候需要重点解决的6个问题 和 如何把BERT的两种预训练任务扩展到多模态数据中去；\n1. ViLBERT架构是什么样子的？\n首先我们来聊第一个问题：ViLBERT架构是什么样子的？\n直接看图：\n\n这个图其实很不错，我简单来概述一下，如下：\n首先ViLBERT包含两个并行的流，上面的那个是图片流，下面那个是文本流；\n每个流是由一些TRM Blocks和  co-attentional TRM layers【Co-TRM】组成；\n需要注意的是TRM Blocks 和Co-TRM 可以是多层的；\n这里面最主要的部分其实就是这个Co-TRM；\n在那个虚线框中，我们可以看到Co-TRM有两个部分，真正的Co-TRM和后连接的TRM；\n首先我们要明确，从图片流前半部分【未交互之前】出来的是一个个图片regions的embeddings；\n从文本流前半部分出来的是一个个文本tokens的embeddings；【需要注意的是文本这有一个L-K X的符号，其实代表的就是构建多层的TRM，在本文就是一个BERT-Base】；\n知道各自流前半部分出来的是什么之后，就到了重头戏上的Co-TRM这个架构，直接来看论文中的图：\n\n其实这个结构很简单，就是在做attention的时候，做一些改动；\n在上面这个图片流，我的Q矩阵来自图片信息，但是我的K和V矩阵来自文本信息；\n在下面这个文本流，我的Q矩阵来自文本信息，但是我的K和V矩阵来自图片信息；\n简单说，就是做了一个在文本条件下的图片的attention和在图片条件下的文本的attention；\n也就是在文本和图片之间做了一个信息的交互；\n这里需要注意的是，在交互之后，各自走自己独立的TRM结构，而并没有拼接在一起走TRM结构；\n我自己在之前的多模态落地讲解文章中有谈到，我的baseline架构和这个很类似，只不过，我是做了双方面的attentinon之后，直接拼接接了任务相关的结构；\n2. ViLBERT预训练任务是什么？\n然后我们再来看ViLBERT预训练任务是什么？\n之前文章谈到，多模态的预训练任务从BERT演化而来，可以分为两类任务：重建任务和匹配任务；\n那么在ViLBERT也是这两类；\n重建任务就是文本重建和图片重建；\n匹配任务是是否匹配；\n需要注意的是重建任务构建的时候并么有保持另一个模态数据保持完整；匹配任务是H_cls和H_img相乘接了一个MLP做分类；\n也是直接来看图：\n\n这么看文本和图片的任务是合在一起训练了，其实从模型架构我们可以看到两个流在最后是各自分支输出的，这点需要注意；\n3. ViLBERT实现细节有哪些？\n实现细节这里其实可说的没有多，主要是ViLBERT本身的预训练和在四个下游任务进行迁移学习；\n在预训练的时候，数据使用的是330万个图像-字幕对；\n这个很有意思，相当于是一种无监督的语料，但是怎么处理文本和字母不相关的问题，因为并不是每时每刻都是相关的，想一下电视剧的情景；所以这种数据噪声估计很严重，需要清理；\n论文使用的数据来自ACL2018论文搞出来的数据，比较干净一点；\n由于担心训练时间，ViLBERT中的BERT这个流使用的是bert-base，后来发现bert-large可能会有更好的表现；\n使用FasterRCNN，通过卡阈值的方式来提取图像中的置信度比较高的候选框【10-36个】，使用 mean-pooled convolutional feature 作为这个候选区域的特征向量；\n其他的:8个TitanX GPUs / batch size of 512 /10 epochs / Adam optimizer / initial learning rates of 1e-4.\n下游任务中的几个任务：Visual Question Answering (VQA)；Grounding Referring Expressions;Caption-Based Image Retrieval;‘Zero-shot’ Caption-Based Image Retrieval;\n做了两个对比实验：\n\n第一个是使用了单流的bert-videobert；没怎么改变bert的架构；\n\n这个其实对照到文本相似度这边，其实属于交互式模型，所以这种模型存在的一个问题是没有办法很好的缓存单个文本或者单个图片的embedding，这样在做一些检索任务的时候就非常的不方面；\n为啥DSSM 架构这么有名，效果是一方面，速度更加的被大家看重；\n\n第二个实验是相同的 ViLBERT架构，但是并没有在我们的图像-字幕数据集中进行预训练；\n\n这个实验是为了 看一下 架构和预训练数据的作用，从而来证明，架构是有用的，预训练也是有用的；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/多模态/多模态之ViLBERT：双流网络，各自为王/"},{"title":"","date":"2024-06-21T03:48:17.855Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.855Z","content":"多模态资源汇总：\n实战类文章：\nhttps://github.com/DA-southampton/Tech_Aarticle\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/多模态/多模态资源汇总/"},{"title":"","date":"2024-06-21T03:48:17.915Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.915Z","content":"大家好，我是DASOU；\n回到2018年BERT刚刚横空出世，如果想快速搞一篇BERT的多模态预训练论文，应该从哪些方面去考虑呢？\n本文讲两个问题，把多模态BERT知识点串起来【绝对原创，至少我还没看到这么讲过的博文】：\n\n如何将MLM和多模态数据融合\n如何将NSP任务和多模态数据融合\n\nBERT中的大部分模块都是已经有的，它最大的作用就是证明了可以通过文本重建的方式从大量的无监督语料中获取到知识；\n那么我们现在思考的问题就是如何从多模态数据中，使用BERT的架构，学习到有用的知识；\nBERT有两个任务，一个是MLM。一个是NSP；\nMLM是做文本重建，NSP是做句间关系；\n1. 如何将MLM和多模态数据融合\nMLM我们需要从三个方面去考虑：\n\nMLM输入形式是什么？\nmask的时候需要注意什么？\n输出形式是什么，损失函数是什么？\n\n在多模态场景下，对MLM任务，需要分为两个方向，一个是对文本的重建，称之为Masked Language Modeling (MLM)，一个是对图像的重建，称之为Masked Region Modeling（MRM）；\n文本这边的MLM很简单，和BERT原始本身没区别，就不赘述了；\n有意思的是图像重建：MRM；\n首先拿到一张图片，要想把这个图片送入到TRM中去，需要的是多个图片tokens；\n有几种方式可以做到这一点，首先第一个就是将图片分为一个个的patch，这个老生常谈了，TRM在CV中的应用大部分都是这种方式；\n还有一种就是使用Faster-RCNN对图片做目标检测，获取到一个个的含有物体的regions，那么这个regions就是可以认为是一个个的tokens；\n这个时候会出现一个问题，我们思考BERT中的文本tokens的输入，不仅仅是embeddings，而且还有position embeddings；\n这是因为TRM中tokens之间是无序的，需要使用position embeddings来标明顺序；\n那么回到图像这里，用什么来标明顺序呢？一般来说使用的是Faster-RCNN中输出的regions的locations信息【5维或者是7维度】；\n仿照文本，我们需要把图片regions的表征和地理位置的表征加起来，由于维度不一致，所以加起来之前需要各自过一个全链接层；\n那么【mask】怎么去操作呢，在操作的时候需要注意什么呢？\n文本这边还是直接使用【mask】符号去mask掉子词就可以；\n那么在图片这边，直接使用全零向量替代掉mask掉的图片regions就可以了；\n这里有一个细节很有意思，在mask的时候我们有两种选择，就是文本和图片是混合mask的或者文本和图片是conditional masking；\n文本和图片是混合的，就是说明我们在mask的时候不区分图片或者文本，随机mask；\n文本和图片是conditional mask，就是说我在mask文本的时候，保持图片是完整的，在mask图片的时候，保持文本是完整的；\n这两方式哪种好呢？\n我们这么来想：\n假如你的句子中存在【苹果】这个单词，而且图片中有【苹果】这个region，那么在mask的时候，会不会存在在mask掉【苹果】这个词汇的时候，同时mask掉了【苹果】这个区域图像呢？\n肯定有概率存在这种情况。\n所以conditional mask一般来说会更好一点。\n我们在来说MLM的第三个问题，输出形式是什么或者说损失函数是什么？\n文本这边就是softmax之后找是哪一个单词，从而进行更新梯度；\n图片这边会更复杂一点，一般来说分为三种形式，这主要是对于一个图片我们可以使用三种方式描述它；\n首先第一种就是使用Faster-RCNN的ROI pooled feature去描述这个图片区域，那么我们就可以使用mask的图片区域的TRM输出的向量接一个全连接打到相同维度，和ROI pooled feature进行一个L2；\n第二个就是，比如说我现在有图片中物体类别有50个类别，那么当前图片区域的输出就可以是一个50个类别软标签（做了softmax的归一化到概率），这样可以和TRM的输出做KL散度；\n第三个是承接第二个，我们可以使用概率最大的那个作为当前区域的类别，也就是得到了一个one-hot，也就是要给硬标签，这个直接做交叉熵就可以；\n2. 多模态数据如何做NSP任务呢？\n其实很简单，NSP任务本质上是做句子间的关系，那么我们只需要类比的做一个图片和文本之间是否匹配的任务就可以了，也就是ITM任务；\nITM本质上是从文本整体和图片整体来做关系，还有的会从字和单个图片区域做关系学习，比如Word-Region Alignment (WRA) ；\n多模态这块有点乱，但是大体上就是按照MLM和NSP任务扩展到多模态数据上，这么理解会更容易一些；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/多模态/如何将多模态数据融入到BERT架构中-多模态BERT的两类预训练任务/"},{"title":"","date":"2024-06-21T03:48:17.935Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.935Z","content":"层次体系的构建\n爱奇艺短视频分类技术解析：https://www.infoq.cn/article/f49e-Gb1xQxh8DttFDgb\n一般来说弹珠模型就可以，爱奇艺这里使用了多任务联合训练的方式\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/多模态/层次体系的构建-多模态解析/"},{"title":"","date":"2024-06-21T03:48:17.975Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:17.975Z","content":"层次分类体系的必要性-多模态讲解系列(1)\n对文章的详细解读：爱奇艺短视频分类技术解析  https://www.infoq.cn/article/f49e-Gb1xQxh8DttFDgb\n这个文章首先上来就给了一个例子出来：\n这只是一个视频的抽帧，也就是一个图片。算法结果：游戏 - 题材 - 角色扮演，与人工结果一致。\n这句话其实挺重要的。如果我们不看这个图片，只是看这个文本，其实很容易会被认为是属于影视这个类别。但是我们在注意图片这个画质，其实影视一般不会是这种画质。（当然，算法给出属于游戏这个类别，很大概率是基于整个视频，我这里只讲这个图片并不全面，大家理解就可以）\n这里其实就点出来了多模态的一个作用。多模态使用不同类型的数据（文本+图片+视频抽帧），对信息进行一个补充或者说融合，从而获取视频更加全面的语义表达。\n其实这个很容易理解。我之前说过一个更加容易理解的例子。比如我们有一个博文，博文的文本内容是“这个苹果真的是太好了”。如果我们做一个单独的文本算法，我们会对此打上“科技”或者“美食”的标签。\n这个时候，如果有图片，图片内容是“真正的苹果的图片”，那么此博文的类别标签就是“美食”。如果图片内容是“苹果手机的相关内容图片”，那么这个博文很大概率就是会打上是“科技”的标签。\n我上面这个例子，其实更加的容易去理解多模态的含义。\n然后说回来，我们看它这个结果的描述：游戏 - 题材 - 角色扮演。 有没有发现一个特点，它不是单单给出了“游戏”这个标签，还给出了在游戏下面，题材属于“角色扮演”这个子标签。\n业内一般把这个叫做，一级标签/二级标签/三级标签/…\n简单来说，短视频分类体系是一种层次结构，在标签下不停的去细分子类。\n我们可以想一下这样做的好处是什么？举个简单例子，比如你最近准备考公务员，那么对你的一个短期兴趣对应的标签就是“教育”（我自己定的，可能不同公司不同分法）这个一级标签。想一下，\n这个标签有没有精准的表达你的需求？并没有，如果按照“教育”这个标签的内容推荐给你，比如除了公务员的内容，还会大量推给你“计算机培训”这种东西，你很大概率是不感兴趣的。\n所以我们需要对兴趣进行划分。\n如果深入想这个问题，还存在一个问题。\n为什么不直接构建子标签，还需要一级标签？也就是为什么构建标签体系的时候不直接一步到位，还需要一层层的细分？\n这个问题其实有很多原因？比如有历史遗留问题，在一些公司初创的时候，是没有这么多分类的，只能先划分大类。不过在这里，我给一个更加的简单的解释。就拿爱奇艺举例子吧，在头部顶栏，一般会有不同类型，比如电影，综艺，电视剧等等的划分。\n这些就是一级标签。如果不进行一级划分，大家可以想一下，怎么把那么多的细分领域让大家知道？屏幕大小是固定的，细分领域那么多，怎么确保让细节领域被看到？有的app一级领域也很好多，所以顶部栏目可以滑动，或者可以点击一个按钮叫做更多。\n所以基于一个曝光的考虑，一级标签是有必要存在的。\n其实还有一个原因我想说一下。在之前这个文章中提到的，在意图分类的框架中，我们一般是先做\n如果新增一个类别，我们重新训练模型会非常的费力。一级标签基本是固定不动的，所以我们使用一个分类模型就可以了。所以在对一个视频进行分类的时候，使用这种层次分类架构，其实是减少了计算量的。\n然后，重点来了！！！\n这个层次分类架构，大家有没有想到层序softmax类似的感觉。理论上肯定是不等价的，但是从感觉上来说，我自己觉得真的很类似，大家可以思考一下。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/多模态/层次分类体系的必要性-多模态讲解系列/"},{"title":"","date":"2024-06-21T03:48:18.015Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:18.015Z","content":"文本图像特征表示和融合-多模态讲解系列文章\n接上一个文章聊一下在多模态中文本和图像是如何做到模型特征表示和融合的。\n最直觉的一个思路是，我们分别对文本和图像进行特征建模，然后对特征使用某种方式进行融合。所以就分成两个模块：特征表示模块和特征融合模块。\n特征表示模块\n对于文本特征的表示，我们这么去想：对一个视频，我们能够想到的文本一般是：标题+简介+字幕。\n那么我么如何对文本进行建模呢？之前写一个关于句向量综述的文章，里面有介绍一部分内容。\n\n\n词袋模型（基于统计和基于词向量），这种建模问题在于忽略了词序信息，可以使用n-gram进行缓解。\n\n\n基于任务（CNN/RNN），存在的问题是迁移性较差。如果是分类网络训练出来的CNN表达的句子向量迁移到情感分类效果不会很好。然后我们分开来说，CNN存在一个问题就是对长距离处理的不是很好。\n因为它的本质是重视的n-gram内的语序信息。RNN存在的问题是训练速度慢，这没什么可说的，不能并行是硬伤。\n\n\n还有其他建模方式就不多说了。我们来看爱奇艺的处理方式，一句话简单描述是“采用的是 BOW 和 CNN+Attention 方式完成文本表示的建模”\nBow使用一些人工特征加n-gram缓解自带的问题。CNN使用两个优化，提取信息使用一定步长的pooling，然后基于这个带有文本信息的表达做self-attention。\n对于短视频来说的图像表示是什么？是封面。封面一般是从短视频精选出来的一帧，一定程度可以对文本信息进行补充。\n对图像进行特征的抽取一般是有三种方式：\n\n直接抽取特征\n实现方式：把 ImageNet 预训练的模型作为特征抽取器，将模型的某一层或者某几层特征作为类型标签模型特征提取源。缺点是效果比较差。\n\n一般对应到NLP，大家可以想一下我们直接用Bert抽取出来的词向量做文本分类，效果也比较差。\n\nfinetune+抽取特征\n把 ImageNet 预训练的模型以类型标签为目标进行 FineTune，然后将模型的某一层或者某几层特征作为类型标签模型特征提取源（因训练目标一致，一般选择最后一层即可达到较好的效果）。\n\n大家仔细琢磨一下这个过程。如果我先在要做一个文本分类的任务，想使用LR做一个baseline。那么我的输入可以是这样的，使用bert对我要使用分类数据（注意是和LR一样的训练数据）进行FineTune\n，然后使用这个模型做特征的抽取。\nFineTune的任务和我LR要做的是一样的，那么bert抽取的特具有充足的意义表达，能够很好的迁移过来。\n基于此，大家可以想一下，如果我使用bert做了文本情感分析的FineTune，然后抽取的特征做文本分类，效果会好吗？想一下。\n这还有一个问题，从bert抽取出来的特征，我们需不需要随着模型进行修改？？？\n\n把 ImageNet 预训练的模型嵌入到类型标签的模型当中，让图像的表示和其他特征的表示同时进行训练。\n\n其实这种方式缺点很明显，耗时太大了，有种尾大不掉的感觉。\n爱奇艺选择的第二种，模型选择是 Xception进行特征的抽取。\n接下来我们聊一下文本和图像的特征融合怎么做，也就是文本图像的特征怎么联系在一起？\n我给大家两个思路，第一个就是直接concat，然后接你的各种网络。第二种是两个特征模块做 attention，这个更常见一点。\n爱奇艺还给出其他两种方式CentralNet 和LMF，我没了解过，就不多说了。\n对于attention，一般来说做双向的attention更合适一点。也就是说做一个文本到图像的attention，然后做一个图像到文本的attention，两者再concat，效果会更好。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/多模态/文本和图像特征表示模块详解-多模态讲解系列/"},{"title":"","date":"2024-06-21T03:48:18.215Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:18.215Z","content":"今天分享一个论文Text Classification with Negative Supervision；\n论文思路比较简单，可以用一句话来说明就是：使用负监督+多任务的方式，可以扩大文本表达在输入语句语义相似的情况下的差异性。\n接下来详细说一下。\n1. 语义相似标签不同的问题\n文本分类中存在这样一种情况，有两个句子：\n\n句子A：感冒是个常见的疾病。这句话在标签数据中属于无标签数据。\n句子B: 我得了感冒。这句话在标签数据中数据标签为【感冒】的类别。\n但是我们的文本分类器，把两句话都分为了【感冒】这个类别。\n对于这种现象，可能是由于数据不充足，可能是由于其他原因，结果就是导致两个句子的【CLS】输出向量很接近，之后接一个分类器，分出的类别就是同一个。\n作者想解决的一个问题就是，想要通过一种方式，让模型知道，这两句话语义是不相似的。\n这个方式归到【ClS】这里，就是想要两个句子的【CLS】的输出是区分度大的。\n现在问题落在了了如何度量【CLS】的区分度？\n我读论文的时候第一想法是用KL散度或者交叉熵，然后发现作者使用的是两个【CLS】向量的余弦相似度进行度量。\n模型架构\n先总览一下模型架构：\n\n这个架构其实很容易理解，初看命名Discriminator以为作者用的是对抗网络。。。\n作者使用的是一个很简单的多任务架构，分为了两个任务：\n\nMain Task：主要任务，做常规的文本分类任务\nAuxiliary Task：辅助任务，输入负样本（不同类别的样本），计算余弦下相似度。\n\n总体的损失函数如下：\n\n辅助任务损失函数：\n\n其实比较细节的一个点是辅助任务的输入样本是什么样子的。\n我们的本质是为了度量语义相似性的句子之间的文本表达向量尽可能的大。\n一个很朴素的想法就是，辅助任务中输入的是同类标签的数据，然后同类标签的输出向量和主要任务的输出向量做相似度度量，计算损失。\n但是，想一下这个过程，有没有解决作者最想解决的问题。\n我们会看一下最初的例子，两个句子是虽然有着相近的意思，但是有着不同的标签。所以我们这个关于辅助任务的输入样本的选择是有问题的。\n作者这边选择的是，与主要任务输入样本不同的标签数据作为辅助任务的输入，然后进行相似度的度量。\n当然作者又细分了两种模式：\n\nAAN：辅助任务输入的全部是与主要任务不同的标签数据\nAM：辅助任务包含一个与主要任务相同标签的数据，剩下的是不同标签数据。\n\n结果分析\n直接来看结果分析：\n\n我其实比较感兴趣的是ACE这个为啥不行？按道理交叉熵应该也可以吧。\n作者的解释是，ACE效果不可以，恰恰说明了简单的多任务是不可行的，基于负监督的多任务是很必要的。\n总结\n总结一下从这个论文学到的东西，主要是就是一点，对使用不同标签数据的【CLS】输出向量进行余弦相似度的损失计算（作为多任务的辅助任务），可以提升表达向量的差异化。\n这个思路用在普通的编码器，应该也是适用的，感兴趣的可以试试。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/ACL2020-多任务负监督方式增加CLS表达差异性/"},{"title":"","date":"2024-06-21T03:48:18.245Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:18.245Z","content":"TextCNN中的卷积核在文本进行处理的时候，是在文本长度上进行卷积。卷积核的大小\n不同，带来的直接后果是这个卷积核每次滑动的时候处理的单词长度不同。\n卷积核大小为2的时候，一次处理2-gram。卷积核大小为3-gram，一次处理三个大小的单词。\n所以卷积核在对文本进行卷积的操作，更像是对在提取文本在n-gram上的特征。卷积核权重的更新\n只是为了能够更好的提取n-gram上的特征。卷积核权重的更新\n大小为2的卷积核提取的是2-gram特征，大小为3的卷积核提取的是3-gram特征，以此类推。\n取不同卷积核大小进行卷积操作的原因，我的理解是可以提取这个句子多个维度不同的信息，使得特征更加的丰富。\n还有一点需要去注意的是，以2-gram为例，每次都是提取两个单词文本，但是如果文本很长，最后两个字和最开始的维度的单词\n联系就很小，唯一的联系就是卷积核的权重是共享的。\n举个例子：\n今天天气不错，适合出去旅游\n在这句话中，如果卷积核大小为2，我们这里不考虑中文分词，那么今天 天天 两个词组中间出了有卷积核权重的联系还有天这个单词的共有性。\n但是今天和旅游两个单词联系性在CNN中并没有体现出来。\n这也就是为什么CNN不适合处理长文本的原因。\n卷积之后，接了一个最大池化。论文中给出的原因是因为输入句子长度不一定，经过卷积之后长度不一定，\n如果直接操作的话，后面的全连接层权重形状不固定，不利于训练。\n其实感觉这一点站不住脚，处理文本的时候，一般会固定长度，阶段长度，不存在卷积之后大小不一定的原因。\n但是如果我们在处理文本的时候，没有截断长度，而是排序然后按照batch中长读补长，是存在上述问题的，所以需要最大池化。\n上面这个原因感觉是最重要的，其实还有一个原因，论文中是说想要获取一个卷积核提取出来特征中的最重要的特征。我的可理解是\n这个原因不太好，因为我直接用所有特征肯定比选取其中一个最重要的效果是好的。\n论文中把一个卷积核抽取特征，然后接一个最大池化的操作，形象的比喻为一个卷积核抽取一个特征。\n有一个人把特点总结的很到位，叫做CNN的卷积核实现了捕捉局部相关性\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/CNN文本分类解读/"},{"title":"","date":"2024-06-21T03:48:18.465Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:18.465Z","content":"今天分享一个论文LCM；这个论文掌握以下几点，使用LCM模型：\n\n可以捕捉标签与标签之间的关系\n可以捕捉标签和样本之间的关系\n在噪声数据集，效果比LS要好\n\n1. 文本分类普遍存在一个问题\n深度学习模型进行文本分类有一个共性：\n\n\n首先使用一个比较深的模型去做text representation；\n\n\n然后使用一个简单的分类层（比如全连接）去预测标签分布；\n\n\n之后计算预测标签分布和真实one-hot标签向量的交叉熵。\n\n\n这个流程其实是有问题的；\n从标注规则来看，使用one-hot的前提是假设你的数据集中的标签是相互独立的。\n但是这种假设在现实中基本不会有，只是或多或少，有的界限比较清晰，有的不清晰的问题而已；\n还有一个问题从样本来看，如果是单标签分类，同一个样本真实情况下可能对应多个类别。\n比如【今天去公园野炊一下，吃点烧烤呗】；类别可能是【美食】，也可能是【旅游】，也可能是其他的类别。\n这个可能并不明显，我举个最明显的例子：【郭麒麟相声说的是真棒啊，综艺是真好看啊，综艺感真实爆棚了】；\n上面这个例子，你说它的类别是【相声】？【综艺】？【娱乐明星】？\n还有一个问题，就是标注错误的问题。这种情况一般使用标签平滑。\n标签平滑让真实标签不那么极端化，给与标签一定的容错概率。\n但是标签平滑本质上加了一个噪声，并不是真实反映标签的分布情况。\n从这出发，就可以看出下面LCM主要去解决以下问题：\n\n\n标签之间并不相互独立，所以我们需要一种方式能够度量标签之间的关系\n\n\n样本可能对应多个标签，所以我们需要一种方式能够度量样本和每个标签之间的关系\n\n\n标签可能标注错误，所以我们尽量不适用one-hot硬标签，而是使用软化之后的标签。\n\n\n2. LCM-架构图\n先来看架构图\n\n架构图最核心的部分注意看紫色的Similarity Layer层，这一层主要做的是对经过深度学习模型学到的句子表达和label的表达进行相似性度量。\n然后把这个相似性的度量加到one-hot标签中。\n看一下公式就明白了，左半部分比较简单，就不说了，看右半部分：\n\n$f^{L}$是对labels进行encode，得到每个label的表达向量，方便和句子向量做 dot product。\n注意图中的参数$\\alpha$，代表了相似性这个信息对原始标签的影响。\n损失函数使用的是KL散度\n3. 实验结果\n方法有效，就不放实验图了。\n我比较感兴趣的是 label embedding究竟有没有学到相似性，看图：\n\n不同颜色代表将类别根据语义分为不同的组，可以看到同个颜色的labels很大情况下还是挨得很近的。\n说明架构图有半部分的下半部分，也就是那个label encoder确实是有作用的。\n还比较感兴趣的是LCM和标签平滑的对比，看图确实比LS更好一点：\n\n总结\n简单总结一下，\n\nLCM挖掘了标签之间的关系和标签与样本之间的关系\n样本数据如果噪声标签（标注错误），LCM有效\n样本数据的标签如果界限比较模糊（根据语义可以划分为多个组），LCM有效\n\n整个论文最核心的点，我认为是对lables做了编码，从而有机会去和句子编码进行交互，度量相似性。并将整个相似性信息加入到了原始标签中。\n所以网络在训练的时候，学习到的信息更加的丰富。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/LCM-缓解标签不独立以及标注错误的问题/"},{"title":"","date":"2024-06-21T03:48:18.075Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:18.075Z","content":"对比学习 学习笔记：\nMoco论文解读细节：\nMoco 论文简单阐述\nMoco是视觉领域使用对比学习一个里程碑的工作；对比学习从2019年开始到现在一直都比较火，；\nMoco作为一个无监督的表征工作，不仅在分类任别务上逼近了有监督的基线模型，在其他任务，检测，分割，人体关键点检测都超越了有监督的预训练模型，也就是imagenet上的预训练模型；\nMoco证明了一点，无监督学习真的可以，我们并不需要大量的标注好的数据；\n什么是对比学习？\n首先说对比学习想要做到一点是什么呢？我们现在有三张图，第一个图是人高兴，第二个图片是人悲伤，第三个图片是狗。\n我们想得到一个一个结果，就是我们不需要知道前两个图片是人这个类别，不需要知道第三个图片是狗这个类别。但是我们能够需知道前两个图片是一个类别，第三张图片是不是一个类别。\n\n换句话说，我们现在把三个图片过一个模型，我们得到三个表征，我们需要让这个三个表征在特征空间中，前两个图片的表征距离比较近，第三个图片和他们的距离比较远。\n\n一句话说，我们希望在特征空间里，同一个类别的物体在特征空间中在相邻的区域，不同类别的物体在特征空间中不相邻的区域。\n在这个过程中，我们需要知道的是，我们并没有用到标签信息，我们不需要第一个和第二个图片是人，第三个是狗。\n但是我们用到了另外一种信息，就是第一个图片和第二个图片是同一个类别，第三个通篇不是同一个类别这个信息。这其实也是一种标签信息。\n不过这种标签信息，我们可以使用一些代理任务，巧妙构造出来的这种信息，而不需要人为的去标注这种标签信息。这些代理任务，会去定义一些规则，这些规则可以去定义哪些图片是相似的，哪些图片是不相似的，从而可以提供一些监督信号给到模型去训练。这个过程其实也是自监督训练的一个过程。\n个体判别代理任务\n一个最经典的代理任务就是：instance discrimination。叫做个体判别\n这个代理任务，就是如果我们有一个没有标注的数据集，里面有n个图片。\n从这个数据集中，我们随机选择一个图片，xi；在这个图片上我们做随机裁剪（或者其他的数据增广操作，我们称之为traansformation）；从而得到另外两张图；\n一个是xi1 一个是xi2；这样我们会得到两个不太一样的照片。但是由于这两个图片是从同一个图片经过某种变化得到的，语义信息不应该发生变化。所以这两个图片就可以称之为正样本，也就是同一个类别的图片。\n这个代理任务，同时认为，这个数据集中剩余的所有图片都是负样本\n\n为什么叫做个体判别呢？因为它认为每个图片自成一个类别，剩余的图片都不是同一个类别。\n（这个粒度其实是很细，你在图片分类的时候是很多照片是同一个类别，其余的照片又分为了很多类别，所以个体判别这个代理任务经过模型训练，表征会很细）\n对于imgenet这数据集来说，如果个体判别任务，就是一千个类别，而是100多万个类别。\n所以个体判别这个代理任务定义了什么是正样本，什么负样本，接下来就很简单了，我们只需要经过模型，然后做一个对比学习的函数去训练模型就可以了。比如说NCEloss\n\n在这个过程中，其实有一个很有意思的点，就是代理任务是多样性的，是很灵活的。只要你能够得到一个判断正样本和负样本的规律，后续的损失函数之类的训练就很常规了。\n比如说在视频领域，同一个视频里的任意两帧是正样本，其他视频里的帧是负样本；NLP中的simcse，你可以通过dropout判断不同句子。\n精读Moco论文\nMomentum Contrast\nMoco这个名字就是来源于前两个单词的前两个字母，就是基于动量的对比学习。\n动量是一种加权移动平均；\n![image-20220813170958764](/Users/zida/Library/Application%20Support/typora-u](…/image-20220813170958764.png)\ny(t-1)是上一个时刻的输出，m是动量超参数，xt是当前时刻的输入。\n说白了，就是不想让我当前时刻的输出只是依赖于我当前时刻的输入，我还希望和什么有关系呢？和之前时刻的输出有关系。动量这个超参数是0-1的一个参数；如果m是趋近于1的一个数，那么我的yt改变是非常缓慢的。\n因为（1-m）是趋近于零的。\nMoco是利用这个动量的特性，去缓慢的更新这个编码器，从而让中间学习到的字典特征尽可能保持的一致（这句话没看懂没关系，一会详细讲）\nMoco摘要部分：\nMoco把对比学习看成了是一个字典查询的东西，他们做了一个动态的字典，这个动态的字典分为两个部分，第一个部分是我们有一个队列，第二个部分是我们有一个移动平均的编码器。\n队列里的样本呢，我们不需要做到梯度回传，所以我们可以往队列里放很多的负样本，从而让字典很大。\n为什么还有一个移动平均的编码器呢，我们是想让字典里的特征尽可能的保持一致。\n在训练过程中，我们发现，如果你有一个很大而且特征比较一致的字典，会让这个无监督的对比学习学的很好。\nMoco从结果来说，在imagenet数据集上，如果采用linear pro去测试，Moco是可以取得和之前最好的无监督方式差不多或者更好的结果；linear pro指的是，我先预训练好一个骨干模型，然后我把这个骨干网络冻住，只取学习最后的全连接层，然后看在不同数据集上的表现结果。这样其实类似于把骨干网络当成了一个特征提取器，只从这里提取特征，这其实和我们使用resne差不多。\nMoco一个很大的卖点，我们学习到的特征，在下游任务上有很好的迁移性，我们看重无监督优点就是它可以从大量无标注上的数据上学习到特征，可以迁移到没有那么多标注数据的任务上。\nMoco在7个下游任务，分割，检测之类的超越之前的有监督预训练模型；举个例子，Moco使用同样的Resnet50，去做无监督，然后和有监督训练的模型去做比较。\n引言部分：\nGPT和BERT，已经证明无监督学习在NLP任务上是行得通的。但是CV领域，有监督预训练还是占据主导地位；\n之前也有很多优秀的无监督工作，但是表现都会比无监督要差，作者认为这是因为CV领域NLP领域不同的原始信号空间。\n对于自然原因来说，他们是离散的信号，也就是原始的信号空间，是有单词组成，或者更细一点，是由单词词缀组成的，所我们可以很容的去建立一个字典，然后让模型去学习特征。那么字典中的每个key就是一个类别，我们可以根据这个类别去学习模型（比如BERT就是最后一个softmax操作吗，不就是分类操作吗）\n但是对于CV领域来讲，完全不一样。CV领域的信号是在一个连续而且高维的空间，它并不像单词那样有很强的的语义信息而且浓缩的非常好，没有那么简洁；所以CV领域并不适合去建立一个字典，去学习模型；如果没有这个字典，无监督就很难去建模。所以在CV领域，出现无监督还不如有监督学习。\n在之前有很多优秀的对比学习工作，都可以归纳为一种字典查询的工作。\n我们之前来看图：\n\n两个编码器，一个是E11，一个是E12；然后我们x1这个图片经过数据增强T1得到的图片X11，然后经过E11这个编码器，得到了图片表征f11;同理，我们这个图片x1，经过数据增强T2，得到的图片x12，然后经过E12这个编码器，得到了f12这个图片。\n我们把X11这个图片叫做archor，瞄点，x12叫做x11的正样本。\n什么是负样本呢？就是图片里剩余的所有的图片都是负样本，那么负样本走哪个编码器呢？走的是E12这个编码器，因为我们正样本和负样本我们都是相对于瞄点来说的，所以正样本和负样本要走同一个编码器，从而让特征的获取过程保持一致性。于是这样负样本x2，x3，x4等等也经过E12得到了真正的负样本表征f2,f3,fn；\n那么我们把f11叫做query，把f12,f2，f3，fn叫做key；\n那么对比学习的过程就是想要在特征空间里，正样本的key和我query近，其余的key离我远。\n我们其实可以把key集合看成字典。那么对比学习的过程，就是想得到一个模型，让query在字典中那个和自己匹配正样本更近。\n如果把对比学习的过程看成一个动态字典的过程，如果想要得到一个比较好的效果，那么字典最好需要满足两个条件，第一个就是字典足够的大，第二个就是在训练的时候尽量保持一致性。\n首先第一个我们在做对比学习的时候，肯定不是一个batch一个batch的去做，所以如果key这个字典足够的大，那么我们从中抽样的可能性组合就很大，那么模型的泛化性就很大。如果字典很少，泛化性就不足，相当于数据来那个不够。\n第二个是保持一致性，是因为我们需要字典中的特征尽可能使用同一个或者相近的编码器进行表征。因为如果不这样做。那么模型可能就学习到和query使用同样的编码器的那个key，导致模型泛化性不足，走了捷径。\n所以Moco要做的就是一句话：在对比学习框架中，提供一个又大又一致的字典；框架图如下：\n\n大字典是怎么做到的：维护一个队列，把每次训练的batch-size和队列大小分离开；具体来说就是这个队列可以很大，但是我们每次更新这个队列，是一点点的更新的，也就是说当我们用一个很小的batchsize的时候，那么我们把现在batch中的特征进入队列，把最老的batch-size的特征抽离队列；那么我们的队列就可以设置的很大，比几万。这样我们用一个GPU也可以很好的训练模型；\n那么一致性是如何做到的？刚才说了，每次都是使用新的编码器更新batch大小的队列特征，除了这个之外的，我们都是使用的之前的编码器得到的，这不就不一致了吗？那么就用动量更新就可以，我们最开始的右边分支的编码是由左边的初始化而来，后续更新使用对右边这个编码器参数进行动量更新，m足够大，保障右边编码器更新的非常缓慢，从公式来说，就是这个图：\n\n可以看到，右边编码器会被之前的k编码，和当前时刻的q编码影响，m足够大，无限接近于1，那么就是可以认为无限被k控制，更新的就会非常缓慢。\n（有个疑问，直接不更新不就可以了吗，不进行梯度回传？）\nMoco只是建立中间模型的一个方式，是很灵活的，可以和很多代理任务结合，这里使用个体判别，之前讲过。\n无监督最大的一个卖点，就是我们的模型在大量无标注的数据集上进行训练之后，我们得到的特征，可以很好的迁移到下游任务中（比如标注数据很少的任务中）；\nMoco结论部分：\nMoco论文在imagenet得到了很好的结果，然后在自己facebook自己数据集是上也得到了很好的结果，但是提升不大，在数据集从100万到10个亿，提升不大，作者认为大规模数据没有被利用起来，可能一个更好的代理任务会有更好的效果。所以作者谈到，除了个体判别这个任务，有没有可能把moco和mask encoded这个任务结合起来，就是类似BERT这种操作，使用mlm自监督的方式去学习。（这不就是MAE模型吗，我之前讲过）；\n这个其实在开头有讲CV和NLP信号空间不一致，直接把bert方式搬过来，可能不太行，具体去看MAE模型；\nMoco相关工作部分：\n一般来说自监督可以有两部分可以去做，一个是在损失函数部分深挖，一个是在代理任务上做文章。\n（注解：自监督学习是无监督学习的一种）\nNCE损失函数把一个超级大的多分类（这个时候softmax是工作不了，计算量太大）转变成一系列的二分类问题，从而让大家可以正常使用softmax，（这个是w2c很类似）\nInfoNCE是NCE的一个变体，如果只\n温度超参数\n在看INfoNCE 的损失函数的时候，首先从softmax看起，这个是softmax的公式：\n\n然后我们加一个-log就是交叉熵损失函数：\n\n这个公式其实可以直接用在对比学习中。\n什么意思呢？\n交叉熵是一个多分类问题的损失函数，一个one-hot向量，和我真实输出做一个损失，目标是为了让真正标签的输出尽可能的大。\n那么有一个问题，如果把这个损失函数，直接套到对比学习中去，那么是什么意义呢？\n比如imagenet100万个图片，那么我当前图片的这个数据增强之后的图片经过编码器1得到了瞄点特征，经过比编码器2得到了正样本，也就是我的groud-turth；\n那么除了我当前这个图片，100万个图片之外的所有图片经过编码器2这个得到的表征都是负样本，也就是会得到这样一个向量：\n1 0 0 0 （1个1,100万-1个0）\n在这个上面我做交叉熵，其实就是可以用在对比学习上。\n但是这样做softamx计算量太大了，其实bert这种模型，也就是几万个类别，没啥问题，几百万太难了。\n这个时候NCE就是一种很好的解决方式，化成一个二分类问题，就是我现在只有两个类别，一个是正常样本，除此之外的都是噪声样本。（计算量没降低下来，这个我待定在看词向量的时候再去看）\n但是这样做不太清楚，所以INFONCE就出来了。\n也是与其你在整个数据集去走loss，不如我抽样一部分去做loss。如果你选取的抽样的这部分很少，那么就没啥意义，不能模拟整个数据集，所以抽样的部分还是要大一点。那么这个字典的大小就很重要，也就是我字典的大小就是我们的分母下方的类别数量；那么这个过程中InfoNCE就把NCE的一系列二分类又转为了多分类。\n\nq就是我query表征，也就是瞄点那个图片特征，k+就是正样本，分母累加那里的K，就是我们的负样本数量，分类累加了K+1，因为K个负样本+一个正样本。\n温度参数T（其实是tao），在蒸馏那里其实我讲过，如果t很大，那么softmax分布会很平滑，看不出区别，就是把所有的负样本一视同仁，导致模型学习没有轻重；如果tao很小，分布会更尖锐，会让模型只关注那个困难的负样本，其实那些负样本很有可能是潜在的正样本，如果模型过度的关注这个困难的负样本，会导致模型很难收敛，或者学号的特征不太好去泛化。\n去除这个温度超参数，InfoNCE本质就是一个交叉熵损失函数，只不过类别和所有样本相比，做了个近似，做了个个随机抽样，就是字典大小。Moco伪代码InfoNCE直接就是用的交叉熵损失函数代码。\n有个细节，为什么使用队列这种数据结构存储字典呢？\n因为先进先出，每次一个batch进来，最老的那个部分batch数据会出去，这部分数据是过时的，从而能够保持队列中的特征尽可能的一致性。\n另一个细节：\n第二个分支不能随着走这一支的样本使用梯度回传进行更新，为什么呢？因为如果这样做了，第二个分支的编码器就更新的太快了，这些特征，我们是放到字典中去的，就会导致特征不一致。\n为什么第二个分支直接就不更新，反而还缓慢更新（我自己理解是不太可以的，因为正样本的定义规则，经过编码器之后语义空间类似，所以是正样本。如果第二个分支一直不变，其实模型在训练的时候就很样本，因为可能到后来，第一个分支和第二个分支编码器差距越来越大，其实是本来是正样本的，损失也很大，就很难训练了。）\n两个贡献：\n一个是很大的字典：计算损失的时候使用多分类，能够很近似整个数据集上做多分类损失\n一个是字典内特征一致性，使用动量更新：\n需要注意的一点：就是infonce损失计算的是整个字典做多分类。minibatch大小和字典大小剥离开，batch可以设置为256，然后进来256个样本，每个样本都需要做一个瞄点，走一遍对比学习的流程。\n动量设置为了0.99，很大了。字典大小是65536\n在Moco之前的工作， 字典和字典特征一致性经常不能同时满足。\n端到端的框架：\n\n端到端的框架就是两个编码器都可以通过梯度回传进行更新，因为xq和xk都是从同一个batch中来的，我们通过一次forward就可以拿到所有样本的特征，我们直接梯度回传就可以了。这要求，我们batc大小要足够的大，那么infonce才能起作用，做到一个近似的全部数据集的多分类。SIMCLR就是这个端到端。这样字典是高度一致的。在这种情况下，batch大小和字典大小是等价的。simclr就是用了8192作为batch大小。\n另一流派，更关注字典的大，然后牺牲一些一致性，就是memory bank；在这个流派只有一个编码器，就是query的编码器，可以进行梯度回传进行更新。对于query这边，是没有一个单独的编码器。\nmemory bank就是把整个数据集的特征，都存到了一起。对于imagenet来说，这里就是128万个特征（作者说到，每个特征128维度，只需要600M的空间，还好。）\n然后每次训练的时候，从memroy bank中随机抽样字典大小就可以了。右边的编码是在线下执行。\n在执行的时候，比如字典是3，那么抽出三个来，左边做一次梯度回传之后，我们把字典中的3个用新的编码器做一个编码，放回到memroy bankl中去。\n（首先，我认为为了保持正样本的定义，肯定得更新样本特征）\n因为你这个更新操作，导致字典内编码特征不一致。\n\nMoco伪代码，讲解的非常好：\n几个中点：\n第一个动量是0.99，字典大小是65536\n第二个是损失函数底下类别是65536+1个=65537，是把所有字典中的都是当成了负样本（这样其实很有可能存在潜在的正样本，不过影响不大 ，一定要注意，这个时候我这次更新的样本特征，还没有放入到字典中去，所以仅仅是可能存在正样本，当前这正样本是一定不在字典中的）。\n参考：https://www.bilibili.com/video/BV1C3411s7t9/?spm_id_from=333.788\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/对比学习/Moco1论文解析/"},{"title":"","date":"2024-06-21T03:48:18.725Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:18.725Z","content":"文本分类\n\n\n文本分类资源总结\n\n\n\n\n\n多分类模型Accuracy, Precision, Recall和F1-score的超级无敌深入探讨\n\n\n\n\n\n\n\n\n\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/README/"},{"title":"","date":"2024-06-21T03:48:19.465Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.465Z","content":"在知乎看到这样一个问题【如何将关键词信息融入到文本分类任务】，简单说一下自己的经验，供大家参考；\n首先说，现在基本各组都有自己的关键词词库，构造方法也都基本上相似。\n简单点的就是TF-IDF筛选，复杂的就是构建挖掘特征，关键词二分类模型；\n基于此，大家一般也会加上新词发现+实体挖掘进行候选词库的补充；\n然后我们再来说，如何把关键词信息融入到文本分类任务中去。\n如果说关键词类别未知，这种情况不常见，但是也会有，一般是两种处理方式。\n一种是直接拼接在文本后面，增强信息，很常见。\n举个例子【今天出去旅游吗】，关键词是【旅游】，文本输入就是【今天出去旅游吗旅游】\n另一种是将关键词构造维稀疏特征加入到文本中去，缺点就是维度会比较高；\n如果说关键词类别已知，这种场景比较常见；\n先说个题外话，在挖掘语料的时候，关键词匹配挖掘语料是一个很常见的手段，但是容易造成语料太过简单单一+语料噪声比价大，所以冷启动的情况下，可以用关键词挖掘语料，之后还是上一批人工的标注会好一点；\n关键词类别已知的情况下，也可以使用两种方式来融入到文本分类任务中去；\n第一种就是，把关键词往上抽象化，转为对应的类别，然后作为特征结合文本输入到网络中去；\n第二种，也是我比较常用的就是对文本分类之后，对文本做关键词匹配，对应类别提升分值，简单说加规则，这个手段有点不好控制的地方就是分值的确定。\n但是我为啥爱用呢？最大的原因就是容易在和运营讲道理【撕】的时候获胜，百试不爽~~~\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/关键词信息如何融入到文本分类任务中/"},{"title":"","date":"2024-06-21T03:48:19.495Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.495Z","content":"伪标签，是啥？\n今天分享的论文是 [Pseudo-Label](“The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks”)\n从这个论文，主要是解决三个知识点：\n\n什么是伪标签\n怎么使用伪标签\n伪标签为啥有用\n\n伪标签\n先说第一个问题，假设我们现在有一个文本分类模型（先不用管分类模型是怎么来的以及怎么训练的），以及大量的无标注数据。\n我们现在使用文本分类模型对无标注数据进行预测，挑选softmax之后概率最大的那个类别为当前无标注数据对应的标签。\n因为是无标注数据而且我们模型准确不可能是百分之百，从而导致预测的这个标签我们并不清楚是不是精准，所以我们称之为&quot;伪标签&quot;。\n怎么使用伪标签\n“伪标签”可以帮助模型学习到无标注数据中隐藏的信息。\n我们先来看模型的损失函数是如何定义的：\n\n公式的前半部分针对的是标签数据的损失。我们重点来看后半部分伪标签的损失函数。\n$C$ 是类别数目，$n^{,}$ 是batch数据中伪标签（无标注）数据的数量大小。$y^{,m}$ 是无标注数据的伪标签，$f^{,m}$是无标注数据的输出。$\\alpha(t)$是未标注数据的权重，更新如下：\n\n这个更新公式值得看看，从这里可以看到，在$T_{1}$ steps之前，只是在训练数据上进行训练。随着模型的训练，无标注数据的损失函数权重在慢慢的增加。\n简单来说，就是模型现在标注数据上进行训练，到一定steps之后，开始使用无标签数据的损失函数。\n伪标签为啥有用\n其实，从上面这个损失函数，最好奇的一点就是为什么我加了后半部分的无标签数据的损失之后（也就是在训练的时候使用无标签数据的伪标签计算损失之后），模型的表现会比只是使用标签数据要好。\n损失函数的第二项，利用了熵最小化的思想。\n从形式上来看，它的这个损失是在强迫模型在无标签数据上的输出更加的集中，逼近其中的一个类别，从而使得伪标签数据的熵最小。\n在这个过程中，什么时候加入对伪标签的考量就很重要，因为如果太早的话，模型在训练数据上训练的并不是很好，那么模型在预测数据上的输出置信度其实就很低，误差会慢慢积累变大。\n所以$\\alpha(t)$是一个很重要的部分。\n总结\n伪标签在我的理解中，就是在模型已经训练的还可以的时候，对无标签数据进行预测，我们通过损失函数，让无标签数据逼近其中某一类（其实本质也是在做GT的文本分类）\n想一下，Bert在小样本上进行finetuning之后，我们也可以把它放在无标签数据上直接预测。\n由于Bert强大的能力，这样预测出来的标签置信度是很高的，我们一般可以直接拿这个结果作为冷启动的一部分。\n伪标签数据半监督入门的思想，之后有时间会慢慢深入的分享几个论文。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/半监督入门思想之伪标签/"},{"title":"","date":"2024-06-21T03:48:19.045Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.045Z","content":"今天分享一个论文，UDA，效果惊人：通过UDA，在IMDb文本分类数据集上，使用20个标签数据，相当于使用25000个标签数据。\n先说一个概念，贯穿在整个论文：consistency training\n直译过来就是一致性训练，我自己的理解就是，对于无标注数据，加入噪声，标签不变（或者说数据代表的含义没有发生太大的变化）\nUDA这个论文就是做了一个事情，验证监督学习中的数据增强方式放在半监督中作为一种噪声输入是有效的，是可以提升模型表现能力的。\n1. UDA\n先说有监督情况下的数据增强\n数据增强的目的是通过对示例进行转换而无需更改其标签，从而扩充数据。\n然后说一下半监督学习本质上在解决一个什么问题？\n我的数据中，有标注数据和无标注数据，半监督学习本质上是在从无标注数据上学下一个信息和知识从而使得模型效果更好，更佳的平滑和健壮。\n一般的半监督模式是这样的：\n\n输入数据为$x$，分别计算两个东西，原本的输出分布：$p_{\\theta}(y|x))$，输入数据注入噪声之后的输出分布：$p_{\\theta}(y|x,\\varepsilon)$。\n最小化两个差异之间的度量:$D(p_{\\theta}(y|x)) || p_{\\theta}(y|x,\\varepsilon))$\n\n此过程使模型对噪声不敏感，因此相对于输入（或隐藏）空间的变化更平滑。从另一个角度看，将一致性损失降至最低会逐渐将标签信息从已标记的示例传播到未标记的示例。\n这句话我是这理解的，输入空间越平滑，输入向量发生细微的变化，也就是加入扰动或者说噪声之后，代表的含义没咋变，标签就没咋变。\n之前的一些工作，为了达到上述这点，也就是为了增强一致性，通常采用简单的噪声注入方法，例如将高斯噪声，简单的输入增强添加到未标记噪声的示例中。\n与之相反，这里使用的是将监督学习中的数据增强的方法在这里作为噪声加入到原始数据中。\n在监督学习中，数据增强的做法的一个前提是，增强之后，标签不变。\n半监督中，我们加入噪声的目的是为了，加入噪声之后，原始数据的输出和加入噪声之后的输入向量的度量差异越小越好，越接近越好。\n这两个其实可以很类似，这也是我觉得谷歌在这个论文 中探讨的一点，就是监督学习中的数据增强可不可以作为一种半监督中的噪声扰动。\n先来看损失函数：\n\n重点是后半部分，是在无标签数据上的一致性损失函数。需要注意，后半部分的参数是固定的，从前部分模型直接复制过来的，简单来说就是不参与训练。\n整体架构如下：\n\n2. 训练技巧\n2.1 数据增强\n我以NLP中的任务为例：\n\n回译\n注意些主题分类任务的时候，有些单词很重要，通过TF-IDF来保留这些单词。\n\n这里其实很想说一下这个TF-IDF替换词这个东西，在附录里找到，好好的翻看了一下。\n概括一样是做了两个步骤（不知道我有没有理解准确）：\n\n对于句子中的每个单词计算替换概率\n替换的时候是从语料中抽取单词（简单粗暴），所以还计算了一个语料中每个单词的是不是关键词的概率。如果是关键词，那么就不能要，因为如果替换到当前的句子，可能让当前句子的类别发生改变。\n\n具体的计算公式，大家可以去看一下原论文。\n2.2 Confidence-based masking\n我们发现掩盖当前模型不确定的示例会有所帮助。具体而言，在每个小批量中，仅对分类类别中最高概率大于阈值β的示例计算一致性损失项。将阈值β设定为较高的值。\n简单来说，一致性损失应该针对的是无标签的数据，我们在训练的时候，只是计算那些输出概率高于阈值的样本，其余样本直接抛弃掉。\n2.3 Sharpening Predictions\n如果训练的时候使用了Confidence-based masking，我们可以结合Sharpening Predictions来提升模型的表现。\n什么是Sharpening Predictions？就是通过设定温度参数，改变最后softmax的分布，使它更加的尖锐，也就是熵更小，分布的越集中。\n这个东西蒸馏的时候也有用到，只不过在蒸馏的时候我们需要的是扩大不同类别的相似性，温度参数是大于1比较好的。但是这里我们希望是集中输出的分布，让它的熵更小，所以温度参数应该小于1。\n从另一个角度来说，使用了温度参数之后，阈值大于0.8 的概率应该也会提升，这就让Confidence-based masking变得没有那么难以操作。\n2.4 Domain-relevance Data Filtering\n首先我们要明白的是我们的数据中是含有无标签数据的，那么基于此，我们很容易有这么一个想法：\n既然有无标签数据了，那么为了增大模型的表达能力，能不能我自己从别的地方收集更多无标签数据补充进来，仍然使用同样的流程，这样最终模型效果是不是更好。\n这个想法很朴素，有一个问题就是我们从外部收集的数据是不是和我们已经有的数据同分布的。\n举个简单的例子，如果我们当前的数据的10个类别分类的数据，你从外部收集的数据类别是超出这个10个类别的，那么这样的数据加入进来，是副作用。\n一个很简单的方法就是，我们使用在域内数据上训练的基线模型来推断大型域外数据集中的数据标签，并选择模型最有信心的示例。具体而言，对于每个类别，我们根据分类属于该类别的概率，并选择概率最高的示例。\n这其实本质上也是无监督的一种方式，具体看一下这个文章。\n2.5 TSA\n全称是Training Signal Annealing for Low-data Regime，直译过来是低数据状态的训练信号退火；\n为什么使用这个东西？它本质上是为了解决一个问题就是无标签数据和有标签数据的数量巨大的不平衡问题。\n也就是说标签数据少，无标签数据多，在训练的时候很容易在标签数据上造成过拟合。\nTSA基本思想就是我们首先定义一个阈值，在训练的时候，我们只是使用模型对于当前标注数据的输出置信度低于阈值的样本。\n比如说，阈值如果你定的是0.5，那么在训练的时候，标注数据的输入最大概率如果是0.9，那么这个样本的是不计算在损失内，如果是0.1，那么我们计算损失。\n这个阈值是随着训练不停的增加的，有三种方式，如图：\n\n如果说标注数据很容易学习，或者说标注数据很少，我们使用指数增加。\n因为标注数据少或者容易学习，刚开始很容易过拟合，置信度高的样本占比多，所以我么最开始增长的慢。\n相反，当模型不太可能过拟合时，例如，当我们有大量带标签的示例或模型采用有效的正则化时，对数计划可以很好地发挥作用。\n总的来说，TSA本质上给人的直观感觉更像是在训练初期，压迫模型，不要尽快的学到最好的表现能力（使劲的扯后腿。。。）\n3. 实验结果\n\n有效，不想多分析\n4. 总结\n说一下从这个论文学到的东西。\n\n数据增强在半监督中可以作为噪声输入提升模型表现，文中使用的回译和TF-IDF。其实NLP中监督学习中有效的数据增强方法很多，猜测其他方法也是有效的，可以去尝试。\n上一个文章提到，为了担心模型在无标签数据上表现不好，在最开始，只是使用的标签数据，然后无标签数据才慢慢加进来的。当时使用的是一个$\\alpha(t)$来控制。在这里，为了缓解这个问题，使用的是masking结合Sharpening，Sharpening来控制输出的熵，让它比较尖锐，masking让它舍弃低置信度的样本，防止误差累积。\n第2点注意是针对的一致性损失，也就是无标签数据做的这个操作，对于有标签数据，我们使用TSA，来缓解过拟合，核心思想就是抛弃掉置信度高的样本，压迫模型在初期不要表现的那么好。\n\n半监督这块，我只是在工作中简单涉及到了，所以没有深入的读大量的论文，只是针对我用到的一些东西，看了一些论文。\n文中有不对的内容，欢迎大家拍砖讨论。\n参考资料：\n谷歌惊艳的无监督数据增强方法–Unsupervised Data Augmentation for Consistency Training\nhttps://www.jianshu.com/p/5d4e18b8de04\n半监督学习在金融文本分类上的探索和实践 - 李渔的文章 - 知乎 https://zhuanlan.zhihu.com/p/151021586\nGoogle无监督数据增强方法UDA在文本分类上的实验 - 延陵既智的文章 - 知乎 https://zhuanlan.zhihu.com/p/186211797\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/UDA/"},{"title":"","date":"2024-06-21T03:48:19.575Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.575Z","content":"今天分享的论文主要是讲Bert如何在文本分类上获得比较好的效果，比较简单：How to Fine-Tune BERT for Text Classification?：不涉及什么复杂公式，也比较早了，里面很多东西对于当下已经司空见惯，我就直接就分享论文结论，攒个思路。\n1. 如何处理长文本\n我比较感兴趣的是一点是Bert处理长文本的思路。\n首先数据集是IMDB，文本分类任务，超过512个token的12.69%，最大长度为3045；\n1.1 截断方法：\n\n保留头部：保留头部最开始的510个tokens\n保留尾部：保留最后的610个tokens\n头部加尾部：头部128+尾部382\n\n1.2 分层的方法：\n简单来说就是把文本分为 k = L/510个小段落，每个都喂进去Bert，然后得到的K个【CLS】的输出向量，我们对这个K个向量做：\n\nmean pooling\nmax pooling\nself-attention\n\n直接看结果：\n\n看结果，我们知道，头部加尾部会获得更好的结果。\n2. 其他结论\n\nBERT 顶层对于文本分类任务更加有效\n每层适当的逐层降低学习速率，可以提高文本分类效果\n任务内和领域内（和任务内数据分布相似）的进一步预训练可以提升文本分类效果\n\n对于第二点，降低学习率来说，论文中是从顶层到底层逐渐降低，越靠近输出学习率越高，越靠近输入层，学习率越低，这一点还是挺有意思的。\n对于第三点，任务内数据和领域内数据，对提升效果都有用，通用领域基本没啥用，因为Bert本来就是在通用领域训练的。\n还有意思的一点是，并不是在任务内的数据训练的越多step越好，直接看图：\n\n也就是说，在任务领域数据预训练可以提升效果，但是也有注意预训练的步数，不能是过分（有点过拟合的感觉，但是感觉说过拟合有点不准确）。\n3. 总结\n掌握以下几点：\n\n如何处理长文本：head/tail/combine two\n不同层不同学习率提升效果，越靠近输入层学习率应该越低\n领域内和任务内数据进一步预训练提升效果：注意进一步预训练步数控制\n\n参考链接：\n文本 × 分类：让 BERT 适配短句分类任务 - 小莲子的文章 - 知乎 https://zhuanlan.zhihu.com/p/148501319\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/在文本分类上微调Bert/"},{"title":"","date":"2024-06-21T03:48:19.545Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.545Z","content":"没有标注数据不用怕，只用标签名称就可以文本分类！韩家炜组出品\n对于实际的文本分类需求，没有标注数据是一件很常见的事情。\n针对这种情况，有一个最朴素的思路可以做：\n\n首先，根据对应的标签名称，使用W2C找到对应的相近词\n通过相近词，对文本数据做关键词命中，进而映射到对应的类别\n使用上述的标注数据训练文本分类模型\n使用3步骤的文本分类模型对新数据预测，获得置信度高的文本，之后做半监督。\n\n上面这个思路，非常的简陋，最终的结果也不会很好。实际工作中，需要有大量的规则去补充。\n今天分享的这个论文【Text Classification Using Label Names Only: A Language Model Self-Training Approach】，来自韩家炜组。\n针对上面的场景，这个论文给出了一个思路。\n整个论文读下来，先给个简单的评价，思路不错，但是也并不成熟，不过其中有很多细节点可以让我学习到。\n1. 背景\n人们在对一个文本分类的时候，不会看到任何带标签的标注数据，而只是通过一些关于描述分类类别的单词，就可以做出判断。\n举个例子，人去对文本进行分类的时候，假如文本有一个类别属于计算机。脑海中其实是有先验知识，比如如果句子中出现人工智能，深度学习，NLP等词汇的时候，人们基于此可以很大概率的判断出当前这个文本是属于计算机这个类别。\n随后呢，注意上面只是说的是很大的概率，还会出现苹果属于科技类别，但是不属于水果这个类别，也就是单单第一步还会出现语义歧义的错误，所以人们还会通读一遍句子，根据上下文语义再对句子分类。\n作者类比这个思路，提出了三个步骤：\n（1）找到和标签名称语义相关性较高的词汇；\n（2）查找类别指示性单词并基于这些单词训练单词分类模型\n（3）自训练提升模型\n2. 步骤\n2.1 Category Understanding via Label Name Replacement\n直译过来就是通过标签名称替换理解类别。\n这句话直译过来的话可能不好理解，更好的表述是找到与标签名称语义相关性较高的词汇。\n就像我们上面说的，人们在看到标签名称的时候，会联想到很多与之相关的词汇。\n类别到NLP中，预训练模型其实就相当于模型的先验知识，可以从中知道标签名称的相近词汇。\n我们知道，Bert 训练的时候是这样的：mask掉一部分词汇，然后通过语言模型预测mask部分的输出，计算损失函数；\n现在我当前输入是人工智能（为了方便理解，我们可以认为输入它是一个词作为整体输入），那么输出的时候其实是在整个词汇表上做softmax;\n基于此，从中挑选出概率最大的50个词汇，也就是当前这个位置最有可能出现的50个单词。\n进一步的，因为包含人工智能这个词的肯定不只是一个句子，我们对每个句子中的人工智能做同样的操作，然后都获取对应的前50个词汇。\n最后把这所有的50个词汇累积起来，按照频率从大到小，选取前100个词汇，作为可以替换人工智能这个标签名称的相近词汇。\n这个思路，简单来说，就是从预训练模型Bert获取标签名称的近义词或者更准确的说是获取与标签相关词汇的过程。\n其实，看到这里，我想到了一点，就是这个过程和我们使用GLove或者Word2Vec获取近似词的过程很相似。\n只不过Bert是一个动态的权重，受上下文影响，所以获得结果更加的准确，泛化性也更强。\n在论文，作者也做了实验论证这个道理。\n这一步，我们得到的结果是类似这种:\n\n简单总结一下：\n两个步骤：\n\n找到每个句子中存在的标签名字，使用Bert预训练模型，找到与之最接近的50个单词。\n每个标签中所有单词汇总，按照频率，找出排在前100个单词，作为当前标签名称(Label Name)的类别词汇（category vocabulary/category indicative words ）\n\n2.2 word-level classification via masked category prediction\n这个步骤，简单来说是使用Bert这类的预训练模型在单词这个级别训练分类模型。\n上个步骤中，针对每个Label Name，我们会得到对应的category vocabulary/category indicative words 。\n这个时候一个最简单的办法，就是只要当前的句子出现了category vocabulary中的词汇，我们就认为当前的句子属于相对应的Label Name。\n也就是我们开头说到的关键词命中规则。\n但是这样做是有很大问题的。\n首先，我们知道每个单词的词汇意义是与语境有关系的。一个句子出现苹果这样的单词，你很难武断的认为这个句子是属于科技还是水果。\n其次，我们得到的每个Label Name的category vocabulary都是有数量限制的。有的单词其实也能表达当前Label Name的含义，但是并未包含在category vocabulary中。\n为了缓解这两个问题，作者提出了Masked Category Prediction (MCP)任务；\n简单讲，它分为两个步骤：\n\n针对句子中的每个单词，使用Bert这种预训练模型，找到与之最近接的前50个相关词汇（很类似第一大步骤的第一小步骤）；然后将这50个相关和每个标签的category vocabulary进行比较，当交集超过20个时候，此时这个句子中的这个单词对应的类别就是对应的这个Label Name\n句子经过第一个步骤之后，句子中的部分单词就有了类别。那么mask掉这些单词，然后Bert相对应的每一个单词尾端接一个分类器对当前单词做类别的分类。\n\n整体流程，如下图：\n\n2.3 self-training on unlabeled corpus for generalization\n经过第二个步骤，当前模型仍然存在问题：\n\n有的句子没有被找到有类别的单词，所以这些没有被训练到\n训练到文本分类模型使用的是对应类别单词mask那里的输出，而不是cls。而ClS一般可以看到整个句子的全部信息。\n\n针对这两个问题，作者提出使用全部的无标签数据，进行自训练。\n这一块我自己知识积累的不多，就不多说了。具体的大家可以去看一下论文。\n3. 模型架构总结\n整体的算法流程如下图所示：\n\n4. 实验结果分析\nDatasets使用了四种：AG News;DBPedia;IMDB;Amazon；\n实验效果图如下：\n\nBERT w. simple match情况是这样：句子只要含有标签名称的相近词，就认为当前句子是对应的标签类别，以此进行训练。\nLOTClass w/o. self train是代表LOTClass只走前两步骤，不进行自训练。\n从图中可以看到，如果不进行sefl-training，LOTClass效果在所有数据集上效果也都不错。\n使用了sefl-training之后，LOTClass 可以和半监督和监督模型结果媲美。\n4.1 细节1\n有一个问题，LOTClass这种方法，相当于在使用Bert的情况下，标注了多少数据？\n作者做了一个实验图，如下图：\n\n从效果图可以看到，LOTClass的效果和Bert在每个类别有48个标注数据的情况下训练的效果相当。\n我大概算了一下，AG News有4个类别，每个类别48个，也就是总共192个标注样本。\n4.2 细节2\n这个论文我比较感兴趣的是第一个步骤，获取标签名称的相近词汇。\n针对这个步骤，做两个方面的修改:\n\n\n修改标签词汇：分别使用commerce;economy;business作为label name；标签的名称虽然变化了，但是每个标签名称得到的100个相近词汇表有一半是重复的，另一半的词汇表意思都很类似。\n这说明，这个方法是有鲁棒性的。不会出现，标签名称换了一个相近的名字表示，而得到的词汇表出现了剧烈的抖动。\n\n\n分别使用300维度的Glove和LOTClass获取标签名称相近词汇，GLove得到的词汇非常的贫乏，而LOTClass效果很好，模型具有很好的泛化性;\n\n\n这个思路也给自己赞个思路，获取同义词或者近义词可以使用这种方法。\n5. 简单总结\n说一下自己学到的东西。\n其实看到细节1的时候，LOTClass方法得到的模型表现相当于使用192个标注数据对Bert进行监督训练。\n从这里来看，标注的成本并不大；不过，应该可以使用此方法为半监督积累数据。\n这个方法还不成熟，不过里面有些思路可以积攒。\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/只使用标签名称就可以文本分类/"},{"title":"","date":"2024-06-21T03:48:19.765Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.765Z","content":"文本分类资源总结\nSGM：用序列生成的方法来处理多标签文本分类问题 - SimpleJian的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/58076177\n半监督学习在金融文本分类上的探索和实践\nhttps://mp.weixin.qq.com/s/7EazF26teBSg0_XvWtKdUg\n多分类模型Accuracy, Precision, Recall和F1-score的超级无敌深入探讨 - NaNNN的文章 - 知乎 https://zhuanlan.zhihu.com/p/147663370\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/文本分类资源总结/"},{"title":"","date":"2024-06-21T03:48:21.265Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:21.265Z","content":"文本纠错\n医疗健康领域的短文本解析探索（三) ----文本纠错\nhttps://mp.weixin.qq.com/s/p9UMvy_VSW5g9IqDDjK6GA\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本纠错/文本纠错资源总结/"},{"title":"","date":"2024-06-21T03:48:19.915Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.915Z","content":"DSSM\nhttps://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf\n架构图\n架构图很简单，也有点老了\n\n核心细节点有两个：一个是使用了cosine做了查询和文档的相似度量\n\n第二个就是，softmax\n\n第三个是损失函数，使用最大似然估计，只计算了正样本：\n\n对于DSSM，主要是想提几个小细节，也是我自己的思考，不准确的地方，欢迎拍砖。\n首先，为什么采用(Query,D+,D-1,D-2,D-3)的方式作为输入，而不是采用(Query,D+)；(Query,D-1)；(Query,D-2)；(Query,D-3)；作为单独的pair样本对作为输入；\n这个问题，其实还可以换个问法，为什么DSSM的损失函数，使用的是一个正样本多个负样本归一化之后对正样本求交叉熵，而不是单个pair对作为输入，去求二分类的交叉熵；\n我的理解是，这个其实适合业务场景相关的一个问题；参考下面这个回答的答案：\nDSSM 为什么以一个正样本几个负样本softmax归一化然后正样本交叉熵的方式算loss? - xSeeker的回答 - 知乎 https://www.zhihu.com/question/425436660/answer/1522163398\n我直接截图过来：\n\n本质上，还是在学习一种顺序关系，正样本排在负样本之前\nDSSM在各大公司的实战\n实践DSSM召回模型 - 王多鱼的文章 - 知乎 https://zhuanlan.zhihu.com/p/136253355\n深度语义模型以及在淘宝搜索中的应用:https://developer.aliyun.com/article/422338  写的很好\n百度NLP | 神经网络语义匹配技术：https://www.jiqizhixin.com/articles/2017-06-15-5\n语义匹配 - 乐沐阳的文章 - 知乎 https://zhuanlan.zhihu.com/p/57550660\n损失函数\nDSSM通过推导公式，可以得到最大化似然估计和交叉熵损失函数是一致的。\n【辩难】DSSM 损失函数是 Pointwise Loss 吗？ - xSeeker的文章 - 知乎 https://zhuanlan.zhihu.com/p/322065156\n交叉熵损失函数原理详解：\nhttps://blog.csdn.net/b1055077005/article/details/100152102\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/DSSM论文-公司实战文章/"},{"title":"","date":"2024-06-21T03:48:19.975Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.975Z","content":"\nBERT推理速度慢，导致落地困难；找到效果不错，推理速度快的模型是一个方向，ESIM是一个很好的选择；\n\nESIM 推理速度快，效果不错，堪称文本匹配的利器；\n对于ESIM，重点掌握就一点：是两个句子之间究竟是如何交互的.\n0 整体架构\n先来看整体结构是什么样子：\n\n对于这个架构，我们主要是看左边这个就可以；\n可以看到，从架构上来看，这个模型大概可以范围四层：最底层是一个双向LSTM，作为句子的的编码器，随后是一个交互层，然后又是一个双向LSTM，最后是一个输出层。\n原论文中，也是将ESIM分为四个部分，Input Encoding，Local Inference Modeling， Inference Composition和Prediction，我们一个个说。\n1. Input Encoding\n先假设，我现在有两个句子：\n$a=(a_{1},a_{2},…,a_{l_{a}})$ 和$b=(b_{1},b_{2},…,b_{l_{b}})$；\n我要判断它是否表达同样的意思：0或者1；\n首先第一步是Input Encoding ，这是一个常规操作，就是tokens的embeddings接BiLSTM；注意，我们是两个句子都进入到这同一个BiLSTM中，而不是进入到两个；\n公式如下：\n\n作者同时也测试了使用GRU对句子进行编码，但是结果在NLI任务上效果并不好；不过我认为，在实际工作中，两者都可尝试，毕竟玄学。\n2. Local Inference Modeling\n首先这里回顾一下符号：$ \\bar{a_{i}} $是句子a在i时刻的是输出，$\\bar{b_{j}}$是句子b在j时刻的输出；\n那么我们使用如下公式计算两个单词输出之间的交互：\n\n举个很简单的例子，比如说隐层维度为256，那么$ \\bar{a_{i}}=[1,256] $，$ \\bar{b_{j}}=[1,256] $\n那么相乘之后，就是维度[1,1]的值；\n这只是两个单词输出之间的交互，我们知道a和b句子长度是不一样的（当然也可能一样）；\n这里我们假设a长度为10，b长度为20；\n那么经过（11）的计算，我们会得到一个[10,20]的矩阵，用来描述两个句子之间不同单词之间的交互。\n核心点就是在于对于这个[10,20]的矩阵，如何对它进行操作，公式如下：\n\n一定要注意看这里的$\\widetilde a_{i}$，我们得到的是[10,20]的矩阵，然后对每一行做softmax操作，得到相似度，然后乘以$b_{j}$。\n（12）和（13）本质上是对这个[10,20]的矩阵分别做了按照行的相似度和按照列的相似度；\n有点难理解，还是举个例子（这里的例子就不举长度为10和20了，简单点）。\na：【我今天去吃饭了】\nb：【他为什么还在学习】\na中的【我】依次对【他为什么还在学习】中的每个单词做乘法，就是得到了$e_{ij}$；然后softmax之后，每个相似度对应乘以【他为什么还在学习】的每个单词输出encoding从而得到加权和，作为$\\widetilde a_{i}$\n之后就是对特征进行拼接，分为两种，对位相减和对位相乘：\n\n3. inference composition和Prediction\n这一步也是常规操作，就是把$m_{a}和m_{b}$输入到第二层的BiLSTM，并把输出做最大池化和平均池化，然后拼接特征，然后输出到全连接，得到结果：\n\n4. 总结\n最核心的点还是在于理解如从两个句子单词之间的交互矩阵获得两个句子之间的交互结果；\n也就是需要对单词之间的交互矩阵，比如[10,20]，分别按照行做softmax和列做softmax；\n这个思想其实在后期很多模型中都有用到，值得思考。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/ESIM/"},{"title":"","date":"2024-06-21T03:48:20.325Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:20.325Z","content":"最近抽时间把SIMCSE用Pytorch复现了一把，中途涉及到的几个思考点，和大家分享一下：\n注：原作者有开源论文代码，不过有些复杂，大家可以看一下自己魔改一下；\n全文思路如下：\n\nSIMCSE理论介绍以及代码实现的部分细节点\nTextCNN是否可以借鉴SIMCSE的思路，来训练模型从而你获取比较好的Sentence embedding\n是否可以借鉴Dropout数据增强，使用amsoftmax，减少同类距离，增大不同类距离\n\n1. SIMCSE论文理论介绍\n当时读完SIMCSE论文之后，没时间写文章，赶紧发了个朋友圈把思路简单的记录了一下；\n\n感兴趣的朋友加我微信【dasounlp】，互看朋友圈啊，笑；\n论文分为四个部分来讲，对比学习，无监督SIMCSE，有监督SIMCSE，评价指标；\n1.1 对比学习\n对比学习的目的是，是减少同类距离，增大不同类之间的距离，借此获得一个文本或者图片更好的表示向量；\n定义句子对:$D={(x_{i},x_{i}^{+})}{i=1}^{N}$；其中N是一个Batch中句子对样本数量，$x{i},x_{i}^{+}$是语义相似的样本，$h_{i},h_{i}^{+}$分别是$x_{i},x_{i}^{+}$经过编码器Encoder之后得到的表示向量；\n那么对比学习的训练目标就是：\n\n这个公式看着比较唬人，其实本质就是一个多分类softamx的交叉熵损失函数；\n需要注意的是参数 $\\tau$ 是个超参数，$sim(h_{1},h_{2})$是一个相似性度量函数，原论文使用的cosine，其实使用一些其他的相似性函数应该也没问题；\n注意一下分母这里：其实一个batch，比如有N个句子对，那么就有2N个句子，其中正例是1个，负样本应该是总样本数目2N减去样本本身加上样本的正例，也就是2N-2；\n不过，看公式，作者这里用到的是一个batch中的N个样本，也就是使用的是每个句子对中的其中一个；\n关于这个问题，是否使用更多的负样本是不是会获得更好的效果，作者回复说并没有。\n我自己在复现的时候，使用的是2N-1个样本【正例+负例总和】；\n那么在落地到代码的时候，怎么实现这个交叉熵呢？我画了一个简单的图，比如batch是2：\n\n1.2 正例和负例的构建\n上面谈到的整个过程，全程没离开正例和负例；\n在图像中，一个图像经过平移旋转等数据增强的方式，可以看成是生成了图像的正例；\n在文本上，一些常规的数据增强的手段就是删减单词，替换同义词等等；\n文本的数据增强存在的一个问题就是，一个简单的操作可能就会导致语义的改变；\n在无监督的SIMCSE中，正例的构造很有意思，就是通过添加一个Dropout的噪声；\nDropout是在随机失活神经元，每次句子经过网络，失活的神经元是不一致的，导致生成的embedding是不一致的；\n这一点其实大家应该都懂，但是能联想到把这个作为数据增强的一个手段，确实很强。\n在有监督的SIMCSE中，其实是借助了NLI数据集中自带的标签，来构造正例和负例；\n直接来看作者原文中的图吧；\n\n1.3 句子向量评价指标\n句子向量的评价指标这里，用两个东西来量化一下，alignment和Uniformity；\n直接来看图：\n\n2. TextCNN和Dropout的融合\nSIMCSE中，BERT作为Encoder未免太复杂了，这时候按照常规思路，我会去思考可不可以使用简单网络比如textcnn代替bert；\n那么实现方式就可以分为两种：\n一种是我使用textcnn直接作为encoder，然后仿照无监督simcse的训练方式进行训练就可以了；\n第二种方式就是知识蒸馏，无监督simcse训练一个bert的encoder出来之后，使用简单网络textcnn进行学习就可以了；\n我针对第一种方式做了个实验。\n在实验之前，我就没报什么大的希望，只是想亲眼试一下究竟可行不可行；\n为什么没有报太大希望呢，很简单，我自己认为dropout作为一种数据增强的形式，太过简单了，textcnn这种简单网络，不足以学习到其中的差异；\n我在中文的LCQMC和ATEC数据集上做了一个简单的测试，Spearman作为评价指标，结果如下：\n\n之后，我看情况能不能把这部分代码开源出来~~，自己实现也挺简单的；\n3. Amsoftmax的引入\n第三个小思路是这样的，dropout可以看做是一个最小化的文本数据增强的形式。同一个句子，经过encoder，得到的embeding不同，但是语义是相似的，所以可以看做是一个正例；\n进一步的，如果我同一个句子经过多次encoder，比如经过10次，那么我得到的就是10个embedding；\n也就是说，在同一个语义下面，我得到的是10个语义近似但是embedding不同的向量；\n如果我有10万个句子，可以把这个10万个句子当做是10万个类别，每个类别下有10个样本；\n想一下这个感觉，不就是人脸识别的操作吗？\n那么可不可以使用这种方式，得到更好的语义表达呢？\n这个我没做实验，只是一个思路，之后有时间再去做实验，有兴趣的朋友可以做一下；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/SIMCSE论文解析/"},{"title":"","date":"2024-06-21T03:48:19.885Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.885Z","content":"大家好，我是DASOU；\n因为业务场景常常用到无监督语义匹配，所以一直在关注这方面的进展；\n现在大家都比较熟知的就是：BERT-Flow;BERT-Whitening和SimCSE；\n之前梳理了一下BERT-Whitening的理论和代码，分享给大家，希望有帮助；\n文章大体脉络如下：\n\nBERT-Whitening 公式推导+注解\nPCA和SVD简单梳理\n协方差矩阵的几何意义\n对BERT-Whitening 代码的简单梳理\n\n1. BERT-Whitening 解读\nBERT的输出向量在计算无监督相似度的时候效果很差是一个共识了，具体原因这里不多说，去看我之前这个文章；\n然后一个改进措施就是想要把BERT的输出向量变成高斯分布，也就是让输出向量满足各向同性；\n什么是各向同性呢？就是向量矩阵的协方差矩阵是一个单位矩阵乘以一个常数，换句话说在每个向量维度上方差是一样的；\n现在大家比较熟知的是两种方式：\n一个是bert-flow模型，采用了基于流的生成模型来做这个数据分布的转变；\n第二个是bert-whitening。\n这个文章重点聊一下BERT的白化，也就是第二种。\n它做的事情就是直接将bert的输出向量矩阵变成均值为0，协方差矩阵为单位矩阵；\n补充两个知识点，方便后续大家理解；\n第一个是，协方差矩阵是单位矩阵，说明数据分布是在一个二维的圆上，三维的球上。\n第二个是对于取值确定的矩阵A，经过W=AX变换后，协方差矩阵将变换为$V [ W ] = A V [ X ] A^{T}$\n公式推导如下:\n我们原始的向量矩阵是$X$，变化之后的矩阵是$X_{new}$；\n我们执行的变化是:\n$X_{new} = (X-\\mu)W$\n上面这个操作，是我们想让$X_{new}$的均值为0，协方差矩阵为单位阵；\n我们知道:  $V[X_{new}]=W^{T}V[X]W$\n那么就可以推导出:  $W^{T}V[X]W=E$\n进而可以推导出：$V[X]=(W^{T})^{-1}W^{-1}$\n对$V[X]$做SVD奇异值分解，有:  $V[X]=U\\Sigma V^{T}$\n因为$V[X]$是一个实对称矩阵，有:  $V[X]=U\\Sigma U^{T}$ 这是因为实对称矩阵的$X^{T}X$和$XX^{T}$相等，U和V也就相等\n也就是有：  $(W^{T})^{-1}W^{-1}=U\\Sigma U^{T}$\n求解W就好了：$W^{-1}=\\sqrt(\\Sigma)U^{T}$\n$W=U\\sqrt(\\Sigma)$\n下面我这个解读\n其实这个操作本质上就是PCA的操作，为啥这么说呢？\nPCA是得到协方差矩阵的特征向量，然后挑选出来前k个特征值对应的特征向量，然后做一个转化；这里我们把对PCAde运用停留在得到并使用全部的特征向量.\n其实核心点在于理解：\n对$V[X]$做SVD奇异值分解，有:  $V[X]=U\\Sigma V^{T}$\n因为$V[X]$是一个实对称矩阵，有:  $V[X]=U\\Sigma U^{T}$\n这个是对协方差矩阵做的奇异值分解，如果是PCA的话，就应该是协方差矩阵的特征分解；\n$V[X]=H\\Sigma H^{-1}$\n因为V[X]是要给对称矩阵，所以\n$V[X]=H\\Sigma H^{T}$\n这么一看H和U是等价的，所以之前的操作其实本质就是在做PCA；按道理我们求出来U就做PCA变化就可以了，但是我们最终的结果却是$W=U\\sqrt(\\Sigma)$;\n这是因为PCA之后数据协方差是对角矩阵，并不是一个单位矩阵；\n可以对PCA后的数据做标准化，就可以变成单位矩阵；\n苏剑林这里，直接就是强制等于单位矩阵，所以结果是$W=U\\sqrt(\\Sigma)$;\n太乱了~~\n2. 简单梳理PCA和SVD\n先总体说一下我的觉得最重要的一个知识点：\nSVD是直接对原始矩阵进行奇异值分解，得到左奇异向量，奇异值和右奇异向量；\nPCA是对矩阵的协方差矩阵进行特征分解，得到对应的特征向量和特征值，其中特征向量和SVD中的右奇异值是一个东西(如果我没记错的话~~)；\n2.1 特征分解\n先说一下特征值分解：\n一个方阵A，一般来说可以被对角化为如下式子:\n$$\nA=X\\Sigma X^{-1}\n$$\nX是A特征向量构造的矩阵，$\\Sigma$ 是一个对角阵，也就是只有对角线上有值，同时这个值是A的特征值；\n如果说这个A除了是方阵，还是一个对称阵，那么式子中的X就变成了正交矩阵，我们使用M来表示，可以对角化如下式子:\n$$\nA=M\\Sigma M^{T}\n$$\n有两个变化，一个是变成了正交矩阵，一个是最后面的是转置矩阵符号，而不是逆矩阵符号；\n2.2 PCA\nPCA分为两个步骤：\n第一个步骤是找到一组新的正交基去表示数据，比如我原来是使用n个正交基来表示数据中的向量，我现在找到另外的新的n个正交基来重新表示向量；这n个新的正交基是怎么找到呢？一个比较形象的表述是第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。\n第二个步骤经过上面这个过程，我们会发现，越到后面的基，方差越小，几乎接近于0，也就是说这些后面的基没啥作用。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。这个步骤就是在降维；\n在这里想要说一个细节点，就是经过第一个步骤之后，并不进行第二个步骤，从公式角度就是$Y_{1}=P*X$，而不是$Y_{2}=P_{k}*X$；那么得到的$Y_{1}$的协方差矩阵是一个对角化矩阵，以三维为例子，在空间上的分布是一个椭圆球体；\n这个时候如果协方差矩阵想要变成单位矩阵，就是对向量矩阵做一个标准化就可以了；\n现在有一个问题，上面我们是形象化的描述如何找到这些基，那么从实际出发，如果找到呢？\n我们是这么做的：通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。\n在这里，需要注意的特征值最大，代表的就是在这个特征值对应的特征向量方向方差最大；\nPCA大体流程：\n\n我自己简单的总结就是，首先对数据进行中心化，然后计算协方差矩阵，然后计算对应的特征值和特征向量等等；\n需要注意的是，第一个步骤之后，如果我们不想去降低维度，那么这个全部的特征向量也可以使用，简单说就是$Y=A*P_{全部}$; 这个操作就是对原始数据做了一个旋转变化，协方差矩阵会变成对角矩阵；\n2.3 SVD分解：\n奇异值分解是一个能适用于任意矩阵的一种分解的方法，对于任意矩阵A总是存在一个奇异值分解：\n$$\nA=U\\Sigma V^{T}\n$$\n假设A是一个$mn$的矩阵，那么得到的U是一个$mm$的方阵，U里面的正交向量被称为左奇异向量。Σ是一个$m*n$的矩阵，Σ除了对角线其它元素都为0，对角线上的元素称为奇异值。\n$V^{T} $是v的转置矩阵，是一个n*n的矩阵，它里面的正交向量被称为右奇异值向量。而且一般来讲，我们会将Σ上的值按从大到小的顺序排列。\n在这里有几个点想要强调一下：\nU这里对应的是$A*A^{T}$ 对应的特征向量；\nV这里对应的是$A^{T}A$对应的特征向量，也就是A矩阵的协方差矩阵对应的特征向量；这一点比较重要，我们在使用PCA降低维度的时候，想要拿到的那个变化矩阵就是这个V（注解，挑选前K个），也就是变化之后为$AV$;\n通过$A=U\\Sigma V^{T}$，我们也可以得到这样一个结果$AV=U\\Sigma$\n所以在降低维度的时候我们这两种都可以；\n具体讲解看这里：\n\nSVD一般流程【也会拿计算协方差矩阵的方法，不是改进的方法】：\n\n2.4 协方差矩阵的几何意义：\n协方差矩阵是一个单位矩阵，数据是分布在一个圆上；\n协方差矩阵是一个对角化矩阵，我们可以将原始数据标准化，这样对应的数据的协方差矩阵就会变成单位矩阵，还可以对原始数据进行平移，移动到原点附近的圆上；\n如果协方差矩阵是一个普通的矩阵，我们可以做PCA【不降低维度的那种】，将其转化为对角化矩阵，之后做标准化，这样协方差矩阵变成了单位矩阵，然后对数据做平移，移动到原点附近；\n3. 梳理BERT白化代码：\n有两个版本的代码，\n一个是苏剑林的Keras版本：https://github.com/bojone/BERT-whitening\n一个是Pytorch版本：https://github.com/autoliuweijie/BERT-whitening-pytorch\n我看了一遍Pytorch版本，主要的细节点我罗列在下面；\n首先就是下载数据和下载一些英文预训练模型。\n之后就是跑代码，分为三种方向：\n第一种就是不使用白化的方式，直接在任务中使用BERT的输出向量；\n第二种是在任务中数据中使用白化方式，也就是在任务数据中计算kernel和bias，然后在任务数据中使用此参数，\n去对BERT系列预训练模型的输出向量做转化；\n第三种是在大数据中，在这里也就是NLI数据，计算相应的kernel和bias，然后在任务数据中使用这个参数，去做对应的转化；\n第三种方式很方便，如果实际工作真的使用bert白化，肯定是我在训练数据中计算出来参数，然后在测试数据中使用这个参数直接去做转化，这样效率最高。\n把测试数据补充进来然后再重新计算对应的参数，感觉总是多了一个步骤，效率不高；\n这就要求我们在大数据中计算参数的时候，确保大数据具有普适性，能够很好的适配任务数据；这样计算出来的参数才有使用的可能；\n在实际运行代码的时候，我只是使用了SICKRelatednessCosin这个任务，整体代码写的相当的不错，大致的过一遍也就可以了；\n最核心的代码是这个：\n12345678910111213141516171819def compute_kernel_bias(vecs, n_components):    &quot;&quot;&quot;计算kernel和bias    最后的变换：y = (x + bias).dot(kernel)    &quot;&quot;&quot;    vecs = np.concatenate(vecs, axis=0)    mu = vecs.mean(axis=0, keepdims=True)    cov = np.cov(vecs.T)    u, s, vh = np.linalg.svd(cov)    W = np.dot(u, np.diag(s**0.5))    W = np.linalg.inv(W.T)    W = W[:, :n_components]    return W, -mudef transform_and_normalize(vecs, kernel, bias):    &quot;&quot;&quot;应用变换，然后标准化    &quot;&quot;&quot;    if not (kernel is None or bias is None):        vecs = (vecs + bias).dot(kernel)    return vecs / (vecs**2).sum(axis=1, keepdims=True)**0.5\n参考：\n苏剑林Bert-whitening：https://kexue.fm/archives/8069\n奇异值分解(SVD)原理 - 鱼遇雨欲语与余的文章 - 知乎 https://zhuanlan.zhihu.com/p/32600280\nPCA 通过 SVD 分解替代协方差矩阵的特征值分解\n协方差矩阵的几何意义：https://blog.csdn.net/nstarLDS/article/details/104874622/\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/bert白化简单的梳理/"},{"title":"","date":"2024-06-21T03:48:20.935Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:20.935Z","content":"在学习DSSM的时候，很容易和孪生网络搞混，我这里稍微总结一下自己的思考；\n对于孪生网络，并不是一个网络的名称，而是一种网络的名称；\n它的特点就是encoder参数共享，也就是在对句子或者图像编码的网络权重共享；\n它的网络的输入形式是这样的:：(X1,X2,Y)；X1，X2是两个输入文本，Y是我们的标签数据；\n对于孪生网络来说，一般的损失函数是对比损失函数：Contrastive Loss\n什么是对比损失函数呢？公式如下：\n\n这个公式需要注意两个细节点，一个是d，代表的是距离度量，一个是margin，代表的是一个超参；\n那么我感兴趣的是孪生网路可以不可以使用其他的损失函数？如果可以，又是哪些呢？\n首先当然是可以，只要能够优化都可以；\n\n参考自这里：\nSiamese network 孪生神经网络–一个简单神奇的结构 - mountain blue的文章 - 知乎 https://zhuanlan.zhihu.com/p/35040994\n三种损失函数形式\n我把自己思考的三种损失函数形式总结在这里，之后有问题再回来修改:\n两个向量做consine相似度或者欧氏距离度量函数，然后归一化到0-1，然后做二分类交叉熵损失函数；\n两个向量做cosine相似度或者欧氏距离，带入到对应的对比损失函数\n两个向量做拼接或者其他操作，然后接单个或者多个全连接，然后可以做逻辑回归或者做softmax；\nfassi做向量召回-样本重复和consine度量疑惑点\n我其实一直有一个疑问，在做向量召回的时候，一般的操作就是双塔模型，然后存储对应的样本的向量，存储到fassi中，然后搜索的时候使用找最近的向量就够了；\n这里面我最开始理解的时候有两个疑惑点，一个是如果做到同一个样本不会有多个向量；\n我的误解原因是没有对向量的是如何落地有很好的理解；我开始的理解是模型训练好了之后，进入一个pair（query，d），分别计算向量，然后存储，这样当然会出现同一个d，出现在不同的pair；\n但是，由于之间没有交互，右边这个塔模型参数不变，得到的向量当然也不变，也就是同一个d不会出现多个向量；\n而且在计算的时候，不用输入一个pair对，只需要对右边这个单塔输入去重之后的d就可以；\n第二个问题，为什么在faiss中使用最近的向量可以（先不用计较度量方式）得到相似的向量，而在模型中，我们在得到cosine或者其他度量结果之后，还会再接一个sigmoid；\n优化sigmoid的输出的时候，cosine的值或者其他度量的值也会同等趋势变化，所以可以起到作用；\n然后多说一下，如果用到向量召回，中间还是需要一个度量的值的，需要和faiss对应上，如果接一个MLP，感觉就够呛。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/聊一下孪生网络和DSSM的混淆点以及向量召回的一个细节/"},{"title":"","date":"2024-06-21T03:48:21.005Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:21.005Z","content":"RE2 这个名称来源于该网络三个重要部分的合体：Residual vectors；Embedding vectors；Encoded vectors;\n掌握这个论文，最重要的一个细节点就是了解如何将增强残差连接融入到模型之中。\n1.架构图\n先来看架构图，如下：\n\n这个架构图很精简，所以不太容易理解。\n大体上区分可以分为三层。第一层就是输入层，第二个就是中间处理层，第三个就是输出层。\n中间处理层我们可以称之为block，就是画虚线的部分，可以被循环为n次，但是需要注意的是每个block不是共享的，参数是不同的，是独立的，这点需要注意。\n2.增强残差连接\n其实这个论文比较有意思的点就是增强残差连接这里。架构图在这里其实很精简，容易看糊涂，要理解还是要看代码和公式。\n2.1 第一个残差\n首先假设我们的句子长度为$l$，然后对于第n个block（就是第n个虚线框的部分）。\n它的输入和输出分别是:$x^{(n)}=(x_{1}^{(n)},x_{2}^{(n)},…,x_{l}^{(n)})$ 和$o^{(n)}=(o_{1}^{(n)},o_{2}^{(n)},…,o_{l}^{(n)})$;\n首先对一第一个block，也就是$x^{(1)}$，它的输入是embedding层，注意这里仅仅是embedding层；\n对于第二个block，也就是$x^{(2)}$，它的输入是embedding层（就是初始的embedding层）和第一个block的输出$o^{(1)}$拼接在一起；\n紧接着对于n大于2的情况下，也就是对于第三个，第四个等等的block，它的输入形式是这样的;\n\n理解的重点在这里：在每个block的输入，大体可以分为两个部分，第一部分就是初始的embedding层，这个永远不变，第二个部分就是此时block之前的两层的blocks的输出和；这两个部分进行拼接。\n这是第一个体现残差的部分。\n2.2第二个残差\n第二个残差的部分在block内部：\nalignment层之前的输入就有三个部分：第一部分就是embedding，第二部分就是前两层的输出，第三部分就是encoder的输出。\n这点结合着图就很好理解了。\n3.Alignment Layer\nattention这里其实操作比较常规，和ESIM很类似，大家可以去看之前这个文章。\n公式大概如下：\n\n\n这里有一个细节点需要注意，在源码中计算softmax之前，也是做了类似TRM中的缩放，也就是参数，放个代码：\n1234567#核心代码def __init__(self, args, __):        super().__init__()        self.temperature = nn.Parameter(torch.tensor(1 / math.sqrt(args.hidden_size)))def _attention(self, a, b):        return torch.matmul(a, b.transpose(1, 2)) * self.temperature\n4.Fusion Layer\n融合层，就是对attentino之前和之后的特征进行一个融合，具体如下：\n\n三种融合方式分别是直接拼接，算了对位减法然后拼接，算了对位乘法然后拼接。最后是对三个融合结果进行拼接。\n有一个很有意思的点，作者说到减法强调了两句话的不同，而乘法强调了两句话相同的地方。\n5.Prediction Layer\nPooling层之后两个句子分别得到向量表达：$v_{1}$和$v_{2}$\n三个表达方式，各取所需就可以：\n\n\n\n6. 总结\n简单总结一下，这个论文最主要就是掌握残差连接。\n残差体现在模型两个地方，一个是block外，一个是block内；\n对于block，需要了解的是，每一个block的输入是有两部分拼接而成，一个是最初始的embeddding，一个是之前两层的输出和。\n对于block内，需要注意的是Alignment之前，有三个部分的输入一个是最初始的embeddding，一个是之前两层的输出和，还有一个是encoder的输出。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/阿里RE2-将残差连接和文本匹配模型融合/"},{"title":"","date":"2024-06-21T03:48:28.845Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:28.845Z","content":"机器翻译\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/README/"},{"title":"","date":"2024-06-21T03:48:21.435Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:21.435Z","content":"bpe论文的我的阅读感受\nNeural Machine Translation of Rare Words with Subword Units\n提出这个算法的直觉是这样的，作者发现翻译一个单词，有时候不需要这个单词的全部信息，可能只需要一部分信息就可以知道大致信息。还有一种可能是翻译这个单词与，可能通过组成\n这个单词的多个小单元信息来翻译就可以了。\n这个方法是为了解决稀少单词（也就是频率在人为规定下的单词）和未登录词没有办法有效翻译的问题。\n这里作者在摘要中提到了一个back-off 字典，就是说在之前翻译模型在处理未登录词汇的时候，处理办法是使用一个词典，把source-target中未登陆词汇一一对应起来，我们在翻译过程中\n如果出现了未登录词汇，直接使用字典中的对应关系进行替换就可以了。但是这样存在一个问题，就是说，最低频率是我们人为规定的，有些时候在调参的时候，这个频率是一个超参，，那么\n我们在准备词典的时候，就是一个动态的长度，这样很不方便，但是如果我们准备所有单词的back-off就得不偿失。还有一个问题是我们不确定source-target对应的关系是一一对应的，可能对应不上，可能对应\n是多种，在不同句子环境中，我们需要选择不同的单词翻译，这也是存在的一个问题。\n基于word-level的模型还存在一个问题，就是不能产生没有看见过的单词，也就是说在翻译端，没有出现在词汇表中的在翻译模型测试的时候是不会出现的。这其实是一个很重要的问题，就是我不能确定\n我的训练语料包含所有情况下的翻译。\n作者在摘要中说明，自己使用了字符级的n-gram和bpe方法，在WMT 15 英文德文翻译中提升1.1，在英文俄罗斯中提升1.3。\n翻译是一个开放词汇表的问题，我们在翻译模型中，一般把翻译模型词汇表限制在30000–50000（基于词）。\n作者通过实验发现，使用subeword模型，比使用大量词汇表的模型和使用back-off模型效果很好更简单。\n我在博客中看到了总结这个论文不错的博客，总结在下面\n通过BPE解决OOV问题----Neural machine Translation of Rare Words with Subword Units\nhttps://blog.csdn.net/weixin_38937984/article/details/101723700\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/bpe-subword论文的我的阅读总结/"},{"title":"","date":"2024-06-21T03:48:20.895Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:20.895Z","content":"[TOC]\n五千字梳理文本相似度判定(无监督+有监督)\n1. 为什么需要文本匹配/文本相似度判定使用场景\n先问一个核心问题，为啥需要文本相似度/文本匹配？换句话说，文本匹配的应用场景有哪些？\n举个例子，比如说海量文本去重。\n在一些社交媒体，某个话题之下，存在大量的营销号的文本，这些文本存在一个特点，内容大同小异，都在说同一个事情。\n我在对这些文本进行处理的时候，本质上只需要处理其中的一条就可以，这样可以极大提高我的处理速度。\n那么问题来了，我如何判定句子a和句子b/c/d/e等等是在说同一个事情？\n再举个例子，老生常谈，搜索场景，你在某度搜索“深度学习如何入门？”，某度如何返回和你这个问题最接近的问题/文章/博客等等内容网页？\n这些本质上都是在做文本相似度的判定或者说文本匹配，只是在不同场景下，有着不同办法不同的特色。\n文本匹配，并不是简简单单的词汇层次的匹配：比如“深度学习如何入门”和“如何入门深度学习”。\n还会有语义方面的匹配考虑，比如说“我想撒尿去哪里啊？”和“我想去卫生间”。这两句话在问答系统中，匹配到标准问题都是“卫生间在哪里”这个问题。\n对文本匹配方法来说，我们一般可以使用两种：无监督和有监督。\n2.无监督文本匹配\n首先我们来谈一下无监督的方法。\n对于无监督的文本匹配，我们需要实时把握两个重点：文本表征和相似函数的度量。\n文本表征指的是我们将文本表示为计算机可以处理的形式，更准确了来说是数字化文本。而这个数字化文本，必须能够表征文本信息，这样才说的通。\n相似函数的度量就是你选择何种函数对文本相似度进行一个判定，比如欧氏距离，余弦距离，Jacard相似度，海明距离等等\n我大概梳理了一下无监督的几种比较典型的方法，，如下所示：\n\n\nTF-IDF/IDF+词向量(word2vec/fasttext/glove)\n\n\nBM25（提前计算IDF矩阵，无需使用词向量）\n\n\nWMD\n\n\nSIF\n\n\nTF-IDF/IDF+词向量比较简单，我就不多说了。我们先来看一下BM25。\n2.1 BM25\n对于BM25，有搜索Query q，分词之后单词 w，候选文档 d。\n掌握BM25，核心要点有三个：分词之后w的权重，w和q的相似性，w和d的相似性。\n对于搜索场景，显而易见的一个问题就是，我们的搜索query和候选文档的长度是不一样甚至差距很大，所以BM25在计算相似性的时候需要对文档长度做一定的处理。\n2.2 WMD\n话说回来，其次我们谈一谈WMD，项目中并没有用到 WMD 这个获取句子向量的方法，所以这里只是对它有一个简单的了解即可，在这里做一个记录，等以后需要用到的时候， 会在这里继续更新，深挖进入。文章末尾会列出相关参考资源，感兴趣的可以看一看。\nWMD Word Mover’s Distance的缩写，翻译为词移距离，WMD距离越大相似度越小，WMD距离越小文本相似度越大。\n理解这个概念，分为两个步骤。\n首先第一步，有两个文档A和B，A中的每个词遍历和B中每个词的距离，挑出A中每个词和B中每个词中最小的距离，最后相加，得到A到B的WMD。\n这个时候，需要明白第一步是存在巨大问题的。什么问题呢？A，B，C三个文档。A全部词和音乐相关。B中一个词和音乐相关，C中一个词和音乐相关，其余词和音乐有点关系，而且定义B和C中和音乐相关的词是同一个词。\n根据我们的直觉，A和B相关性肯定小于A和C的相关性，但是如果按照第一步的算法去做，会得到相关性相等的结果，这是不对的。\n这是因为A中的所有词匹配到的B中的那个和音乐相关的词（遍历取最小），A中所有的词匹配到C中和音乐相关的词（遍历取最小），B和C中和音乐相关的词一样，就导致相关性相等。\n怎么解决这个问题，这就是第二步，让A中的所有词在匹配的时候，不是遍历取最小，而是让每个词匹配到C中的所有词，只不过匹配的权重不同。\n这个权重从两个部分去看：一个是从A看，要符合分配出去的权重等于自身这个词在本文档的权重，一个是从B看，分配到B的某个词的权重要等于这个词在B文档的权重。\n大概理解到这里。之后关于WMD加速（因为复杂度比较高）的内容就没有进一步的去了解\n2.3 SIF\n这个时候，我想到了一个比较有意思的点。我们知道有监督，比如说SiaGRU 这个方法，也是对句子进行编码，只不过这里的编码我们使用的是基于损失函数和标注语料训练出来的编码。\n注意，这句话里我认为是有个重点的，就是句子编码是基于损失函数训练出来的。\n换句话说，不同的损失函数我们训练出来的句子编码肯定是不一样的，效果也就有好有坏。\n换句话说，我们使用不同的损失函数，比如调整正样本对应的损失权重和正常损失函数训练出来的文本编码肯定是不一样的。\n为什么说这么呢？我们想一下，如果我使用SiaGRU训练出来的句子对的编码，我们直接使用cosine进行相似度的度量，效果会怎么样？\n对于这个问题，大家可以自己实践一下。我想提到的一点就是，我们的句子对的编码并不是基于我们的cosine这种相似度量训练出来的，所以不存在完全匹配，所以效果肯定是比不上有监督的。\n我们更近一步的想一个问题。通过实践，我们在使用bert做句子对的相似度的时候会发现一个问题，就是效果可能还没有使用word2vec的效果好。\n我自己猜测这个原因可能就是因为cosine这个函数没有办法表征出bert训练的句子编码信息，因为bert训练过程复杂，不是consien这种简单函数就可以表达的。这是我自己的一个理解。\n然后，我们详细聊一下SIF这个方法。\n我先说一下这个方法最容易出问题的一个地方：数据预处理的时候不要去除停用词等高频词汇\n掌握SIF，就抓住两个核心要点：\n\n\n词向量加权求和\n\n\n句子词向量矩阵减去主成分投影\n\n\n我们先说核心要点1。其实词向量加权求和这个方法并不是特别的陌生，比如等权求和，IDF权重求和，TF-IDF权重求和等等，这些我们平时都会用到。至于用哪一个就看你的数据集上的效果了。\n我们知道word2vec这种词向量是含有语义信息的（在我看来其实它本质上是一种位置信息，相同句子结构下不同词的词向量可能很相似）。所以等权求和或者加权求和更像是在做Pooling（最大池化，平均池化，加权池化）等等，就是从我们各个词语信息中提取中有用的部分。\n因为我们是无监督，这个权重的设定我们没有办法像在CNN处理图片一样，把参数学出来，所以只能人工的去定这个参数。\n说到这里，想要插一句话，上面说到因为无监督我们无法将权重参数学出来。换句话说，如果是有监督，这个参数是可以学出来的，那怎么学呢？\n最简单的一个方法就是全连接，或者说逻辑回归啊，只要有数据，我们就可以使用机器学习，复杂就使用神经网络基于标注数据把这个参数学出来。\n现在没有标注数据，这个参数我们需要人工去定。这个参数需要含有一定的意义，不是我们一拍脑袋就定个参数，它需要加权求和之后使句子编码含有的语义信息可以对下游任务有帮助或者说在下游任务中效果好才可以。\nSIF这里使用的权重函数是这样的：a/a+p(w) p(w)代表的是词在句子中的频次或者频率。用大白话去描述这个公式就是词在语料中出现的越多，对应的权重越小。这么做就减少了高频词的作用。也是我们不需要在数据处理的时候去除高频词的原因。\n想一下这个公式的作用，其实本质上和IDF是很类似的。它并没有涉及到IF这个概念，也就是词在本句话出现的情况。\n我们再来看核心要点2。什么叫主成分？具体的原理不多说，PCA本质上是找到原始数据存在最大方差的那个方向，让原始数据的投影尽可能的分散，从而可以起到降维的作用。\n核心要点2就是减去了矩阵中共有的一部分信息，比如都含有的高频词汇信息等等。比如原始句子是”我爱吃红烧肉“和”我超不爱吃红烧肉“。减去共有信息之后，剩下的词向量矩阵就回对不相似的信息比较敏感。\n总体来说。SIF这个算法还是很不错，很适合做个基线初期上线的。\n3.有监督文本相似度方法\n我主要是讲两个模型：SiamGRU和ESIM模型\n3.1 SiamGRU\n主要是参考了论文：Siamese Recurrent Architectures for Learning Sentence Similarity.\n从三个核心要点来掌握这个模型：\n1.使用神经网络对句子进行编码；\n2.两个句子共享一个神经网络；\n3.对句子编码进行相似性的度量进行训练。\n首先我们来看核心要点1。\n对于这个我觉得应该这么去想。首先，我们考虑一个使用神经网络进行文本分类的场景。我们使用交叉熵损失函数，基于此，训练神经网络。\n在这个过程中，神经网络可以认为依赖了两个东西，一个是标注数据，\n比如属于娱乐领域或者音乐领域等等，我们通过更新神经网络的参数让我们的模型预测结果不停的逼近这个标注数据。\n其次，我们使用损失函数来度量预测结果和真实数据的差距。损失函数在我看来更像是一种解题方法，\n不同的解题方法可能对应不同的结果，有的解题方法得到的结果既高效又准确，有的方法就很差。\n为什么讲上面这些话？神经网络在上面这个过程中究竟起到了什么作用？在上面这个例子中，神经网络就是一个载体，接受文本数据，输出一个文本编码，从而判定属于哪个类别。所以神经网络的输出是带有语义信息。\nSiamGRU的损失函数应该是什么？数据格式是输入一个句子对，标注结果是两个句子是不是表达同一个含义。这个属于0/1的二分类问题，也就是损失函数和二分类问题的损失函数是一样的，基于此损失函数，我们对模型进行训练。\n对于核心要点2，两个句子共享一个神经网络。最大的好处是可以减少参数量。对于这个，我们可以想一下，能不能不共享参数，而是每个句子对应一个自己的Bilstm/GRU等编码器。\n当然可以，我们的神经网络只是对句子进行编码，每个句子对应自己的编码器也可以做到这样的效果，所以没问题。我们再想一下，有没有必要这么做？关于这一点，我并没有去具体的做实验。我只是说一下自己的理解。\n我觉得是没有必要的。首先，如果每个句子都对应自己的编码器，我想到的一个问题是需不需要对训练数据进行一个翻转重新输入的问题。换句话说，&lt;a,b&gt;作为输入，分别对应&lt;encode1,endode2&gt;。\n那么我需不需要&lt;b,a&gt;作为一次输入再训练一次增加泛化性，我认为这个步骤是需要，这样就增加了训练来量。\n简单来说，我们的句子对的输入不应该对句子对的输入顺序敏感，所以共享编码反而很好的解决了这个问题。\n最后，我们来看核心要点3，我觉得这里还是值得注意的。\n这里我认为有两个细节点需要注意。首先，这里做了一个两个句子编码的交互。对的，SiaGRU在我看来是对两个句子编码进行了交互的，只不过是在后期做的交互。\n为什么这么说呢？这里的相似性的度量使用的是曼哈顿距离，也就是对应坐标差值。注意，这里涉及到了对应坐标位置的操作，所以可以看做是一种交互，而且这种差值在模型训练过程中是会随着模型参数的变化而变化的。\n其次一个细节点，还是曼哈顿距离，这里不是一个标量，也就是不是一个和。简单来说就是对应位置相减之后，每个位置对应一个神经元，然后直接接全连接层就可以。\n如果你认为这里是一个标量，就是很大的问题，标量如何接全连接层最终做二分类呢？\n代码实现部分参考：\nhttps://github.com/DA-southampton/NLP_ability/blob/master/深度学习自然语言处理/文本匹配_文本相似度/src/models.py\n3.2 ESIM\n理解ESIM模型最核心的要点就在于两个句子word层次的attentino就可以了。\n区别于SiamGRU 在后期进行两个句子之间的交互，ESIM在模型中期进行了word层级的两个句子之间的交互。这种交互在我看来是一种attention。\n最近在做多模态的东西，其中一部分的attention和这里的很类似，更准确的说attention形式很类似。\n整个ESIM可以分为三个部分。\n首先第一部分是对两个句子的初期编码，这里和SiamGRU没有什么区别，都是使用神经网络对句子进行信息的提取。\n需要注意的一点是在这里，我们编码之后，比如说LSTM，我们是可以得到每一个时刻或者说每一个单词的输出的，这就为交互中用到的Q/K/V提供了基础。\n这里，我们使用LSTM作为编码器，然后得到编码向量，得到的维度是这样的[batch_size,seq_len,embedding_dim]。\n为了下面解释的更加的方便，我们举个简单的例子，句子a，长度为10，句子b，长度为20.\n现在经过初期编码，句子a：[1,10,300],句子b: [1,20,300]\n随后，我们的操作就是进行交互操作，为了简单，我们省略掉第一个维度。它的交互就是矩阵相乘，得到[10,20]一个矩阵。\n这个矩阵需要根据针对a句子横向做概率归一化，针对b句子纵向做概率归一化。\n上面这句话其实就是ESIM的核心要点。它是一个两个item之间互相做attention，简单称之为both attention。\n这样针对句子a我们就获得了attention之后的语境向量，针对句子b我们也获得了attention之后的语境向量，然后各自在做差和点击，然后结果拼接。\n最后我们用获得的向量拼接输入到另一个编码器，输出pool接全连接就可以了，这块没有什么好说的。\n代码参考链接：\nhttps://github.com/DA-southampton/NLP_ability/blob/master/深度学习自然语言处理/文本匹配_文本相似度/src/models.py\n总结\n针对不同的业务场景挑选不同的匹配模型很考验一个工程师的能力，所以需要掌握每个模型的特点和优缺点。\n参考链接：\n参考链接：常见文本相似度计算方法简介 - 李鹏宇的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/88938220\n短文本相似度算法研究 - 刘聪NLP的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/111414376\n现在工业界有哪些比较实用的计算短文本相似度的算法或者模型？ - vincent的回答 - 知乎\nhttps://www.zhihu.com/question/342548427/answer/806986596\n推荐系统中的深度匹配模型 - 辛俊波的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/101136699\n四种计算文本相似度的方法对比 - 论智的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/37104535\n深度文本匹配发展总结 - xiayto的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/40741576\n常见文本相似度计算方法简介 - 李鹏宇的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/88938220\n参考链接：\nhttps://zhuanlan.zhihu.com/p/48188731\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/五千字全面梳理文本相似度和文本匹配模型/"},{"title":"","date":"2024-06-21T03:48:29.285Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:29.285Z","content":"假如手上有一个文本分类任务，我们在提升模型效果的时候一般有以下几个思路：\n\n\n增大数据集，同时提升标注质量\n\n\n寻找更多有效的文本特征，比如词性特征，词边界特征等等\n\n\n更换模型，使用更加适合当前任务或者说更加复杂的模型，比如FastText–&gt;TextCNN–Bert\n\n\n…\n之后接触到了知识蒸馏，学习到了简单的神经网络可以从复杂的网路中学习知识，进而提升模型效果。\n之前写个一个文章是TextCNN如何逼近Bert，当时写得比较粗糙，但是比较核心的点已经写出来。\n这个文章脱胎于这个论文：Distilling Task-Specific Knowledge from BERT into Simple Neural Networks\n整个训练过程是这样的：\n\n在标签数据上微调Bert模型\n使用三种方式对无标签数据进行数据增强\nBert模型在无标签数据上进行推理，Lstm模型学习Bert模型的推理结果，使用MSE作为损失函数。\n\n目标函数\n知识蒸馏的目标函数：\n\n一般来说，我们会使用两个部分，一个是硬目标损失函数，一个是软目标损失函数，两者都可以使用交叉熵进行度量。\n在原论文中，作者在计算损失函数的时候只是使用到了软目标，同时这个软目标并不是使用softmax之前的logits进行MSE度量损失，也就是并没有使用带有温度参数T的sotmax进行归一化。\n数据增强\n为了促进有效的知识转移，我们经常需要一个庞大的，未标记的数据集。\n三种数据增强的方式：\n\n\nMasking：使用概率$P_{mask}$随机的替换一个单词为[MASK].\n需要注意的是这里替换之后，Bert模型也会输入这个数据的。从直觉上来讲，这个规则可以阐明每个单词对标签的影响。\n\n\nPOS-guided word replacement.使用概率$P_{pos}$随机替换一个单词为另一个相同POS的单词。这个规则有可能会改变句子的语义信息。\n\n\nn-gram sampling\n\n\n整个流程是这样的：对于每个单词，如果概率p&lt;$p_{mask}$，我们使用第一条规则，如果p&lt;$p_{mask}+p_{pos}$，我们使用第二条规则，两条规则互斥，也就是同一个单词只使用两者之间的一个。当对句子中的每个单词都过了一遍之后，我进行第三条规则，之后把整条句子补充道无标签数据集中。\n知识蒸馏结果图\n效果图：\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/模型蒸馏/Bert蒸馏到简单网络lstm/"},{"title":"","date":"2024-06-21T03:48:30.345Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:30.345Z","content":"PKD  核心点就是不仅仅从Bert（老师网络）的最后一层学习知识去做蒸馏，它还另加了一部分，就是从Bert的中间层去学习。\n简单说，PKD的知识来源有两部分：中间层+最后输出。\n它缓解了之前只用最后softmax输出层的蒸馏方式出现的过拟合而导致泛化能力降低的问题。\n接下来，我们从PKD模型的两个策略说起：PKD-Last 和 PKD-Skip。\n1.PKD-Last and PKD-Skip\nPKD的本质是从中间层学习知识，但是这个中间层如何去定义，就各式各样了。\n比如说，我完全可以定位我只要奇数层，或者我只要偶数层，或者说我只要最中间的两层，等等，不一而足。\n那么作者，主要是使用了这么多想法中的看起来比较合理的两种。\nPKD-Last，就是把中间层定义为老师网络的最后k层。\n这样做是基于老师网络越靠后的层数含有更多更重要的信息。\n这样的想法其实和之前的蒸馏想法很类似，也就是只使用softmax层的输出去做蒸馏。但是从感官来看，有种尾大不掉的感觉，不均衡。\n另一个策略是 就是PKD-Skip，顾名思义，就是每跳几层学习一层。\n这么做是基于老师网络比较底层的层也含有一些重要性信息，这些信息不应该被错过。\n作者在后面的实验中，证明了，PKD-Skip 效果稍微好一点（slightly better）；\n作者认为PKD-Skip抓住了老师网络不同层的多样性信息。而PKD-Last抓住的更多相对来说同质化信息，因为集中在了最后几层。\n2. PKD\n2.1架构图\n两种策略的PKD的架构图如下所示，注意观察图，有个细节很容易忽视掉:\n\n我们注意看这个图，Bert的最后一层（不是那个绿色的输出层）是没有被蒸馏的，这个细节一会会提到。\n2.2 怎么蒸馏中间层\n这个时候，需要解决一个问题：我们怎么蒸馏中间层？\n仔细想一下Bert的架构，假设最大长度是128，那么我们每一层Transformer encoder的输出都应该是128个单元，每个单元是768维度。\n那么在对中间层进行蒸馏的时候，我们需要针对哪一个单元？是针对所有单元还是其中的部分单元？\n首先，我们想一下，正常KD进行蒸馏的时候，我们使用的是[CLS]单元Softmax的输出，进行蒸馏。\n我们可以把这个思想借鉴过来，一来，对所有单元进行蒸馏，计算量太大。二来，[CLS] 不严谨的说，可以看到整个句子的信息。\n为啥说是不严谨的说呢？因为[CLS]是不能代表整个句子的输出信息，这一点我记得Bert中有提到。\n2.3蒸馏层数和学生网络的初始化\n接下来，我想说一个很小的细节点，对比着看上面的模型架构图：\nBert（老师网络）的最后一层 (Layer 12 for BERT-Base) 在蒸馏的时候是不予考虑；\n原因的话，其一可以这么理解，PKD创新点是从中间层学习知识，最后一层不属于中间层。当然这么说有点牵强附会。\n作者的解释是最后一层的隐层输出之后连接的就是Softmax层，而Softmax层的输出已经被KD Loss计算在内了。\n比如说，K=5，那么对于两种PKD的模式，被学习的中间层分别是：\nPKD-Skip: $I_{pt} = {2,4,6,8,10}$;\nPKD-Last: $I_{pt} = {7,8,9,10,11}$\n还有一个细节点需要注意，就是学生网络的初始化方式，直接使用老师网络的前几层去初始化学生网络的参数。\n2.4 损失函数\n首先需要注意的是中间层的损失，作者使用的是MSE损失。如下：\n\n整个模型的损失主要是分为两个部分：KD损失和中间层的损失，如下：\n\n超参数问题：\n\n$T:{5,10,20}$\n$\\alpha:{0.2,0.5,0.7}$\n$LR :{5e-5, 2e-5, 1e-5}$\n$\\beta :{10, 100, 500, 1000} $\n\n3. 实验效果\n实验效果可以总结如下：\n\nPKD确实有效，而且Skip模型比Last效果稍微好一点。\nPKD模型减少了参数量，加快了推理速度，基本是线性关系，毕竟减少了层数\n\n除了这两点，作者还做了一个实验去验证：如果老师网络更大，PKD模型得到的学生网络会表现更好吗？\n这个实验我很感兴趣。\n直接上结果图：\n\nKD情况下，注意不是PKD模型，看#1 和#2，在老师网络增加的情况下，效果有好有坏。这个和训练数据大小有关。\nKD情况下，看#1和#3，在老师网络增加的情况下，学生网络明显变差。\n作者分析是因为，压缩比高了，学生网络获取的信息变少了。\n也就是大网络和小网络本身效果没有差多少，但是学生网络在老师是大网络的情况下压缩比大，学到的信息就少了。\n更有意思的是对比#2和#3，老师是大网络的情况下，学生网络效果差。\n这里刚开始没理解，后来仔细看了一下，注意#2 的学生网络是$Bert_{6}[Base]-KD$，也就是它的初始化是从$Bert_{12}[Base]$来的，占了一半的信息。\n好的，写到这里\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/模型蒸馏/PKD-Bert基于多层的知识蒸馏方式/"},{"title":"","date":"2024-06-21T03:48:29.245Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:29.245Z","content":"大家好，我是DASOU；\n今天从代码角度深入了解一下知识蒸馏，主要核心部分就是分析一下在知识蒸馏中损失函数是如何实现的；\n之前写过一个关于BERT知识蒸馏的理论的文章，感兴趣的朋友可以去看一下：Bert知识蒸馏系列(一)：什么是知识蒸馏。\n知识蒸馏一个简单的脉络可以这么去梳理：学什么，从哪里学，怎么学？\n学什么：学的是老师的知识，体现在网络的参数上；\n从哪里学：输入层，中间层，输出层；\n怎么学：损失函数度量老师网络和学生网络的差异性；\n从架构上来说，BERT可以蒸馏到简单的TextCNN，LSTM等，也就可以蒸馏到TRM架构模型，比如12层BERT到4层BERT；\n之前工作中用到的是BERT蒸馏到TextCNN；\n最近在往TRM蒸馏靠近，使用的是 Textbrewer 这个库（这个库太强大了）；\n接下来，我从代码的角度来梳理一下知识蒸馏的核心步骤，其实最主要的就是分析一下损失函数那块的代码形式。\n我以一个文本分类的任务为例子，在阅读理解的过程中，最需要注意的一点是数据的流入流出的Shape，这个很重要，在自己写代码的时候，最重要的其实就是这个；\n首先使用的是MNLI任务，也就是一个文本分类任务，三个标签；\n输入为Batch_data：[32,128]—[Batch_size,seq_len];\n老师网络：BERT_base：12层，Hidden_size为768；\n学生网络：BERT_base：4层，Hidden_size为312；\n首先第一个步骤是训练一个老师网络，这个没啥可说。\n其次是初始化学生网络，然后将输入Batch_data流经两个网络；\n在初始化学生网络的时候，之前有的同学问到是如何初始化的一个BERT模型的；\n关于这个，最主要的是修改Config文件那里的层数，由正常的12改为4，然后如果你不是从本地load参数到学生网络，BERT模型的类会自动调用初始化；\n关于代码实现，我之前写过一个文章，大家可以看这里的代码解析，更加的清洗一点：Pytorch代码验证–如何让Bert在finetune小数据集时更“稳”一点；\n然后我们来说数据首先流经学生网络，我们得到两个东西，一个是最后一层【CLS】的输出，此时未经softmax操作，所以是logits，维度为：[32,3]-[batch_size,label_size];\n第二个东西是中间隐层的输出，维度为:[5,32,128,312]，也就是 [隐层数量,batch_size,seq_len,Hidden_size];\n需要注意的是这里的隐层数量是5，因为正常的隐层在模型定义的时候是4，然后这里是加上了embedding层；\n还有一点需要注意的是，在度量学生网络和老师网络隐层差异的时候，这里是度量的seq_len，也就是对每个token的输出都做了操作；\n如果在这里我们想做类似【CLS】的输出的时候，只需要提取最开始的一个[32,312]的向量就可以；不过，一般来说我们不这么做；\n其次流经老师网络，我们同样得到两个东西，一个是最后一层【CLS】的输出，此时未经softmax操作，所以是logits，维度为：[32,3]-[batch_size,label_size];\n第二个东西是中间隐层的输出，维度为:[5,32,128,768]，也就是 [隐层数量,batch_size,seq_len,Hidden_size];\n这里需要注意的是老师网络和学生网络隐层数量不一样，一个是768，一个是312。\n这其实是一个很常见的现象；就是我们的学生网络在减少参数的时候，不仅会变矮，有时候我们也想让它变窄，也就是隐层的输出会发生变化，从768变为312；\n这个维度的变化需要注意两点，首先就是在学生模型初始化的时候，不能套用老师网络的对应层的参数，因为隐层Hidden_size发生了变化。所以一般调用的是BERT自带的初始化方式；\n其次就是在度量学生网络和老师网络差异性的时候，因为矩阵大小不一致，不能直接做MSE。在代码层面上，需要做一个线性映射，才能做MSE。\n而且还需要注意的一点是，由于老师网络已经固定不动了，所以在做映射的时候我们是要对学生网路的312加一个线性层转化到768层，也就是说这个线性层是加在了学生网络；\n整个架构的损失函数可以分为三种：首先对于【CLS】的输出，使用KL散度度量差异；对于隐层输出使用MSE和MMD损失函数进行度量；\n对于损失函数这块的选择，其实我觉得没啥经验可说，只能试一试；\n看了很多论文加上自己的经验，一般来说在最后面使用KL，中间层使用MSE会更好一点；当然有的实验也会在最后一层直接用MSE；玄学。\n在初看代码的时候，MMD这个之前我没接触过，还特意去看了一下，关于理论我就不多说了，一会看代码吧。\n首先对【CLS】的输出，代码如下：\n12345678def kd_ce_loss(logits_S, logits_T, temperature=1):    if isinstance(temperature, torch.Tensor) and temperature.dim() &gt; 0:        temperature = temperature.unsqueeze(-1)    beta_logits_T = logits_T / temperature    beta_logits_S = logits_S / temperature    p_T = F.softmax(beta_logits_T, dim=-1)    loss = -(p_T * F.log_softmax(beta_logits_S, dim=-1)).sum(dim=-1).mean()    return loss\n首先对于 logits_S，就是学生网络的【CLS】的输出，logits_T就是老师网络【CLS】的输出，temperature 在代码中默认参数是1，例子中设置为了8；\n整个代码其实很简单，就是先做Temp的一个转化，注意这里我们对学生网络的输出和老师网络的输出都做了转化，然后做loss计算；\n其次我们来看比较复杂的中间层的度量；\n首先需要掌握一点，就是学生网络和老师网络层之间的对应关系；\n学生网络是4层，老师网络12层，那么在对应的时候，简单的对应关系就是这样的：\n12345layer_T : 0, layer_S : 0,layer_T : 3, layer_S : 1, layer_T : 6, layer_S : 2, layer_T : 9, layer_S : 3,layer_T : 12, layer_S : 4，\n这个对应关系是需要我们认为去设定的，将学生网络的1层对应到老师网络的12层可不可以？当然可以，但是效果不一定好；\n一般来说等间隔的对应上就好；\n这个对应关系其实还有一个用处，就是学生网络在初始化的时候【假如没有变窄，只是变矮，也就是层数变低了】，那么可以从依据这个对应关系把权重copy过来；\n学生网络的隐层输出为：[5,32,128,312],老师网络隐层输出为[5,32,128,768]\n那么在代码实现的时候，需要做一个zip函数把对应层映射过去，然后每一层计算MSE，然后加起来作为损失函数；\n我们来看代码：\n12345678910111213141516171819202122inters_T = &#123;feature: results_T.get(feature,[]) for feature in FEATURES&#125;inters_S = &#123;feature: results_S.get(feature,[]) for feature in FEATURES&#125;for ith,inter_match in enumerate(self.d_config.intermediate_matches):    if type(layer_S) is list and type(layer_T) is list: ## MMD损失函数对应的情况        inter_S = [inters_S[feature][s] for s in layer_S]        inter_T = [inters_T[feature][t] for t in layer_T]        name_S = &#x27;-&#x27;.join(map(str,layer_S))        name_T = &#x27;-&#x27;.join(map(str,layer_T))        if self.projs[ith]: ## 这里失去做学生网络隐层的映射            #inter_T = [self.projs[ith](t) for t in inter_T]            inter_S = [self.projs[ith](s) for s in inter_S]    else:## MSE 损失函数        inter_S = inters_S[feature][layer_S]        inter_T = inters_T[feature][layer_T]        name_S = str(layer_S)        name_T = str(layer_T)        if self.projs[ith]:            inter_S = self.projs[ith](inter_S) # 需要注意的是隐层输出是312，但是老师网络是768，所以这里要做一个linear投影到更高维，方便计算损失函数            intermediate_loss = match_loss(inter_S, inter_T, mask=inputs_mask_S)  ## loss = F.mse_loss(state_S, state_T)    total_loss += intermediate_loss * match_weight\n这个代码里面比如迷糊的是【self.d_config.intermediate_matches】，打印出来发现是这个东西：\n12345678910IntermediateMatch: layer_T : 0, layer_S : 0, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : 3, layer_S : 1, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : 6, layer_S : 2, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : 9, layer_S : 3, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : 12, layer_S : 4, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : [0, 0], layer_S : [0, 0], feature : hidden, weight : 1, loss : mmd, proj : None, IntermediateMatch: layer_T : [3, 3], layer_S : [1, 1], feature : hidden, weight : 1, loss : mmd, proj : None, IntermediateMatch: layer_T : [6, 6], layer_S : [2, 2], feature : hidden, weight : 1, loss : mmd, proj : None, IntermediateMatch: layer_T : [9, 9], layer_S : [3, 3], feature : hidden, weight : 1, loss : mmd, proj : None, IntermediateMatch: layer_T : [12, 12], layer_S : [4, 4], feature : hidden, weight : 1, loss : mmd, proj : None\n简单说，这个变量存储的就是上面我们谈到的层与层之间的对应关系。前面5行就是MSE损失函数度量，后面那个注意看，层数对应的时候是一个列表，对应的是MMD损失函数；\n我们来看一下MMD损失的代码形式：\n12345678910111213141516def mmd_loss(state_S, state_T, mask=None):    state_S_0 = state_S[0] # (batch_size , length, hidden_dim_S)    state_S_1 = state_S[1] # (batch_size , length, hidden_dim_S)    state_T_0 = state_T[0] # (batch_size , length, hidden_dim_T)    state_T_1 = state_T[1] # (batch_size , length, hidden_dim_T)    if mask isNone:        gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(2)  # (batch_size, length, length)        gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(2)        loss = F.mse_loss(gram_S, gram_T)    else:        mask = mask.to(state_S[0])        valid_count = torch.pow(mask.sum(dim=1), 2).sum()        gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(2)  # (batch_size, length, length)        gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(2)        loss = (F.mse_loss(gram_S, gram_T, reduction=&#x27;none&#x27;) * mask.unsqueeze(-1) * mask.unsqueeze(1)).sum() / valid_count    return loss\n看最重要的代码就可以：\n1234state_S_0 = state_S[0]#  32 128 312 (batch_size , length, hidden_dim_S)state_T_0 = state_T[0] #  32 128 768 (batch_size , length, hidden_dim_T)gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(2) gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(2)\n简单说就是现在自己内部计算bmm，然后两个矩阵之间做mse；这里如果我没理解错使用的是一个线性核函数；\n损失函数代码大致就是这样，之后有时间我写个简单的repository，梳理一下整个流程；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/模型蒸馏/BERT知识蒸馏代码解析-如何写好损失函数/"},{"title":"","date":"2024-06-21T03:48:30.375Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:30.375Z","content":"大家好，我是DASOU，今天介绍一下：BERT-of-Theseus\n这个论文我觉得还挺有意思，攒个思路。\n读完这个文章，BERT-of-Theseus 掌握以下两点就可以了：\n\n\n基于模块替换进行压缩\n\n\n除了具体任务的损失函数，没有其他多余损失函数。\n\n\n效果的话，与$Bert-base$相比，$BERT-of-Theseus$：推理速度 $1.94$；模型效果 98%；\n模块替换\n举个例子，比如有一个老师网络是12层的Bert，现在我每隔两层Transformer，替换为学生网络的一层Transformer。那么最后我的学生网络也就变成了6层的小Bert，训练的时候老师网络和学生网络的模块交替训练。\n直接看下面这个架构图：\n\n作者说他是受 Dropout 的启发，仔细想了想还真的挺像的。\n我们来说一下这样做的好处。\n我刚才说每隔老师网络的两层替换为学生网络的一层。很容易就想到PKD里面，有一个是PKD-Skip策略。\n就是每隔几层，学生网络的层去学习老师网络对应层的输出，使用损失函数让两者输出接近，使用的是CLS的输出。\n在这里提一下蒸馏/压缩的基本思想，一个最朴素的想法就是让学生网络和老师网络通过损失函数在输出层尽可能的靠近。\n进一步的，为了提升效果，可以通过损失函数，让学生网络和老师网络在中间层尽可能的靠近，就像PKD这种。\n这个过程最重要的就是在训练的时候需要通过损失函数来让老师网络和学生网络尽可能的接近。\n如果是这样的话，问题就来了，损失函数的选取以及各自损失函数之前的权重就需要好好的选择，这是一个很麻烦的事情。\n然后我们再来看 BERT-of-Theseus，它就没有这个问题。\n它是在训练的时候以概率 $r$ 来从老师网络某一层和学生网络的某一层选择一个出来，放入到训练过程中。\n在这个论文里，老师网络叫做  $predecessor$， 学生网络叫做 $successor$ ；\n训练过程\n对着这个网络架构，我说一下整体训练的过程：\n\n在具体任务数据上训练一个 BERT-base 网络作为 $predecessor$；\n使用 $predecessor$  前六层初始化一个 6层的Bert作为 $successor$ ；\n在具体任务数据上，固定 $predecessor$ 相应权重，以概率$r$（随着steps，线性增加到1），对整个网络（$predecessor$加上$successor$ ）进行整体的训练。\n为了让$successor$  作为一个整体，单独抽离出来$successor$ （其实$r$设置为1就可以了），作为一个单独的个体，在训练数据上继续微调。直至效果不再增加。\n\n简单总结，在训练数据上，老师网络和学生网络共同训练，因为存在概率问题，有的时候是老师网络的部分层加入训练，有的时候是学生网络的部分层加入训练。在这一步训练完成之后，为了保证学生网络作为一个整体（因为在第一步训练的时候大部分情况下学生网络的层都是分开加入训练过程的），在具体任务数据上，对学生网络继续微调，直至效果不再增加。\n结果分析\n不同方法的损失函数\n论文提供了一个不同Bert蒸馏方法使用的损失函数的图，值得一看，见下图：\n\n值得注意的是，这里的 $Finetuning$应该是选取前六层，在具体任务微调的结果。\n效果\n\n整体来说，BERT-of-Theseus 思路很简单，效果也还不错。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/模型蒸馏/Theseus-模块压缩交替训练/"},{"title":"","date":"2024-06-21T03:48:28.955Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:28.955Z","content":"为什么需要做模型蒸馏？\nBert类模型精读高，但是推理速度慢，模型蒸馏可以在速度和精读之间做一个平衡。\n\n从蒸馏方法\n\n从蒸馏方法来看，一般可以分为三种：\n\n\n参数的共享或者剪枝\n\n\n低秩分解\n\n\n知识蒸馏\n\n\n对于1和2，可以参考一下 Albert。\n而对于知识蒸馏来说，本质是通过一种映射关系，将老师学到的东西映射到或者说传递给学生网络。\n在最开始的时候，一般都会有一种疑问？ 我有训练数据了，训练数据的准确度肯定比你大模型的输出结构准确度高，为什么还需要从老师网络来学习知识？\n我觉得对于这个问题，我在李如的文章看到这样一句话：”好模型的目标不是拟合训练数据，而是学习如何泛化到新的数据“\n我觉得写的很好。对于这个问题，我们这么去想，我们的大模型的输出对于logits不仅仅是类别属于哪一个，还有一个特点就是会给出不同类别之间的一个关系。\n比如说，在预测”今天天气真不错，现在就决定了，出去浪一波，来顿烧烤哦“。\n文本真实标签可能就直接给出了”旅游“这个标签，而我们的模型在概率输出的时候可能会发现”旅游“和”美食“两个标签都还行。\n这就是模型从数据中学习到的一种”暗知识“（好像是这么叫，忘了在哪里看到了）、\n而且还存在一个问题，有些时候是没有那么多训练数据的，需要的是大模型Bert这种给出无监督数据的伪标签作为冷启动也是不错的。\n\n从蒸馏结构\n\n从蒸馏结构来说，我们可以分为两种：\n\n\n从transformer到transformer结构\n\n\n从transformer结构到别的模型（CNN或者lstm结构）\n\n\n我主要是想聊一下 Bert 到 TextCNN模型的蒸馏。\n为啥选择textcnn？最大的原因就是速度快精读还不错。\n论文参考 Distilling Task-Specific Knowledge from BERT into Simple Neural Networks\n对于这个蒸馏，对于我而言，最重要的掌握一个点就是损失函数的设定，别的地方我暂且不考虑。\n对于损失函数，分为两个部分，一个是我当前lstm输出结果和真实标签的交叉熵损失，一个是我的当前lstm输出结果和大模型bert的输出logits的平方损失。\n至于为啥一个是交叉熵一个是平方损失，是因为其实前面的看做分类问题，后面的看做回归问题。当然只是谁更合适的选择问题。\n因为是加权两个部分做损失，我这边选择为都是0.5。\n当然在李如的文章中谈到，可能真实标签这边的权重小一点会更好一点，因为蒸馏本质上还是想多关注bert的输出多一点。\n关于这个论文有一个很好的解释：\n知识蒸馏论文选读（二） - 小禅心的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/89420539\n关于模型蒸馏，我就简单了解到这里，可能之后会花费大量精力看看背的蒸馏方式，放上开源代码：\nbert到lstm的蒸馏\nbert到textcnn/lstm/lkeras/torch\n一个pytorch实现的模型蒸馏库\n罗列一下关于Bert模型蒸馏的文章和博客：\n首先一个讲的比较好的文章就是下面这个文章，比较系统的讲了一遍\nBERT知识蒸馏综述 - 王三火的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/106810758\n还有一个文章讲的比较好的是：BERT 模型蒸馏 Distillation BERT\nhttps://www.jianshu.com/p/ed7942b5207a\n这个文章就是比较系统的对比了Bert的两个蒸馏操作：DistilBERT 和 Distilled BiLSTM  我觉得写得还不错\n从实战的角度来说，我觉得写得很好的就是：BERT 蒸馏在垃圾舆情识别中的探索\nhttps://blog.csdn.net/alitech2017/article/details/107412038\n这个文章是对bert的蒸馏，到textcnn，使用了多种方式并且比较了最终的结果。\n接下来是李如的这个文章，很概括，确实大佬，写得很好：\n【DL】模型蒸馏Distillation - 李如的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/71986772\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/模型蒸馏/bert2textcnn模型蒸馏/"},{"title":"","date":"2024-06-21T03:48:30.395Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:30.395Z","content":"大家好，我是DASOU，今天说一下 TinyBert；\nTinyBert 主要掌握两个核心点：\n\n\n提出了对基于 transformer 的模型的蒸馏方式：Transformer distillation；\n\n\n提出了两阶段学习框架：在预训练和具体任务微调阶段都进行了 Transformer distillation（两阶段有略微不同）；\n\n\n下面对这两个核心点进行阐述。\n1. Transformer distillation\n1.1整体架构\n整体架构如下：\n\nBert不严谨的来划分，可以分为三个部分：词向量输入层，中间的TRM层，尾端的预测输出层。\n在这个论文里，作者把词向量输入层 和中间的TRM层统一称之为中间层，大家读的时候需要注意哈。\nBert的不同层代表了学习到了不同的知识，所以针对不同的层，设定不同的损失函数，让学生网络向老师网络靠近，如下：\n\nebedding层的输出\n多头注意力层的注意力矩阵和隐层的输出\n预测层的输出\n\n1.2 Transformer 基础知识：\n注意力层：\n\n多头注意力层：\n\n前馈神经网路：\n\n1.3 Transformer 的蒸馏\n对 Transformer的蒸馏分为两个部分：一个是注意力层矩阵的蒸馏，一个是前馈神经网络输出的蒸馏。\n注意力层矩阵蒸馏的损失函数：\n\n这里注意两个细节点：\n一个是使用的是MSE；\n还有一个是，使用的没有归一化的注意力矩阵，见(1)，而不是softmax之后的。原因是实验证明这样能够更快的收敛而且效果会更好。\n前馈神经网络蒸馏的损失函数\n\n两个细节点：\n第一仍然使用的是MSE.\n第二个细节点是注意，学生网路的隐层输出乘以了一个权重矩阵$w_{h}$，这样的原因是学生网络的隐层维度和老师网络的隐层维度不一定相同。\n所以如果直接计算MSE是不行的，这个权重矩阵也是在训练过程中学习的。\n写到这里提一点，其实这里也可以看出来为什么tinybert的初始化没有采用类似PKD这种，而是使用GD过程进行蒸馏学习。\n因为我们的tinybert 在减少层数的同时也减少了宽度（隐层的输出维度），如果采用PKD这种形式，学生网络的维度和老师网络的维度对不上，是不能初始化的。\n词向量输入层的蒸馏：\n\n预测层输出蒸馏：\n\n1.4 总体蒸馏损失函数\n\n2. 两阶段蒸馏\n2.1 整体架构\n整体架构如图：\n\n2.2 为什么需要GD:\n说一下我自己的理解哈，我觉得有两个原因：\n首先，就是上文说到的，tinybert不仅降低了层数，也降低了维度，所以学生网络和老师网络的维度是不符的，所以PKD这种初始化方式不太行。\n其次，一般来说，比如PKD，学生网络会使用老师网络的部分层进行初始化。这个从直觉上来说，就不太对。\n老师网络12层，学到的是文本的全部信息。学生网络是6层，如果使用老师的12层的前6层进行初始化，这个操作相当于认为这前6层代表了文本的全部信息。\n当然，对于学生网络，还会在具体任务上微调。这里只是说这个初始化方式不太严谨。\nTiny bert的初始化方式很有意思，也是用了蒸馏的方式。\n老师网络是没有经过在具体任务进行过微调的Bert网络，然后在大规模无监督数据集上，进行Transformer distillation。当然这里的蒸馏就没有预测输出层的蒸馏，翻看附录，发现这里只是中间层的蒸馏。\n简单总结一下，这个阶段，使用一个预训练好的Bert（ 尚未微调）进行了3epochs的 distillation；\n2.3 TD：\nTD就是针对具体任务进行蒸馏。\n核心点：先进行中间层（包含embedding层）的蒸馏，再去做输出层的蒸馏。\n老师网络是一个微调好的Bert，学生网络使用GD之后的tinybert，对老师网络进行TD蒸馏。\nTD过程是，先在数据增强之后的数据上进行中间层的蒸馏-10eopchs，learning rate 5e-5；然后预测层的蒸馏3epochs，learning rate 3e-5.\n3. 数据增强\n在具体任务数据上进行微调的时候，进行了数据增强。\n(感觉怪怪的)\n两个细节点：\n\n\n对于 single-piece word 通过Bert找到当前mask词最相近的M个单词；对于 multiple sub-word pieces 使用Glove和Consine找到最相近的M个词\n\n\n通过概率P来决定是否替换当前的词为替换词。\n\n\n对任务数据集中的所有文本数据做上述操作，持续N次。\n\n\n伪代码如下：\n\n4. 实验效果\n其实我最关心的一个点就是，数据增强起到了多大的作用。\n作者确实也做了实验，如下，数据增强作用还是很大的：\n\n我比较想知道的是，在和PKD同等模型架构下，两者的比较，很遗憾，作者好像并没有做类似的实验(或者我没发现)。\n这里的tinybert参数如下：\n\nthe number of layers M=4, the hidden size d 0=312, the feedforward/filter size d 0 i=1200 and the head number h=12.\n\n5. 简单总结\n先说一下，我读完论文学到的东西：\n首先是transformer层蒸馏是如何涉及到的损失函数：\n\n注意力矩阵和前馈神经层使用mse；\n蒸馏的时候注意力矩阵使用未归一化\n维度不同使用权重矩阵进行转化\n\n其次，维度不同导致不能从老师Bert初始化。GD过程为了解决这个问题，直接使用学生网络的架构从老师网络蒸馏一个就可以，这里并不是重新学一个学生网络。\n还有就是数据增强，感觉tinyebert的数据增强还是比较简陋的，也比较牵强，而且是针对英文的方法。\nTD过程，对不同的层的蒸馏是分开进行的，先进行的中间层的蒸馏，然后是进行的输出层的蒸馏，输出层使用的是Soft没有使用hard。\n这个分过程蒸馏很有意思，之前没注意到这个细节点。\n在腾讯的文章中看到这样一句话：\n\n并且实验中，softmax cross-entropy loss 容易发生不收敛的情况，把 softmax 交叉熵改成 MSE, 收敛效果变好，但泛化效果变差。这是因为使用 softmax cross-entropy 需要学到整个概率分布，更难收敛，因为拟合了 teacher BERT 的概率分布，有更强的泛化性。MSE 对极值敏感，收敛的更快，但泛化效果不如前者。\n\n是有道理的，积累一下。\n值得看的一些资料：\n比 Bert 体积更小速度更快的 TinyBERT - 腾讯技术工程的文章 - 知乎 https://zhuanlan.zhihu.com/p/94359189\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/模型蒸馏/tinybert-全方位蒸馏/"},{"title":"","date":"2024-06-21T03:48:30.835Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:30.835Z","content":"Bert知识蒸馏系列(一)：什么是知识蒸馏\n全文参考的论文是：Distilling the Knowledge in a Neural Network\n参考的讲解的比较的博文是：\n《Distilling the Knowledge in a Neural Network》知识蒸馏 - musk星辰大海的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/75031938\n这个含有Hiton的PPT介绍\n【经典简读】知识蒸馏(Knowledge Distillation) 经典之作 - 潘小小的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/102038521\n这个把其中的公式推导写的比较明白\n如何理解soft target这一做法？ - YJango的回答 - 知乎\nhttps://www.zhihu.com/question/50519680/answer/136406661\nBert 系列文章\n\n\nBert 模型压缩\n什么是知识蒸馏？知识蒸馏基础概念一览。\n\n\nBert 的后续改进\nAlbert\nRobert\n\n\n什么是蒸馏\n一般来说，为了提高模型效果，我们可以使用两种方式。一种是直接使用复杂模型，比如你原来使用的TextCNN，现在使用Bert。一种是多个简单模型的集成，这种套路在竞赛中非常的常见。\n这两种方法在离线的时候是没有什么问题的，因为不涉及到实时性的要求。但是一旦涉及到到部署模型，线上实时推理，我们需要考虑时延和计算资源，一般需要对模型的复杂度和精度做一个平衡。\n这个时候，我们就可以将我们大模型学到的信息提取精华灌输到到小模型中去，这个过程就是蒸馏。\n什么是知识\n对于一个模型，我们一般关注两个部分：模型架构和模型参数。\n简答的说，我们可以把这两个部分当做是我们模型从数据中学习到的信息或者说是知识（当然主要是参数，因为架构一般来说是训练之前就定下来的）\n但是这两个部分，对于我们来说，属于黑箱，就是我们不知道里面究竟发生了什么事情。\n那么什么东西是我们肉眼可见的呢？从输入向量到输出向量的一个映射关系是可以被我们观测到的。\n简单来说，我输入一个example，你输出是一个什么情况我是可以看到的。\n区别于标签数据格式 [0,0,1,0],模型的输出结果一般是这样的：[0.01,0.01,0.97,0.01]。\n举个比较具象的例子，就是如果我们在做一个图片分类的任务，你的输入图像是一辆宝马，那么模型在宝马这个类别上会有着最大的概率值，与此同时还会把剩余的概率值分给其他的类别。\n这些其他类别的概率值一般都很小，但是仍然存在着一些信息，比如垃圾车的概率就会比胡萝卜的概率更高一些。\n模型的输出结果含有的信息更丰富了，信息熵更大了，我们进一步的可以把这种当成是一种知识，也就是小模型需要从大模型中学习到的经验。\n这个时候我们一般把大模型也就是复杂模型称之为老师网络，小模型也就那我们需要的蒸馏模型称之为学生网络。学生网络通过学习老师网络的输出，进而训练模型，达到比较好的收敛效果。\n为什么知识蒸馏可以获得比较好的效果\n在前面提到过，卡车和胡萝卜都会有概率值的输出，但是卡车的概率会比胡萝卜大，这种信息是很有用的，它定义了一种丰富的数据相似结构。\n上面谈到一个问题，就是不正确的类别概率都比较小，它对交叉熵损失函数的作用非常的低，因为这个概率太接近零了，也就是说，这种相似性存在，但是在损失函数中并没有充分的体现出来。\n第一种就是，使用sofmax之前的值，也就是logits，计算损失函数\n第二种是在计算损失函数的时候，使用温度参数T，温度参数越高，得到的概率值越平缓。通过升高温度T，我们获取“软目标”，进而训练小模型\n其实对于第一种其实是第二种蒸馏方式的的一种特例情况，论文后续有对此进行证明。\n这里的温度参数其实在一定程度上和蒸馏这个名词相呼应，通过升温，提取精华，进而灌输知识。\n带温度参数T的Softmax函数\n软化公式如下：\n\n说一下为什么需要这么一个软化公式。上面我们谈到，通过升温T，我们得到的概率分布会变得比较平缓。\n用上面的例子说就是，宝马被识别为垃圾车的概率比较小，但是通过升温之后，仍然比较小，但是没有那么小（好绕口啊）。\n也就是说，数据中存在的相似性信息通过升温被放大了，这样在计算损失函数的时候，这个相似性才会被更大的注意到，才会对损失函数产生比较大的影响力。\n损失函数\n\n损失函数是软目标损失函数和硬目标损失函数的结合，一般来说，软目标损失函数设置的权重需要大一些效果会更好一点。\n如何训练\n整体的算法示意图如下：\n\n整体的算法示意图如上所示：\n\n首先使用标签数据训练一个正常的大模型\n使用训练好的模型，计算soft targets。\n训练小模型，分为两个步骤，首先小模型使用相同的温度参数得到输出结果和软目标做交叉熵损失，其次小模型使用温度参数为1，和标签数据（也就是硬目标）做交叉损失函数。\n预测的时候，温度参数设置为1，正常预测。\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/模型蒸馏/什么是知识蒸馏/"},{"title":"","date":"2024-06-20T11:38:15.751Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:38:15.751Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/IMG%20Dos');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/IMG Dos/"},{"title":"","date":"2024-06-21T03:48:30.995Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:30.995Z","content":"本文是对苏神文章的解读，主要是关于公式推导中省略的部分细节记录了自己的理解，希望能帮助大家更好的理解。\n模型训练的时候，我们会把数据分为训练数据和开发数据。\nLoss变化一般是这样的：训练集损失在不停的降低，开发集先降低随后上升。\n我们一般选择两条线的交叉点（其实也没有交叉），也就是开发数据集开始上升的那个点作为我们的最终模型的选择，这样既可以得到最好的结果，也可以避免过拟合。\n这个论文思路是这样的，当损失函数降低的一定程度（足够小）的时候，改变损失函数为:\n$$\n\\widetilde{J_{\\theta}} =|J_{\\theta}-b|+b\\tag{1}\n$$\n公式 $(1)$ 中 $J_{\\theta}$  为原始的损失函数， $\\widetilde{J_{\\theta}}$ 改变之后的损失函数。\n观察这个公式，其实可以这样去描述：\n\n\n当 $J_{\\theta} \\geq b$ 时，损失函数就是$J_{\\theta}$；\n\n\n当$J_{\\theta} &lt; b$ 的时候，损失函数就是$\\widetilde{J_{\\theta}}=2b - J_{\\theta}$。\n\n\n这个时候，我们想一下梯度下降算法公式，如下:\n$$\n\\theta_{n}=\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})\\tag{2}\n$$\n所以，当损失函数为 $\\widetilde{J_{\\theta}}$ 的时候，符号就会发生变化，这个时候我们就使用就不是梯度下降而是梯度上升算法。也就是说，以$b$ 为临界点，在交替的进行梯度上升和梯度下降算法。\n论文发现，在某些任务上，使用这个方法，开发集上的损失函数会发生二次下降。\n再次说一下，关于这一点，苏剑林给出来相关的数学推导（参考链接放在文章末尾）。不过有个关于泰勒公式的展开跳过了，我简单做了一个补充，帮助自己和大家理解。\n首先如果交替做梯度上升和梯度下降算法，参数更新公式如下所示:\n$$\n\\theta_{n}=\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})\n$$\n$$\n\\theta_{n+1}=\\theta_{n}+ \\alpha\\nabla J(\\theta_{n}) \\tag{3}\n$$\n对此公式上下消参 中，我们可以得到：\n$$\n\\theta_{n+1}= \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})+ \\alpha\\nabla J( \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1}))\\tag{4}\n$$\n对于公式$(4)$ ，重点是对 $ J(\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1}))$ 这个损失函数进行一个剖析化简，这里用到了泰勒公式的展开。\n先回忆一下泰勒公式，这里直接给出一个一阶泰勒公式的展开:\n$$\nJ(\\omega) \\approx   J(\\omega_{0}) + (\\omega - \\omega_{0})*J^{'}(\\omega_{0}) + \\epsilon  \\qquad  \\omega_{0} 和 \\omega 足够接近\\tag{5}\n$$\n注意，这个时候，我们仔细观察公式  $ J(\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1}))$  和 公式$(5)$。\n首先，我们知道的是，$\\alpha\\nabla J(\\theta_{n-1}))$ 是每次参数更新时候的增量，在损失函数足够小的时候，我们每次参数更新的增量可以认定是一个极小值。\n换句话说，$\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})$可以对应到我们公式(5) 中的$\\omega_{0}$，$\\theta_{n-1}$ 对应的就是公式(5) 中的 $\\omega$\n也就是说，\n$$\nJ( \\theta_{n-1}) \\approx  J( \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})) +\\alpha\\nabla J(\\theta_{n-1})*\\nabla J(\\theta_{n-1})  \\tag{6}\n$$\n这里需要注意的，公式最后面一个$\\nabla J(\\theta_{n-1})$ 的由来。按道理，这里应该是对$J^{'}(\\omega_{0})$进行求导 。但是这里因为$\\omega 和 \\omega_{0}$ 非常的相近，我们直接使用对$J(\\omega)$的求导结果就可以，这一点是个比较重要的细节点。\n基于此，我们可以继续往下推导：\n$$\n\\theta_{n+1}= \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})+ \\alpha\\nabla J( \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1}))\n$$\n$$\n\\approx \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})+ \\alpha\\nabla (J (\\theta_{n-1})- \\alpha\\nabla J(\\theta_{n-1})*\\nabla J(\\theta_{n-1}))\n$$\n$$\n= \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})+ \\alpha\\nabla J (\\theta_{n-1})- \\alpha^{2}\\nabla (\\nabla J(\\theta_{n-1})*\\nabla J(\\theta_{n-1}))\n$$\n$$\n= \\theta_{n-1}- \\alpha^{2}\\nabla ||\\nabla J(\\theta_{n-1})||^{2}  \\tag{7}\n$$\n这里，我还想提一点就是 $||\\nabla J(\\theta_{n-1})||^{2}$ ，它是两个求微分函数的乘积，所以结果是一个带参数的函数，也就是求得一个微分之后，做一个平方，得到的函数，这个时候在参数更新的时候，我们带入相应的值就可以了。\n我们针对这个公式(7)，会发现一个很奇怪的现象，就是参数更新的模式没有发生变化，都是进行了梯度下降（注意开头我们单从损失函数看是认为梯度下降和梯度上升是交替进行的，两个理解其实都没有问题）。\n只是，当前步骤的参数更新不再是取决于上一个步骤，而是取决于上上一个步骤的参数。\n这一点，我是这么理解的。使用普通的损失函数，相当于此时我们站在上一个步骤往山下看。当损失函数非常小的时候，极有可能会陷入局部最小值，并不是全局最优点。此时寻找出来的更新的方向，还是局限于局部最优点。而使用新的损失函数，我们通过公式，最直观的感受就是，是站在了上上一个步骤，是脱离了当前的视线(虽然只是差了一个步骤)，相当于视野变大了，有更大的可能跳出当前的局部最优点，从而寻找到全局最优点。\n我的理解就是这样的，当然苏神给出了另一个解释。大家可以去看一下。我这个文章主要是对他的公式推导中的跳过的泰勒公式的展开做了一个比较详细的阐述，记录下来，方便自己和大家理解。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/论文解读/模型训练需不需要将损失降低为零/"},{"title":"","date":"2024-06-21T03:48:31.455Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:31.455Z","content":"CBOW和skip-gram相较而言，彼此相对适合哪些场景\n先用一句话来个结论：CBOW比Skip-gram 训练速度快，但是Skip-gram可以得到更好的词向量表达。\n为什么这么说？\n因为我们知道两种优化方式只是对softmax的近似优化，不会影响最终结果，所以这里，我们讨论的时候，为了更加的清晰讲解，不考虑优化的情况。\n使用一句话作为一个例子： “我/永远/爱/中国/共产党”\n先说CBOW，我们想一下，它的情况是使用周围词预测中心词。如果“爱”是中心词，别的是背景词。对于“爱”这个中心词，只是被预测了一次。\n对于Skip-gram，同样，我们的中心词是“爱”，背景词是其他词，对于每一个背景词，我们都需要进行一次预测，每进行一次预测，我们都会更新一次词向量。也就是说，相比CBOW，我们的词向量更新了2k次（假设K为窗口，那么窗口内包含中心词就有2k+1个单词）\n想一下是不是这么回事？Skip-gram被训练的次数更多，那么词向量的表达就会越丰富。\n如果语料库中，我们的的低频词很多，那么使用Skip-gram就会得到更好的低频词的词向量的表达，相应的训练时长就会更多。\n简单来说，我们视线回到一个大小为K的训练窗口（窗口内全部单词为2k+1个），CBOW只是训练一次，Skip-gram 则是训练了2K次。当然是Skip-gram词向量会更加的准确一点，相应的会训练的慢一点。\n欢迎大佬拍砖\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/CBOW和skip-gram相较而言，彼此相对适合哪些场景/"},{"title":"","date":"2024-06-21T03:48:31.515Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:31.515Z","content":"我先说一个小问题，估计很多人也有疑惑。\n看了很多文章，有的说是fasttext是CBOW的简单变种，有的说是Skip-gram的变种。究竟哪个是对的？\n带着这个问题，我们来聊一聊Fasttext。首先Fasttext涉及到两个论文：\n\n第一个是Bag of Tricks for Efficient TextClassification(201607)。它解决的问题是使用Fasttext进行文本分类\n第二个是Enriching Word Vectors with Subword Information(201607) 。它解决的是使用Fasttext训练词向量。\n\n今天这个文章，主要谈一下Bag of Tricks for Efficient Text Classification 这个论文 ，主要涉及到的就是文本分类的问题。\nFasttext用作文本分类，做到了速度和精读的一个平衡：标准多核CPU情况下，不到十分钟，可以训练超过十亿个单词。不到一分钟，可以对50万个句子在312千个类别中进行分类。\n这么说，其实不太明显，简单算一下。假设每个句子含有20个单词，那么十亿个单词对应就是五千万个句子，换句话讲在多核CPU的条件下，一分钟左右可以训练500万个句子。\n和Bert比较一下，在GPU条件下，8个小时训练300万条数据左右。相比之下Fasttext的这个速度是真的太快了。\n在这个论文中，也就是使用做文本分类的Fasttext，使用的是CBOW的架构。\n注意哦，强调一遍，Fasttext用在文本分类，模型架构使用的是CBOW的变种。（我这句话的意思不是说使用Skip-gram不可以，而是CBOW在理解文本分类的时候更加的容易理解）\n这里和Word2vec的CBOW有两个区别：\n\n第一，使用类别标签替换了中心词。\n第二，使用句子中所有单词作为输入，而不再是单单的针对滑动窗口中的单词。\n\n这两个点如果我们自己考虑，也很容易想到。\n为什么这么说呢？先说第二点。我现在要做的是针对文本进行分类，所以对于我的输入需要转换到整体这个句子来看，才能使对一个句子的特征表达。\n再说第一点，我们知道在Wrod2vec中，我们使用的是中心词作为输出，而且使用了霍夫曼作为输出层。\n非叶子点上的向量为了我的二分类提供计算，叶子节点为整个词汇表中所有词汇的向量。两个向量都会随着模型而训练。\n如果要做分类，我们可以想一下叶子节点和非叶子节点的变化。\n首先叶子节点对应的是所有类别。如果说我们的类别有5000个，那么对应到Word2vec，我们就有着5000个词汇。想一下是不是这么对应。\n非叶子节点其实没什么变化，因为它没有什么实际含义，只是为二分类提供计算。\n在这里还想说一下，word2vec中的叶子节点也就是词向量更新之后我们最后是要的，但是对于fasttext其实不会用到这个，因为我们是对文本进行分类，只需要保存了模型权重在预测的时候可以预测就可以了。\n还想谈一下词向量初始化的问题，模型训练开始的时候，词向量随机初始化就可以，模型训练结束之后，我们在预测阶段直接使用这个词向量就可以（就是随着模型训练而更新的这个词向量）。\n对这个论文还有一个很有意思的点，就是N-gram就是fasttext的模型的输入不仅仅针对的是每个单词，为了加入词序信息，还加入了n-gram信息。\n需要注意的一个细节是，这里的n-gram针对的是word，而不是char。对应到中文，应该对应的是分词之后的词，而不是字。但是我自己认为这么对应过来不太好理解。\n中文的字做n-gram貌似也有词序信息。但是英文的char-level的n-gram很难说针对这个句子提供一个语序信息。大家理解一下就好。\n还有一个问题想说一下，使用了n-gram信息之后，词表肯定是变大了的。你需要的n的内容越多，比如你想要n=1orn=2orn=3等等吧，n的取值范围越大，你的词表越大。\n这就会出现一个问题训练非常缓慢。这点很容易理解，参数越多训练当然越慢。\n针对这个问题，怎么解决呢？使用哈希。\n我举个简单的例子，不一定准确，“我/爱/中国/共产党”，我在更新的时候，把’我’,‘爱’,‘中国’,'共产党’我们都使用同一个参数来代表（这种情况很难遇见，理解一下就好），那么在更新训练参数的时候，我只需要更新一个参数就把这个四个词都更新了，当然会快一点。\n但是会出现一个问题，就是精度的问题。这个过程，不知道大家有咩有想到和albert很类似。哈希这个过程我自己感觉有点共享参数的意思。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/Fasttext解读1/"},{"title":"","date":"2024-06-21T03:48:31.545Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:31.545Z","content":"这个文章主要是谈一下Enriching Word Vectors with Subword Information 这个论文。\n有了上一个文章的打底，（上一个文章点击这里）这个论文理解起来就比较简单，所以我写的也比较短。\n对于这个论文，我先给出它最核心的部分： 使用负采样的skip-gram的基础上，将每个中心词视为子词的集合，并学习子词的词向量。\n这句话涉及到的一个最关键的部分就是子词subword，也是这个论文的核心。\n举个例子，现在我们的中心词是&quot;where&quot;，设定子词大小为3，那么子词集合分为两个部分，注意是两个部分。\n第一部分形如这样：“&lt;wh”，“whe”，“her”，“ere”，“re&gt;”，第二部分就是特殊子词，也就是整词“”。\n那么对应到模型是，原来我的输入是“where”的词向量，现在在Fasttext就是所有子词的词向量的和。\n注意哦，这里是所有子词，是包含特殊子词，也就是整词的。\n对于背景词，直接使用整词就可以。\n简单来说，就是输出层使用子词（普通子词加上整词），输出层使用整词。\n如果遇到了OOV怎么办？使用普通子词的向量和来表示就可以。\n其实这里的子词，在名字上和上一个文章的ngram很类似，不过，这里使用的是就char的n-gram，缓解的问题并不是语序，而是利用了词序形态的规律。\n对应到中文，其实就是偏旁部首。 我记得阿里好像有发一个关于fasttext的中文版本，训练的就是偏旁部首。大家有兴趣可以去看一看。\n写完了，我对两个文章做个小总结，顺便对文章开头的问题做个回答: fasttext 训练词向量的时候一般是使用Skip-gram模型的变种。在用作文本分类的时候，一般是使用CBOW的变种。\n在这里，我想要强调一下，上一段我说的是一般情况，是为了方便大家了解，并不代表说CBOW架构不能训练词向量，skip-gram不能用作文本分类，需要注意这一点哦。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/Fasttext解读2/"},{"title":"","date":"2024-06-21T03:48:31.825Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:31.825Z","content":"词向量\nWord2vec\nFasttext\nGlove\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/README/"},{"title":"","date":"2024-06-21T03:48:31.965Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:31.965Z","content":"Word2vec为什么需要二次采样？\n说到 Word2vec 的采样，首先会想起来的是负采样，属于对Word2vec的一个近似训练方法。\n其实它还涉及到一个采样方法，就是subsampling，中文叫做二次采样。\n用最简单的一句话描述二次采样就是，对文本中的每个单词会有一定概率删除掉，这个概率是和词频有关，越高频的词越有概率被删掉。\n二次采样的公式如下所示：\n\n注意: t为超参数，分母 f(w) 为单词w的词频与总词数之比\n首先说一下，我们需要对文本数据进行二次采样？\n举个简单例子，“他/是/个/优秀/的/学生”。如果此时中心词为&quot;学生&quot;，背景词为&quot;的&quot;。\n那么，我们的背景词对于我们这个中心词其实是没有什么作用的，并没有什么语义信息上的补充。\n但是像“的”这种高频词，出现的机会还很大，所以对于这一句话信息是存在冗余的。\n也就是说，在一个背景窗口中，一个词和较低频词同时出现比和较高频词同时出现对训练词嵌入模型更有益。\n举个生活中的例子，现实生活中自律优秀的人比较少，堕落不努力人的人比较多，当然是优秀的人出现在我们身边会对我们自身的成长更加的有益。\n所以我们的想法就是减少和堕落的人出现的次数，远离他们，让优秀的人出现在我们生活中的概率上升。\n那么二次采样之后文本数据变成了什么样子？\n还是上面那句话，“他/是/个/优秀/的/学生”，在这个时候，就变成了“他/是/个/优秀/学生”。也就是说高频词“的”在我们的训练数据中消失了。\n当然这个消失正如上文所说，是一个概率，可能在之后的另一个句子中，它还是存在的，只不过它出现在文本中的词频肯定是降低了的。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/Word2vec为什么需要二次采样？/"},{"title":"","date":"2024-06-21T03:48:32.005Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.005Z","content":"Word2vec模型究竟是如何获得词向量的?\n问大家一个问题：Word2vec模型是如何获得词向量的?\n很多文章在解释的时候，会说对一个词通过One-hot编码，然后通过隐层训练，得到的输入到隐层的矩阵就对应的词表词向量。\n我不能说这么解释是不对的，但是我认为是不准确的。\n在前面文章也说过了，Word2vec是不涉及到隐层的，CBOW有投影层，只是简单的求和平均，Skip-gram没有投影层，就是中心词接了一个霍夫曼树。\n所以，很多文章涉及到的隐层的权重矩阵也就无从谈起。\n在此情况下，词向量是怎么来的？\n从源码的角度来看，我们是对每个词都初始化了一个词向量作为输入，这个词向量是会随着模型训练而更新的，词向量的维度就是我们想要的维度，比如说200维。\n以Skip-gram为例，我们的输入的中心词的词向量其实不是One-hot编码，而是随机初始化的一个词向量，它会随着模型训练而更新。\n需要注意的一个细节点是，每个单词都会对应两个词向量，一个是作为中心词的时候的词向量，一个是作为背景词的时候的词向量。大家一般选择第一种。\n这一点需要注意区别Glove中的中心词向量和背景词向量。Glove中的中心词向量和背景词向量从理论上来说是等价的，只不过由于初始化的不同，最终结果会略有不同。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/Word2vec模型究竟是如何获得词向量的/"},{"title":"","date":"2024-06-21T03:48:32.085Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.085Z","content":"Word2vec的负采样\n负采样的特点\n首先对基于负采样的技术，我们更新的权重只是采样集合，减少了训练量，同时效果上来说，中心词一般来说只和上下文有关，更新其他词的权重并不重要，所以在降低计算量的同时，效果并没有变差。\n负采样具体实施细节\n我自己的总结就是创建两个线段，第一个线段切开词表大小的份数，每个份数的长度和频率正比。\n第二个线段均分M个，然后随机取整数，整数落在第二个线段那里，然后取第一个线段对应的词，如果碰到是自己，那么就跳过。\n欢迎拍砖\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/Word2vec的负采样/"},{"title":"","date":"2024-06-21T03:48:32.145Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.145Z","content":"Word2vec训练参数的选定?\n首先根据具体任务，选一个领域相似的语料，在这个条件下，语料越大越好。然后下载一个 word2vec 的新版（14年9月更新），语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型。最后记得设置迭代次数为三五十次，维度至少选 50，就可以了。（引自 《How to Generate a Good Word Embedding》）\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/Word2vec训练参数的选定/"},{"title":"","date":"2024-06-21T03:48:30.915Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:30.915Z","content":"本文首发公众号：【DASOU】\n涉及到的代码部分，可以去我仓库里找，已经1.2k了：\nDA-southampton/NLP_abilitygithub.com\n文中内容有不同见解大家及时沟通\n目录如下：\n\n知识蒸馏简单介绍\nBert 蒸馏到 BiLSTM\nPKD-BERT\nBERT-of-Theseus\nTinyBert\n\n1. 知识蒸馏简单介绍\n1.1 什么是蒸馏\n一般来说，为了提高模型效果，我们可以使用两种方式。一种是直接使用复杂模型，比如你原来使用的TextCNN，现在使用Bert。一种是多个简单模型的集成，这种套路在竞赛中非常的常见。\n这两种方法在离线的时候是没有什么问题的，因为不涉及到实时性的要求。但是一旦涉及到到部署模型，线上实时推理，我们需要考虑时延和计算资源，一般需要对模型的复杂度和精度做一个平衡。\n这个时候，我们就可以将我们大模型学到的信息提取精华灌输到到小模型中去，这个过程就是蒸馏。\n1.2 什么是知识\n对于一个模型，我们一般关注两个部分：模型架构和模型参数。\n简答的说，我们可以把这两个部分当做是我们模型从数据中学习到的信息或者说是知识（当然主要是参数，因为架构一般来说是训练之前就定下来的）\n但是这两个部分，对于我们来说，属于黑箱，就是我们不知道里面究竟发生了什么事情。\n那么什么东西是我们肉眼可见的呢？从输入向量到输出向量的一个映射关系是可以被我们观测到的。\n简单来说，我输入一个example，你输出是一个什么情况我是可以看到的。\n区别于标签数据格式 [0,0,1,0],模型的输出结果一般是这样的：[0.01,0.01,0.97,0.01]。\n举个比较具象的例子，就是如果我们在做一个图片分类的任务，你的输入图像是一辆宝马，那么模型在宝马这个类别上会有着最大的概率值，与此同时还会把剩余的概率值分给其他的类别。\n这些其他类别的概率值一般都很小，但是仍然存在着一些信息，比如垃圾车的概率就会比胡萝卜的概率更高一些。\n模型的输出结果含有的信息更丰富了，信息熵更大了，我们进一步的可以把这种当成是一种知识，也就是小模型需要从大模型中学习到的经验。\n这个时候我们一般把大模型也就是复杂模型称之为老师网络，小模型也就那我们需要的蒸馏模型称之为学生网络。学生网络通过学习老师网络的输出，进而训练模型，达到比较好的收敛效果。\n1.3 为什么知识蒸馏可以获得比较好的效果\n在前面提到过，卡车和胡萝卜都会有概率值的输出，但是卡车的概率会比胡萝卜大，这种信息是很有用的，它定义了一种丰富的数据相似结构。\n上面谈到一个问题，就是不正确的类别概率都比较小，它对交叉熵损失函数的作用非常的低，因为这个概率太接近零了，也就是说，这种相似性存在，但是在损失函数中并没有充分的体现出来。\n第一种就是，使用sofmax之前的值，也就是logits，计算损失函数\n第二种是在计算损失函数的时候，使用温度参数T，温度参数越高，得到的概率值越平缓。通过升高温度T，我们获取“软目标”，进而训练小模型\n其实对于第一种其实是第二种蒸馏方式的的一种特例情况，论文后续有对此进行证明。\n这里的温度参数其实在一定程度上和蒸馏这个名词相呼应，通过升温，提取精华，进而灌输知识。\n1.4 带温度参数T的Softmax函数\n软化公式如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n说一下为什么需要这么一个软化公式。上面我们谈到，通过升温T，我们得到的概率分布会变得比较平缓。\n用上面的例子说就是，宝马被识别为垃圾车的概率比较小，但是通过升温之后，仍然比较小，但是没有那么小（好绕口啊）。\n也就是说，数据中存在的相似性信息通过升温被放大了，这样在计算损失函数的时候，这个相似性才会被更大的注意到，才会对损失函数产生比较大的影响力。\n1.5 损失函数\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n损失函数是软目标损失函数和硬目标损失函数的结合，一般来说，软目标损失函数设置的权重需要大一些效果会更好一点。\n1.6 如何训练\n整体的算法示意图如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n整体的算法示意图如上所示：\n\n首先使用标签数据训练一个正常的大模型\n使用训练好的模型，计算soft targets。\n训练小模型，分为两个步骤，首先小模型使用相同的温度参数得到输出结果和软目标做交叉熵损失，其次小模型使用温度参数为1，和标签数据（也就是硬目标）做交叉损失函数。\n预测的时候，温度参数设置为1，正常预测。\n\n2. Bert 蒸馏到 BiLSTM\n2.1 简单介绍\n假如手上有一个文本分类任务，我们在提升模型效果的时候一般有以下几个思路：\n\n增大数据集，同时提升标注质量\n寻找更多有效的文本特征，比如词性特征，词边界特征等等\n更换模型，使用更加适合当前任务或者说更加复杂的模型，比如FastText–&gt;TextCNN–Bert\n\n…\n之后接触到了知识蒸馏，学习到了简单的神经网络可以从复杂的网路中学习知识，进而提升模型效果。\n之前写个一个文章是TextCNN如何逼近Bert，当时写得比较粗糙，但是比较核心的点已经写出来。\n这个文章脱胎于这个论文：Distilling Task-Specific Knowledge from BERT into Simple Neural Networks\n整个训练过程是这样的：\n\n在标签数据上微调Bert模型\n使用三种方式对无标签数据进行数据增强\nBert模型在无标签数据上进行推理，Lstm模型学习Bert模型的推理结果，使用MSE作为损失函数。\n\n2.2 目标函数\n知识蒸馏的目标函数：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n一般来说，我们会使用两个部分，一个是硬目标损失函数，一个是软目标损失函数，两者都可以使用交叉熵进行度量。\n在原论文中，作者在计算损失函数的时候只是使用到了软目标，同时这个软目标并不是使用softmax之前的logits进行MSE度量损失，也就是并没有使用带有温度参数T的sotmax进行归一化。\n2.3 数据增强\n为了促进有效的知识转移，我们经常需要一个庞大的，未标记的数据集。\n三种数据增强的方式：\n\nMasking：使用概率随机的替换一个单词为[MASK]. 需要注意的是这里替换之后，Bert模型也会输入这个数据的。从直觉上来讲，这个规则可以阐明每个单词对标签的影响。\nPOS-guided word replacement.使用概率随机替换一个单词为另一个相同POS的单词。这个规则有可能会改变句子的语义信息。\nn-gram sampling\n\n整个流程是这样的：对于每个单词，如果概率p&lt;，我们使用第一条规则，如果p&lt;，我们使用第二条规则，两条规则互斥，也就是同一个单词只使用两者之间的一个。当对句子中的每个单词都过了一遍之后，我进行第三条规则，之后把整条句子补充道无标签数据集中。\n2.4 知识蒸馏结果图\n效果图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n3. PKD-BERT\nPKD  核心点就是不仅仅从Bert（老师网络）的最后一层学习知识去做蒸馏，它还另加了一部分，就是从Bert的中间层去学习。\n简单说，PKD的知识来源有两部分：中间层+最后输出。\n它缓解了之前只用最后softmax输出层的蒸馏方式出现的过拟合而导致泛化能力降低的问题。\n接下来，我们从PKD模型的两个策略说起：PKD-Last 和 PKD-Skip。\n3.1 PKD-Last and PKD-Skip\nPKD的本质是从中间层学习知识，但是这个中间层如何去定义，就各式各样了。\n比如说，我完全可以定位我只要奇数层，或者我只要偶数层，或者说我只要最中间的两层，等等，不一而足。\n那么作者，主要是使用了这么多想法中的看起来比较合理的两种。\nPKD-Last，就是把中间层定义为老师网络的最后k层。\n这样做是基于老师网络越靠后的层数含有更多更重要的信息。\n这样的想法其实和之前的蒸馏想法很类似，也就是只使用softmax层的输出去做蒸馏。但是从感官来看，有种尾大不掉的感觉，不均衡。\n另一个策略是 就是PKD-Skip，顾名思义，就是每跳几层学习一层。\n这么做是基于老师网络比较底层的层也含有一些重要性信息，这些信息不应该被错过。\n作者在后面的实验中，证明了，PKD-Skip 效果稍微好一点（slightly better）；\n作者认为PKD-Skip抓住了老师网络不同层的多样性信息。而PKD-Last抓住的更多相对来说同质化信息，因为集中在了最后几层。\n3.2. PKD\n3. 2.1架构图\n两种策略的PKD的架构图如下所示，注意观察图，有个细节很容易忽视掉:\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n我们注意看这个图，Bert的最后一层（不是那个绿色的输出层）是没有被蒸馏的，这个细节一会会提到。\n3. 2.2 怎么蒸馏中间层\n这个时候，需要解决一个问题：我们怎么蒸馏中间层？\n仔细想一下Bert的架构，假设最大长度是128，那么我们每一层Transformer encoder的输出都应该是128个单元，每个单元是768维度。\n那么在对中间层进行蒸馏的时候，我们需要针对哪一个单元？是针对所有单元还是其中的部分单元？\n首先，我们想一下，正常KD进行蒸馏的时候，我们使用的是[CLS]单元Softmax的输出，进行蒸馏。\n我们可以把这个思想借鉴过来，一来，对所有单元进行蒸馏，计算量太大。二来，[CLS] 不严谨的说，可以看到整个句子的信息。\n为啥说是不严谨的说呢？因为[CLS]是不能代表整个句子的输出信息，这一点我记得Bert中有提到。\n3.2.3蒸馏层数和学生网络的初始化\n接下来，我想说一个很小的细节点，对比着看上面的模型架构图：\nBert（老师网络）的最后一层 (Layer 12 for BERT-Base) 在蒸馏的时候是不予考虑；\n原因的话，其一可以这么理解，PKD创新点是从中间层学习知识，最后一层不属于中间层。当然这么说有点牵强附会。\n作者的解释是最后一层的隐层输出之后连接的就是Softmax层，而Softmax层的输出已经被KD Loss计算在内了。\n比如说，K=5，那么对于两种PKD的模式，被学习的中间层分别是：\nPKD-Skip: ;\nPKD-Last:\n还有一个细节点需要注意，就是学生网络的初始化方式，直接使用老师网络的前几层去初始化学生网络的参数。\n3.2.4 损失函数\n首先需要注意的是中间层的损失，作者使用的是MSE损失。如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n整个模型的损失主要是分为两个部分：KD损失和中间层的损失，如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n3.3. 实验效果\n实验效果可以总结如下：\n\nPKD确实有效，而且Skip模型比Last效果稍微好一点。\nPKD模型减少了参数量，加快了推理速度，基本是线性关系，毕竟减少了层数\n\n除了这两点，作者还做了一个实验去验证：如果老师网络更大，PKD模型得到的学生网络会表现更好吗？\n这个实验我很感兴趣。\n直接上结果图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\nKD情况下，注意不是PKD模型，看#1 和#2，在老师网络增加的情况下，效果有好有坏。这个和训练数据大小有关。\nKD情况下，看#1和#3，在老师网络增加的情况下，学生网络明显变差。\n作者分析是因为，压缩比高了，学生网络获取的信息变少了。\n也就是大网络和小网络本身效果没有差多少，但是学生网络在老师是大网络的情况下压缩比大，学到的信息就少了。\n更有意思的是对比#2和#3，老师是大网络的情况下，学生网络效果差。\n这里刚开始没理解，后来仔细看了一下，注意#2 的学生网络是，也就是它的初始化是从来的，占了一半的信息。\n好的，写到这里\n4. BERT-of-Theseus\n大家好，我是DASOU，今天介绍一下：BERT-of-Theseus\n这个论文我觉得还挺有意思，攒个思路。\n读完这个文章，BERT-of-Theseus 掌握以下两点就可以了：\n\n基于模块替换进行压缩\n除了具体任务的损失函数，没有其他多余损失函数。\n\n效果的话，与相比，：推理速度 ；模型效果 98%；\n4.1 模块替换\n举个例子，比如有一个老师网络是12层的Bert，现在我每隔两层Transformer，替换为学生网络的一层Transformer。那么最后我的学生网络也就变成了6层的小Bert，训练的时候老师网络和学生网络的模块交替训练。\n直接看下面这个架构图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n作者说他是受 Dropout 的启发，仔细想了想还真的挺像的。\n我们来说一下这样做的好处。\n我刚才说每隔老师网络的两层替换为学生网络的一层。很容易就想到PKD里面，有一个是PKD-Skip策略。\n就是每隔几层，学生网络的层去学习老师网络对应层的输出，使用损失函数让两者输出接近，使用的是CLS的输出。\n在这里提一下蒸馏/压缩的基本思想，一个最朴素的想法就是让学生网络和老师网络通过损失函数在输出层尽可能的靠近。\n进一步的，为了提升效果，可以通过损失函数，让学生网络和老师网络在中间层尽可能的靠近，就像PKD这种。\n这个过程最重要的就是在训练的时候需要通过损失函数来让老师网络和学生网络尽可能的接近。\n如果是这样的话，问题就来了，损失函数的选取以及各自损失函数之前的权重就需要好好的选择，这是一个很麻烦的事情。\n然后我们再来看 BERT-of-Theseus，它就没有这个问题。\n它是在训练的时候以概率  来从老师网络某一层和学生网络的某一层选择一个出来，放入到训练过程中。\n在这个论文里，老师网络叫做  ， 学生网络叫做  ；\n4.2 训练过程\n对着这个网络架构，我说一下整体训练的过程：\n\n在具体任务数据上训练一个 BERT-base 网络作为 ；\n使用   前六层初始化一个 6层的Bert作为  ；\n在具体任务数据上，固定  相应权重，以概率（随着steps，线性增加到1），对整个网络（加上 ）进行整体的训练。\n为了让  作为一个整体，单独抽离出来 （其实设置为1就可以了），作为一个单独的个体，在训练数据上继续微调。直至效果不再增加。\n\n简单总结，在训练数据上，老师网络和学生网络共同训练，因为存在概率问题，有的时候是老师网络的部分层加入训练，有的时候是学生网络的部分层加入训练。在这一步训练完成之后，为了保证学生网络作为一个整体（因为在第一步训练的时候大部分情况下学生网络的层都是分开加入训练过程的），在具体任务数据上，对学生网络继续微调，直至效果不再增加。\n4.3 结果分析\n不同方法的损失函数\n论文提供了一个不同Bert蒸馏方法使用的损失函数的图，值得一看，见下图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n值得注意的是，这里的 应该是选取前六层，在具体任务微调的结果。\n效果\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n整体来说，BERT-of-Theseus 思路很简单，效果也还不错。\n5. Tiny-Bert\n大家好，我是DASOU，今天说一下 TinyBert；\nTinyBert 主要掌握两个核心点：\n\n提出了对基于 transformer 的模型的蒸馏方式：Transformer distillation；\n提出了两阶段学习框架：在预训练和具体任务微调阶段都进行了 Transformer distillation（两阶段有略微不同）；\n\n下面对这两个核心点进行阐述。\n5.1. Transformer distillation\n5.1.1整体架构\n整体架构如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\nBert不严谨的来划分，可以分为三个部分：词向量输入层，中间的TRM层，尾端的预测输出层。\n在这个论文里，作者把词向量输入层 和中间的TRM层统一称之为中间层，大家读的时候需要注意哈。\nBert的不同层代表了学习到了不同的知识，所以针对不同的层，设定不同的损失函数，让学生网络向老师网络靠近，如下：\n\nebedding层的输出\n多头注意力层的注意力矩阵和隐层的输出\n预测层的输出\n\n5.1.2 Transformer 基础知识：\n注意力层：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n多头注意力层：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n前馈神经网路：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.1.3 Transformer 的蒸馏\n对 Transformer的蒸馏分为两个部分：一个是注意力层矩阵的蒸馏，一个是前馈神经网络输出的蒸馏。\n注意力层矩阵蒸馏的损失函数：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n这里注意两个细节点：\n一个是使用的是MSE；\n还有一个是，使用的没有归一化的注意力矩阵，见(1)，而不是softmax之后的。原因是实验证明这样能够更快的收敛而且效果会更好。\n前馈神经网络蒸馏的损失函数\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n两个细节点：\n第一仍然使用的是MSE.\n第二个细节点是注意，学生网路的隐层输出乘以了一个权重矩阵，这样的原因是学生网络的隐层维度和老师网络的隐层维度不一定相同。\n所以如果直接计算MSE是不行的，这个权重矩阵也是在训练过程中学习的。\n写到这里提一点，其实这里也可以看出来为什么tinybert的初始化没有采用类似PKD这种，而是使用GD过程进行蒸馏学习。\n因为我们的tinybert 在减少层数的同时也减少了宽度（隐层的输出维度），如果采用PKD这种形式，学生网络的维度和老师网络的维度对不上，是不能初始化的。\n词向量输入层的蒸馏：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n预测层输出蒸馏：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.1.4 总体蒸馏损失函数\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.2. 两阶段蒸馏\n5.2.1 整体架构\n整体架构如图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.2.2 为什么需要GD:\n说一下我自己的理解哈，我觉得有两个原因：\n首先，就是上文说到的，tinybert不仅降低了层数，也降低了维度，所以学生网络和老师网络的维度是不符的，所以PKD这种初始化方式不太行。\n其次，一般来说，比如PKD，学生网络会使用老师网络的部分层进行初始化。这个从直觉上来说，就不太对。\n老师网络12层，学到的是文本的全部信息。学生网络是6层，如果使用老师的12层的前6层进行初始化，这个操作相当于认为这前6层代表了文本的全部信息。\n当然，对于学生网络，还会在具体任务上微调。这里只是说这个初始化方式不太严谨。\nTiny bert的初始化方式很有意思，也是用了蒸馏的方式。\n老师网络是没有经过在具体任务进行过微调的Bert网络，然后在大规模无监督数据集上，进行Transformer distillation。当然这里的蒸馏就没有预测输出层的蒸馏，翻看附录，发现这里只是中间层的蒸馏。\n简单总结一下，这个阶段，使用一个预训练好的Bert（ 尚未微调）进行了3epochs的 distillation；\n5.2.3 TD：\nTD就是针对具体任务进行蒸馏。\n核心点：先进行中间层（包含embedding层）的蒸馏，再去做输出层的蒸馏。\n老师网络是一个微调好的Bert，学生网络使用GD之后的tinybert，对老师网络进行TD蒸馏。\nTD过程是，先在数据增强之后的数据上进行中间层的蒸馏-10eopchs，learning rate 5e-5；然后预测层的蒸馏3epochs，learning rate 3e-5.\n5.3. 数据增强\n在具体任务数据上进行微调的时候，进行了数据增强。\n(感觉怪怪的)\n两个细节点：\n\n对于 single-piece word 通过Bert找到当前mask词最相近的M个单词；对于 multiple sub-word pieces 使用Glove和Consine找到最相近的M个词\n通过概率P来决定是否替换当前的词为替换词。\n对任务数据集中的所有文本数据做上述操作，持续N次。\n\n伪代码如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.4. 实验效果\n其实我最关心的一个点就是，数据增强起到了多大的作用。\n作者确实也做了实验，如下，数据增强作用还是很大的：\n\n编辑\n添加图片注释，不超过 140 字（可选）\n我比较想知道的是，在和PKD同等模型架构下，两者的比较，很遗憾，作者好像并没有做类似的实验(或者我没发现)。\n这里的tinybert参数如下：\n\nthe number of layers M=4, the hidden size d 0=312, the feedforward/filter size d 0 i=1200 and the head number h=12.\n\n5.5. 简单总结\n先说一下，我读完论文学到的东西：\n首先是transformer层蒸馏是如何涉及到的损失函数：\n\n注意力矩阵和前馈神经层使用mse；\n蒸馏的时候注意力矩阵使用未归一化\n维度不同使用权重矩阵进行转化\n\n其次，维度不同导致不能从老师Bert初始化。GD过程为了解决这个问题，直接使用学生网络的架构从老师网络蒸馏一个就可以，这里并不是重新学一个学生网络。\n还有就是数据增强，感觉tinyebert的数据增强还是比较简陋的，也比较牵强，而且是针对英文的方法。\nTD过程，对不同的层的蒸馏是分开进行的，先进行的中间层的蒸馏，然后是进行的输出层的蒸馏，输出层使用的是Soft没有使用hard。\n这个分过程蒸馏很有意思，之前没注意到这个细节点。\n在腾讯的文章中看到这样一句话：\n\n并且实验中，softmax cross-entropy loss 容易发生不收敛的情况，把 softmax 交叉熵改成 MSE, 收敛效果变好，但泛化效果变差。这是因为使用 softmax cross-entropy 需要学到整个概率分布，更难收敛，因为拟合了 teacher BERT 的概率分布，有更强的泛化性。MSE 对极值敏感，收敛的更快，但泛化效果不如前者。\n\n是有道理的，积累一下。\n值得看的一些资料：\n比 Bert 体积更小速度更快的 TinyBERT - 腾讯技术工程的文章 - 知乎 https://zhuanlan.zhihu.com/p/94359189\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/模型蒸馏/知识蒸馏综述万字长文/"},{"title":"","date":"2024-06-21T03:48:32.175Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.175Z","content":"微信公众号：NLP从入门到放弃\n\n有没有使用自己的数据训练过Word2vec，详细说一下过程。包括但是不限于：语料如何获取，清理以及语料的大小，超参数的选择及其原因，词表以及维度大小，训练时长等等细节点。\nWord2vec模型是如何获得词向量的？聊一聊你对词嵌入的理解？如何理解分布式假设？\n如何评估训练出来的词向量的好坏\nWord2vec模型如何做到增量训练\n大致聊一下 word2vec这个模型的细节，包括但不限于：两种模型以及两种优化方法（大致聊一下就可以，下面会详细问）\n解释一下 hierarchical softmax 的流程(CBOW and Skip-gram)\n基于6，可以展开问一下模型如何获取输入层，有没有隐层，输出层是什么情况。\n基于6，可以展开问输出层为何选择霍夫曼树，它有什么优点，为何不选择其他的二叉树\n基于6，可以问该模型的复杂度是多少，目标函数分别是什么，如何做到更新梯度（尤其是如何更新输入向量的梯度）\n基于6，可以展开问一下 hierarchical softmax 这个模型 有什么缺点\n聊一下负采样模型优点（为什么使用负采样技术）\n如何对输入进行负采样（负采样的具体实施细节是什么）\n负采样模型对应的目标函数分别是什么（CBOW and Skip-gram）\nCBOW和skip-gram相较而言，彼此相对适合哪些场景\n有没有使用Word2vec计算过句子的相似度，效果如何，有什么细节可以分享出来\n详细聊一下Glove细节，它是如何进行训练的？有什么优点？什么场景下适合使用？与Word2vec相比，有什么区别（比如损失函数）？\n详细聊一下Fasttext细节，每一层都代表了什么？它与Wod2vec的区别在哪里？什么情况下适合使用Fasttext这个模型？\nELMO的原理是什么？以及它的两个阶段分别如何应用？（第一阶段如何预训练，第二阶段如何在下游任务使用）\nELMO的损失函数是什么？它是一个双向语言模型吗？为什么？\nELMO的优缺点分别是什么？为什么可以做到一词多义的效果？\n\n本面试题词向量资源参考：\nword2vec、glove、cove、fastext以及elmo对于知识表达有什么优劣？ - 霍华德的回答 - 知乎\nhttps://www.zhihu.com/question/292482891/answer/492247284\n面试题：Word2Vec中为什么使用负采样？ - 七月在线 七仔的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/66088781\n关于word2vec，我有话要说 - 张云的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/29364112\nword2vec（二）：面试！考点！都在这里 - 我的土歪客的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/133025678\nnlp中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert - JayLou娄杰的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/56382372\n史上最全词向量讲解（LSA/word2vec/Glove/FastText/ELMo/BERT） - 韦伟的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/75391062\nword2vec详解（CBOW，skip-gram，负采样，分层Softmax） - 孙孙的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/53425736\nWord2Vec详解-公式推导以及代码 - link-web的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/86445394\n关于ELMo，面试官们都怎么问：https://cloud.tencent.com/developer/article/1594557\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/史上最全词向量面试题梳理/"},{"title":"","date":"2024-06-21T03:48:31.875Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:31.875Z","content":"总结不易，请大力点赞，感谢\n上一个文章，Word2vec-负采样/霍夫曼之后模型是否等价-绝对干货是字节的面试真题，建议朋友们多看几遍，有问题及时沟通。\n私下有几个朋友看完之后还是有点懵，又问了一下具体细节。基于此，我重新写了一个简短的文章，希望能让大家明白，大家可以结合上一个文章看。\n我们再看一下题目：W2V经过霍夫曼或者负采样之后，模型与原模型相比，是等价的还是相似的？\n首先，我们要明确，这里的原模型指的是什么？原模型就是我们的没有经过优化的W2V（当然我们也说过它是一个工具不是一个模型）。\n也就是只是使用Skip-gram模型或者CBOW模型而没有进行优化的原始版本。对于这个原始版本，是在最后一层进行了Softmax。\n我们的目标函数中，最核心的一个部分就是在给定中心词的条件下生成正确背景词的概率，我们要最大化这个东西，公式如下：\n\n仔细看，在分母涉及到了一个V，这里的V就是我们的词典大小。也就是说，为了计算这个条件概率，我们需要对整个词典进行操作，复杂度就是O(|V|)\n所以，负采样和霍夫曼就是针对这一个计算开销大的地方进行了优化。当然W2V为了减少计算量，还是去掉了隐层。比如CBOW直接是输入向量求和平均然后接霍夫曼树。比如Skip-gram直接是中心词的词向量接霍夫曼树。\n这不是我这个文章的重点，就不细细展开了。\n我们先说负采样。负采样的本质在于生成K个噪声。它的本质是基于中心词生成正确的背景词概率为1，生成噪声词概率为0，这个是我们的优化方向。公式如下：\n\n仔细看这个公式，V已经消失，取而代之的是K，也就是我们的噪声词的数量，换句话讲，我们的复杂度被K这个大小限制住了，降低为了O(|K|)\n然后我们再来看层序Softmax。它的核心本质是在一条路径上不停的做二分类，概率连乘就会得到我们的条件概率。公式如下：\n\n注意看，这个公式中，V也已经消失了，被霍夫曼树中到达背景词的路径限制住了，这也就是上个文章中说到的，复杂度变成了二叉树的高度: O(log|V|)\n既然只是针对的部分节点，那么与原始版本相比，当然是近似。\n简单的总结一下：\n其实可以这样理解，以跳字模型为例，条件概率是中心词生成背景词的概率，也就是我们优化函数中最核心的部分。没有使用优化的，分母涉及到全部词汇，训练开销大。负采样近似训练，把复杂度限制在了k个噪声词，层序softmax也属于近似训练，在它的条件概率中，不断的二分类，涉及到的是能够达到背景词的那个路径上的非叶子结点，也就是没涉及到其他节点，这一点和负采样很类似，都是从全部词汇降低复杂度，只不过负采样是被k限制，层序是被路径编码限制(0,1,1,1,0)这种限制住。\n不知道大家有没有注意到，负采样和霍夫曼都是讲Softmax转化为二分类的问题从而降低了复杂度。负采样是针对是不是背景词做二分类，霍夫曼是在对是不是正确路径上的节点做二分类。这么说有点不严谨，但是意思就是这么个意思，大家理解一下。\n总结不易，请大力点赞，感谢\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/word2vec两种优化方式的联系和区别/"},{"title":"","date":"2024-06-21T03:48:32.265Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.265Z","content":"本文大概需要阅读 4.75 分钟\n先问大家两个问题，看能不能解答\n\n\nGlove 中词向量的表达是使用的中心词向量还是背景词向量还是有其他方法？\n\n\n能不能分别用一句话概括出Glove和Fasttext 的核心要点？\n\n\n先来谈Glove。中文全称 Global Vectors for Word Representation。它做的事情概括出来就是：基于全局语料，获得词频统计，学习词语表征。\n我们从语料之中，学习到X共现词频矩阵，词频矩阵中的每个元素$x_{ij}$，代表的是词\n$x_{j}$出现在$x_{i}$的环境中的次数。注意，对于共现词频矩阵来说，它是一个对称矩阵。\n这一点非常的重要，也很容易理解，词A出现在词B周围的次数肯定是等价于词B出现在词A周围的次数的。\n类比于Word2vec，对于词$x_{i}$，就是中心词，对于词$x_{j}$也就是背景词。\n理论上，一个词作为中心词向量和一个词作为背景学到的两种向量应该是完全相同的。\n但是现实中，由于我们初始化的不同，所以我们最终学习到的两种词向量是有些许不同。\n为了增加模型的鲁棒性，在Glove中，使用两个词向量的和作为我们一个词的词向量的表达。\n这一点是区别于Word2vec，对于Word2vec，中心词向量和背景词向量是不等价的，我们一般使用中心词向量代表一个词最终的语义表达。\nGlove 论文中的推导过程其实不是很严谨，大致流程就是从大语料中发现了一个规律，即条件概率的比值可以比较直观的表达出词与词之间的关系。\n随后可以构建词向量函数去拟合条件概率的比值。基于此一步步的进行延伸推导，在我看了就是在基于一些假设，寻找出一种可能性的存在。\n在这里就不细说，直接给出Glove的损失函数：\n$\\sum_{i \\in V} \\sum_{j \\in V} h(x_{ij})(u_{j}^Tv_{i}+b_{i}+b_{j}-log(x_{ij}))^2$\n详细讲一下这个损失函数，它是一个平方损失函数，很值得琢磨。\n我把它分为了三个部分，权重函数h(x),词向量表达是$u_{j}^Tv_{i}+b_{i}+b_{j}$，共现词频的对数 $log(x_{ij})$\n从这里，我插一句我的理解，就是GLove基于的是无标签的语料，属于无监督的训练过程，但是从损失函数看，我觉得可以认为它是一个有监督的过程。\n标签就是$log(x_{ij})$，这个是我们从语料之中计算出来的，换句话说，在模型训练之前，我们可以对语料的计算，得到相应的标签数据，所以我自己认为这个可以看做是一个有监督的过程。\n我们使用一句话去描述这个损失函数可以这么去说：随着模型不停的优化，词向量的表达在不断的拟合共现词频的对数。\nh(x)是权重函数，代表的含义是表达一个词对的重要性，在值域[0,1]上单调递增。直观上理解就是一对词语共现的词频越多，那么它的重要性也就越大。\n论文中给出的函数是这样的，在x&lt;c(比如c=100)的情况下，h(x)=(x/c)^\\alpha (\\alpha=0.75)，在其余情况下，h(x)=1。也就是重要度不能无限增大，给了一个上限。\n我们看这个损失函数，有一个很有意思的点，就是h(0)=0。想一下，这个代表着什么？\n也就是说，当我们一对词的共现词频为0的时候，损失函数是为0，换句话讲，我们的损失函数的复杂度是和共现词频矩阵中的非零元素是线性关系。\n所以在训练的时候，我们只需要对非零元素采样，使用随机梯度就可以对词向量和两个偏置项进行学习更新。\n这里还有一个细节点，需要注意，偏置项是不可以省略的。为什么呢？因为如果去掉，在公式推导中的对称性假设就不满足，感兴趣的同学可以自己推导一系。\n参考链接：\n视频：\nhttps://www.bilibili.com/video/BV154411S7Tf?p=17\n文档：http://zh.d2l.ai/chapter_natural-language-processing/glove.html\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/聊一下Glove/"},{"title":"","date":"2024-06-21T03:48:32.305Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.305Z","content":"本文大概需要阅读 5.25 分钟\n大概用三篇文章好好谈一下Word2vec，这篇主要是关于 Word2vec 的两种模型。\n先问大家个问题：一个词通过Word2vec训练之后，可以得到几个词向量？\n如果您有点懵，说明我写对了，您慢慢看下去，码字不易，请多多点赞，让更多人看到，谢谢。\n词向量一般被认为是一个词的特征向量，也就是说可以代表一个词的含义。一个中文词，比如&quot;中国&quot;这个词，是很难被神经网络直接作为输入，我们需要将词向量化（或者说数字化），才能更好的喂进神经网络。\n这个过程很类似于计算机在处理我们输入的文字的时候，会转化为二进制进行处理。\n只不过这个二进制表达规则我们会自己人为规定，而词向量这个向量表达根据不同的方式会有着不同的结果。想一下，两者是不是很类似。\n谈到向量表达单词，一般首先想到的都是One-Hot编码。但是它是有缺点的。我这里谈两个缺点。\n首先是维度灾难：如果我们有1万个单词，那么你的One-hot去表示每个单词的的时候，会转变为一个1万维度的向量。\n那么在计算的时候会带来巨大的不便，而且向量矩阵极其稀疏，占据了太多不必要的内存。当然对于维度灾难，我们一般可以使用PCA等降维手段来缓解。\n其次是语义表达不足。这一点很简单，”娱乐“与”八卦“两个词，通过One-hot编码得到的向量，计算Consine得到相似度为0。\n这显然是不合适的。One-Hot编码表示出来的词向量是两两正交的，从余弦相似度的角度来看，是互不相关的，所以 One-Hot 不能很好的表达词语的相似性。\n这个时候，我们再来看 Word2vec。首先，要明确一点，Word2vec 不是一个模型，而是一个工具。这个点虽然无伤大雅，但是每每看到有的文章说它是个模型，就有点…\nWord2vec 从整体上理解，就是包含两个模型（CBOW and Skip-gram）和两个高效的优化训练方式（负采样和层序softmax）\n这个文章主要谈一下两种模型。\n先假定我们的窗口大小为2，也就是说中心词前面有两个词，后面有两个词，比如&quot;我/永远/爱/中国/共产党&quot;，抽象出来就是：\nW_{t-2}，W_{t-1}，W_{t}，W_{t+1}，W_{t+2}\n对于Skip-gram，中文我们叫跳字模型，使用中心词预测背景词。也就是用&quot;爱&quot;去预测其他的四个词。\n对于CBOW，中文我们叫连续词袋模型，使用背景词预测中心词，也就是用&quot;我&quot;，“永远”，“中国”，“共产党” 去预测&quot;爱&quot;。\nCBOW的模型架构，从整体上，我们可以分为三层：输入层，投影层，和输出层。\n对于输入层，对应的是窗口中的单词，也就是例子中&quot;我&quot;，“永远”，“中国”，“共产党” 四个词的词向量，在投影层，将四个词的词向量进行相加求平均，输出层在没有优化的前提下，维度为词表大小，随后做 Softmax即可。\nSkip-gram模型架构很类似，只不过可以省去投影层，因为输入为中心词，是一个单词的词向量，无从谈起求和与平均了。\n接下来，我们来详细谈一下Skip-gram。\n对于一个神经网络模型，我们需要确定我们的优化目标或者说目标函数是什么？\n对于Skip-gram，其实很容易理解，就是我要最大化在给出中心词的条件下背景词出现的概率，公式如下\n\\prod_{t=1}^T \\ \\prod_{-m\\leq j \\leq m,j\\neq 0}  P(w^{t+j}|w^{t})\n这个公式对应着两个连乘，第一个连乘是对应 T 个输入样本，也就是说我们文本中的每个单词都要做中心词。第二个连乘代表着给定一个中心词的情况下，窗口中的单词出现的概率，内含相互独立的假设。\n在这里，有一个细节点想要提醒大家，在词汇表中的每个单词，都是对应两个词向量的，一个是在作为中心词的时候的中心词向量，一个是在作为背景词时候的背景词向量。\n优化目标函数的时候，为了减少复杂度（也就是忽略 T），我们可以使用随机梯度下降，针对每一个样本我们都更新一次参数。\n通过公式推导（略），我们能够观察到一个特点，就是每次更新参数，都会涉及到词典中的全部词汇，这是因为我们在做 Softmax 的时候，是分母是针对所有词汇的操作。\n这样的操作的复杂度是 O(|V|)，其中|V|就是我们词汇表的大小。CBOW 其实是很类似的情况，每次更新都会涉及到我们的全部词汇。\n于是，我们就需要对此进行优化，使用了两种近似的训练方式来高效的训练Word2vec，降低训练的复杂度。\n下一个文章谈一下优化方式。\n如果觉得写的还行，帮忙点个赞或者在看，让更多人关注到我吧，感谢。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/聊一下Word2vec-模型篇/"},{"title":"","date":"2024-06-21T03:48:32.395Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.395Z","content":"Word2vec 涉及到两种优化方式，一种是负采样，一种是层序Softmax\n先谈一下负采样，以跳字模型为例。中心词生成背景词可以由两个相互独立事件的联合组成来近似（引自李沐大神的讲解）。\n第一个事件是，中心词和背景词同时出现在窗口中。第二个事件是，中心词和K个噪声词不同时出现在窗口数据中，其中噪声词由噪声分布随机生成。\n这里我们就可以知道上一个文章开头说到的，负采样是一种等价操作还是近似操作？我们在第二个事件中，使用了K个噪声词。但是实际上呢？应该远远大于K。\n还是那个例子，句子为&quot;我/永远/爱/中国/共产党&quot;，中心词为’爱’，我们在选择噪声词的时候，选择了K个，但是实际上，在词汇表中，排除掉’我’，‘永远’，‘中国’，‘共产党’ 这四个词汇的其他词都可以算做我的噪声词，然而为了减少复杂度，我只选择了其中的K个，所以当然应该是近似了。\n接下来，我们看层序Softmax。\n层序Softmax 对应的就是在输出层使用一个霍夫曼树，代替了原本在输出层统一进行的softmax。\n首先，我们需要了解霍夫曼树在这里是如何构建的。\n简单讲，霍夫曼树是一个二叉树，以语料中出现过的词当做叶子节点，以各词在语料中出现的次数当做权值进行构造。其中叶子节点有N个，就是词典的大小，非叶子节点有N-1个（包括根节点）。\n比如说我的所有文章中，“共产党”这个词出现了 100次，是最大的，那么根节点的左分支（或者右分支）就对应着”共产党“这个词，另一个分支做与根节点相同的操作，找到排除”共产党“这个词之外的所有词中最大的词，比如”中国“作为其中的左分支（或者右分支），以此类推，一个霍夫曼树就成功构建。\n霍夫曼树中，我们需要注意的是，每个非叶子节点对应一个向量，每个叶子节点对应一个向量。两种向量都会随着模型的训练进行更新。\n其中叶子节点的向量就是我们的词向量，而非叶子节点上的向量就是没有什么实际含义，它的作用就是帮助我们计算模型在霍夫曼树上不断的进行二分类时候的概率。\n以上面那句话为例，我们现在中心词为‘爱’，然后，我要预测背景词‘中国’。首先我们要确定的是我的叶子节点是包含所有单词的，也就是包含了我这个简单句子的五个单词（不考虑前期数据清洗低频率词的情况）。\n也就是说，在这个霍夫曼树上，有且仅有一条路径，让我从根节点出发，经过多次判断（也就是说走过了多个非叶子节点），最终走到了“中国”这个叶子节点，对应的概率就是每个节点概率的连乘。\n然后这个时候，我们想一下霍夫曼树是不是一种近似？\n当然，我们每更新一个词向量，只是涉及到了可以到达叶子节点的这一条路径上节点。所以复杂度就是树的高度，也就是 O(log|V|)\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/聊一下Word2vec-训练优化篇/"},{"title":"","date":"2024-06-21T03:48:32.355Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.355Z","content":"本文大概需要阅读 2.75 分钟\n以Q&amp;A的形式，聊一聊Word2vec的细节点\nWord2vec训练参数的选定?\n首先根据具体任务，选一个领域相似的语料，在这个条件下，语料越大越好。然后下载一个 word2vec 的新版（14年9月更新），语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型。最后记得设置迭代次数为三五十次，维度至少选 50，就可以了。（引自 《How to Generate a Good Word Embedding》）\nWord2vec模型是如何获得词向量的?\n很多文章在解释的时候，会说对一个词通过One-hot编码，然后通过隐层训练，得到的矩阵就对应的词表词向量。这个我觉得是不准确的。\n首先，在前面文章也说过了，Word2vec是不涉及到隐层的，CBOW有投影层，只是简单的求和平均，Skip-gram没有投影层，就是中心词接了一个霍夫曼树。\n在此情况下，词向量是怎么来的？首先，要明确一点，以Skip-gram为例，我们的输入的中心词的词向量其实不是One-hot编码，而是随机初始化的一个词向量，它会随着模型训练而更新。\n需要注意的一个细节点是，每个单词都会对应两个词向量，一个是作为中心词的时候的词向量，一个是作为背景词的时候的词向量。大家一般选择第一种。\nCBOW和skip-gram相较而言，彼此相对适合哪些场景\nCBOW比Skip-gram 训练速度快，但是Skip-gram可以得到更好的词向量表达。\n为什么这么说？Skip-gram 是中心词预测背景词，假如我们有K个背景词，那么对于一个中心词来说，就是做了K次训练，那么词向量就回得到更加充分的训练。\n而对于CBOW，我们是背景词预测中心词，相当于只是做了一次训练。想一下是不是这么回事？\n简单来说，我们视线回到一个大小为K的训练窗口（窗口内全部单词为2k+1个），CBOW只是训练一次，Skip-gram 则是训练了2K次。当然是Skip-gram词向量会更加的准确一点，相应的会训练的慢一点。\n负采样的特点\n首先对基于负采样的技术，我们更新的权重只是采样集合，减少了训练量，同时效果上来说，中心词一般来说只和上下文有关，更新其他词的权重并不重要，所以在降低计算量的同时，效果并没有变差。\n负采样具体实施细节\n就是创建两个线段，第一个线段切开词表大小的份数，每个份数的长度和频率正比。\n第二个线段均分M个，然后随机取整数，整数落在第二个线段哪里，然后取第一个线段对应的词，如果碰到是自己，那么就跳过。\n在代码实现中，第一个线段的长度不仅仅是频率，而是一个3/4的幂次方，第二个线段切分为10的8次方个段数\n如果觉得对您有所帮助，帮忙点个在看或者赞，谢谢!\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/聊一下Word2vec-细节篇/"},{"title":"","date":"2024-06-21T03:48:32.485Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.485Z","content":"词向量资源总结\n一文总结词向量的计算、评估与优化\nhttps://mp.weixin.qq.com/s/RXY5A4gcx60BN6bFdeccKQ\n比较系统的介绍了常见的生成词向量的神经网络模型有NNLM模型,C&amp;W模型,CBOW模型和Skip-gram模型。不错\n深入浅出词嵌入技术\nhttps://mp.weixin.qq.com/s/UE7ClHu7kiY_HXoJrZ0CwA\nword2vec bert gpt的简单介绍\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/词向量资源总结/"},{"title":"","date":"2024-06-20T11:37:07.621Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:37:07.621Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Dos/CIV_474_01');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Dos/CIV_474_01/"},{"title":"","date":"2024-06-20T11:37:17.201Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:37:17.201Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Dos/CIV_474_03');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Dos/CIV_474_03/"},{"title":"","date":"2024-06-20T11:37:27.121Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:37:27.121Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Dos/CIV_474_04');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Dos/CIV_474_04/"},{"title":"","date":"2024-06-20T11:37:36.931Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:37:36.931Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/DosCIV_474_05');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Dos/CIV_474_05/"},{"title":"","date":"2024-06-20T11:37:45.461Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:37:45.461Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Dos/CIV_474_05Fr');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Dos/CIV_474_05Fr/"},{"title":"","date":"2024-06-21T03:48:32.425Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:32.425Z","content":"[TOC]\n1. 灵魂20问帮你彻底搞定词向量\n\n有没有使用自己的数据训练过Word2vec，详细说一下过程。包括但是不限于：语料如何获取，清理以及语料的大小，超参数的选择及其原因，词表以及维度大小，训练时长等等细节点。\nWord2vec模型是如何获得词向量的？聊一聊你对词嵌入的理解？如何理解分布式假设？\n如何评估训练出来的词向量的好坏\nWord2vec模型如何做到增量训练\n大致聊一下 word2vec这个模型的细节，包括但不限于：两种模型以及两种优化方法（大致聊一下就可以，下面会详细问）\n解释一下 hierarchical softmax 的流程(CBOW and Skip-gram)\n基于6，可以展开问一下模型如何获取输入层，有没有隐层，输出层是什么情况。\n基于6，可以展开问输出层为何选择霍夫曼树，它有什么优点，为何不选择其他的二叉树\n基于6，可以问该模型的复杂度是多少，目标函数分别是什么，如何做到更新梯度（尤其是如何更新输入向量的梯度）\n基于6，可以展开问一下 hierarchical softmax 这个模型 有什么缺点\n聊一下负采样模型优点（为什么使用负采样技术）\n如何对输入进行负采样（负采样的具体实施细节是什么）\n负采样模型对应的目标函数分别是什么（CBOW and Skip-gram）\nCBOW和skip-gram相较而言，彼此相对适合哪些场景\n有没有使用Word2vec计算过句子的相似度，效果如何，有什么细节可以分享出来\n详细聊一下Glove细节，它是如何进行训练的？有什么优点？什么场景下适合使用？与Word2vec相比，有什么区别（比如损失函数）？\n详细聊一下Fasttext细节，每一层都代表了什么？它与Wod2vec的区别在哪里？什么情况下适合使用Fasttext这个模型？\nELMO的原理是什么？以及它的两个阶段分别如何应用？（第一阶段如何预训练，第二阶段如何在下游任务使用）\nELMO的损失函数是什么？它是一个双向语言模型吗？为什么？\nELMO的优缺点分别是什么？为什么可以做到一词多义的效果？\n\n2. W2C模型篇–一个词通过Word2vec训练之后，可以得到几个词向量?\n先问大家个问题：一个词通过Word2vec训练之后，可以得到几个词向量？\n如果您有点懵，说明我写对了，您慢慢看下去，码字不易，请多多点赞，让更多人看到，谢谢。\n词向量一般被认为是一个词的特征向量，也就是说可以代表一个词的含义。一个中文词，比如&quot;中国&quot;这个词，是很难被神经网络直接作为输入，我们需要将词向量化（或者说数字化），才能更好的喂进神经网络。\n这个过程很类似于计算机在处理我们输入的文字的时候，会转化为二进制进行处理。\n只不过这个二进制表达规则我们会自己人为规定，而词向量这个向量表达根据不同的方式会有着不同的结果。想一下，两者是不是很类似。\n谈到向量表达单词，一般首先想到的都是One-Hot编码。但是它是有缺点的。我这里谈两个缺点。\n首先是维度灾难：如果我们有1万个单词，那么你的One-hot去表示每个单词的的时候，会转变为一个1万维度的向量。\n那么在计算的时候会带来巨大的不便，而且向量矩阵极其稀疏，占据了太多不必要的内存。当然对于维度灾难，我们一般可以使用PCA等降维手段来缓解。\n其次是语义表达不足。这一点很简单，”娱乐“与”八卦“两个词，通过One-hot编码得到的向量，计算Consine得到相似度为0。\n这显然是不合适的。One-Hot编码表示出来的词向量是两两正交的，从余弦相似度的角度来看，是互不相关的，所以 One-Hot 不能很好的表达词语的相似性。\n这个时候，我们再来看 Word2vec。首先，要明确一点，Word2vec 不是一个模型，而是一个工具。这个点虽然无伤大雅，但是每每看到有的文章说它是个模型，就有点…\nWord2vec 从整体上理解，就是包含两个模型（CBOW and Skip-gram）和两个高效的优化训练方式（负采样和层序softmax）\n这个文章主要谈一下两种模型。\n先假定我们的窗口大小为2，也就是说中心词前面有两个词，后面有两个词，比如&quot;我/永远/爱/中国/共产党&quot;，抽象出来就是：\n$W_{t-2}，W_{t-1}，W_{t}，W_{t+1}，W_{t+2}$\n对于Skip-gram，中文我们叫跳字模型，使用中心词预测背景词。也就是用&quot;爱&quot;去预测其他的四个词。\n对于CBOW，中文我们叫连续词袋模型，使用背景词预测中心词，也就是用&quot;我&quot;，“永远”，“中国”，“共产党” 去预测&quot;爱&quot;。\nCBOW的模型架构，从整体上，我们可以分为三层：输入层，投影层，和输出层。\n对于输入层，对应的是窗口中的单词，也就是例子中&quot;我&quot;，“永远”，“中国”，“共产党” 四个词的词向量，在投影层，将四个词的词向量进行相加求平均，输出层在没有优化的前提下，维度为词表大小，随后做 Softmax即可。\nSkip-gram模型架构很类似，只不过可以省去投影层，因为输入为中心词，是一个单词的词向量，无从谈起求和与平均了。\n接下来，我们来详细谈一下Skip-gram。\n对于一个神经网络模型，我们需要确定我们的优化目标或者说目标函数是什么？\n对于Skip-gram，其实很容易理解，就是我要最大化在给出中心词的条件下背景词出现的概率，公式如下\n$\\prod_{t=1}^T \\ \\prod_{-m\\leq j \\leq m,j\\neq 0} P(w^{t+j}|w^{t})$\n这个公式对应着两个连乘，第一个连乘是对应 T 个输入样本，也就是说我们文本中的每个单词都要做中心词。第二个连乘代表着给定一个中心词的情况下，窗口中的单词出现的概率，内含相互独立的假设。\n在这里，有一个细节点想要提醒大家，在词汇表中的每个单词，都是对应两个词向量的，一个是在作为中心词的时候的中心词向量，一个是在作为背景词时候的背景词向量。\n优化目标函数的时候，为了减少复杂度（也就是忽略 T），我们可以使用随机梯度下降，针对每一个样本我们都更新一次参数。\n通过公式推导（略），我们能够观察到一个特点，就是每次更新参数，都会涉及到词典中的全部词汇，这是因为我们在做 Softmax 的时候，是分母是针对所有词汇的操作。\n这样的操作的复杂度是 O(|V|)，其中|V|就是我们词汇表的大小。CBOW 其实是很类似的情况，每次更新都会涉及到我们的全部词汇。\n于是，我们就需要对此进行优化，使用了两种近似的训练方式来高效的训练Word2vec，降低训练的复杂度。\n3. W2C优化方式篇\nWord2vec 涉及到两种优化方式，一种是负采样，一种是层序Softmax\n先谈一下负采样，以跳字模型为例。中心词生成背景词可以由两个相互独立事件的联合组成来近似（引自李沐大神的讲解）。\n第一个事件是，中心词和背景词同时出现在窗口中。第二个事件是，中心词和K个噪声词不同时出现在窗口数据中，其中噪声词由噪声分布随机生成。\n这里我们就可以知道上一个文章开头说到的，负采样是一种等价操作还是近似操作？我们在第二个事件中，使用了K个噪声词。但是实际上呢？应该远远大于K。\n还是那个例子，句子为&quot;我/永远/爱/中国/共产党&quot;，中心词为’爱’，我们在选择噪声词的时候，选择了K个，但是实际上，在词汇表中，排除掉’我’，‘永远’，‘中国’，‘共产党’ 这四个词汇的其他词都可以算做我的噪声词，然而为了减少复杂度，我只选择了其中的K个，所以当然应该是近似了。\n接下来，我们看层序Softmax。\n层序Softmax 对应的就是在输出层使用一个霍夫曼树，代替了原本在输出层统一进行的softmax。\n首先，我们需要了解霍夫曼树在这里是如何构建的。\n简单讲，霍夫曼树是一个二叉树，以语料中出现过的词当做叶子节点，以各词在语料中出现的次数当做权值进行构造。其中叶子节点有N个，就是词典的大小，非叶子节点有N-1个（包括根节点）。\n比如说我的所有文章中，“共产党”这个词出现了 100次，是最大的，那么根节点的左分支（或者右分支）就对应着”共产党“这个词，另一个分支做与根节点相同的操作，找到排除”共产党“这个词之外的所有词中最大的词，比如”中国“作为其中的左分支（或者右分支），以此类推，一个霍夫曼树就成功构建。\n霍夫曼树中，我们需要注意的是，每个非叶子节点对应一个向量，每个叶子节点对应一个向量。两种向量都会随着模型的训练进行更新。\n其中叶子节点的向量就是我们的词向量，而非叶子节点上的向量就是没有什么实际含义，它的作用就是帮助我们计算模型在霍夫曼树上不断的进行二分类时候的概率。\n以上面那句话为例，我们现在中心词为‘爱’，然后，我要预测背景词‘中国’。首先我们要确定的是我的叶子节点是包含所有单词的，也就是包含了我这个简单句子的五个单词（不考虑前期数据清洗低频率词的情况）。\n也就是说，在这个霍夫曼树上，有且仅有一条路径，让我从根节点出发，经过多次判断（也就是说走过了多个非叶子节点），最终走到了“中国”这个叶子节点，对应的概率就是每个节点概率的连乘。\n然后这个时候，我们想一下霍夫曼树是不是一种近似？\n当然，我们每更新一个词向量，只是涉及到了可以到达叶子节点的这一条路径上节点。所以复杂度就是树的高度，也就是 O(log|V|)\n4. W2C-负采样/霍夫曼之后模型是否等价\n私下有几个朋友看完之后还是有点懵，又问了一下具体细节。基于此，我重新写了一个简短的文章，希望能让大家明白，大家可以结合上一个文章看。\n我们再看一下题目：W2V经过霍夫曼或者负采样之后，模型与原模型相比，是等价的还是相似的？\n首先，我们要明确，这里的原模型指的是什么？原模型就是我们的没有经过优化的W2V（当然我们也说过它是一个工具不是一个模型）。\n也就是只是使用Skip-gram模型或者CBOW模型而没有进行优化的原始版本。对于这个原始版本，是在最后一层进行了Softmax。\n我们的目标函数中，最核心的一个部分就是在给定中心词的条件下生成正确背景词的概率，我们要最大化这个东西，公式如下：\n![条件概率_中心词生成背景词](https://github.com/DA-southampton/NLP_ability/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%9](https://github.com/DA-southampton/NLP_ability/blob/master/深度学习](…/images/条件概率_中心词生成背景词.png)\n仔细看，在分母涉及到了一个V，这里的V就是我们的词典大小。也就是说，为了计算这个条件概率，我们需要对整个词典进行操作，复杂度就是O(|V|)\n所以，负采样和霍夫曼就是针对这一个计算开销大的地方进行了优化。当然W2V为了减少计算量，还是去掉了隐层。比如CBOW直接是输入向量求和平均然后接霍夫曼树。比如Skip-gram直接是中心词的词向量接霍夫曼树。\n这不是我这个文章的重点，就不细细展开了。\n我们先说负采样。负采样的本质在于生成K个噪声。它的本质是基于中心词生成正确的背景词概率为1，生成噪声词概率为0，这个是我们的优化方向。公式如下：\n![负采样_条件概率_中心词生成背景词](https://github.com/DA-southampton/NLP_ability/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%9](https://github.com/DA-southampton/NLP_ability/blob/master/深度学习](…/images/负采样_条件概率_中心词生成背景词.png)\n仔细看这个公式，V已经消失，取而代之的是K，也就是我们的噪声词的数量，换句话讲，我们的复杂度被K这个大小限制住了，降低为了O(|K|)\n然后我们再来看层序Softmax。它的核心本质是在一条路径上不停的做二分类，概率连乘就会得到我们的条件概率。公式如下：\n![层序softmax_条件概率](https://github.com/DA-southampton/NLP_ability/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%9](https://github.com/DA-southampton/NLP_ability/blob/master/深度学习](…/images/层序softmax_条件概率.png)\n注意看，这个公式中，V也已经消失了，被霍夫曼树中到达背景词的路径限制住了，这也就是上个文章中说到的，复杂度变成了二叉树的高度: O(log|V|)\n既然只是针对的部分节点，那么与原始版本相比，当然是近似。\n简单的总结一下：\n其实可以这样理解，以跳字模型为例，条件概率是中心词生成背景词的概率，也就是我们优化函数中最核心的部分。没有使用优化的，分母涉及到全部词汇，训练开销大。负采样近似训练，把复杂度限制在了k个噪声词，层序softmax也属于近似训练，在它的条件概率中，不断的二分类，涉及到的是能够达到背景词的那个路径上的非叶子结点，也就是没涉及到其他节点，这一点和负采样很类似，都是从全部词汇降低复杂度，只不过负采样是被k限制，层序是被路径编码限制(0,1,1,1,0)这种限制住。\n不知道大家有没有注意到，负采样和霍夫曼都是讲Softmax转化为二分类的问题从而降低了复杂度。负采样是针对是不是背景词做二分类，霍夫曼是在对是不是正确路径上的节点做二分类。这么说有点不严谨，但是意思就是这么个意思，大家理解一下。\n5. Word2vec训练参数的选定?\n首先根据具体任务，选一个领域相似的语料，在这个条件下，语料越大越好。然后下载一个 word2vec 的新版（14年9月更新），语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型。最后记得设置迭代次数为三五十次，维度至少选 50，就可以了。（引自 《How to Generate a Good Word Embedding》）\n6. W2C为什么需要二次采样？\n说到 Word2vec 的采样，首先会想起来的是负采样，属于对Word2vec的一个近似训练方法。\n其实它还涉及到一个采样方法，就是subsampling，中文叫做二次采样。 用最简单的一句话描述二次采样就是，对文本中的每个单词会有一定概率删除掉，这个概率是和词频有关，越高频的词越有概率被删掉。 二次采样的公式如下所示：\n![二次采样](https://github.com/DA-southampton/NLP_ability/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%9](https://github.com/DA-southampton/NLP_ability/blob/master/深度学习](…/images/二次采样.png)\n注意: t为超参数，分母 f(w) 为单词w的词频与总词数之比\n首先说一下，我们需要对文本数据进行二次采样？ 举个简单例子，“他/是/个/优秀/的/学生”。如果此时中心词为&quot;学生&quot;，背景词为&quot;的&quot;。\n那么，我们的背景词对于我们这个中心词其实是没有什么作用的，并没有什么语义信息上的补充。\n但是像“的”这种高频词，出现的机会还很大，所以对于这一句话信息是存在冗余的。 也就是说，在一个背景窗口中，一个词和较低频词同时出现比和较高频词同时出现对训练词嵌入模型更有益。\n举个生活中的例子，现实生活中自律优秀的人比较少，堕落不努力人的人比较多，当然是优秀的人出现在我们身边会对我们自身的成长更加的有益。\n所以我们的想法就是减少和堕落的人出现的次数，远离他们，让优秀的人出现在我们生活中的概率上升。\n那么二次采样之后文本数据变成了什么样子？\n还是上面那句话，“他/是/个/优秀/的/学生”，在这个时候，就变成了“他/是/个/优秀/学生”。也就是说高频词“的”在我们的训练数据中消失了。\n当然这个消失正如上文所说，是一个概率，可能在之后的另一个句子中，它还是存在的，只不过它出现在文本中的词频肯定是降低了的。\n7. Word2vec的负采样\n负采样的特点\n首先对基于负采样的技术，我们更新的权重只是采样集合，减少了训练量，同时效果上来说，中心词一般来说只和上下文有关，更新其他词的权重并不重要，所以在降低计算量的同时，效果并没有变差。\n负采样具体实施细节\n我自己的总结就是创建两个线段，第一个线段切开词表大小的份数，每个份数的长度和频率正比。\n第二个线段均分M个，然后随机取整数，整数落在第二个线段那里，然后取第一个线段对应的词，如果碰到是自己，那么就跳过。\n欢迎拍砖\n8. W2C模型究竟是如何获得词向量的\n问大家一个问题：Word2vec模型是如何获得词向量的?\n很多文章在解释的时候，会说对一个词通过One-hot编码，然后通过隐层训练，得到的输入到隐层的矩阵就对应的词表词向量。\n我不能说这么解释是不对的，但是我认为是不准确的。\n在前面文章也说过了，Word2vec是不涉及到隐层的，CBOW有投影层，只是简单的求和平均，Skip-gram没有投影层，就是中心词接了一个霍夫曼树。\n所以，很多文章涉及到的隐层的权重矩阵也就无从谈起。\n在此情况下，词向量是怎么来的？\n从源码的角度来看，我们是对每个词都初始化了一个词向量作为输入，这个词向量是会随着模型训练而更新的，词向量的维度就是我们想要的维度，比如说200维。\n以Skip-gram为例，我们的输入的中心词的词向量其实不是One-hot编码，而是随机初始化的一个词向量，它会随着模型训练而更新。\n需要注意的一个细节点是，每个单词都会对应两个词向量，一个是作为中心词的时候的词向量，一个是作为背景词的时候的词向量。大家一般选择第一种。\n这一点需要注意区别Glove中的中心词向量和背景词向量。Glove中的中心词向量和背景词向量从理论上来说是等价的，只不过由于初始化的不同，最终结果会略有不同。\n9. CBOW和skip-gram相较而言，彼此相对适合哪些场景\n先用一句话来个结论：CBOW比Skip-gram 训练速度快，但是Skip-gram可以得到更好的词向量表达。\n为什么这么说？\n因为我们知道两种优化方式只是对softmax的近似优化，不会影响最终结果，所以这里，我们讨论的时候，为了更加的清晰讲解，不考虑优化的情况。\n使用一句话作为一个例子： “我/永远/爱/中国/共产党”\n先说CBOW，我们想一下，它的情况是使用周围词预测中心词。如果“爱”是中心词，别的是背景词。对于“爱”这个中心词，只是被预测了一次。\n对于Skip-gram，同样，我们的中心词是“爱”，背景词是其他词，对于每一个背景词，我们都需要进行一次预测，每进行一次预测，我们都会更新一次词向量。也就是说，相比CBOW，我们的词向量更新了2k次（假设K为窗口，那么窗口内包含中心词就有2k+1个单词）\n想一下是不是这么回事？Skip-gram被训练的次数更多，那么词向量的表达就会越丰富。\n如果语料库中，我们的的低频词很多，那么使用Skip-gram就会得到更好的低频词的词向量的表达，相应的训练时长就会更多。\n简单来说，我们视线回到一个大小为K的训练窗口（窗口内全部单词为2k+1个），CBOW只是训练一次，Skip-gram 则是训练了2K次。当然是Skip-gram词向量会更加的准确一点，相应的会训练的慢一点。\n欢迎大佬拍砖\n10. Fasttext解读-文本分类\n我先说一个小问题，估计很多人也有疑惑。\n看了很多文章，有的说是fasttext是CBOW的简单变种，有的说是Skip-gram的变种。究竟哪个是对的？\n带着这个问题，我们来聊一聊Fasttext。首先Fasttext涉及到两个论文：\n\n第一个是Bag of Tricks for Efficient TextClassification(201607)。它解决的问题是使用Fasttext进行文本分类\n第二个是Enriching Word Vectors with Subword Information(201607) 。它解决的是使用Fasttext训练词向量。\n\n今天这个文章，主要谈一下Bag of Tricks for Efficient Text Classification 这个论文 ，主要涉及到的就是文本分类的问题。\nFasttext用作文本分类，做到了速度和精读的一个平衡：标准多核CPU情况下，不到十分钟，可以训练超过十亿个单词。不到一分钟，可以对50万个句子在312千个类别中进行分类。\n这么说，其实不太明显，简单算一下。假设每个句子含有20个单词，那么十亿个单词对应就是五千万个句子，换句话讲在多核CPU的条件下，一分钟左右可以训练500万个句子。\n和Bert比较一下，在GPU条件下，8个小时训练300万条数据左右。相比之下Fasttext的这个速度是真的太快了。\n在这个论文中，也就是使用做文本分类的Fasttext，使用的是CBOW的架构。\n注意哦，强调一遍，Fasttext用在文本分类，模型架构使用的是CBOW的变种。（我这句话的意思不是说使用Skip-gram不可以，而是CBOW在理解文本分类的时候更加的容易理解）\n这里和Word2vec的CBOW有两个区别：\n\n第一，使用类别标签替换了中心词。\n第二，使用句子中所有单词作为输入，而不再是单单的针对滑动窗口中的单词。\n\n这两个点如果我们自己考虑，也很容易想到。\n为什么这么说呢？先说第二点。我现在要做的是针对文本进行分类，所以对于我的输入需要转换到整体这个句子来看，才能使对一个句子的特征表达。\n再说第一点，我们知道在Wrod2vec中，我们使用的是中心词作为输出，而且使用了霍夫曼作为输出层。\n非叶子点上的向量为了我的二分类提供计算，叶子节点为整个词汇表中所有词汇的向量。两个向量都会随着模型而训练。\n如果要做分类，我们可以想一下叶子节点和非叶子节点的变化。\n首先叶子节点对应的是所有类别。如果说我们的类别有5000个，那么对应到Word2vec，我们就有着5000个词汇。想一下是不是这么对应。\n非叶子节点其实没什么变化，因为它没有什么实际含义，只是为二分类提供计算。\n在这里还想说一下，word2vec中的叶子节点也就是词向量更新之后我们最后是要的，但是对于fasttext其实不会用到这个，因为我们是对文本进行分类，只需要保存了模型权重在预测的时候可以预测就可以了。\n还想谈一下词向量初始化的问题，模型训练开始的时候，词向量随机初始化就可以，模型训练结束之后，我们在预测阶段直接使用这个词向量就可以（就是随着模型训练而更新的这个词向量）。\n对这个论文还有一个很有意思的点，就是N-gram就是fasttext的模型的输入不仅仅针对的是每个单词，为了加入词序信息，还加入了n-gram信息。\n需要注意的一个细节是，这里的n-gram针对的是word，而不是char。对应到中文，应该对应的是分词之后的词，而不是字。但是我自己认为这么对应过来不太好理解。\n中文的字做n-gram貌似也有词序信息。但是英文的char-level的n-gram很难说针对这个句子提供一个语序信息。大家理解一下就好。\n还有一个问题想说一下，使用了n-gram信息之后，词表肯定是变大了的。你需要的n的内容越多，比如你想要n=1orn=2orn=3等等吧，n的取值范围越大，你的词表越大。\n这就会出现一个问题训练非常缓慢。这点很容易理解，参数越多训练当然越慢。\n针对这个问题，怎么解决呢？使用哈希。\n我举个简单的例子，不一定准确，“我/爱/中国/共产党”，我在更新的时候，把’我’,‘爱’,‘中国’,'共产党’我们都使用同一个参数来代表（这种情况很难遇见，理解一下就好），那么在更新训练参数的时候，我只需要更新一个参数就把这个四个词都更新了，当然会快一点。\n但是会出现一个问题，就是精度的问题。这个过程，不知道大家有咩有想到和albert很类似。哈希这个过程我自己感觉有点共享参数的意思。\n11. Fasttext解读-获取词向量\n这个文章主要是谈一下Enriching Word Vectors with Subword Information 这个论文。\n有了上一个文章的打底，（上一个文章点击这里）这个论文理解起来就比较简单，所以我写的也比较短。\n对于这个论文，我先给出它最核心的部分： 使用负采样的skip-gram的基础上，将每个中心词视为子词的集合，并学习子词的词向量。\n这句话涉及到的一个最关键的部分就是子词subword，也是这个论文的核心。\n举个例子，现在我们的中心词是&quot;where&quot;，设定子词大小为3，那么子词集合分为两个部分，注意是两个部分。\n第一部分形如这样：“&lt;wh”，“whe”，“her”，“ere”，“re&gt;”，第二部分就是特殊子词，也就是整词“”。\n那么对应到模型是，原来我的输入是“where”的词向量，现在在Fasttext就是所有子词的词向量的和。\n注意哦，这里是所有子词，是包含特殊子词，也就是整词的。\n对于背景词，直接使用整词就可以。\n简单来说，就是输出层使用子词（普通子词加上整词），输出层使用整词。\n如果遇到了OOV怎么办？使用普通子词的向量和来表示就可以。\n其实这里的子词，在名字上和上一个文章的ngram很类似，不过，这里使用的是就char的n-gram，缓解的问题并不是语序，而是利用了词序形态的规律。\n对应到中文，其实就是偏旁部首。 我记得阿里好像有发一个关于fasttext的中文版本，训练的就是偏旁部首。大家有兴趣可以去看一看。\n写完了，我对两个文章做个小总结，顺便对文章开头的问题做个回答: fasttext 训练词向量的时候一般是使用Skip-gram模型的变种。在用作文本分类的时候，一般是使用CBOW的变种。\n在这里，我想要强调一下，上一段我说的是一般情况，是为了方便大家了解，并不代表说CBOW架构不能训练词向量，skip-gram不能用作文本分类，需要注意这一点哦。\n12. Glove细节详解解读\n本文大概需要阅读 4.75 分钟 先问大家两个问题，看能不能解答\n\nGlove 中词向量的表达是使用的中心词向量还是背景词向量还是有其他方法？\n能不能分别用一句话概括出Glove和Fasttext 的核心要点？\n\n先来谈Glove。中文全称 Global Vectors for Word Representation。它做的事情概括出来就是：基于全局语料，获得词频统计，学习词语表征。\n我们从语料之中，学习到X共现词频矩阵，词频矩阵中的每个元素$x_{ij}$，代表的是词\n$x_{j}$出现在$x_{i}$的环境中的次数。注意，对于共现词频矩阵来说，它是一个对称矩阵。\n这一点非常的重要，也很容易理解，词A出现在词B周围的次数肯定是等价于词B出现在词A周围的次数的。\n类比于Word2vec，对于词$x_{i}$，就是中心词，对于词$x_{j}$也就是背景词。\n理论上，一个词作为中心词向量和一个词作为背景学到的两种向量应该是完全相同的。\n但是现实中，由于我们初始化的不同，所以我们最终学习到的两种词向量是有些许不同。\n为了增加模型的鲁棒性，在Glove中，使用两个词向量的和作为我们一个词的词向量的表达。\n这一点是区别于Word2vec，对于Word2vec，中心词向量和背景词向量是不等价的，我们一般使用中心词向量代表一个词最终的语义表达。\nGlove 论文中的推导过程其实不是很严谨，大致流程就是从大语料中发现了一个规律，即条件概率的比值可以比较直观的表达出词与词之间的关系。\n随后可以构建词向量函数去拟合条件概率的比值。基于此一步步的进行延伸推导，在我看了就是在基于一些假设，寻找出一种可能性的存在。\n在这里就不细说，直接给出Glove的损失函数：\n$\\sum_{i \\in V} \\sum_{j \\in V} h(x_{ij})(u_{j}^Tv_{i}+b_{i}+b_{j}-log(x_{ij}))^2$\n详细讲一下这个损失函数，它是一个平方损失函数，很值得琢磨。\n我把它分为了三个部分，权重函数h(x),词向量表达是$u_{j}^Tv_{i}+b_{i}+b_{j}$，共现词频的对数 $log(x_{ij})$\n从这里，我插一句我的理解，就是GLove基于的是无标签的语料，属于无监督的训练过程，但是从损失函数看，我觉得可以认为它是一个有监督的过程。\n标签就是$log(x_{ij})$，这个是我们从语料之中计算出来的，换句话说，在模型训练之前，我们可以对语料的计算，得到相应的标签数据，所以我自己认为这个可以看做是一个有监督的过程。\n我们使用一句话去描述这个损失函数可以这么去说：随着模型不停的优化，词向量的表达在不断的拟合共现词频的对数。\nh(x)是权重函数，代表的含义是表达一个词对的重要性，在值域[0,1]上单调递增。直观上理解就是一对词语共现的词频越多，那么它的重要性也就越大。\n论文中给出的函数是这样的，在x&lt;c(比如c=100)的情况下，h(x)=(x/c)^\\alpha (\\alpha=0.75)，在其余情况下，h(x)=1。也就是重要度不能无限增大，给了一个上限。\n我们看这个损失函数，有一个很有意思的点，就是h(0)=0。想一下，这个代表着什么？ 也就是说，当我们一对词的共现词频为0的时候，损失函数是为0，换句话讲，我们的损失函数的复杂度是和共现词频矩阵中的非零元素是线性关系。\n所以在训练的时候，我们只需要对非零元素采样，使用随机梯度就可以对词向量和两个偏置项进行学习更新。\n这里还有一个细节点，需要注意，偏置项是不可以省略的。为什么呢？因为如果去掉，在公式推导中的对称性假设就不满足，感兴趣的同学可以自己推导一系。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/词向量/词向量/"},{"title":"","date":"2024-06-20T11:37:53.001Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:37:53.001Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Dos/CIV_475_01');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Dos/CIV_475_01/"},{"title":"","date":"2024-06-20T11:38:09.881Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:38:09.881Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Dos/CIV_475_02G');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Dos/CIV_475_02G/"},{"title":"","date":"2024-06-20T11:38:13.761Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:38:13.761Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/IMG%20Dos/IMG_474_01');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/IMG Dos/IMG_474_01/"},{"title":"","date":"2024-06-20T11:38:02.651Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:38:02.651Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Dos/CIV_475_01Rus');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Dos/CIV_475_01Rus/"},{"title":"","date":"2024-06-20T11:38:14.341Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:38:14.341Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/IMG%20Dos/IMG_474_04');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/IMG Dos/IMG_474_04/"},{"title":"","date":"2024-06-20T11:38:15.141Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:38:15.141Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/IMG%20Dos/IMG_475_01');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/IMG Dos/IMG_475_01/"},{"title":"","date":"2024-06-20T11:38:15.711Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:38:15.711Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/IMG%20Dos/IMG_475_02G');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/IMG Dos/IMG_475_02G/"},{"title":"","date":"2024-06-20T11:39:27.116Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:39:27.116Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Update/PowerMac%20Fix');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Update/PowerMac Fix/"},{"title":"","date":"2024-06-20T11:39:17.456Z","date_formatted":{"ll":"Jun 20, 2024","L":"06/20/2024","MM-DD":"06-20"},"updated":"2024-06-20T03:39:17.456Z","content":"\n  \n    \n      (async () => {\n        const response = await fetch('https://api.github.com/repos/RedShakespeare/redshakespeare.github.io/contents/files/hxh_civ/Scenario/JustForWin');\n        const data = await response.json();\n        let htmlString = '';\n        \n        for (let file of data) {\n\t\t  if (file.name != 'index.html') {\n            htmlString += `${file.name}`;\n          }\n\t\t}\n\n\n        htmlString += '';\n        document.getElementsByTagName('body')[0].innerHTML = htmlString;\n      })()\n    \n  \n\n","plink":"http://www.ephesus.top/files/hxh_civ/Scenario/JustForWin/"},{"title":"","date":"2024-06-21T03:20:41.863Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:41.863Z","content":"0.背景\nwide&amp;deep 理论的介绍可以参考我之前写的那个文章。\nWDL在实际应用的时候，有很多细节需要注意。\n在我自己的应用中，对WDL模型做了一个简单的修改，加入了多模态（图片加标题）的特征，效果比单纯的xgboost要提升不少。\n因为涉及到具体业务，所以不能详细展开。\n不过我之前有读一个很不错的文章：，顺着这个文章的脉络，我们可以来看看WDL需要注意的地方。\n全文思维导图：\n1. wide &amp; deep 在贝壳推荐场景的实践\nWDL应用场景是预测用户是否点击推荐的房源。\nhttps://mp.weixin.qq.com/s/rp6H_HydTbKiSanijDZwBQ\n1.1 如何构建正负样本\n首先，模型离不开样本，样本一般从日志中获取。一般是通过埋点，记录用户行为到日志，然后清洗日志，获得用户行为。\n贝壳这里样本格式是三元组：userid，itemid和label；\n至于对应的特征，一般是需要根据userid，itemid到对应hive表格获取整合。\n\n正样本：用户点击的房源\n负样本：用户点击最大位置以上曝光未点击的房源；从未点击的用户部分曝光未点击的房源。\n\n样本构建细节整理\n在这里，想要详细说一下正负样本构建的细节。\n首先是对于日志的处理，需要区分web端和app端。不要增加无效的负样本\n其次，用户点击最大位置以上曝光未点击的房源，这个方法其实叫做Skip Above，也就是过滤掉最后一次的点击。这样做我们是基于用户对之后的item是没有观测到的。\n其次为了避免高度活跃用户的对loss的影响，在训练集中对每个用户提取相同数量的样本。\n然后我们来想一下这个问题：从未点击的用户部分曝光未点击的房源。\n首先，去除了这部分用户，会出现什么问题？\n模型学习到的只有活跃用户和有意向用户的行为习惯，这样线上和线下的数据分布会不一致。我们在线上的遇到的数据，肯定会出现那种不活跃用户。\n如果不去除这部分用户，会出现什么情况？\n这样的用户在本质上是无效用户。为什么这么说呢？我们模型的作用是为了提升用户点击。\n如果频繁给这个用户推荐物品，他一直不点击，也就是说没有正反馈。两种情况，一种是我们推荐的是有很大问题的，但是这种概率极低。还有一种情况，就是这个用户是有问题的。\n所以简单来说，我们需不需要对这样的用户做为样本？\n很简单，做A/B测试，看是否需要这部分用户以及需要多少这部分用户作为样本。\n还有一定需要注意的是，特征需要控制在样本时间之前，防止特征穿越。\n1.2 如何控制样本不平衡\n一般来说，负样本，也就是未点击的房源肯定是更多的。所以在训练模型的时候，肯定是存在样本不平衡的问题。\n贝壳这里采用的是下采样负样本和对样本进行加权。\n之前写个一个简单的文章，来讲述了一下如何缓解样本不平衡，可以参考这里：\n文章总结的结论就是，无论你使用什么技巧缓解类别不平衡，其实都只能让模型有略微的提升。最本质的操作还是增加标注数据。\n就拿贝壳的操作来说，下采样和样本加权，本质上都修改了样本的分布情况。\n就会出现训练样本分布和线上真实分布不一致的情况，那么你现在训练的模型究竟在线上真实环境能不能有好的提升，就看模型在真实数据上的评估情况了。\n1.3 解决噪声样本\n贝壳指的噪声样本指的是：\n\n在我们的业务场景下，用户在不同时间对同一房源可能会存在不同的行为，导致采集的样本中有正有负。\n\n我自己的感受是很奇怪的是，只是猜测而已哈，样本特征中没有加入时间特征吗？加入时间特征应该可以学到用户短期兴趣变化。\n1.4 特征处理：\n\n\n缺失值与异常值处理：常规操作；不同特征使用不同缺失值填补方法；异常值使用四分位；\n\n\n等频分桶处理：常规操作；比如价格，是一个长尾分布，这就导致大部分样本的特征值都集中在一个小的取值范围内，使得样本特征的区分度减小。\n不过有意思的是，贝壳使用的是不同地区的等频分布，保证每个城市下特征分布均匀。\n\n\n归一化：常规操作；效果得到显著提升；\n\n\n低频过滤：常规操作；对于离散特征，过于低频的归为一类；\n\n\nembedding：常规操作；小区，商圈id做embedding；\n\n\n1.5 特征工程\n预测目标是用户是否点击itme，所以特征就是从三方面：用户，item，交互特征；\n\n用户：\n\n\n注册时长、上一次访问距今时长等基础特征，最近3/7/15/30/90天活跃/浏览/关注/im数量等行为特征，以及画像偏好特征和转化率特征。\n\n\n房源：\n\n\n价格、面积、居室、楼层等基础特征，是否地铁房/学区房/有电梯/近医院等二值特征，以及热度值/点击率等连续特征。\n\n\n交叉：\n\n\n将画像偏好和房源的特征进行交叉，主要包含这几维度：价格、面积、居室、城区、商圈、小区、楼层级别的交叉。交叉值为用户对房源在该维度下的偏好值。\n\n1.6 模型离线训练\n\n数据切分：采用7天的数据作为训练集，1天的作为测试集\nembedding：尝试加入，没有获得很好的效果\n模型调优：\n\n防止过拟合：加入dropOut 与 L2正则\n加快收敛：引入了Batch Normalization\n保证训练稳定和收敛：尝试不同的learning rate（wide侧0.001，deep侧0.01效果较好）和batch_size（目前设置的2048）\n优化器：我们对比了SGD、Adam、Adagrad等学习器，最终选择了效果最好的Adagrad。\n\n\n\n1.7 模型上线\n\n模型部署：使用TensorFlow Serving，10ms解决120个请求\n解决线上线下特征不一致：将离线特征处理的参数存储在redis中\n效果提升：\n\nCTR：提升6.08%\nCVR:：提升15.65%\n\n\n\n2. WDL代码实现\nGithub上有太多了，TF也有官方的实现，我就不多说了\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/推荐/WDL/WDL在贝壳中的应用实践总结/"},{"title":"","date":"2024-06-21T03:48:16.235Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.235Z","content":"中文分词中基于词典的正向最大匹配和逆向最大匹配\n正向最大匹配和逆向最大匹配步骤类似，只是方向不同，我以正向匹配为例，先用一句话去总结它：\n在做整个正向成词的过程中，我们做了两个步骤，首先按照字典最大长度进行对原始文本进行切分，然后逐渐去掉右边一个单字，去查看剩余文本在字典是否存在，依次迭代。\n上面这句话只看不太好理解，我来个简单的例子，如下：\n我要被切分的句子是这样的：”今天天气真不错啊“\n我的字典是这样的：[今天,天天,天气,真不错,不错,啊,哈哈哈哈哈]\n对于字典这一块，最粗糙的就是存成列表，优化一点可以存成字典树，这里简化一点，我们存成列表。\n我字典的最大长度是 “哈哈哈哈哈”，为5\n所以我第一次正向匹配就是：\n今天天气真 # 取原始文本前五个单词，查看是否存在于字典，否，删除底部\n今天天气 # 查看是否存在于字典，否，删除底部\n今天天 # 查看是否存在于字典，否，删除底部\n今天 #匹配到字典中的“天气”这个词\n第二次正向匹配是这样的：\n天气真不错啊 # 因为”今天“已经被匹配到了，所以我们不在考虑，取剩余文本的前五个单词，查看是否存在于字典，否，删除底部\n天气真不错 #查看是否存在于字典，否，删除底部\n天气真不 #查看是否存在于字典，否，删除底部\n天气真 #查看是否存在于字典，否，删除底部\n天气 #匹配到字典中的“天气“这个单词\n第三次正向匹配的过程：\n真不错啊 # 剩余文本不够5个，我们取小，取4个，查看是否存在于字典，否，删除底部\n真不错 # 匹配到”真不错“ 这个单词\n第四次正向匹配的过程：\n啊 # 字典中没有与之相关的单词，由于长度已经为1，直接单独成词就可以\n在做整个正向成词的过程中，我们做了两个步骤，首先按照字典最大长度进行对原始文本进行切分（需要比对最大长度和文本的长度，如果文本长度不够的话，就取文本长度，总之取小。比如第三次正向匹配”真不错啊“这剩余的四个字就不够5个），\n然后逐渐去掉右边一个单字，去查看剩余文本在字典是否存在，依次迭代。\n其实逆向匹配是很类似的过程，只不过方向变了，需要注意的是我们始终删除的是底部单词：\n第一次逆向匹配：\n气真不错啊 # 查看是否存在于字典，否，删除底部\n真不错啊 # 查看是否存在于字典，否，删除底部\n不错啊 # 查看是否存在于字典，否，删除底部\n错啊 # 查看是否存在于字典，否，删除底部\n啊 # 字典中没有与之相关的单词，由于长度已经为1，直接单独成词就可以\n…\n…\n…\n双向最大匹配算法就是两种方法都切一遍，从中选择一种比较好的，标准就是：大颗粒度词越多越好，非词典词和单字词越少越好.\n对于代码的实现，我记得是好久之前从网上down下来的，具体来源忘了，不过都大同小异，自己写也没啥问题。\n我在这里啰嗦的讲一下大致思路，如果您觉得比较简单，或者只想看代码，跳过就可以：\n基本思路是这样的，我有一个存储我词典的列表，以词典中最大长度为基线顺序对原始文本进行切分，迭代查看当前切分词是否在词典，在就算一个词，不在的话，当前词长度减一，就是往前缩小一个词，继续进行上述活动。直至长度为1，是最后的一个迭代条件。\n在写代码的时候，我自己觉得从两个方面来掌握，一个是从小方面，怎么讲，就是比如说我的字典最大的长度是5个单词，我在5个单词迭代的去找有没有在字典的中的词，这是一个while循环。\n还有一个方面是大的方面，就是我现在5个单词迭代完了，比如找到了一个长度为2的在字典中的词（需要注意的是如果没有在字典中，那么长度就是1的单字就可以加进去了），然后我要做的就是把这两个单词之后的字段作为输入，再重复上面这个过程，这个是大的方面，是另一个While循环\n12345678910111213141516171819202122232425262728## 正向最大匹配算法def cut_words(split_sentence,words_dic):    #统计词典中最长的词    max_length = max(len(word) for word in words_dic)    sentence = split_sentence.strip() ## 简单清理一下    #统计序列长度    words_length = len(sentence) ## 在第二个循环的时候，我需要不停的和字典最大长度比较，取最小值作为基线    #存储切分好的词语    cut_word_list = []    while words_length &gt; 0: ## 第二个循环，找到一个之后，循环的去找下一个符合要求的        max_cut_length = min(max_length, words_length)        subSentence = sentence[0 : max_cut_length]        while max_cut_length &gt; 0: ## 第一个循环，迭代找到符号字典的            if subSentence in words_dic:                cut_word_list.append(subSentence)                break            elif max_cut_length == 1:                cut_word_list.append(subSentence)                break            else:                max_cut_length = max_cut_length -1                subSentence = subSentence[0:max_cut_length]        sentence = sentence[max_cut_length:]        words_length = words_length - max_cut_length    return cut_word_listinput_str=&quot;今天天气真不错啊，适合出去旅游&quot;bmm_word_list = cut_words(input_str, words_dic)print(bmm_word_list)\n123456789101112131415161718192021222324252627##逆向最大匹配def cut_words(raw_sentence,words_dic):    #统计词典中词的最长长度    max_length = max(len(word) for word in words_dic)    sentence = raw_sentence.strip()    #统计序列长度    words_length = len(sentence)    #存储切分出来的词语    cut_word_list = []    #判断是否需要继续切词    while words_length &gt; 0:        max_cut_length = min(max_length, words_length)        subSentence = sentence[-max_cut_length:]        while max_cut_length &gt; 0:            if subSentence in words_dic:                cut_word_list.append(subSentence)                break            elif max_cut_length == 1:                cut_word_list.append(subSentence)                break            else:                max_cut_length = max_cut_length -1                subSentence = subSentence[-max_cut_length:]        sentence = sentence[0:-max_cut_length]        words_length = words_length -max_cut_length    cut_word_list.reverse()    return  cut_word_list\n参考链接：\n中文分词中的正向最大匹配与逆向最大匹配：https://blog.csdn.net/chengzheng_hit/article/details/54752673\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/关键词提取/中文分词/基于词典的正向最大匹配和逆向最大匹配中文分词/"},{"title":"","date":"2024-06-21T03:20:41.753Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:41.753Z","content":"更多NLP文章在这里：\nhttps://github.com/DA-southampton/NLP_ability\n谈到WDL，一个经常看到的总结是：Wide and Deep 模型融合 wide 模型的记忆能力和 Deep 模型的泛化能力，进行两个模型的联合训练，从而兼顾推荐的准确性和多样性。\n理解上面这句话，还是要先弄清楚：什么是记忆能力，什么是泛化能力？\n1. 什么是记忆能力与泛化能力\n1.1记忆能力\n我们先说记忆能力，从中文的角度理解，记忆能力就是之前做过的事情，在后面做同样的事的时候会利用到之前的经验和教训。\n进一步，记忆能力就是对之前学习到的经验或者说规律的遵守。\n原论文是这么说的：\n\nMemorization can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data.\n\n从这段话可以看出来记忆能力分为两个部分：\n\n从历史数据中学习共现的物体/特征组合—&gt;这就对应到上面谈到的经验规律\n在预测的时候利用到这种学习到的这种相关性—&gt;这就对应到上面谈到的对经验的遵守。\n\n在这里，我想提一下，在第一点中提到的 “学习共现的物体/特征组合” 的主体是谁？\n最开始我认为是模型，后来认为不是。\n因为LR模型属于广义线性模型，本身不具备对特征之间非线性关系进行建模。\n所以需要我们从历史数据中找到有用的特征组合（当然我们也可以使用一些工具来找到哪些特征组合是有效的），人为的加入到模型中，给LR模型增添非线性建模能力。\n简单来说，记忆能力是一种共现规律，表现方式为特征交叉，它需要人为或者通过工具从历史数据中找到，并放入到模型中作为新的特征，从而增加非线性建模能力。\n记忆能力过强会出现一个问题，就是推荐物体的单一化。\n原文是这么说的：\n\nRecommendations based on memorization are usually more topical and directly relevant to the items on which users have already performed actions.\n\n1.2泛化能力\n对于泛化能力，原论文是这么说的：\n\nGeneralization, on the other hand, is based on transitivity of correlation and explores new feature combinations that have never or rarely occurred in the past.\n\n关键词是：从未或者很少出现的特征组合\n神经网络无需人为构建组合特征，有着自动做特征组合的方式。可以通过对类别特征做embedding，这样就是在测试集中出现在训练集中没有出现过的特征组合方式，也可以使用embedding进行计算得到对应的值。\n综合来说，LR模型有着更强的记忆能力，Deep模型有着更强的泛化能力。\n2.模型架构图\n\n整个模型分为三个部分，左边的Wide模型，右边的Deep模型，最后输出的Softmax/sigmoid函数。\nWide使用的是LR模型，这里需要注意的点是LR的输入特征包含两部分：\n\n原始特征\n特征交叉之后的特征（特征交叉之前各自特征需要one-hot）\n\nDeep模型使用的是前馈神经网络，会对类别特征做embedding，连续特征不动直接输入就好（需要提前做好特征工程）。\n联合训练，Wide使用FTRL算法进行优化，Deep模型使用AdaGrad进行优化。\n在实际中，Wide和Deep部分直接使用一个优化器就可以。\n3.实践\n3.1 实践架构\n\n这个是原论文中的架构图，我们自己在实践的时候不一定完全遵守。比如架构图中Wide部分只是使用了交叉特征，我们在使用的时候可以把原始的离散特征或者打散后的连续特征加过来。\n3.2 多模态特征的加入\n有些时候对于用户或者待推荐的物体会有Text和Image，为了增加效果，可能会使用到多模态特征。\n（是否需要加入多模态特征需要大家亲自试，很有可能吭哧吭哧写了好久代码调试通了，最后发现效果提升不多甚至还会降低，哭了）\n我这里给几个简单的思路。\n\nText 和 Image 的 embedding 向量，采用 和Wide模型一样的方式加入到整体模型中就可以了。至于 两者的Embedding向量如何获取，就看你自己了。\nText和Image之间使用attention之后再加入\nText和Image 和Deep 模型的输出拼接之后再做一次处理\n多看 Paper-给个关键词：Multimodal Fusion\n\n3.3 特征工程小记录\n在详细写一个特征工程的文章，写完之后会放出来。\n后记\n读完整个论文，让我去回顾整个模型，给我这样一个感觉：\n对于隐藏在历史数据中的共现特征关系，Deep模型是可以学习到的。但是WDL模型做的是，把其中的一部分（容易观察出来或者通过其他工具找出来的特征组合）放到LR这边，从而显式的加入到模型中。\n往极端的方面想一下，LR模型这边更像是一种规则，是对Deep模型输出的补充。\n","plink":"http://www.ephesus.top/links/NLP_ability/推荐/推荐/WDL/WDl/"},{"title":"","date":"2024-06-21T03:48:21.835Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:21.835Z","content":"Contributors\nOpenNMT-py is a community developed project and we love developer contributions.\nGuidelines\nBefore sending a PR, please do this checklist first:\n\nPlease run onmt/tests/pull_request_chk.sh and fix any errors. When adding new functionality, also add tests to this script. Included checks:\n\nflake8 check for coding style;\nunittest;\ncontinuous integration tests listed in .travis.yml.\n\n\nWhen adding/modifying class constructor, please make the arguments as same naming style as its superclass in PyTorch.\nIf your change is based on a paper, please include a clear comment and reference in the code (more on that below).\n\nDocstrings\nAbove all, try to follow the Google docstring format\n(Napoleon example,\nGoogle styleguide).\nThis makes it easy to include your contributions in the Sphinx documentation. And, do feel free\nto autodoc your contributions in the API .rst files in the docs/source folder! If you do, check that\nyour additions look right.\n12345cd docs# install some dependencies if necessary:# recommonmark, sphinx_rtd_theme, sphinxcontrib-bibtexmake htmlfirefox build/html/main.html  # or your browser of choice\nSome particular advice:\n\n\nTry to follow Python 3 typing module conventions when documenting types.\n\nException: use “or” instead of unions for more readability\nFor external types, use the full “import name”. Common abbreviations (e.g. np) are acceptable.\nFor torch.Tensor types, the torch. is optional.\nPlease don’t use tics like (`str`) or rst directives like (:obj:`str`). Napoleon handles types\nvery well without additional help, so avoid the clutter.\n\n\n\nGoogle docstrings don’t support multiple returns.\nFor multiple returns, the following works well with Sphinx and is still very readable.\n12345678910111213141516def foo(a, b):    &quot;&quot;&quot;This is my docstring.    Args:        a (object): Something.        b (class): Another thing.    Returns:        (object, class):        * a: Something or rather with a long          description that spills over.        * b: And another thing.    &quot;&quot;&quot;    return a, b\n\n\nWhen citing a paper, avoid directly linking in the docstring! Add a Bibtex entry to docs/source/refs.bib.\nE.g., to cite “Attention Is All You Need”, visit arXiv, choose the\nbibtext link, search docs/source/refs.bib\nusing CTRL-F for DBLP:journals/corr/VaswaniSPUJGKP17, and if you do not find it then copy-paste the\ncitation into refs.bib. Then, in your docstring, use :cite:`DBLP:journals/corr/VaswaniSPUJGKP17` .\n\nHowever, a link is better than nothing.\n\n\n\nPlease document tensor shapes. Prefer the format\nb, c)`` ```. This style is easy to read, allows using ``x`` for multplication, and is common1234567891011121314  (PyTorch uses a few variations on the parentheses format, AllenNLP uses exactly this format, Fairseq uses  the parentheses format with single ticks).    - Again, a different style is better than no shape documentation.- Please avoid unnecessary space characters, try to capitalize, and try to punctuate.  For multi-line docstrings, add a blank line after the closing ``&quot;&quot;&quot;``.  Don&#x27;t use a blank line before the closing quotes.  ``&quot;&quot;&quot; not this &quot;&quot;&quot;`` ``&quot;&quot;&quot;This.&quot;&quot;&quot;``  ```python  &quot;&quot;&quot;      Not this.  &quot;&quot;&quot;\n1&quot;&quot;&quot;This.&quot;&quot;&quot;\nThis note is the least important. Focus on content first, but remember that consistent docs look good.\n\n\nBe sensible about the first line. Generally, one stand-alone summary line (per the Google guidelines) is good.\nSometimes, it’s better to cut directly to the args or an extended description. It’s always acceptable to have a\n“trailing” citation.\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/CONTRIBUTING/"},{"title":"","date":"2024-06-21T03:48:21.635Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:21.635Z","content":"Notes on versioning\n[Unreleased]\nFixes and improvements\n1.0.0.rc1 (2019-10-01)\n\nFix Apex / FP16 training (Apex new API is buggy)\nMultithread preprocessing way faster (Thanks François Hernandez)\nPip Installation v1.0.0.rc1 (thanks Paul Tardy)\n\n0.9.2 (2019-09-04)\n\nSwitch to Pytorch 1.2\nPre/post processing on the translation server\noption to remove the FFN layer in AAN + AAN optimization (faster)\nCoverage loss (per Abisee paper 2017) implementation\nVideo Captioning task: Thanks Dylan Flaute!\nToken batch at inference\nSmall fixes and add-ons\n\n0.9.1 (2019-06-13)\n\nNew mechanism for MultiGPU training “1 batch producer / multi batch consumers”\nresulting in big memory saving when handling huge datasets\nNew APEX AMP (mixed precision) API\nOption to overwrite shards when preprocessing\nSmall fixes and add-ons\n\n0.9.0 (2019-05-16)\n\nFaster vocab building when processing shards (no reloading)\nNew dataweighting feature\nNew dropout scheduler.\nSmall fixes and add-ons\n\n0.8.2 (2019-02-16)\n\nUpdate documentation and Library example\nRevamp args\nBug fixes, save moving average in FP32\nAllow FP32 inference for FP16 models\n\n0.8.1 (2019-02-12)\n\nUpdate documentation\nRandom sampling scores fixes\nBug fixes\n\n0.8.0 (2019-02-09)\n\nMany fixes and code cleaning thanks @flauted, @guillaumekln\nDatasets code refactor (thanks @flauted) you need to r-preeprocess datasets\n\nNew features\n\nFP16 Support: Experimental, using Apex, Checkpoints may break in future version.\nContinuous exponential moving average (thanks @francoishernandez, and Marian)\nRelative positions encoding (thanks @francoishernanndez, and Google T2T)\nDeprecate the old beam search, fast batched beam search supports all options\n\n0.7.2 (2019-01-31)\n\nMany fixes and code cleaning thanks @bpopeters, @flauted, @guillaumekln\n\nNew features\n\nMultilevel fields for better handling of text featuer embeddinggs.\n\n0.7.1 (2019-01-24)\n\nMany fixes and code refactoring thanks @bpopeters, @flauted, @guillaumekln\n\nNew features\n\nRandom sampling thanks @daphnei\nEnable sharding for huge files at translation\n\n0.7.0 (2019-01-02)\n\nMany fixes and code refactoring thanks @benopeters\nMigrated to Pytorch 1.0\n\n0.6.0 (2018-11-28)\n\nMany fixes and code improvements\nNew: Ability to load a yml config file. See examples in config folder.\n\n0.5.0 (2018-10-24)\n\nFixed advance n_best beam in translate_batch_fast\nFixed remove valid set vocab from total vocab\nNew: Ability to reset optimizer when using train_from\nNew: create_vocabulary tool + fix when loading existing vocab.\n\n0.4.1 (2018-10-11)\n\nFixed preprocessing files names, cleaning intermediary files.\n\n0.4.0 (2018-10-08)\n\n\nFixed Speech2Text training (thanks Yuntian)\n\n\nRemoved -max_shard_size, replaced by -shard_size = number of examples in a shard.\nDefault value = 1M which works fine in most Text dataset cases. (will avoid Ram OOM in most cases)\n\n\n0.3.0 (2018-09-27)\n\n\nNow requires Pytorch 0.4.1\n\n\nMulti-node Multi-GPU with Torch Distributed\nNew options are:\n-master_ip: ip address of the master node\n-master_port: port number of th emaster node\n-world_size = total number of processes to be run (total GPUs accross all nodes)\n-gpu_ranks = list of indices of processes accross all nodes\n\n\ngpuid is deprecated\nSee examples in https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/FAQ.md\n\n\nFixes to img2text now working\n\n\nNew sharding based on number of examples\n\n\nFixes to avoid 0.4.1 deprecated functions.\n\n\n0.2.1 (2018-08-31)\nFixes and improvements\n\nFirst compatibility steps with Pytorch 0.4.1 (non breaking)\nFix TranslationServer (when various request try to load the same model at the same time)\nFix StopIteration error (python 3.7)\n\nNew features\n\nEnsemble at inference (thanks @Waino)\n\n0.2 (2018-08-28)\nimprovements\n\nCompatibility fixes with Pytorch 0.4 / Torchtext 0.3\nMulti-GPU based on Torch Distributed\nAverage Attention Network (AAN) for the Transformer (thanks @francoishernandez )\nNew fast beam search (see -fast in translate.py) (thanks @guillaumekln)\nSparse attention / sparsemax (thanks to @bpopeters)\nRefactoring of many parts of the code base:\n\n\nchange from -epoch to -train_steps -valid_steps (see opts.py)\nreorg of the logic train =&gt; train_multi / train_single =&gt; trainer\n\n\nMany fixes / improvements in the translationserver (thanks @pltrdy @francoishernandez)\nfix BPTT\n\n0.1 (2018-06-08)\nFirst and Last Release using Pytorch 0.3.x\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/CHANGELOG/"},{"title":"","date":"2024-06-21T03:48:23.425Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:23.425Z","content":"MIT License\nCopyright © 2017-Present OpenNMT\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the “Software”), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/LICENSE/"},{"title":"","date":"2024-06-20T19:48:23.295Z","updated":"2024-06-20T19:48:23.295Z","content":"{\"env\":\"pytorch-0.4\",\"machine\":\"cpu\"}"},{"title":"","date":"2024-06-21T03:48:26.755Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:26.755Z","content":"OpenNMT-py: Open-Source Neural Machine Translation\n\n\nThis is a Pytorch\nport of OpenNMT,\nan open-source (MIT) neural machine translation system. It is designed to be research friendly to try out new ideas in translation, summary, image-to-text, morphology, and many other domains. Some companies have proven the code to be production ready.\nWe love contributions. Please consult the Issues page for any Contributions Welcome tagged post.\n\nBefore raising an issue, make sure you read the requirements and the documentation examples.\nUnless there is a bug, please use the Forum or Gitter to ask questions.\nTable of Contents\n\nFull Documentation\nRequirements\nFeatures\nQuickstart\nRun on FloydHub\nAcknowledgements\nCitation\n\nRequirements\nInstall OpenNMT-py from pip:\n1pip install OpenNMT-py\nor from the sources:\n123git clone https://github.com/OpenNMT/OpenNMT-py.gitcd OpenNMT-pypython setup.py install\nNote: If you have MemoryError in the install try to use pip with --no-cache-dir.\n(Optionnal) some advanced features (e.g. working audio, image or pretrained models) requires extra packages, you can install it with:\n1pip install -r requirements.opt.txt\nNote:\n\nsome features require Python 3.5 and after (eg: Distributed multigpu, entmax)\nwe currently only support PyTorch 1.2 (should work with 1.1)\n\nFeatures\n\nSeq2Seq models (encoder-decoder) with multiple RNN cells (lstm/gru) and attention (dotprod/mlp) types\nTransformer models\nCopy and Coverage Attention\nPretrained Embeddings\nSource word features\nImage-to-text processing\nSpeech-to-text processing\nTensorBoard logging\nMulti-GPU training\nData preprocessing\nInference (translation) with batching and beam search\nInference time loss functions.\n[Conv2Conv convolution model]\nSRU “RNNs faster than CNN” paper\nMixed-precision training with APEX, optimized on Tensor Cores\n\nQuickstart\nFull Documentation\nStep 1: Preprocess the data\n1onmt_preprocess -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\nWe will be working with some example data in data/ folder.\nThe data consists of parallel source (src) and target (tgt) data containing one sentence per line with tokens separated by a space:\n\nsrc-train.txt\ntgt-train.txt\nsrc-val.txt\ntgt-val.txt\n\nValidation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.\nAfter running the preprocessing, the following files are generated:\n\ndemo.train.pt: serialized PyTorch file containing training data\ndemo.valid.pt: serialized PyTorch file containing validation data\ndemo.vocab.pt: serialized PyTorch file containing vocabulary data\n\nInternally the system never touches the words themselves, but uses these indices.\nStep 2: Train the model\n1onmt_train -data data/demo -save_model demo-model\nThe main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder.\nIf you want to train on GPU, you need to set, as an example:\nCUDA_VISIBLE_DEVICES=1,3\n-world_size 2 -gpu_ranks 0 1 to use (say) GPU 1 and 3 on this node only.\nTo know more about distributed training on single or multi nodes, read the FAQ section.\nStep 3: Translate\n1onmt_translate -model demo-model_acc_XX.XX_ppl_XXX.XX_eX.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose\nNow you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into pred.txt.\n!!! note “Note”\nThe predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for translation or summarization.\nAlternative: Run on FloydHub\n\nClick this button to open a Workspace on FloydHub for training/testing your code.\nPretrained embeddings (e.g. GloVe)\nPlease see the FAQ: How to use GloVe pre-trained embeddings in OpenNMT-py\nPretrained Models\nThe following pretrained models can be downloaded and used with translate.py.\nhttp://opennmt.net/Models-py/\nAcknowledgements\nOpenNMT-py is run as a collaborative open-source project.\nThe original code was written by Adam Lerer (NYC) to reproduce OpenNMT-Lua using Pytorch.\nMajor contributors are:\nSasha Rush (Cambridge, MA)\nVincent Nguyen (Ubiqus)\nBen Peters (Lisbon)\nSebastian Gehrmann (Harvard NLP)\nYuntian Deng (Harvard NLP)\nGuillaume Klein (Systran)\nPaul Tardy (Ubiqus / Lium)\nFrançois Hernandez (Ubiqus)\nJianyu Zhan (Shanghai)\n[Dylan Flaute](http://github.com/flauted (University of Dayton)\nand more !\nOpentNMT-py belongs to the OpenNMT project along with OpenNMT-Lua and OpenNMT-tf.\nCitation\nOpenNMT: Neural Machine Translation Toolkit\nOpenNMT technical report\n123456789101112@inproceedings&#123;opennmt,  author    = &#123;Guillaume Klein and               Yoon Kim and               Yuntian Deng and               Jean Senellart and               Alexander M. Rush&#125;,  title     = &#123;Open&#123;NMT&#125;: Open-Source Toolkit for Neural Machine Translation&#125;,  booktitle = &#123;Proc. ACL&#125;,  year      = &#123;2017&#125;,  url       = &#123;https://doi.org/10.18653/v1/P17-4012&#125;,  doi       = &#123;10.18653/v1/P17-4012&#125;&#125;\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/README_old/"},{"title":"","date":"2024-06-21T03:48:26.695Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:26.695Z","content":"机器翻译竞赛-唱园杯-Pytorch代码-Baseline\n几天前看到一个机器翻译竞赛-唱园杯，奖金60万，真是吓了一跳。\n不过我不是冲奖金，因为这么高奖金可以想一下竞争程度。我本意想要积累一下中英文翻译数据，后来发现是编码之后的数据…\n有点失望，就没有然后了。所以就没有花太多时间去做这个东西，简单跑了一个baselines。\n官方评测指标简单粗暴，一个句子有一个单词翻译错了就pass。这个比赛数据量不小，迭代20万步，目测需要一周。所以现在排行榜的分数都很低，大佬们估计在等后期发力吧。\n没时间打比赛，一些相关代码也不想浪费掉，就分享给大家，希望对您有所帮助。\nBaseline代码很简单，就是用 OpenNMT-py 这个库做的机器翻译，不过中文关于这个库的资料很少，当初啃这个库也是一点点看的源代码，细节还挺多的。\n默默吐槽一句代码组织架构有点乱，有些地方真的是让人摸不到头脑…\n我也用这个文章做一个简单的 OpenNMT-py 的教程。如果是参加竞赛的话，后期可能需要修改源代码，所以建议大家不用安装 OpenNMT-py 库，而是直接下载源代码，方便修改。\n首先，使用环境如下，大家照此下载就可以:\ntorchtext==0.4\nOpenNMT-py==1.0\npython==3.5\ncuda==9.0\n如果 OpenNMT-py==1.0 这个版本大家找不到，直接来我github上下载下来用就可以。\n数据预处理\n在 /data 目录下，需要包含四个文件，分别是 src-train.txt  src-val.txt  tgt-train.txt  tgt-val.txt\n假如我们是中文翻译成英文，那么我们的 src-train.txt 和 src-val.txt 就是中文文件， tgt-train.txt  和 tgt-val.txt 就是英文文件。\n其中文件内容格式为每行为一句文本，以空格进行分割。\n对于唱园杯的数据，我们需要对其进行一些简单的修改，以满足上面的要求，我这边给出一个简单的处理代码，以字为单位，代码文件名称为「process_ori_data.py」：\n12345678910111213141516171819202122232425262728293031323334353637file=open(&#x27;train_data.csv&#x27;,&#x27;r&#x27;)lines=file.readlines()src_train=open(&#x27;src-train.txt&#x27;,&#x27;w&#x27;)tgt_train=open(&#x27;tgt-train.txt&#x27;,&#x27;w&#x27;)src_val=open(&#x27;src-val.txt&#x27;,&#x27;w&#x27;)tgt_val=open(&#x27;tgt-val.txt&#x27;,&#x27;w&#x27;)chinese_lists=[]english_lists=[]index=0for line in lines:    if index ==0:        index+=1        continue    line=line.strip().split(&#x27;,&#x27;)    chinese=line[1].strip().split(&#x27;_&#x27;)    english=line[2].strip().split(&#x27;_&#x27;)    chinese_lists.append(&#x27; &#x27;.join(chinese))    english_lists.append(&#x27; &#x27;.join(english))    index+=1assert len(chinese_lists)==len(english_lists)split_num=int(0.85*index)for num in range(len(english_lists)):    if num&lt;=split_num:        src_train.write(chinese_lists[num]+&#x27;\\n&#x27;)        tgt_train.write(english_lists[num]+&#x27;\\n&#x27;)    else:        src_val.write(chinese_lists[num]+&#x27;\\n&#x27;)        tgt_val.write(english_lists[num]+&#x27;\\n&#x27;)src_train.close()tgt_train.close()src_val.close()tgt_val.close()\n在对原始数据进行处理之后，我们还需要进一步处理，代码如下：\n1nohup python preprocess.py -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/data  -src_seq_length 500 -tgt_seq_length 500 &gt;preposs_datalog &amp;\n在这里需要提一个很重要的小细节，就是 src_seq_length 参数 和tgt_seq_length 参数的设定问题。默认这里是50。它的含义是如果句子长度小于50，不会被读入dataset！！！因为唱园杯的数据普遍比较长，所以你如果这里保持默认的话，会出现你只处理了一小部分原始数据的问题。\n具体这个数值你设定为多少，看你自己具体情况。因为唱园杯在数据说明中说到已经去掉了特殊字符等，所以我就全部保留了。\n模型进行预测\n直接使用 Transformer 进行训练。Opennmt使用特定参数复现了 Transformer 的效果，这里我们直接套用就可以。\n123456789nohup python train.py -data data/data -save_model data-model \\        -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \\        -encoder_type transformer -decoder_type transformer -position_encoding \\        -train_steps 200000  -max_generator_batches 2 -dropout 0.1 \\        -batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2 \\        -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 \\        -max_grad_norm 0 -param_init 0  -param_init_glorot \\        -label_smoothing 0.1 -valid_steps 10000 -save_checkpoint_steps 10000 \\        -world_size 4 -gpu_ranks 0 1 2 3  &amp;\n预测\n在预测之前，我们需要看一下测试数据，发现是双向预测，所以我们需要将上面的数据颠倒过来再来一次，训练另一个模型即可。\n按道理也可以使用全部数据（颠倒混合），这样训练一个模型就可以，不过我没试过，不知道效果如何，感兴趣的可以试一试。\n预测代码如下：\n1python  translate.py  -model demo-model_200000.pt -src data/src-test.txt -output pred.txt \n优化思路\n因为是编码之后的数据，所有常规的优化没啥用，这里简单提两个：\n\n\n使用全部数据（训练数据和测试数据）训练Word2vec/Glove/Bert 等，然后作为输入，从而加入先验信息\n\n\n如果不想自己训练，可以使用词频对应到编码之后的数据，得到一个大致的结果，从而可以使用我们正常的word2vec/glove/bert\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/README/"},{"title":"","date":"2024-06-21T03:20:42.073Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.073Z","content":"BERT模型的压缩大致可以分为：1. 参数剪枝；2. 知识蒸馏；3. 参数共享；4. 低秩分解。\n其中，对于剪枝，比较简单，但是容易误操作降低精读；\n对于知识蒸馏，之前我写个一系列的文章，重点可以看一下这里：\n对于参数共享和低秩分解，就和今天分享的ALBERT息息相关；\n它减少了BERT的参数，但是需要注意的一个细节点是，同等规格下，ALBERT速度确实变快，但是并不明显（和大量自媒体文章解读给大家的印象差距很大）；\n举个形象的例子就是，（这个例子并不严谨，只是帮助理解）参数共享让它训练的时候把多层压缩为一层去训练，但是在预测的时候，我们需要再展开多层去进行预测。\n主要掌握以下的几个知识点：\n\n词向量嵌入参数分解\n跨层参数分享\n取消NSP，使用SOP\n预训练的时候采用更满的数据/n-gram mask方式\n\n1.词向量嵌入分解\n词向量嵌入参数分解，简单说就是将词向量矩阵分解为了两个小矩阵，将隐藏层的大小和词汇矩阵的大小分离开。\n在Bert中，词汇表embedding大小是$V*H$；\nAlbert 的参数分解是这样的，将这个矩阵分解为两个小矩阵：$VE$和$EH$\n这样做有什么好处呢？\n如果说，我觉得我的模型表达能力不够，我想要通过增大隐层H的大小来提升我们模型能力的表达能力，那么在提升H的时候，不仅仅隐层参数增多，词汇表的embedding矩阵维度也在增多，参数量也在增大。\n矩阵分解之后，我们可以只是做到提升隐层大小，而不去改变表词汇表的大小。\n2.跨层参数分享\n跨层参数分享，这个操作可以防止参数随着网络层数的增大而增加。\n\n分为三种形式，只是共享attentions，只是共享FFN，全部共享。\n共享的意思就是我这部分结构只使用同样的参数，在训练的时候只需要训练这一部分的参数就可以了。\n看表格我们可以发现一个细节，就是只是共享FFN比只是共享attention的参数，模型效果要降低的多。\n小声嘀咕一下，这是不是说明FFN比attention在信息表达上要重要啊。或者说attention在学习信息表达的时候。attention层学习共性比较多。FFN学习到的差异性比较多。这只是我自己的猜测哈。\n3. SOP\n作者认为，NSP不必要。与MLM相比，NSP失效的主要原因是其缺乏任务难度。\nNSP样本如下:\n\n从训练语料库中取出两个连续的段落作为正样本\n从不同的文档中随机创建一对段落作为负样本\n\nNSP将主题预测和连贯性预测合并为一个单项任务；\n但是，与连贯性预测相比，主题预测更容易学习，并且与使用MLM损失学习的内容相比，重叠性更大。\n对于ALBERT，作者使用了句子顺序预测（SOP）损失，它避免了主题预测，而是着重于句间建模。\n其实就是预测句子顺序，正样本是顺着，负样本是颠倒过来。都是来自同一个文档。\n\n其他细节\n\n数据格式：Segments-Pair\n\n这个在RoBERTa中也有谈到，更长的序列长度可以提升性能。\n\nMasked-ngram-LM\n\n\n这就有点类似百度的ERINE和SpanBERT了\n\n推测速度\n\n\n从图中知道，同一规模ALBERT和BERT，比如同为Base：\nBERT base: 4.7x；ALBERT base：5.6x；速度确实变快，但是确实加速并不明显；\n同等效果的情况下，比如BERT base（Avg=82.3）和ALBERT large（Avg=82.4）：\nBERT base：4.7x；ALBERT large：1.7x；速度变慢了\n总结\n总结一下可以学习的思路：\n\n预训练的时候，数据填充的更满，到512这种，有利于提升模型效果，这点在RoBERTa有谈到\nmask n-gram有利于提升效果，这点类似百度的ERINE和SpanBERT了\n词向量矩阵分解能减少参数，但是也会降低性能\n跨层参数分享可以降低参数，也会降低性能，通过实验图知道，attention共享效果还好，FFN共享效果降低有点多\n取消NSP，使用SOP，正负样本来自同一个文档，但是顺序不同。\n推理速度来看，同等规格，ALBERT速度确实变快，但是并不明显，同等效果，速度变慢；https://kexue.fm/archives/7846)\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/ALBERT-更小更少但并不快/"},{"title":"","date":"2024-06-21T03:20:42.173Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.173Z","content":"Bert各种后续预训练模型-预训练模型的改进\n参考资料：\n站在BERT肩膀上的NLP新秀们（PART II） - kaiyuan的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/68362016\n√ XLMs from Facebook\n√ LASER from Facebook\n√ MASS from Microsoft\n√ UNILM from Microsoft\n\n邱锡鹏老师发表了关于NLP预训练模型的综述《Pre-trained Models for Natural Language Processing: A Survey》\n\n这里有一个对这个的解读，写的非常好，在这个文章中，这个作者也列出来了自己的另外另个文章，可以看一看\nNLP算法面试必备！史上最全！PTMs：NLP预训练模型的全面总结 - JayLou娄杰的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/115014536\n当然在这里文章里面，有一个链接，非常重要，就是对预训练模型单模型的精度，注意这里是精度，都是链接到了知乎文章\n写的都是非常好！！！！！非常好，链接地址在这里https://github.com/loujie0822/Pre-trained-Models\n这里链接一定要看\n这里还有一个关于邱老师综述的解读，也很好\n论文笔记 - NLP 预训练模型综述 - 徐阿衡的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/139015428\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/Bert各种后续预训练模型-预训练模型的改进/"},{"title":"","date":"2024-06-21T03:20:42.233Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.233Z","content":"Bert如何融入知识(二)-Bert融合知识图谱\nBert如何融入知识(一)中主要是百度和清华ERINE，其实还有很多的Bert结合知识图谱的文章内容，这里我先列出来一些参考：\n当BERT遇上知识图谱 - kaiyuan的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/91052495\n√ KG-BERT from NWU\n√ K-BERT from PKU\n√ KnowBERT from AI2\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/Bert如何融入知识二-Bert融合知识图谱/"},{"title":"","date":"2024-06-21T03:20:42.253Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.253Z","content":"Bert的可视化-Bert每一层都学到了什么\n首先我罗列一下重要的知识点：\n主要是参考\nBERT句子表示的可视化 - 李如的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/87942922\n理解BERT每一层都学到了什么 - xijiz的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/74515580\n这个是对ACL论文What does BERT learn about the structure of language? 的解读，很好\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/Bert的可视化-Bert每一层都学到了什么/"},{"title":"","date":"2024-06-21T03:20:42.273Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.273Z","content":"Bert\nNLPCC：预训练在小米的推理优化落地\nhttps://mp.weixin.qq.com/s/itOyETgKBoRHOrIfKuphrw\nElasticsearch遇上BERT：使用Elasticsearch和BERT构建搜索引擎\nhttps://mp.weixin.qq.com/s/NV0SR7YveXwkhG3lrhMMQQ\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/Bert资源总结/"},{"title":"","date":"2024-06-21T03:20:42.193Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.193Z","content":"Bert如何融入知识(一)-百度和清华ERINE\n首先想一下Bert是如何训练的？首先我获取无监督语料，随机mask掉一部分数据，去预测这部分信息。\n这个过程其实和W2C很类似，上下文相似的情况下，mask掉的单词的词向量很可能非常相近。\n比如说”今天米饭真好吃“和”今天苹果真好吃“，很有可能”米饭“和”苹果“学出来的向量就很相似。\n我在李如有一篇文章中《BERT句子表示的可视化》有这样一句话，contextual dependent词向量的一个缺点，就是上下文相似的情况下词向量也很接近。\n从这里，我觉得很容易就可以发现一个问题，就是Bert确实抽取能力非常的强，但是他也是在死记硬背的学这些知识。\n想一下，为什么我们需要在Bert中融入一些知识呢？\n我们考虑这么一个例子，比如我要对一个文本进行分类：”库克今日来北京进行商务洽谈活动“\n单从bert做一个文本分类，可能模型很难从语义角度进行决断。\n但是，我现在的知识图谱中有这样一个三元组：库克-CEO-苹果公司\n我把这个三元组的信息融入到我的模型之中，也就是我在文本分类的时候不仅仅使用了你的原始文本，还是使用了知识图谱中的三元组信息，相当于一种信息的增强，这个时候我的模型就可以文本分类为”IT公司“这个类别。\n一般来说，涉及到Bert中融入知识，大家都会涉及到两个文章：百度的 ERNIE from Baidu 和清华的ERNIE from THU\n我先从整体的思路说一下两者：\nERNIE from Baidu 出发点是这样的，Bert 的mask只是 mask掉单字，放在中文中，一般来说词汇会带有比字更多的信息。\n比如说\n哈[mask]滨真冷啊 是Bert基础操作\n[mask][mask][mask]真冷啊 是ERNIE from Baidu的操作\n也就是，我预测的不仅仅是一个单字，而是一个实体词组。\n对于这个操作，我是这么想的，首先从难度来讲，去预测一个词组会比预测一个单字难，而且这个词组是一个实体，所以在学习的时候回学习到实体信息\nERNIE from THU\n对于这个模型，我是这么想的，百度利用的是预测句子中的实体信息。而清华这边的操作是加入了外部的知识信息。\n就像最开始我们的例子，”库克-CEO-苹果公司“，这是外部知识，这个不是我文本中的信息，相当于显示的加入了外部信息。\n当然清华这边应该也只是使用到了实体信息（做了实体对齐）\n我们需要考虑两个问题：\n\n\n如何抽取并且更好的表达知识图谱的信息：知识嵌入算法（如TransE）\n\n\n实体向量和Bert的向量在不同的空间，如何缓解两者之间的Gap：\n\n\n对于这个问题，从模型架构上来解决，使用两种：\ntextual encoder (T-Encoder)：类别Bert\nknowledgeable encoder (K-Encoder)：用于将外部的知识图谱的信息融入到模型中；\n对于Bert融入知识信息，主要是参考以下文章：\n站在BERT肩膀上的NLP新秀们（PART I） - kaiyuan的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/68295881\n写的还不错，介绍了百度和清华的ERINE\nBert 改进： 如何融入知识 - 老宋的茶书会的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/69941989\n写的还不错，介绍了百度和清华的ERINE\nBERT与知识图谱的结合——ERNIE模型浅析 - 段易通的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/75466388\n写的还不错，介绍了百度和清华的ERINE\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/Bert如何融入知识一-百度和清华ERINE/"},{"title":"","date":"2024-06-21T03:20:42.593Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.593Z","content":"Pytorch代码分析–如何让Bert在finetune小数据集时更“稳”一点\n前几天在知乎刷到邱震宇同学的一个文章，如何让 Bert 在 finetune 小数据集时更“稳”一点，主要是分析了一篇论文，感觉很有意思。\n小数据集的时候，很容易就训练震荡。作者主要是分析了两个点进行优化。一个是adam偏移修正的问题，一个是权重初始化的问题。\n作者通过实验给出的结论是：1.使用误差修正，训练效率提高（收敛变快），开发集效果变好。2.使用权重初始化（后6层），训练效率提高（收敛变快），开发集效果变化不大。\n我做了简单的测试，有一个小点和作者结论不太一样，adam的修正并没有加速模型收敛，具体的看正文分析吧。\n我基于 huggingface 的 Bert，做了个两个简单的实验，详细情况如下。\n1. adam偏移修正\n对于adam偏移修正的实验，我这边得到的结果是：使用误差修正，收敛并未变快（反而变慢），训练更加稳定，开发集效果变好。\n对于收敛速度这块和作者结论不太一样，多跑了几个种子，都是收敛速度并未变快，希望有关大佬解惑。\n1.1 代码情况讲解\n首先说一下，在翻看源代码的时候，发现对于抱抱脸的 Bert 的实现，默认偏移修正是开启的，具体代码的位置在这里：\nhttps://github.com/huggingface/transformers/blob/84be482f6698fac822a5113735f2242c6d3abc76/src/transformers/optimization.py#L107\n抱抱脸使用的是 AdamW:  Adam algorithm with weight decay fix\n代码如下\n12345class AdamW(Optimizer):    def __init__(self,...,correct_bias=True):    ...    ...\n所以在测试偏移修正的对比的实验的时候，一个是保持默认不变得出一个结果；一个是修改这个参数，你可以在调用函数的时候修改参数传入值，也可以修改源代码，如果是修改的源代码，记得做完实验把代码改回来，别对之后工作造成影响。\n1.2 任务基本情况\n任务基本情况是这样的：\n\n任务类别：文本分类/15个类别\n数据来源: 今日头条新闻数据\n数据量级：训练集1K/开发集50k\n训练参数：\n\nBert : chinese_l-12_h-768_a-12，使用原始版本未做修改\nbatchsize：16\nmax_seq_length ：128\nlearning_rate：2e-5\nEpoches: 10\n\n\n\n因为数据量较小，并且想要观察变化，没使用 earlly stopping，完整的跑完了10个epoch，一共是 600 steps 左右，文中所示图以每隔 10 steps 打点显示，所以最大显示为 60/30。\n1.3 结果展示\n结果展示分为两个，一个是 Loss 变化图，一个是在开发集 Acc 展示图。\nLoss 变化如下图：\n\n可以看到，没有使用偏移纠正的 loss 收敛的更加的迅速一点，反而使用了修正的模型收敛的慢一点。但是同时可以观测到，修正之后，模型的收敛更加的稳定，相比较而言，并没有特别大的震荡。\nAcc变化如下图（后期没怎么变化，所以截取了前300steps）：\n\n对于在开发集来说，经过修正的收敛速度慢，但是比较稳定，没有大幅度的震荡，准确度相比，有可观收益（图中看不不太明显，无修正最好效果:0.80，加入修正最好效果: 0.82）\n2. 权重初始化\n权重初始化比较简单，平常任务也试过，因为是文本分类任务，所以在这里只是简单的测试了一下重新初始化 Pooler 层。\nLoss结果如下图：\n\n从图中可以看出，重新初始化，收敛速度变快了，但是不明显。\nAcc没什么变化，就不上图了，没什么变化（主要是被我无意删了，懒得再重跑一次了，不影响大局）\n3. 简单总结\n简单总结一下：\n与没有修正的adam之后，修正之后，模型收敛速度变慢，收敛过程变得稳定，效果提升比较明显。\n与没有重新初始化的模型相比，初始化最后一层pooler之后，模型收敛速度有所变快，但是不明显，效果也没有明显变化。\n上面这两个实验只是基于邱震宇同学的文章做的，在这里感谢作者。关于收敛速度这里，结果有一点不一样，希望有大佬可以解惑，我也会抽空去看看原论文，仔细研读一下，看论文还有没有值得挖的东西，有任何进展，我再和大家说。\n打完收工，看完我这么辛苦画图（真是累死了）的份上，点个赞再撤吧，鞠躬感谢。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/Pytorch代码分析-如何让Bert在finetune小数据集时更“稳”一点/"},{"title":"","date":"2024-06-21T03:20:42.613Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.613Z","content":"RoBERTa：更大更多更强\n今天分享一个Bert的改进工作RoBERTa。RoBERTa是训练充分的Bert。\n主要掌握以下几点，与Bert相比较，RoBERTa预训练的时候：\n\n动态掩码：comparable or slightly better\n去掉NSP任务并且更改数据输入格式为全部填充可以跨越多个文档\n更多数据，更大bsz，更多的步数，更长训练时间\n\n1. 动态掩码\n首先明确Bert使用的是静态掩码。但是这样会存在一个现象，比如我训练40个epoches，那么每次epoches都是使用同一批数据。\n这其实不是什么大问题，我们在深度学习训练模型的时候，每个epoches基本都没咋变过。\n不过对于Bert，其实本质是一个自监督模型。每次的训练输入如果是不同的，对于模型肯定是更好的。\n比如我们句子为：今天去哪里吃饭啊？\nmask之后为：今天去哪里[mask]饭啊？\n每次训练使用同一个mask样本，那么模型见得就少。\n如果换一个mask：[mask]天去哪里吃饭啊？\n模型对于同一个句子，在预测不同的单词，那么模型对句子的表达能力直觉上肯定是会上升的。\n所以为了缓解这种静态掩码的问题，Bert的操作是这样的：\n复制原始样本10份，每份都做不同的静态mask，然后进行训练。\n我们想一下这个过程：比如我仍然是训练40个epoches，复制了十份样本，相当于每4个epoches使用的是同一个mask的样本。\n这个操作确实缓解了静态掩码的问题，但是毕竟还有重复mask的情况出现。\n这个时候其实有个朴素的思想，为啥不直接复制40份，然后分在40个epoches中进行训练，这个到时候写Bert的时候再说。\nRoBERTa 是咋做的呢？\n动态掩码，也就是不是在最开始的时候的数据处理的过程中就生成mask样本，而是在送入到模型之前才进行mask，这样同一个句子，在40epoches中，每次mask都不同。\n效果直接看图\n\n2. NSP和模型数据输入格式\n这一点其实很有意思。\n我们先说RoBERTa 的四种输入形式和实验效果，然后再详细分析：\n\nSEGMENT-PAIR+NSP：就是Bert的输入形式\nSENTENCE-PAIR+NSP：输入的是一对句子，即前后是单个句子\nFULL-SENTENCES：输入为全量的句子，填满512的长度，采集样本的时候可以跨越文章的界限，去除了NSP loss\nDOC-SENTENCES：输入和FULL-SENTENCE类似，但是一个样本不能跨越两个document\n\n然后看一下实验效果：\n\n对上面这个图一个最简单的总结就是NSP没啥用。然后我们来详细说一下这个事情。\n首先Bert的消融实验证明，NSP是应该有的，如果没有NSP，在部分任务上效果有损失。\n但是上图RoBERTa实验证明，NSP没啥效果，可以没有。\n一个直观的解释，或者说猜测是因为，可能是Bert在消融实验去除NSP的时候，仍然保持的是原始的输入，即有NSP任务的时候的输入形式。\n这就相当于，构造了好了符合NSP任务的数据，但是你删去了针对这个任务的损失函数，所以模型并没有学的很好，在部分任务精读下降。\n但是RoBERTa这里不是的，它删除NSP任务的时候，同时改变了输入格式，并不是使用上下两句的输入格式，而是类似文档中的句子全部填满这512个字符的格式进行输入。\n简单说就是，去掉了NSP任务的同时，去掉了构造数据中NSP数据格式。\n比较SEGMENT-PAIR和DOC-SENTENCES两个模式的时候，证明没有NSP效果更好。其实看起来好像并没有控制变量，因为变了两个地方，一个是是否有NSP，一个是输入格式。\n这种情况下，就只能去看在下游任务中的效果了。\n3. 数据+bsz+steps\n\n数据：Bert：16G；RoBERTa：160G；十倍\nbsz：Bert：256；RoBERTa：8K\nsteps：Bert：1M；RoBERTa：300K/500K\n\n4. 总结：\n简单总结一下学到的东西：\n\n动态掩码：comparable or slightly better\n去掉NSP任务并且更改数据输入格式为全部填充可以跨越多个文档\n更多数据，更大bsz，更多的步数，更长训练时间\n动态掩码那里，说到一个复制10份的细节，那里是针对的Bert，RoBERTa是每次输入之前才mask，注意区分，不要搞混\n\n参考资料：RoBERTa: 捍卫BERT的尊严 - yangDDD的文章 - 知乎 https://zhuanlan.zhihu.com/p/149249619\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/RoBERTa/"},{"title":"","date":"2024-06-21T03:20:42.303Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.303Z","content":"论文标题《FastBERT: a Self-distilling BERT with Adaptive Inference Time》。\n关于这个论文已经有不错的解读了，所以我写的侧重点可能和别人的不太一样，具体的往下看吧，欢迎讨论。\n这个论文从两个方面去掌握：\n\n样本自适应推断：使用不确定性指标（a normalized entropy）去过滤样本；\n模型自蒸馏：分支网络分类器学习主干网络分类器，并使用KL散度进行度量；\n\n然后我们聊一聊 FastBert 究竟在做什么事情？\nBert本身有12层，模型在进行推理的时候， 每一个样本都会完整的走过12层。而 FastBert 做到了让简单的样本的不必走过12层，只需要走过3层或者4层（这个数字并不确定）就可以。\n仔细想一下这个过程，简单来讲，就是杀鸡不要用牛刀，一件很简单的样本，不值得我们调用12层去搞定它，3层transformer学的参数就可以消灭掉这个样本。\n1. 整体过程\n整个过程分为三步：\n\n微调主干网络：使用自己下游数据微调主干网络。\n自蒸馏：可以使用无监督数据，用KL散度度量每一层和最后一层分布距离，并使用和作为损失。交叉熵也没差，H(p)不变，两者等价。\n自适应推理：自己设定速度(阈值)，大于这个的往后走，小于这个的输出结果。\n\n2. 需不需要自蒸馏\n但是，看到这里，我们肯定会有一个疑问。\n基于“杀鸡不用牛刀”的想法，我可以简单改造 Bert 模型，在每一层都加一个分类器，使用我自己的下游数据训练模型就可以了，损失函数直接使用每一层的交叉熵损失之和就可以了，为什么需要用到自蒸馏呢？\n简单来说，就是我完全可以不需要使用自蒸馏就可以达到同样的目的。\n基于这一点，作者有做一个实验对比，得出的结论就是没有自蒸馏，会导致精读的降低。\n具体看这个图：\n\n其实从这个图可以看到，如果没有自蒸馏，确实会有精度的下降。\n论文中在自蒸馏的时候，使用的是无监督的数据。\n我们一般可以会有大量的无监督数据，所以这个方法真的很适合少样本的情况的冷启动问题。\n不过，如果有监督数据，使用 labeled data应该会取得更好的效果，也就是不用去学习soft，而是去学习Hard。\n3. LUHA假设证明\n这个论文其实我自己更感兴趣的是文中针对假设：“不确定性越低，分类的准确度越高”的证明。\n作者分析了一个 FastBert 模型的三层：Student-Classifier0, StudentClassifier5, Teacher-Classifier 的分类结果，并在不同的不确定度区间评判分类准确度。\n\n我自己的理解哈，在这里是没有进行speed的筛选的，而是计算所有的样本，不然对于第一层，大于speed的地方准确度应该是0（因为直接就往后走了，并没有使用它的分类结果），不应该出现在图中。\n从这个图中，我们可以得到一个结论，就是不论是是针对分支网络的哪一层，还是针对主干网络，不确定性越高，分类的准确度越低。\n换过来讲，不确定性越低（图中横坐标越靠左），分类的准确度越高。\n当然作者是使用画图的方式来证明的，我自己当时理解的时候是这么想的：\n作者对不确定性的定义就是熵（在类别维度上做了normalized）。\n熵越低，说明分布的越集中，也就是说在做类别判定的时候，出现在某个类别的地方的概率越大，这样当然可以说明分类的准确度越高。\n我这个思路可能并不严谨，不过我觉得还是挺好理解的。\n4. 每层样本分布问题\n从这个图，还有一个问题需要注意：\n就是学生网络的分类层在每个不确定区间内的准确度都是高于主干网络分类层的（看最下面一个和最上面一个对比）。\n这一点真是非常的奇怪，如果按照这个理解，那么完全可以使用第一层替代主干网络。\n当然作者这里也给出了解释，就是每层的样本分布其实是不一样的，越靠近后面的层，样本的不确定度越低，越靠近左边，所以样本走完12层之后使用主干网络整体准确度肯定是比直接使用第一层的要好。\n具体可以看下面这个图（a）:\n\n对于上面这个图，我们还可以仔细去看一看里面的内容。\n需要明确一点，同一个样本，每经过一层计算一次它的不确定度（熵），都是在变化的，而且会逐渐靠近不确定度低的地方。\n也就是这个样本被分出类别的可能性越来越大。\n这一点其实很容易理解，就是Bert抽取能力随着层数越来越强，那么文本被正确分类的可能性当然越来越大，不确定度当然越来越大。\n你的Speed越高，筛选出来的样本越多，留给后面的就越是不确定高的样本，那么分布越高近图中的右侧。\n其他细节李如（她原来是个女生…）有个文章讲的挺好的，在这里：\nFastBERT：又快又稳的推理提速方法 - 李rumor的文章 - 知乎 https://zhuanlan.zhihu.com/p/127869267\n代码地址在这里：\nhttps://github.com/autoliuweijie/FastBERT\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/FastBert/"},{"title":"","date":"2024-06-21T03:20:42.803Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.803Z","content":"UniLM：给Bert插上文本生成的翅膀\n今天分享一个论文UniLM，核心点是掌握三种LM任务形式：单向LM，双向LM，序列到序列LM；\n1. 生成任务\nNLP任务大致可以分为NLU和NLG两种；Bert在NLU任务上效果很好，但是天生不适合处理生成任务。\n原因在于Bert的预训练过程是使用的MLM，和生成任务的目标并不一致。\n生成任务目标是每次蹦出来一个词，只能看到当前位置之前的词汇。\n而Bert采用的是双向的语言模型，除了mask的单词，两个方向的词汇都可以被看到。\n所以对Bert的一个改进思路就是让它在具有NLU能力的时候，同时兼备NLG能力。\n2. 三种LM任务\nUniLM做的就是这样一个事情。\n具体的实现方式是设计了一系列的完形填空任务，这些完形填空任务的不同之处在于对上下文的定义。\n\n从左到右的LM：使用mask单词的左侧单词来预测被遮掩的单词\n从右到左的LM：和上面第一个相比就是方向的变化，使用mask单词的右侧单词来预测遮掩的单词\n双向LM：就是当前mask的左右词汇都可以看到\nsequence-to-sequence LM：这个就是UniLM能够具有生成能力的关键。我们的输入是source句子和target句子，mask单词在target上，那么当前mask的上下文就是source句子的所有单词和target句子中mask单词左侧的词汇可以被看到\n\n我们把从左到右LM和从右到左LM我们归为一种任务叫单向LM；\n有个点需要注意，三个任务是一起优化的，具体来讲是这样做的：\n在训练的时候，1/3的时候使用双向LM，1/3的时候使用序列到序列 LM，1/6的时候使用从左到右的LM，1/6的时间使用从右到做的LM。\n我们是使用不同的Mask矩阵来对应不同任务输入数据形式。\n文中使用的是这样一张图来展示：\n\n3. 其他细枝末节\n\nGelu 激励函数\n24层TRM，最大长度512，1024Hidden Size，16Heads，340M参数量\n初始化使用Bert Large\n15%被mask，其中80%真正替换mask，10%随机替换，10%不动。替换的时候，80% 的时候替换单个token，20%的时候替换bigram 或者 trigram\n\n第四个步骤类似中文实体词的mask，也算是一点改进。\n有个细节点需要注意的是，作者强调，不同的segment embedding用来区分不同LM任务。\nBert的时候，区分上下句子，我们使用0和1，在这里，我们使用这个segment embedding用来区分任务：\n比如说，双向对应0和1；单向left-right对应2；单向right-left对应3；序列对应4和5；\n4. 总结\n掌握以下几个细节点就可以：\n\n联合训练三种任务：单向LM，双向LM，序列LM\n使用不同的attention矩阵控制三种任务形式的参与\nsegment embedding可以区分不同的任务形式\nmask的时候15% 的有被替换的概率，其中80% 被真正替换。在这80%真正替换的里面有80%单个token被替换，20%的二元或者三元tokens被替换\n\n5. 加我微信，点赞之交\n\n参考链接：\nUniLM论文阅读笔记 - 刘聪NLP的文章 - 知乎 https://zhuanlan.zhihu.com/p/113380840\nBERT生成式之UNILM解读 - rumor的文章 - 知乎 https://zhuanlan.zhihu.com/p/68327602\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/UniLM/"},{"title":"","date":"2024-06-21T03:20:42.683Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.683Z","content":"今天分享一个论文ACL2020-tBERT，论文主要融合主题模型和BERT去做语义相似度判定，在特定领域使用这个模型，效果更明显。\n掌握以下几点：\n\n【CLS】向量拼接两个句子各自的主题模型，效果有提升\n尤其是在特定领域的数据集合会有更好的表现。\n\n1. 架构图\n先看架构图：\n\n模型架构比较简单，BERT这边使用的【CLS】输出向量：$C=BERT(S_{1},S_{2})$\n主题模型使用两种，LDA和GSDMM，主要是因为LDA在长文本效果更好；GSDMM在短文本效果更好。\n获取主题模型如下所示：\n$$D_{1} = TopicModel([T_{1},…,T_{N}]) \\in R^{t}$$\n$$D_{2} = TopicModel([T^{,}{1},…,T^{,}{M}]) \\in R^{t}$$\n$t$ 代表的是主题数量，N是$S_{1}$的字数量，M是$S_{2}$的字数量\n进而我们可以得到单词的主题分布:\n$w_{i} = TopicModel(T_{i})$\n$$W_{1} = \\frac{\\sum^{N}{i=1}w{i}}{N} \\in R^{t}$$\n$$W_{2} = \\frac{\\sum^{M}{i=1}w^{,}{i}}{M} \\in R^{t}$$\n所以在最后和【CLS】连接的时候，可以使用文档主题$D_{1}和D_{2}$，也可以使用单词主题$W_{1}和W_{2}$。\n2.实验效果\n\n看实验效果，LDA效果会比GSDMM更好一点。\n其实有一个比较有意思的点是，BERT的建模能力已经足够强了，为啥加上主题模型还会有提升。\n换句话说，主题模型在基于BERT的方向上，能够在哪些方面提升。\n作者是这么做的实验，他选了和主题模型相关的三个属性：实体，特定领域词和不规范拼写。根据三个属性抽取样本，总共500个， 然后让BERT和tBERT做预测。\n\n看实验效果是这样的，发现在特定领域tBERT效果更明显一点。\n作者认为在预训练的时候，可能是BERT碰到特定领域词汇的机会比较少，没有很好的学习到这些信息，所以主题模型很好的补充了这部分信息。\n不过，感觉这个实验并不充分，一个属性这块挑选感觉有点不太充分，还有一个是样本数量感觉太少了，500个…\n总结\n说一下掌握的知识点：\n\n【CLS】向量拼接两个句子各自的主题模型，效果有提升\n尤其是在特定领域的数据集合会有更好的表现。\n\n说一下我自己的思考，关于特定领域这块。一般来说，微调是可以解决这个问题的。\n不过看作者的实验，即使是微调之后的BERT，在特定领域这块，效果也没有tBERT好，说明主题模型在这块还是很有用的。\n进一步思考，可不可以这么推论，如果说我们的任务输入越是特定领域，那么假如tBERT越有明显的提升呢？\n这个感兴趣的大家可以去试一试，比如医疗领域，比如金融领域之类的。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/tBERT-BERT融合主题模型/"},{"title":"","date":"2024-06-21T03:20:42.913Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.913Z","content":"XLNET里面的细节点有很多，重点掌握以下两点：\n\nAR和AE两种无监督预训练的优化目标\n双流自注意力机制：为什么需要把位置信息和内容信息拆分\n\n1. 无监督目标函数\n在NLP中，无监督表示学习已经获得长足发展。一般的流程是先将模型在大量无标签数据上进行预训练，然后在具体的下游任务上进行微调。\n一般来说，无监督预训练有两种目标函数很受重视：AR和AE。\n\nAR，也就是autoregressive，我们称之为自回归模型；用$x_{0},x_{1},x_{2}…$去预测$x_{t}$，可以分为正向和反向，只能考虑单侧的信息，典型的就是GPT\nAE，也就是autoencoding，我们称之为自编码模型；从损坏的输入数据中预测重建原始数据。可以使用上下文的信息，Bert就是使用的AE；\n\nAE模型能够看到句子中的更多的信息，这也是BERT在下游任务中表现很好的原因。\n先来看一下优化目标，对于AR语言模型来说，它是基于概率的一个链式法则，优化如下：\n\n而AE模型，拿BERT举例，它的优化目标是是从损坏的输入数据中重建原始未被破坏的输入，优化目标如下：\n\n举个简单的例子，原始输入为【我爱吃饭】，那么AR模型在做的时候，它的优化是P(我爱吃饭) = P(我)P(爱|我)P(吃|我爱)P(饭|我爱吃)；\n阈值对应的，假设我们mask之后为【我爱mask mask】，那么BERT的优化目标是：P(我爱吃饭|我爱maskmask)=P(吃|我爱)P(饭|我爱)；\n根绝这两个例子，我们首先知道对于AR模型，它在优化的时候，只能看到单向信息。\n对于AE模型，它可以看到双向信息，但是有一个问题就是，它认为mask之间相互独立，也就是上面例子中【吃】和【饭】是相互独立的。\n很显然，这一点是错误的，【吃】和【饭】之间肯定是有联系的，但是BERT在预训练的时候并没有考虑这一点。\nBERT还有一个缺点就是，在预训练的时候，是存在mask字符的，但是在微调的时候，也就是在我们的在具体任务上训练的时候，我们的输入是不存在mask字符的，造成了预训练和微调之间的gap；\nAR（单向缺点）和BERT（mask缺点）都存在缺点，XLNET想办法解决了这两个问题。\n2. Permutation Language Modeling\n为了解决AR模型不能关联上下文信息，提出这个策略。\n如果我们的序列x的长度为T，那么对于这个序列，我们有T!种排列方式。\n比如说，原始排列为1，2，3，4；那么对它进行重排列，就有24中排列方式。\n我们挑两种来看：1，2，3，4和1，4，3，2；\n在这两种排列中，假设我们都处于预测第三个位置3的时刻，那么对于第一种，它能够看到的内容信息来自1和2，对于第二种排列方式，它能够看到的内容信息来自1和4。\n这样一来，对于3 来说，它在训练之后，既看到了前面的信息1和2，又看到了后面的信息4。\n通过这种方式，AR模型可以联系到上下文的信息。\n但是如何做到输入序列进行重排序呢，使用TSSA；这样输入序列顺序不会发生变化，顺序的变化只是发生在内部；\n3. Two-Stream Self-Attention\n假设我们先把某个点的信息分为内容信息和位置信息，为啥这么分，看完例子就知道了。\n先来一个简单的例子，句子序列：1，2，3，4；\n如果要是预测3，那么需要做什么：\n首先，我需要1和2的全部信息（包括它们内容信息和位置信息）；其次需要看到3当前这个点的位置信息，确保知道预测的是哪一个位置，但是我不能看到3这点的内容信息，因为我要预测这个单词，不能做到标签信息的泄露。\n如果要是预测4，那么需要做什么：\n首先，我需要1，2，3的全部信息（包括它们的内容信息和位置信息），其次，我需要看到4当前这个点的位置信息；\n好了，两个例子联合起来看，对于3这个点，有的时候我需要向4提供全部信息（包括内容信息和位置信息），有的时候我需要向自己提供位置信息（不能包含内容信息，防止造成标签的泄露）；\n这就是为什么需要将信息分为内容信息和位置信息，如果不分开，那么对外提供信息的时候就不能有效的隔离。\n仔细琢磨这个例子，对照着这个例子，可以看一下下面这个图：\n\n这个图，细节一点要注意，在计算位置信息的时候，QKV分别代表着什么？\n这点需要大家仔细去看。\n然后看c，在最后预测的时候我们使用的query stream，并没有使用content stream；这点需要注意；\n4.其他细节\n\n使用部分预测：句子预测起始阶段，上文信息较少，担心误差较大，所以只对句子后1/K的tokens被预测\n使用Transformer-XL，用于处理长文本\n\n5. 总结\n说一下值得注意的点，主要就是双流自注意力机制这里很有意思，在初看图的时候很容易看混。\n这么理解会更加的方便，对于同一个token，在预测自身的时候，它需要向外提供自己的位置信息，在预测别的单词的时候，它需要对外提供全部信息。\n所以一个好办法就是把内容信息和位置信息分隔开对外提供。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/XLNET/"},{"title":"","date":"2024-06-21T03:20:43.133Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.133Z","content":"前几天有朋友问了一下【小布助手短文本语义匹配竞赛】的问题，主要是两个；\n\n如何在脱敏数据中使用BERT；\n基于此语料如何使用NSP任务；\n\n比赛我没咋做，因为我感觉即使认真做也打不过前排大佬[囧]，太菜了；不过我可以分享一下我自己的经验；\n对于脱敏语料使用BERT，一般可以分为两种：\n第一种就是直接从零开始基于语料训练一个新的BERT出来使用；\n第二种就是按照词频，把脱敏数字对照到中文或者其他语言【假如我们使用中文】，使用中文BERT做初始化，然后基于新的中文语料训练BERT；\n大家可以先看一下当时我的回复：\n\n\n然后我发现很多朋友对于预训练模型其实理解的还是不深刻，很疑惑为什么在脱敏数据中也可以训练BERT等预训练模型；\n其实这一点很容易理解，就像我截图中说到的：\n最开始BERT是用英文语料训练出来的，然后有朋友基于中文语料开源了中文的BERT；\n那么我的脱敏数字就是类似于中文的一种另外的语言，你可以看成是【X】语言，我们当然可以基于【X】语言的语料去训练一个新的BERT或者其他的预训练模型了；\n有的朋友谈到了NSP任务如何去使用的问题；\n很明显，在当前这个任务中是一个文本匹配的形式；\n语料不是我们自己有主动的去获取的能力，所以构造一个NSP任务的格式比较困难；\n但是NSP任务仅仅是一种任务形式，我们完全可以基于训练语料构造一个是否匹配的任务，可以称之为类NSP任务；\n基于此，测试数据是使用不了的，因为测试数据没有label；\n不过，我自己认为可以测试数据使用MLM任务，训练数据使用MLM+类NSP任务；\n更加具体大家可以看我当时的回复：\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/如何在脱敏数据中使用BERT等预训练模型/"},{"title":"","date":"2024-06-21T03:20:43.173Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.173Z","content":"Bert我们一般使用方法是，加载预训练模型，在我们自己的任务上进行微调。但是我们有些时候会遇到这种情况，比如说，之前文章提到的，\n我不想要你预训练模型中最后三层参数，而是使用我自己的方法重新初始化。\n首先解释一下为什么需要这么做？有的论文发现，bert越靠后面（越靠近顶层，也就是输出层），学到的知识越是笔记抽象高级的知识，越靠近预训练模型的任务情况，和我们自己的任务就不太相符，所以想要重新初始化，基于我们自己的任务从零学习。\n好了，代码是怎么实现？\n一般pytorch的初始化方法我就不说了，这个比较简单，之后可能有时间写一下，这里专门介绍一下bert里面如何去做。\n首先，我们看一下源代码，加载模型的时候是怎么加载的：\n1model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool(&#x27;.ckpt&#x27; in args.model_name_or_path), config=config)\n链接在这里：https://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/run_classifier.py#L462\n再执行到这里之后，会进入并执行这个函数：\n1def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n代码链接在这里看：\nhttps://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/transformers/modeling_utils.py#L224\n这个函数就是我们要修改的函数，核心操作是这个操作：\n1module._load_from_state_dict(state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n代码位置在这里：\nhttps://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/transformers/modeling_utils.py#L404\n主要是两个参数最重要：\nmissing_keys：就是我们自己定义的模型有哪些没在预训练模型中，比如我们的模型现在是 BertForSequenceClassification ，那么这里结果就是 [‘classifier.weight’, ‘classifier.bias’]\nunexpected_keys:预训练模型的参数有很多，这里的结果是定义的我们对哪些参数忽视，并不采用，这里的正常结果是这样的：[‘cls.predictions.bias’, ‘cls.predictions.transform.dense.weight’, ‘cls.predictions.transform.dense.bias’, ‘cls.predictions.transform.LayerNorm.weight’, ‘cls.predictions.transform.LayerNorm.bias’, ‘cls.predictions.decoder.weight’, ‘cls.seq_relationship.weight’, ‘cls.seq_relationship.bias’]\n重点来了，如果我们想要对第一层的query的进行重新初始化，怎么做？分两个步骤，第一步，定义你想要重新初始化哪些参数，第二步代入进去。看代码：\n1unexpected_keys =[&#x27;bert.encoder.layer.0.attention.self.query.weight&#x27;,&#x27;bert.encoder.layer.0.attention.self.query.bias&#x27;]\n就这么简单，这里定义了就可以\n代码位置在这里\nhttps://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/transformers/modeling_utils.py#L364\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/解决老大难问题-如何一行代码带你随心所欲重新初始化bert的某些参数附Pytorch代码/"},{"title":"","date":"2024-06-21T03:20:44.053Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.053Z","content":"关键词提取\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/关键词提取/README/"},{"title":"","date":"2024-06-21T03:20:43.103Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.103Z","content":"今天分享的论文解决了我很久以来的一个疑惑，名字叫做：On the Sentence Embeddings from Pre-trained Language Models：\n这个论文主要探讨两个问题：\n\n为什么Bert做无监督文本匹配效果不好？是因为bert的输出携带的语义信息太少还是我们没有正确的利用挖掘这些语义信息\n如果说是因为没有正确挖掘利用这些语义信息，那么我们怎么使用无监督的方式，让这种语义信息很容易的被利用\n\n本文主要聊第一点，先说一下结论：\nBert向量空间并不平滑，有些区域不好被定义，也就是说是存在问题的，导致简单的相似性度量不能很好起到度量作用。\n简单说就是Bert输出向量语义信息是足够的，但是consine这种简单东西不能很好的度量出来而已。\n那么就有大致两个解决办法：转化Bert的输出空间或者使用其他的有用的相似性度量函数。\n论文使用的是第一种，对Bert的输出空间做了一个转化，这个不细说，大家去看原论文。\n1. 之前遇到的小疑惑\n我简单说一下我之前遇到的问题：\nBert出来之后，在下游任务上，横扫各大榜单。\n一个最基础的应用是使用【cls】的输出向量微调做文本分类任务；\n这样做的前提我们认为因为由于自注意力的存在，【cls】可以看到全局的信息。\n但是需要注意的是，上面这句话绝对不代表没有在具体任务微调过的【CLS】向量可以代表整个句子的语义信息。\n为什么这么说呢？\n我们使用【CLS】向量做无监督的文本匹配，会发现一个非常奇怪的现象。\n一般直觉上认为，由于Bert强大的编码能力，应该可以很好的表示语义信息，所以做无监督语义匹配，准确度应该还不错。\n但是结果会发现相似度都比较大，没有什么区分度。\n更进一步，我们可以对Bert输出，不仅仅是【CLS】向量，可以使用所有Tokens的输出向量做各种操作：max/mean/min Pooling等等吧。\n甚至我们可以使用倒数第1/2/3/4或者把最后几层输出联合起来使用。\n总之各种骚操作之后，会发现，无监督文本匹配效果依然不好。\n究其原因就是👇谈到的词向量的各向异性。\n2. 词向量的各向异性\n我们来详细说说Bert的向量空间存在的问题。\n之前的论文发现Bert的词向量存在各向异性（anisotropic），两个现象\n\n词频影响了词向量空间分布\n\n根据词频来区分不同的单词，然后计算不同词频的单词距离原点的距离，发现词频越低，距离原点越远。\n我们希望句子的embedding表达可以用来度量句子之间的相似性；但是如果单词的embedding的分布是受词频影响的，那么这种相似性是不能让人信服的。\n我举个简单的例子，两句话，A和B：\n\nA：你喜欢吃苹果吗？\nB：你爱吃苹果吗？\n\n不同的单词是【喜欢】和【爱】。我们假设哈，假设【喜欢】出现的频次足够的高，那么它离原点就足够的近。【爱】出现的频率足够的低，那么它离原点就足够的远。\n这样的话，词频信息就误导了我最终句子向量的表达，那么embdding度量相似性就不靠谱了。\n\n词频影响词向量空间稀疏性\n\n基于上面的1，论文观察到，词频高的词汇分布的密集，词频低的词汇分布的稀疏。\n这一点问题很大，因为如果存在稀疏性，那么在词频低的词汇周围，就会存在很多意义不明的地方。\n我们可以直观的想一下，如果分布的很稀疏，那么也就是词与词之前的空间存在大量的不确定性，这些地方是没有明确的语义信息表达的。\n如果我们最终所有单词的加和正合适落在这种语义不明确的地方，那么相似性的度量准确度自然很低。\n为什么sen-emb相似度很大\n其实还有一个小点我想说一下，就是自己做实验的时候，会发现语义匹配效果不好的体现是因为consine计算出来的相似度基本都大于0.9。\n文中解释了为啥效果不好，但是并没有详细说为啥相似度计算出来都很大。（或者是有提到但是我粗心错过了）\n针对这一点，我想说一下我自己的感受。\n我觉得这两点的结合是根本原因：高频离原点近+高频分布紧密。\n怎么说呢？\n这句话其实可以这么理解：高频词基本都挤在一块了。\n句子大部分还是高频词占主要部分，在计算句子向量sen-emb的的时候，我们的操作是直接相加。\n那么计算出来的和当然相似度就很高。\n我这个解释不严谨，没啥严格证明，但是我觉得比较容易理解。\n简单总结\n简单来说，就是Bert的向量空间存在各向异性（anisotropic）：\n\n高频离原点近，低频离原点远\n高频分布紧密，低频分布稀疏\n\n这两个现象的存在导致Bert的语义信息不能很好的表达出来，所以做语义相似度不好。\n一个解决办法，就是把Bert的向量空间转换到另一个更加合适的空间，然后再做相似性度量，这就是论文的另一部分，感兴趣的可以去看原论文吧。\n为啥相似度都很大，主要是因为高频词都挤在一块了。\n加我微信，做个点赞之交\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/为什么Bert做不好无监督语义匹配/"},{"title":"","date":"2024-06-21T03:20:44.153Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.153Z","content":"关键词提取资源总结\nNLP关键词提取方法总结及实现 https://blog.csdn.net/asialee_bird/article/details/96454544\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/关键词提取/关键词提取资源总结/"},{"title":"","date":"2024-06-21T03:20:44.123Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.123Z","content":"关键词的提取，也可以称之为文本标签提取。\n比如说，”今天这顿烧烤是真不错啊“，在这句话中，”烧烤“这个词就可以被认为是一个关键词，或者说这个句子的一个标签。\n这个标签在一定程度上能够表现出这个句子的含义，比如这个”烧烤“，如果用在文本分类任务中，可以隐含带有”美食“这个类别的信息。\n这些标签有些时候也可以用在推荐系统的召回，比如直接按照”烧烤“这个标签做一路召回。\n对于关键词的提取一般来说分为抽取式和生成式。其实类比到摘要，其实也是分为抽取式和生成式。\n生成式有一个缺点就是有些结果不可控，这其实还挺要命的。\n对于抽取式，就是从现有的数据中拿出来词组。最差的结果也就是拿出的单词并不重要，不是我们想要的。\n我们的重点是在抽取式提取关键词。\n关键词的提取可以分为两个步骤：召回+排序\n1.召回\n召回就是得到文本中的候选关键词，也就是得到这个句子中有可能是关键词的词汇。\n这一步，可以做的方法有很多，比如\n我们有积累的关键词词库，在这里直接匹配出来。\n一些符合的词性的候选词，比如我挑选出名词作为候选词\n还可以基于一些统计特征提出候选词，比如TF-IDF（有些时候统计特征也会用在排序中作为特征）\n基于一些规则，比如一个句子出现了人名地名，书名号中词，这些很有可能就是关键词\n召回其实是一个很重要的部分，在这一步骤，尽可能的召回有用的词汇。我自己的标准是宁可多不能少。如果多了，无非就是增加了资源消耗，但是少了，可能在排序阶段就是无米之炊了。\n2.排序\n排序阶段，我们可以将方法大致的分为有监督和无监督的方法\n2.1无监督抽取关键词\n对于无监督，我们分为基于统计和基于图。基于统计就是TF-IDF和各种变种。基于图最常见的就是TextRank。\n关键词提取的一个baseline就是 TF-IDF 提取，这种方法效果已经很好。投入产出比很高，我们一般需要去掉常用的停用词，保留重要的词语。\nTF-IDF基于统计，易于实现，但是缺点就是没有考虑词与词，词与文档之间的关系。是割裂的。\n另一个baseline就是基于图的TextRank, TextRank 由 PageRank 演变而来。\n相比于TF-IDF，TextRank考虑了词与词之间的关系（提取思想就是从窗口之间的词汇关系而来），但是缺点是它针对的是单个文本，而不是整个语料，在词汇量比较少的文本中，也就是短文中，效果会比较差。\n随着数据量的积累，我们需要把模型更换到有监督模型加上。一般来说，有监督分为两种，一种是看做序列标注，一种是看做二分类的问题。\n2.2有监督之二分类\n先说二分类问题，比较简单，就是找到词汇的各种特征，去判断这个词汇是不是这个文本的关键词。\n我大概罗列一些可能会用到的特征。\n位置特征：\n使用位置特征是我们基于文本关键词出现的位置是在大量数据的情况下是有规律可言的，比如微博文本中出现在##符号中部分词汇有很大概率就是文本的一个关键词。\n是否出现在开头，是否出现在中间部分，是否出现在末尾，出现的位置（具体是第几个单词）；相对于整个文本的位置；是否出现在##符号中…\n统计特征：\n共现矩阵信息；词频；逆词频；词性；词跨度；关键词所在句子的最大长度/最小长度/平均长度;\n向量特征：\n关键词词向量和文档向量的相似性\n2.3有监督之序列标注\n关键词的提取，就是一个典型的序列标注的问题。判断句子中关键词的开头中间结尾的位置。\n序列标注最基础的就是HMM和CRF方法，但是特征工程比较复杂。\n为了解决特征工程复杂的问题，我们使用深度学习模型序列标注。\n关于序列标注，大家可以参考我这个文章内容：\n工业级命名体识别经验+代码总结\n3.新词发现\n还会出现一个问题，如果我们使用二分类判定关键词，上述的过程我们都是基于我们的分词器来做的。有可能会出现一些新词，由于分词错误，不能及时的出现在你的候选词库中，比如”爷青结“。\n这个时候，我们需要一个新词发现系统，持续不断的补充到词库中，在召回阶段可以提升召回率。\n对于新词发现来说，基操就是从文本的自由程度和凝固程度来判断是否是新词，这样的问题就是阈值不好调整从而导致召回和精准不好平衡。\n我们还可以通过别的方法离线挖掘实体词补充道词库中，之前有借鉴美团ner的文章实现了一下，效果还不错，在这里，大家可以参考我这个文章：实体库构建：离线大规模新词实体挖掘\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/关键词提取/关键词提取方法综述/"},{"title":"","date":"2024-06-21T03:20:44.253Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.253Z","content":"\n大家最重要的可以看一下第五个问题，一个读者朋友根据我之前分享论文，在无标注语料上做中文文本分类的实践，我觉得比较有借鉴意义。\n\n主要梳理的7问题，梳理如下，大家可以看下有没有自己感兴趣的问题，希望对大家有帮助：\n\n有没有比较好的NLP开源项目\n如何融合BERT所有Tokens输出语义信息\n英文BERT如何加载中文参数-这个待后续更新\n没有机器学习基础是否可以学习NLP深度学习知识\n只用标签无需标注语料就可以进行文本分类在中文语料的效果\n有没有关于Transformer的面试题\n如何在NER的时候加入词汇信息\n\n涉及到敏感信息，比如大家私人idea之类的，以及个人信息，我不会公开的，大家可以放心。有不便公开的，也可以直接和我说的。\n所以大家有问题可以在最后面扫码加我私人微信，我一般都会在我力所能及的范围内，比较详细的回答的。\n1. 有没有比较好的NLP开源项目\n\n2. 如何融合BERT所有Tokens输出语义信息\n\n3. 英文BERT如何加载中文参数-这个待后续更新\n\n4. 没有机器学习基础是否可以学习NLP深度学习知识\n\n5. 只用标签无需标注语料就可以进行文本分类在中文语料的效果\n之前写的这个文章，有读者朋友在中文语料上做了测试，效果还不错，感兴趣的可以看看。\n\n\n\n6. 有没有关于Transformer的面试题\n\n\n7. 如何在NER的时候加入词汇信息\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/其他/20201210一周技术问题答疑汇总/"},{"title":"","date":"2024-06-21T03:20:44.173Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.173Z","content":"实体库构建：离线新词发现流程\n命名体识别我们一般有两个操作：词典匹配+模型预测。\n对于词典匹配来说，速度快，准确度高。但是有一个问题是由于不同人对同一个东西有不同的表达，所以OOV问题比较严重。\n缓解OOV，我们可以使用模型预测增加泛化，还可以离线挖掘实体进行补充实体库。\n美团在这个文章中提到了一种新词离线挖掘补充实体库的方法，我借鉴了其中的思路，并且用到了自己工作中，效果还不错。在这个文章，我主要是详细解读一下整个过程。\n我们聊一下为什么需要做新词发现？\n新词是什么？按照最普通的定义就是我词典中不存在的词汇都属于新词。如果按照这个思路去挖掘新词，我们一般使用两种方法：有监督和无监督。\n无监督一般来说就是使用紧密度加自由度调整阈值就可以提取新词。但是这种方法有一个问题，就是你这个阈值的调整到哪里才可以，这个取决于你的召回和精确的一个平衡。\n有监督的话，一个简单的思路就是序列标注做中文分词，出来的词汇不在字典中的我们就可以作为新词。\n但是我们想一下这样新词出现的是什么情况？\n举个最简单的例子，可能你挖掘出来的就是“爷青结”这样的词汇，确实是新词，不在我们已经有词典中，但是对于我们的实体库有没有帮助呢？\n有没有帮助要看我们的目的。如果说我们的目的是为了分词的准确，那么这个新词完全可以用，直接放到txt文件中，保证下回分类的准确。\n但是在这里，我们是做的事情是为了补充实体库，也就是需要有意义的词汇，比如说“外滩十八号”这种词汇。\n所以，普通的新词发现的有监督和无监督方法只能挖掘词汇，不能保证挖掘的是实体。\n基于此目的，可以借鉴新词挖掘的思路，对词汇做二元分类判断是不是实体的有监督方法就很容易想到。\n总结下来步骤就是这样：\n\n\n挖掘频繁项\n\n\n提取频繁项的各种统计特征\n\n\n频繁项和已经有的实体交集作为正样本，负采样得到负样本。使用多个分类器进行集成，训练多个二元分类器。\n\n\n采用负样本的时候，美团有提到一个论文，大家可以去看一下。\n\n搜索日志中搜索次数比较高的词条和正样本的交集作为高质量短语，负样本减去词条作为低质量短语，使用Bert训练质量打分器。\n\n整个流程通读下来，其实很好理解。\n一般来讲，如果实践过程，第四个步骤其实很难做。\n我是这样想的，首先这个美团搜索很垂直，一般搜索属于短query，你很难去在美团搜索框去搜一个很长的句子。\n这种情况下，就会出顾客的搜索记录本身就是高质量的短语或者实体。想一下是不是这样，你去搜“来杯啤酒烧烤”，这本身就是个商户名称，就是个实体。所以交集才可以作为高质量短语。\n如果你是个大搜的搜索日志，这种情况基本不存在的，有长短语，有短的词汇，你找交集的阈值都无从下手。\n第二个难点就是Bert打分器这个东西的可靠性。一般来说实体的字数都比较少，比如五六个字，字数这么少，这个打分究竟可靠不可靠我没有实践过，只是有这个疑惑。\n整个做完，还有一个问题，实体库是分类别的，比如美食有一个词典，景点有一个词典等等吧。我们上面挖掘出来的是全部的实体，不分类别的，那么怎么分类呢？\n美团提到他们使用的AutoNER，大家可以去看一下相关论文。针对这一块，其实能做的思路还挺多的，由于工作原因，这块我就不说了。大家可以发散思路。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/关键词提取/实体库构建：大规模离线新词实体挖掘/"},{"title":"","date":"2024-06-21T03:20:44.273Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.273Z","content":"如果面试官问【聊一下RNN中的梯度消失】\n盲猜很多同学的回答可以简化成这样形式【由于网络太深，梯度反向传播会出现连乘效应，从而出现梯度消失】\n这样的回答，如果用在普通网络，类似MLP，是没有什么问题的，但是放在RNN中，是错误的。\nRNN的梯度是一个和，是近距离梯度和远距离梯度的和；\nRNN中的梯度消失的含义是远距离的梯度消失，而近距离梯度不会消失，从而导致总的梯度被近的梯度主导，同时总的梯度不会消失。\n这也是为什么RNN模型能以学到远距离依赖关系。\n简单的解释一下原因。\n首先，我们要明白一点，RNN是共享一套参数的（输入参数，输出参数，隐层参数），这一点非常的重要。\n当然，我们在理解RNN的时候，会把RNN按照时间序列展开多个模块，可能会认为是多套参数，这个是不对的哈。\n如下所示：\n\n然后，假设我们现在的时间序列为3，有如下公式存在：\n\n现在假设我们只是使用t=3时刻的输出去训练模型，同时使用MSE作为损失函数，那么我们在t=3时刻，损失函数就是:\n$$L_{3}=\\frac{1}{2}(Y_{3}-O_{3})^{2}$$\n求偏导的时候，就是这样的情况：\n\n其实看到这里，答案已经出来了。\n我们以第二个公式为例，也就是对$w_{x}$ 求偏导，如果时间序列程度为t，我们简化一下成下面这个公式：\n$$W_{x}=a_{1}+a_{2}+…+a_{t}$$\n时间序列越长，出现连乘的部分越集中出现在靠后面的公式上，比如$a_{t}$，但是前面的公式是不受影响的，比如$a_{1}$，也就是梯度是肯定存在的。\n总结一下：RNN中的梯度消失和普通网络梯度消失含义不同，它的真实含义是远距离的梯度消失，而近距离梯度不会消失，同时总的梯度不会消失，从而导致总的梯度被近的梯度主导。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/其他/RNN的梯度消失有什么与众不同的地方/"},{"title":"","date":"2024-06-21T03:20:44.453Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.453Z","content":"句向量\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/句向量/README/"},{"title":"","date":"2024-06-21T03:20:44.473Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.473Z","content":"问：如何判断”今天你吃饭了“和”今天去哪里吃饭“这两个句子的相似度？\n上面这个问题，就是我们为什么需要句子向量的原因。神经网络或者其他的机器学习方法很难直接对中文进行处理，我们需要对句子使用一定的方法进行数字化或者向量化。\n我在讲词向量的时候，说了一个很好的比喻，向量化的过程就非常的类似计算机把我们的输入转为二进制以便处理的过程。\n只不过二进制的转化我们是可以规定，而向量化的过程根据我们任务不同目标不同，有着多种方法。\n我简单花了一个概述图，大家可以看一下：\n\n1.基线模型\n1.1 基于统计的词袋模型\nOne-hot 模型简单来说就是单词出现的位置为1，不出现的位置为0，形如[1,1,1,0,1],来将句子向量化。\nTF-IDF 使用到了单词在句子中出现的词频和在所有文档中出现的频率。相比于One-hot，增加了单词重要性这个维度的特征，所以效果一般来说比One-hot要好。\n1.2 基于词向量的词袋模型\n为什么使用词向量这个特征？相较于One-hot和TF-IDF，词向量能够提取语义信息。\n对词向量最简单的操作就是求平均获取句子的表征。对于词向量，一般可以使用Word2vec/Fasttext/Glove。后期Bert出现之后，我们也可以使用Bert的最后一层（或者某一层）的输出作为词向量。但是效果有待商榷。\n简单求均值简单粗暴，优化方法就是使用各种方法进行加权求均值。\n我们可以使用TF-IDF对词向量做加权求和获得句子的表征。为了简便，我们也可以去掉TF，只是使用IDF做加权求和。\n对于SIF模型，它分为两个步骤。首先使用平滑倒词频为权重求和，随后减去所有句子的共有信息，获得的结果作为句子表征。\n对于Power Mean 均值模型，它引入了幂均值改进加权求均值，通过修改不同的P值拼接不同的句子向量得到最后的句子表征。\n以上都属于我们词袋模型求得句子向量。词袋模型存在一个最大问题，就是忽略了或者没有那么重视句子的语序问题，不管你是不是有用到词向量。\n1.3基于任务\n我们来看一下基于任务的，分为RNN和CNN。举个简单的例子，我们使用RNN和CNN做文本分类任务，然后使用最后一个时刻或者最后一层（或者你使用其他方式）作为句子的向量。\n这种方式很好，但是存在的问题就是句子向量的表达严重依赖任务形式。\n我们用文本分类训练出现的句子向量如果还是用在文本分类任务，效果可能还不错，但是如果用在情感分析任务上，可能就一塌糊涂。\n这是因为我们的模型是依赖于任务的，文本分类模型侧重点和情感分类的侧重点是不同的，导致模型参数也应该是不相同的。\n所以基于任务的句子向量模型迁移性比较差。\n2. 无监督模型\n无监督模型最大的好处就是可以不使用标签数据。这一点真的很重要。\n当然我想提一点就是我这里说的无监督模型是做的是端到端。其实本质上，我们使用基于词袋的模型，也属于无监督模型。仔细想一下是不是这个道理，词袋模型同样没有使用到标签数据。\n拉回来，我们说端到端的无监督模型。主要谈两个：Skip-Thought Vectors 和 Quick-Thought Vectors。\nSkip-Thought Vectors 模型输入为三个连续的句子，然后使用Encoder-Decoder模型，输入中间的句子，分别生成上一个句子和下一个句子。这个过程非常类似于Word2vec。\nQuick-Thought Vectors 是对Skip-Thought Vectors 的改进。首先说为啥需要改进，最大的原因就是 Skip-Thought Vectors 太慢了。首先它是一个生成任务。生成任务在预测阶段很难并行。其次他是做了两个生成任务，一个是上一个句子的生成，一个是下一个句子的生成。\nQuick-Thought Vectors把生成任务改为了分类任务，Decoder从一组句子中选择出正确的上/下一个句子。\n3. 有监督模型\nInferSent模型注意两个细节点就可以。首先就是使用的是自然语言推理（NLI）数据集上训练 Sentence Embedding。\n这一点其实很重要，作者是认为从这个数据集上训练出来的词向量是可以很好的被迁移到别的任务上的。\n其次使用的是LSTM或者其他模型对句子进行编码，作者在论文中对不同编码模型有详细比较。\nUniversal Sentence Encoder 使用多任务，通过在不同的数据集和不同的任务上同时训练，动态地适应各种的 NLP 任务。Encoder使用两种模型，一个是Transformer，是为了获取更高的精度，另一个事DAN (Deep Averaging Network）为了获得更快的速度\n已经尽了最大努力缩减内容，提取重点了，接下来会用几篇文章详细的谈一谈其中的部分模型。\n写文不易，点个在看或者赞让更多人看到吧，谢谢。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/句向量/句向量模型综述/"},{"title":"","date":"2024-06-21T03:20:44.873Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.873Z","content":"HMM\n三个参数\n我们这个HMM模型，含有三种参数，定义如下:\n$$\n\\lambda =（\\pi,A,B）\n$$\n注解:\n首先\n$$\n\\pi： 这里不是我们的圆周率那个符号，而是代表的是初始概率矩阵，具体看下面的讲解\n$$\n其次\n$$\nA：代表的是状态转移概率矩阵，具体看下面的讲解\n$$\n最后\n$$\nB：代表的是发射概率矩阵，具体看下面的讲解\n$$\n现在引入数学符号，首先定义观测变量为符号:\n$$\no: o_{1},o_{2},o_{3},o_{4}…o_{t}…\n$$\n观测变量的值域，也就是观测变量的取值范围：\n$$\nV={v_{1},v_{2},…v_{M}}\n$$\n也就是观测变量我们有M个取值结果。\n同理我们可以得到状态变量为:\n$$\ni: i_{1},i_{2},i_{3}…i_{t}…\n$$\n状态变量的值域，也就是状态变量的取值范围：\n$$\nQ={q_{1},q_{2}…g_{N}}\n$$\n也就是说，我们的状态变量有N个不同的取值。\n我们定义A为状态转移概率矩阵，公式定义为:\n$$\nA=[a_{ij}]，其中a_{ij}=P(i_{t+1}=q_{j}|i_{t}=q_{i})\n$$\n注解：这里我们的状态转移概率矩阵很容易理解，就是说我们上面不是定义了状态变量为符号\n$$\ni\n$$\n，其中状态有N种取值范围。我们以词向标注为例，这里我们假设我们的N有四种方式，分别为[名词，动词，谓词，形容词]。那么A这个矩阵中的每个元素就是其中一个状态转移到另一个状态的概率，比如名词之后接动词（也就是名词转移为动词）的概率，比如动词之后接谓词（也就是动词转移为谓词）的概率，依次类推。\n我们定义B为发射矩阵\n$$\nB=[ b_{j}(k)], 其中b_{j}(k)=P(o_{t}=v_{k}|i_{t}=q_{j})\n$$\n注解：这里简单记住，发射矩阵就是上面状态发射到下面的概率，注意看箭头的方向。\n这个时候，我们再去看\n$$\n\\pi ：这个符号代表的就是 i_{1}={q_{1},q_{2}…q_{n}}时的状态概率{q_{1},q{2}…q_{n}}\n$$\n两个假设：\n\n\n马尔科夫假设：当前时刻的状态变量只与t-1时刻有关，而和别的变量无关。\n$$\np(i_{t+1}|i_{t},i_{t-1}…i_{1},o_{t},o_{t-1}…o_{1})=p(i_{t+1}|i_{t})\n$$\n\n\n齐次性假设，可以理解为时间平移不变\n\n\n![image-20201221164438044](/Users/zida/Library/Application%20Support/typora-u](…/image-20201221164438044.png)\n\n观测独立假设：当前的观测变量只与当前时刻的状态变量有关，而和其他无关\n\n$$\np(o_{t}|i_{t},i_{t-1},…i_{1},o_{t-1},…o_{1})=p(o_{t}|i_{t})\n$$\n三个需要解决的问题\nHMM 需要解决的问题。\n首先求值问题：已经知道三种参数的情况下，那么我一句话出现的概率多大：我爱中共产党\n简单讲就是已知\n$$\n\\lambda\n$$\n求\n$$\no_{1},o_{2}…o{n}\n$$\n这句话出现的概率有多大。\n我们常用的算法是前向后向算法。前向算法后向算法解决的问题是求在给定三个参数的情况下求观测序列出现的概率  注意一定是求得观测序列，也就是放在序列标注中，是求我们本身文字序列出现的概率。\n第二个问题，就是参数如何求？\n$$\n也就是如何求得：\\lambda\n$$\n我们使用EM算法求得这个参数\nEM算法是在估计HMM三个参数的办法。当然之前有谈到如果我们有观测序列和对应的隐藏序列，那么我们直接从数据中去统计就可以了。但是现实情况是我们很难获取标注序列，也就是隐藏序列。这个时候我们就需要使用到EM算法去预估。\n也就是，如果没有标注序列，我们使用EM算法，如果有了标注序列我们直接从语料中统计出来就可以了。\n第三个问题就是解码问题，也就是要找到一个状态序列，可以使得\n$$\nI=argmaxP(I|O)\n$$\n也就是解决当前这个句子最有可能的序列标注结果是什么样子的。\nHMM最可能的额隐藏状态序列求解使用维特比算法。\n使用一句话话可以很精辟的总结出来维特比的过程：\n在每一时刻，计算当前时刻落在每种隐状态的最大概率，并记录这个最大概率是从其哪一个时刻那个隐状态转移过来的，然后再从结尾达到最大概率的那个隐状态回溯，就有可能得到最优路径。\n维特比使用动态规划，解决寻找全局最优路径的问题。\nCRF\n全局归一化避免偏置\n对于CRF我们的目标函数是让正确的标注序列出现的概率在所有路径汇总是最大的。所以分母我们是针对的所有路径。而不是在每一个时刻去计算最优值。因为在某一个时刻计算的最优可能在整体路径上并不是最优。\n分数并不是概率\n在bilstm-crf中，我们包括转移分数，发射分数，我们都是分数而不是概率。并且我们是做了log操作的，所以在计算某个路径的分数的时候我们并不是概率相乘而是分数相加。\n损失函数\n损失函数其实本质很简单，就是正确路径概率最大，拆分之后我们会对应两个部分一个是一元分值，就是在某个时刻成为某个实体标签的分数。一个是二元分值，就是标签之间的转移分数。\n解码-维特比\n在每一时刻，计算当前时刻落在每种隐状态的最大概率，并记录这个最大概率是从其哪一个时刻那个隐状态转移过来的，然后再从结尾达到最大概率的那个隐状态回溯，就有可能得到最优路径。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/命名体识别/HMM_CRF/"},{"title":"","date":"2024-06-21T03:20:44.723Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.723Z","content":"在做中文NER的时候，我们的常规操作是以字为单词。这是因为如果以词为单位，\n很容易造成切分错误，导致误差的积累。\n我举个简单的例子，比如我现在有一句话，【你去北京老哥】\n但是以字为单词，有一个问题就是会忽视词的信息。\n所以，大家很自然就想仍然以字为单词做NER，但是把词的信息补充进来。\n这个时候，一个很朴素的想法就是，我输入的时候过一遍分词，然后把词向量和字向量拼接或者相加或者做别的操作来融合起来。\n这个方法一般来说能够提升准确度，但是不会太多。\n后来还有一种思想就是使用 lattice structure，这种确实做到了词汇信息的增强，但是存在并行化困难以及推理速度慢的缺点，换句话说，方法是好方法，但是落地困难。\n这个论文做了一个什么事情呢？把栅栏式结构通过相对位置编码展平。\n我们知道transformer为了保持位置信息，对于每个token，是使用了位置编码的。在这里，为了这个晶格结构设计了一个巧妙的位置编码，来把复杂结构展开展平：\n如图所示：\n\n看这个图需要注意的是，【重】这个字对应到英文代表的是character-字符，【重庆】这个词组对应到英文代表的是word-单词，这一点，大家在读论文的时候需要注意。\n为每一个token（包含char和word）分配两个位置索引：头位置和尾位置；\n在原来的晶格结构中，比如【店】只能和【人和药店】以及【药店】产生关系，但是在TRM中，由于self-attention的存在，【店】是可以和序列中的每个token都发生关系，不仅仅是和self-matched的词汇。这算是一个意外之喜。\nself-matched的词汇，就是包含当前char的\n谈一下为什么这么转化：\n一般来说，我们有语料，和词典，通过词典，我们可以得到一个晶格\n为什么要把晶格结构压平\n头部的索引就是第一个单词的位置，尾部就是最后一个单词所在的位置，如果是一个char，头尾就是相同的。\n通过这个巧妙的设置，我们是可以把展平的东西再重建到晶格模式的，所以认为是可行的。\n相对位置编码\n通过头尾索引，我们可以把晶格结构压平。\n现在还面临一个问题，就是对于【人和药店】头尾索引是【3】【6】，但是这并不包含位置信息。\n对于NER来说，位置信息是很重要的。\n对于普通的TRM，使用绝对位置编码保持位置信息，但是有研究表示，这种位置信息在self-attention中使用向量内积的时候，会减弱。\n具体的大家可以看我这个文章：原版Transformer的位置编码究竟有没有包含相对位置信息；\n所以，我们现在就要考虑使用相对位置信息来表达位置，同时还要把我们头尾索引融合进来。\n对于句子中的两个spans（包含char和words）$x_{i},x_{j}$，它们可能有三种关系：相交，包含，和分离。\n比如上面那个例子，【药店】和【人和药店】就是包含的关系；【重庆】和【人和药店】就是分离的关系。\n我们使用一个向量来描述两个spans之间的关系。\n先说两个spans之间存在的距离关系可以用如下公式去表达：\n\n上角标的$(hh)$代表的就是两个spans之间的头部索引差值，其他上角标类似的意思。\n具体的实际是什么样子，大家可以看上面的图c；\n然后我们使用如下的公式去生成相对位置编码：\n\n接下来的问题就是利用这个相对位置编码融入到TRM之中。\n\n简单来说，就是利用相对位置编码，生成了一个包含相对位置编码信息的新的attention矩阵，不再使用原始的attention矩阵\n看到这里，其实有注意到一个很有意思的点就是FLAT使用的是一层encoder。\n实验\n实验比较感兴趣的是\n一个是和其他词汇增强的网络结果相比，效果如何。\n还有一个就是使用transformer之后，TRM长距离依赖的优点和每个token之间都可以交互的优点有没有在提升效果上发挥作用\n还有一个其实很自然的会想到能不能使用将FLAT和BERT融合起来。也就是如何将动态的字向量和FLAT这种词向量结合起来。\n先看第一二点\n\n再看第三点\n\n有意思的是，使用了FLAT之后，在Resume和Weibo效果有提升，但是不明显，作者认为可能是因为数据集有点小。在大数据集Ontonotes和MSRA上，效果提升比较明显。\n推理速度的话，和Lattice LSTM相比，BSZ为16的情况下，基本是8倍左右。\n总结\n梳理一下怎么把词汇信息加入进去的：\n\n首先我们知道NER融合词汇信息能提升最终效果，但是一般的Lattice结构落地困难\n然后受TRM位置信息的启发，将Lattice结构展开\n然后由于普通TRM绝对位置信息在self-attention中会被削弱，所以想要使用相对位置信息。\n从头尾索引，我们可以知道tokens之间有三种关系：相交，包含，隔离；从这三种关系，我们可以得到两个tokens的四种距离公式，并且把这个四种距离公式融入到了相对位置信息。\n得到最终的相对位置信息，将相对位置信息融合进入attention矩阵，参与Encoder计算\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/命名体识别/FLAT-Transformer/"},{"title":"","date":"2024-06-21T03:20:44.893Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.893Z","content":"命名体识别\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/命名体识别/README/"},{"title":"","date":"2024-06-21T03:20:45.093Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.093Z","content":"今天介绍复旦的一个论文TENER ；普通的TRM在其他NLP任务中效果很不错，但是在NER中表现不佳。为了解决性能不佳，论文做了几点改进。\n主要掌握以下三点改进：\n\n方向\n距离\n无缩放的注意力\n\n1. 架构图\n先看TENER架构图：\n\n2. 距离和方向信息\n对于NER任务来说，距离和方向都很重要；\n举个简单的例子：【李华住在北京】；李华是人名，北京是地名，如果忽视了方向，那么【北京住在李华】，这个肯定是说不通的。\n换句话说，每类NER实体在哪种位置是有着某种关系或者规则的。所以方向很重要。\n简单概述普通TRM位置编码的问题，如下：\n普通TRM中的正弦位置编码能够捕捉到距离信息，但是不能捕捉到方向信息。而且这种基本性质（distance-awareness）会在sefl-attention消失；\n为了改进这种问题，使用了经过改进的相对位置编码，弃用了绝对位置编码；\n2.1 为什么没有方向信息：\n位置编码的点积可以看做在度量两者之间的距离:$PE^{T}{t}PE{t+k}$\n点积结果画图表示如下：\n\n从这个图，我们可以很清楚的看到，是对称的，也就是说在k=20和k=-20的时候，点击结果相同，换句话说，方向信息没有体现出来。\n公式上体现就是：$PE^{T}{t}PE{t+k}=PE^{T}{t-k}PE{t}$\n2.2 distance-awareness 消失\n再进一步，在self-attention中，distance-awareness 也在消失，这一点，我之前的文章有写，可以看原版Transformer的位置编码究竟有没有包含相对位置信息。\n改进之后的相对位置编码以及attention计算为：\n\n3. attention缩放\n传统TRM的attention分布被缩放了，从而变得平滑。但是对于NER来说，一个更加尖锐或者说稀疏的矩阵是更合适的，因为并不是所有的单词都需要被关注；一个当前的单词的类别，足够被周围几个单词确定出来。\n矩阵越平滑，关注的单词越多，可能会引入更多的噪声信息。\n4. 总结\n\n原始TRM绝对位置编码不含有方向信息，Self-attention之后相对位置信息也会消失；故使用改进的相对位置编码和新的attention计算方式\nattention计算不使用缩放系数，减少了噪声信息\n使用TRM进行char编码，结合预训练的词向量拼接输入TENER\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/命名体识别/TNER-复旦为什么TRM在NER上效果差/"},{"title":"","date":"2024-06-21T03:20:44.543Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.543Z","content":"今天介绍一个论文autoner，主要是为了探索如何在只有词典的情况下，提升NER实际落地效果；\n首先，如果手中含有词典，常规操作就是远程监督打标数据，然后做NER；\n远程监督一个比较常见的操作就是使用我们手中的字典，通过字符匹配的形式对文本中可能存在的实体打标。\n但是对于这种远程监督的形式，存在比较多的问题，这个论文主要探讨两种： 多标签(multi-label tokens) 和标签不完善的问题；\n针对multi-label tokens，论文提出的是Fuzzy-LSTM-CRF，简单讲就是讲LSTM后面的CRF层变为了Fuzzy CRF层，可以在处理tokens对应多标签的情况下，不牺牲计算效率；\n第二个问题标签不完善，是因为字典毕竟是有限的，不可能把所有的实体都覆盖到，那么句子中没有被字典打标成功的词组很有可能也是某种实体，但是远程监督并没有对此做处理。\n针对这个问题，本文提出了一种比较新的标注框架，简单来讲就是在这新的框架中，不去预测单个的token的类别，而是去判断两个相邻的tokens是不是在同一个实体中被tied；\n上面只是我自己简单的分类，其实存在的两个问题和两种解决架构是相互融合在一起的，具体的我们下面谈。\n0. 词典形式简单介绍\n首先定义一下词典形式，包含两个部分，第一部分是实体的表面名称，这个包括规范名称和对应的同义词列表；第二个部分就是实体的类型；\n其次，词典的标注肯定是有限的，肯定存在不在词典中的某些词组但是也属于某种类型的实体；\n对于这部分实体，我自己的理解大体可以包含两个大部分；第一个大部分就是比如说【科技】这个领域覆盖的【科技】实体有有限的，所以有漏网之鱼；第二部分就是词典的实体类型是有限的，比如词典总共包含2个实体类型，但是你真实的文本包含更多的实体类型，存在漏网之鱼。\n对于这些漏网之鱼的实体，我们的策略是这样的。\n首先通过AutoPhrase从文中挖掘出来高质量短语，然后统一赋值为unknown type，也就是未知类型。\n1. Fuzzy-LSTM-CRF\n1.1 标注策略\n梳理一下，我们现在手上有词典；\n词典包含两个部分，一部分是已知实体类型（假设是2个，当然可能更多或者更少）；另一个部分就是我们通过某种方式挖掘出来的高质量实体对应的未知类型；\n然后我们通过手中的词典对原始无标注文本进行打标；\n那么现在对于句子中的某个token，它存在三种可能性；第一它可能是已知实体类型中的一种或者多种；第二它属于未知类型；第三是属于O这种情况，就是non-entity；\n基于传统架构BIlstm-CRF如何解决多标签的问题？\n其实本质解决的思路很简单。对于原来的每个token，只是预测一个类别，现在是预测多个类别就可以了。\n详细点讲就是，首先对于远程监督标注的过程，我们会使用三种策略。\n我们先假设我们使用{I；O；B；E；S}的标注形式；\n第一，对于某个token，如果它对应到了已知类型中的某一个或者多个实体，那么按照对应的位置直接标记上，不要漏掉；也就是说{I；B；E；S}和对应的一个或者多个实体类型对上标；\n第二对于对于某个token，如果属于未知类型，那么对应的这个token就需要把所有已知实体类型（区别于上面的一个或者多个已知实体类型）和 {I, O, B, E, S}对应的打标上；\n注意，这里并没有使用未知实体类型，而是使用的所有的已知实体类型；\n第三个对于既不属于已知类型的，也不属于未知类型的，全部打上O；\n1.2 Fuzzy-LSTM-CRF 模型架构\n其实很好理解，传统的CRF最大化唯一一条有效的标注序列。在这里，我们最大化所有有可能的标注序列。\n公式如下：\n\n看架构图：\n\n2. AutoNER\n区别于Fuzzy-LSTM-CRF 模型沿用传统架构，在这里论文提出一种新的标注架构-Tie or Break；\n这个标注框架更加关注的是当前token和上一个token是否在同一个实体里面；如果在同一个实体里面，那么就标注为Tie；\n如果当前单词和上一个单词至少有一个在unkonw类型的高质量短语，那么标注为unkonw，其他情况标注为Break；\n优化过程：把实体识别和实体类型判定分离开。\n原论文中描述的是先做实体识别，两个Break之间作为一个span，然后做实体类型判定；\n实体识别中，对于当前单词和上一个单词之间类别的的输出，对Tie和Break做二分类损失，如果类别是unkown类别，直接跳过，不计算损失。\n概率公式如下：\n\n\n第二步预测实体类型，包含None实体类型\nunkonw这种，知道这属于实体，在高质量短语词典中，但是不知道短语类型，所在这里我们会标注为None实体类型。\n其他的不在词典中的，当然也就会被标注为None实体类型。\n为了应对多标签，也就是同一个实体对应不同的类别，这里修改了最后的CE损失函数：\n\n\n使用的是软标签的进行的CE的计算，并没有使用硬标签。\n$L_{i}$对应的是在远程监督中，当前实体真实类型标签集合。从公式我们可以知道，尤其是看分母，在不属于这个集合的标签概率我们并没有计算在内。\n总结\n多提一个小细节，就是高质量短语的挖掘使用的是AutoPhrase，大家可以去试一下；\n论文提出两种结构解决多标签和标签不完善的问题。\n首先对于标签不完善，使用上面提到的AutoPhrase去挖掘文本中的高质量短语，作为词典中的未知类型。\n在Fuzzy-LSTM-CRF，需要注意的细节是，对于未知类型的标注，我们使用的策略是标注所有已知类型；\n对于AutoNER，有两个细节需要注意，一个是新的标注框架tie or break，重点在于去看两个相邻单词是否属于同一个实体；第二个细节就是为了解决多标签问题，修改了损失函数，使用的软标签；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/命名体识别/autoner/"},{"title":"","date":"2024-06-21T03:20:45.103Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.103Z","content":"最近在梳理命名体识别和关键词提取的东西，之前有建一个仓库，专门梳理相关内容。新关注的人比较多，分享给大家，地址在这里:\nhttps://github.com/DA-southampton/ner\n点star 不迷路，相关文章在github上更新的会更频繁一点QAQ。\n微信没有外链跳转，涉及到代码的部分，大家去仓库去看。\n之前做过一段时间的命名体识别，项目背景其实也很简单，就是我要做一个关键词匹配的功能，第一步我需要挖掘关键词。数据调研之后发现对于一部分领域文本，比如说娱乐领域，明星领域，财经领域等等吧，这些领域的文本很有特色，一般人名/地名/公司名称/书名/电影名称都可以很好的表示文本关键信息。\n在这种项目背景之下，很自然的就会想到使用命名体识别。我把在做这个项目的过程中，积累的一些资料总结了一下，希望对大家有所帮助。\n关于命名体识别，这是个很大的领域，要是做好，有很多工作要做。标题完全是为了能增加曝光，自己还是知道只是一个小学生，我会把自己看过的有用的东西都列出来，给大家提供一些先验信息。\n之后看到的关于nert的文章会在此基础继续更新（最近存了好多新文章还没看/苦逼码农/QAQ），不过建议大家star一下Github，不迷路，我给自己的计划是精读一些论文和博客，做一些思维导图，复现一些代码，我会努力的。\n经验介绍\n对于命名体识别的代码这一块，我大概的经验就是，工作中很少直接就上复杂模型，一般都是先来简单模型，然后在优化迭代。我给个大概的方向（大家视情况而定）：\n词典匹配–&gt;HMM/CRF–&gt;BiLSTM-CRF–&gt;Bert系列\n一般来说词典匹配是最简单的，也是最快的。不过很依赖于你的词典情况。一般来说，词典的补充需要你自己搞定，比如找相关的运营人员/产品人员，因为他们比较靠近一线工作，手上会积累一些相关的词典。或者使用合法爬虫手段（至于如何合法就自己考虑吧）去专业的垂直领域网站获取数据补充词典。\n我大概分为两个个模块，第一个是各种模型的代码实现相关资源，第二个就是关于命名体识别基础知识之类的相关资源\n代码实现\n代码不再多，把一个反复看，看懂了，自己能写出来做二次开发就可以，不要今天看一个代码明天换一个代码看（小声嘟囔）\n左侧是有链接的，微信点不开，大家去仓库看！！！\nBert系列 (Bert/Albert-softmax/CRF/Span/Span+focal_loss/Span+label_smoothing)做命名体识别\t仓库下面有Bert系列完成命名体识别的效果对比（一般来说看F1就可以）以及训练时间之类的比较，很推荐大家去看一看\nBiLSTM-CRF实现命名体识别(Pytorch版本)\tBiLSTM-CRF我就推荐这一个吧，其他的都是大同小异，大家可以一步步去调试，做二次开发就可以，比如换个损失函数之类的。\nNLP实战-中文命名实体识别-HMM/CRF 代码的实现\t(引用原文)本文章将通过pytorch作为主要工具实现不同的模型（包括HMM，CRF，Bi-LSTM，Bi-LSTM+CRF）来解决中文命名实体识别问题，文章不会涉及过多的数学推导，但会从直观上简单解释模型的原理，主要从零的内容会集中在代码部分。\n隐马尔可夫模型命名实体识别NER-HMM-1  [隐马尔可夫模型命名实体识别NER-HMM-2\t不愿意看书想看视频的同学可以看一下这个，B站首页偶然推荐给我的（推荐算法精准石锤了），讲的确实好\n双向最大匹配和实体标注：你以为我只能分词？------这个是词典方法命名体识别\t这个作者总结了自己实体词典+jieba词性标注进行实体自动打标，有Python代码实现，大家可以关注一下这个博主，名字叫“叫我NLPer”，行文很有意思\n基本上代码，我觉的看上面几个就够了吧，反复咂摸一下。\n博客讲解\n有些时候看到有人说，要想对某个概念真正有所了解，一定要看原论文。这句话肯定没错，但是不是有些时候没时间看论文吗（哭了苦逼码农）。而且有些博客讲的是真的好啊。我大概罗列一些我局的真心不错的文章，主要就是HMM/CRF/Bilstm-CRF\n左侧是有链接的，微信点不开，大家去仓库看！！！\n概率图模型体系：HMM、MEMM、CRF\t这个文章传播的比较广，讲的确实比较详细，不过大佬写的有些地方还是有些小问题，大家自己去挖掘吧。。。\n最通俗易懂的BiLSTM-CRF模型中的CRF层介绍-孙孙的文章\t这个文章讲的是对CRF模型的讲解，翻译的外文，原文很精彩，看译文也可以。大概讲一下，在看的过程中，要多琢磨。比如CRF有个特点全局归一化，这是区别于MEMM模型的；比如在代码实现的时候，我们一般都是使用log，所以乘法会对应加法，这样你在看源代码时候就不会懵逼；比如CRF损失函数有两个部分组成，分别有啥作用。\nner自动化打标方法\t叉烧大佬讲了一下如何用词典+最大逆向匹配做命名体识别整体思路，代码的实现可以参考第一部分那个Python代码\n中文NER任务实验小结报告——深入模型实现细节\t作者写了一下自己在做命名体识别的时候针对Bert的优化:BERT+CE_loss;BERT+lstmcrf;尝试用更少的标签列表;对损失函数进行了优化尝试(解决类别不平衡，因为O类别太多了);BERT+MRC;(改天我可能要精读一下，大佬写了很多内容，感觉有很多细节可以挖)\n如何通俗地讲解 viterbi 算法？\t讲解了维特比算法，维特比一般是用于解码\n小标注数据量下自然语言处理实战经验\t小标注数据如何处理\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/命名体识别/命名体识别资源梳理（代码+博客讲解）/"},{"title":"","date":"2024-06-21T03:20:45.133Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.133Z","content":"背景介绍\n建了仓库，地址在这里:\nhttps://github.com/DA-southampton/ner\n点star 不迷路，相关文章在github上更新的会更频繁一点QAQ\n之前做过一段时间的命名体识别，项目背景其实也很简单，就是我要做一个关键词匹配的功能，第一步我需要挖掘关键词。数据调研之后发现对于一部分领域文本，比如说娱乐领域，明星领域，财经领域等等吧，这些领域的文本很有特色，一般人名/地名/公司名称/书名/电影名称都可以很好的表示文本关键信息。\n在这种项目背景之下，很自然的就会想到使用命名体识别。我把在做这个项目的过程中，积累的一些资料总结了一下，希望对大家有所帮助。\n关于命名体识别，这是个很大的领域，要是做好，有很多工作要做。标题完全是为了能增加曝光，自己还是知道只是一个小学生，我会把自己看过的有用的东西都列出来，给大家提供一些先验信息。\n之后看到的关于nert的文章会在此基础继续更新（最近存了好多新文章还没看/苦逼码农/QAQ），不过建议大家star一下Github，不迷路，我给自己的计划是精读一些论文和博客，做一些思维导图，复现一些代码，我会努力的。\n微信公众号: NLP从入门到放弃\n（我果然是个渣渣，公众号名字都这么渣…欢迎关注）\n经验介绍\n对于命名体识别的代码这一块，我大概的经验就是，工作中很少直接就上复杂模型，一般都是先来简单模型，然后在优化迭代。我给个大概的方向（大家视情况而定）：\n词典匹配–&gt;HMM/CRF–&gt;BiLSTM-CRF–&gt;Bert系列\n一般来说词典匹配是最简单的，也是最快的。不过很依赖于你的词典情况。一般来说，词典的补充需要你自己搞定，比如找相关的运营人员/产品人员，因为他们比较靠近一线工作，手上会积累一些相关的词典。或者使用合法爬虫手段（至于如何合法就自己考虑吧）去专业的垂直领域网站获取数据补充词典。\n我大概分为两个个模块，第一个是各种模型的代码实现相关资源，第二个就是关于命名体识别基础知识之类的相关资源\n代码实现\n代码不再多，把一个反复看，看懂了，自己能写出来做二次开发就可以，不要今天看一个代码明天换一个代码看（小声嘟囔）\n\n\nBert系列 (Bert/Albert-softmax/CRF/Span/Span+focal_loss/Span+label_smoothing)做命名体识别\n仓库下面有Bert系列完成命名体识别的效果对比（一般来说看F1就可以）以及训练时间之类的比较，很推荐大家去看一看\n\n\n\n\nBiLSTM-CRF实现命名体识别(Pytorch版本)\nBiLSTM-CRF我就推荐这一个吧，其他的都是大同小异，大家可以一步步去调试，做二次开发就可以，比如换个损失函数之类的。\n\n\nNLP实战-中文命名实体识别-HMM/CRF 代码的实现\n(引用原文)本文章将通过pytorch作为主要工具实现不同的模型（包括HMM，CRF，Bi-LSTM，Bi-LSTM+CRF）来解决中文命名实体识别问题，文章不会涉及过多的数学推导，但会从直观上简单解释模型的原理，主要的内容会集中在代码部分。\n\n\n隐马尔可夫模型命名实体识别NER-HMM-1  [隐马尔可夫模型命名实体识别NER-HMM-2\n不愿意看书想看视频的同学可以看一下这个，B站首页偶然推荐给我的（推荐算法精准石锤了），讲的确实好\n\n\n双向最大匹配和实体标注：你以为我只能分词？------这个是词典方法命名体识别\n这个作者总结了自己实体词典+jieba词性标注进行实体自动打标，有Python代码实现，大家可以关注一下这个博主，名字叫“叫我NLPer”，行文很有意思\n\n\n基本上代码，我觉的看上面几个就够了吧，反复咂摸一下。\n博客讲解\n有些时候看到有人说，要想对某个概念真正有所了解，一定要看原论文。这句话肯定没错，但是不是有些时候没时间看论文吗（哭了苦逼码农）。\n而且有些博客讲的是真的好啊。我大概罗列一些我局的真心不错的文章，主要就是HMM/CRF/Bilstm-CRF\n\n\n概率图模型体系：HMM、MEMM、CRF\n这个文章传播的比较广，讲的确实比较详细，不过大佬写的有些地方还是有些小问题，大家自己去挖掘吧。。。\n\n\n\n\n最通俗易懂的BiLSTM-CRF模型中的CRF层介绍-孙孙的文章\n这个文章讲的是对CRF模型的讲解，翻译的外文，原文很精彩，看译文也可以。大概讲一下，在看的过程中，要多琢磨。比如CRF有个特点全局归一化，这是区别于MEMM模型的；比如在代码实现的时候，我们一般都是使用log，所以乘法会对应加法，这样你在看源代码时候就不会懵逼；比如CRF损失函数有两个部分组成，分别有啥作用。\n\n\nner自动化打标方法\n叉烧大佬讲了一下如何用词典+最大逆向匹配做命名体识别整体思路，代码的实现可以参考第一部分那个Python代码\n\n\n中文NER任务实验小结报告——深入模型实现细节\n作者写了一下自己在做命名体识别的时候针对Bert的优化:BERT+CE_loss;BERT+lstmcrf;尝试用更少的标签列表;对损失函数进行了优化尝试(解决类别不平衡，因为O类别太多了);BERT+MRC;(改天我可能要精读一下，大佬写了很多内容，感觉有很多细节可以挖)\n\n\n如何通俗地讲解 viterbi 算法？\n讲解了维特比算法，维特比一般是用于解码\n\n\n小标注数据量下自然语言处理实战经验\n小标注数据如何处理\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/命名体识别/工业级命名体识别的做法/"},{"title":"","date":"2024-06-21T03:20:43.233Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.233Z","content":"大概会花一到两周的时间，把 transformer 系统的讲一遍，可能会涉及到到 Bert/GPT 的一些基本知识，每天只讲一个知识点。\n所有的关于NLP知识的文章都会放在下面这个仓库，大家快去看。\nhttps://github.com/DA-southampton/NLP_ability\n预告一下明天内容，是关于transformer位置编码的讲解，很多同学对位置编码这个概念很模糊，只是知道是正余弦函数，别的就不太清楚，我们之后花几篇文章好好聊一聊这个概念。这个已经更新在github，想看的朋友可以提前去看一哈。\n正文\nTransformer 分为两个部分，encoder 侧 和 decoder 侧。今天，我们聊一下 encoder 侧。这部分由 N 个完全相同的大模块堆叠而成（原论文N=6）。\n这个结构怎么理解？这个构造就需要我们确保每一个模块的输入和输出维度是相同的，在实现代码的时候，我们只需要完成一个模块的代码的构造就可以。\n注解：你可以把这个过程想象成 RNN 竖过来的一个流程，是不是就很好理解（当然这样想只是帮助你理解）。\n其次对于这每一个大的模块，又分为两个模块，分别是多头注意力层和前馈神经网络层。进一步拆分，多头注意力层可以分为注意力层和 Add&amp;Norm 层。前馈神经网络可以分为 Linear 层和 Add&amp;Norm 层。\n多头注意力层，核心点在于 Q/K/V 三个矩阵，其中 Q/K 矩阵生成权重矩阵(经由softmax)，随后和V矩阵得到加权和。\n这个过程重复了 n_heads 次，这个 n_heads 代表的就是头的数目，这里需要注意的是我们需要确保 hidden_size/n_heads 需要为一个整数，不然代码会报错。\nAdd 代表一个残差结构。对于残差结构，可以使得信息前后向传播更加顺畅，缓解了梯度破碎问题。在 NLP 角度来看，残差结构一定程度上促进了 NLP 网络结构向窄而深的方向发展。\n我们可以把 Transformer 和之前的模型对比一下，比如 RNN 模型，一般来说，我们会选择 单层RNN 或者 一个 Bilstm，对于这些比较传统的模型，只是在时间长度上进行了延展，并没有在深度上做的太深。\n所以说，残差结构是有助于网路变深的。\n顺便联想一下 Elmo，使用的是 双层双向lstm，训练起来已经非常慢了，所以对于RNN这种比较传统的模型，做深太难了，GNMT也是用了很多的 tricks 进行加速训练。\nNorm 代表的是 Layer Normalization。为什么这里使用 Layer Normalization，而不是BN，这个后面有文章说，这里直白的回答就是，BN的效果差，所以不用。\n随后多头注意力层的输出经过前馈神经网络。对前馈神经网络，比较简单，我们需要注意的是它分为两个 Linear 层，第一层的激活函数为 Relu，第二层没有使用激活函数。\n最后我们谈一下整个encoder的输入和输出。\n先说输入，分为两个部分：word embedding 和 position encoding\nword embedding 没什么可说的，初始化后跟着训练或者使用word2vec这种已经有的看具体任务的效果。\nposition encoding 这里 transformer 使用的是 正余弦函数进行表达。其实这里进行初始化然后进行训练也是可以的，论文原作者的实验表明效果基本没区别。\n对于 position encoding 表示的绝对位置，这点大家都没异议，那么 position encoding 究竟有没有表达相对位置信息，之后会有个文章专门讲讲这个知识点。\n然后说一下 encoder的输出，感觉很少有人谈到这里。\nencoder 的输出需要注意的细节点在于它需要和 decoder做交互，所以它的输出为 K/V 矩阵，记住这个细节点，Q 矩阵来自decoder模块，K/V矩阵来自encoder。\n写到这里，我估摸这三分钟差不多能看完，现在没有留言功能，有问题大家在公众号对话框发送，我后台能看见。\n能点个在看，老铁们 ！！鞠躬感谢！！\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/3分钟从零解读Transformer的Encoder/"},{"title":"","date":"2024-06-21T03:20:45.173Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.173Z","content":"命名体识别\n关于这一块，主要是参考了美团ner的文章，写的非常的好：\n美团搜索中NER技术的探索与实践\nhttps://tech.meituan.com/2020/07/23/ner-in-meituan-nlp.html\n中文NER的正确打开方式: 词汇增强方法总结 (从Lattice LSTM到FLAT) - JayLou娄杰的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/142615620\n实际工作中做实体识别，分为两个方向：词典匹配和模型预测。一般情况下，两者会被同时使用，相辅相成。\n方法：词典匹配+模型预测\n首先聊一下为什么使用词典匹配。对于词典匹配来说，很简单就是，来一个句子，看句子中有没有含有我的字典中的词汇，如果有，直接输出就可以，这个过程匹配速度很快，不存在性能上的瓶颈。\n这个时候，我们要思考一个问题，词典匹配上线不存在性能上的瓶颈，那么它的瓶颈在哪里？\n首先，语义消歧问题：我的词典是分类别的，就是说针对不同的垂直领域我会有不同的词典列表，即美食含有一个词典列表，景点含有一个列表，以此类推，类别越多，你的垂直度就会越好。美团文章有这么一个例子，query是”黄鹤楼美食“，那么在词典匹配的时候会出现这样一种情况，就是景点词典匹配上了”黄鹤楼“，美食词典也匹配上了”黄鹤楼“（可能是北京一个美食商家名称）。\n那么，我们选择哪一个作为输出？这个例子就显示了一个问题，词典匹配不能解决语义消歧功能。而模型预测能够有泛化能力。\n其次，词典数量有限，泛化能力比较差：词典效果再好，但是它的数量是有限的，这就会出现OOV情况。缓解这个问题，有两种办法，一个是来做实体挖掘的方式不同的补充实体库，第二个就是使用深度学习的方式进行模型预测实体。\n这个时候，会思考一个问题，就是词典匹配和模型预测两路的结果，如何合并输出？\n美团是训练了要给CRF打分器，这一步我猜测是这么做的：\n对于我来说，两者完全可以这么做：词典匹配全部输出，模型预测提高阈值，至于这个阈值输出的阈值就看你自己去调了。\n词典匹配\n离线挖掘补充实体库\n词典匹配的一个瓶颈问题就是数量有限，OOV问题会比较明显。也就是说，有些词语不是那么正常，比如”爷青结“，但是这些词有需要补充到实体库。这个时候就需要我们离线的从数据中对实体进行挖掘。\n离线挖掘首先面临的一个问题是数据问题。数据如果是结构化数据，就很好办了。比如说直接从电影榜单获取到电影实体，从电视剧榜单获取电视剧的实体等等吧，这个没有什么难度。\n如果数据是非结构的数据怎么办？什么叫做非结构化数据呢？比说微博的博文文本，这个是UGC内容，就是我们平常说的话。\n从非结构化数据中提取出实体才是我们想要的东西。这个过程应该是分为两个步骤的，首先第一步，提取实体，第二步我们需要对实体分类。也就是我们的实体是需要对应到不同的类别词典中。\n美团在这一点说自己使用的是新词发现的一个流程来做实体识别。其实我仔细思考了他的这个流程，它和常规的新词发现还不太一样。\n首先，我们知道新词发现一般来说分为有监督和无监督。有监督就是序列标注，进行中文分词，结果中不再词库的就是我们的新词。无监督就是凝固度和自由度来评判词汇是不是新词。\n这个是新词发现的流程，但是我们要做的是找到新的实体，如果仅仅是做新词发现，肯定不能保证你挖掘出来的新词就是一个实体类别，也有可能是一些不是实体的那种网络新词。\n所以美团这边只是借鉴了新词发现的一部分。\n它的具体流程是这样的：\n\n\n挖掘频繁集合作为候选\n\n\n候选集中的词语和已有积累的实体交集作为正样本。比如”烧烤“在频繁集合中有，在已有的实体词典中也有，就是一个正样本。基于负采样生成负样本。\n\n\n提取正负样本四个维度特征训练二分类判断是不是一个实体。\n\n\n注意看到这第三点，从这个点，我发现一个问题。学习的目标是什么？二元分类判断词汇是不是实体。如果是实体，这个实体的数据来源于哪里？是交集，所以从本质上是已积累的实体。所以，我们相当于在挖掘UGC内容中，和我已经积累的实体\n有相同特性的实体，至于这个实体是不是一个新词，不是我们考虑的。\n负样本中也有部分是高质量实体，也就是说我挖掘出来的高频繁集合有些也是比较好的实体，但是由于没存在交集中，所以被认为是负样本了，所以这个时候可以使用集成多个弱分类器的方式减少误差。\n接下来，我们使用的是Bert做了一个短语质量评分。对于这一个部分，其实很有意思，经过上面这个步骤，我们获取到了大量的正负实体。我们可以这样想一下，美团搜索其实有这样一个特点，就是说，我们基本上的搜索和大搜很不一样，我们的搜索都很垂直，\n而且很短，都是很有意义的，比如我直接就是找某个商家名称，某个地方，这就是一个实体。\n所以，我们完全可以把搜索次数大于一定阈值的词条作为一个实体，而且这个实体天然就具有高质量，因为是人搜出来的。美团做了这样一件事情，把这个搜索记录和正正实体的交集作为正样本，把负实体中减去搜索记录作为负样本，做一个短语质量评分。\n这里的短语质量评分在我看来更像是一种判断实体是不是符合语言模型的标准。\n在预测的时候，我是这么想的，我们首先筛选出来正实体，然后短语质量打分挑选出高质量实体。\n在得到实体之后，我们要做的一个事情就是对实体进行分类，放到不同类别的词典中去。 这一块美团使用的autoner，这个我待定更新\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/命名体识别/词典匹配+模型预测-实体识别两大法宝/"},{"title":"","date":"2024-06-21T03:20:43.713Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.713Z","content":"Transformer的并行化\n正文\n本文主要谈一下关于 Transformer的并行化。文章比较短，适合大家碎片化阅读。\nDecoder不用多说，没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。\n对于Encoder侧：\n首先，6个大的模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。\n从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。\n当然对于注意力层在做attention的时候会依赖别的时刻的输入，不过这个只需要在计算之前就可以提供。\n然后注意力层和前馈神经层之间是串行，必须先完成注意力层计算再做前馈神经层。\n有点绕，不知道有没有讲清楚。\n简单讲，就是6个encoder之间是串行，每个encoder中的两个子模块之间是串行，子模块自身是可以并行的。\n系列总结\n整个Transformer这一块基本就是讲完了，基本上可以解决之前那个关于transformer面试题百分之八十的题目。\n至于剩下的题目会放在之后别的模块去讲，比如 wordpiece model 会在总结机器翻译知识点的时候写一下，然后 GPT 会在总结词向量知识点的时候写一下。\n写这个系列过程中，很多朋友也有私信我一些问题，交流过程中，对我自己帮助也很大，能回答的问题我都尽力回答了，也感谢大家的关注。平时工作挺忙的，尽量输出干货，也欢迎大家和我交流问题。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/Transformer的并行化/"},{"title":"","date":"2024-06-21T03:20:43.253Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.253Z","content":"BN踩坑记–谈一下Batch Normalization的优缺点和适用场景\n这个问题没有定论，很多人都在探索，所以只是聊一下我自己的理解，顺便为讲 layer-norm做个引子。\nBN的理解重点在于它是针对整个Batch中的样本在同一维度特征在做处理。\n在MLP中，比如我们有10行5列数据。5列代表特征，10行代表10个样本。是对第一个特征这一列（对应10个样本）做一次处理，第二个特征（同样是一列）做一次处理，依次类推。\n在CNN中扩展，我们的数据是N·C·H·W。其中N为样本数量也就是batch_size，C为通道数，H为高，W为宽，BN保留C通道数，在N,H,W上做操作。比如说把第一个样本的第一个通道的数据，第二个样本第一个通道的数据…第N个样本第一个通道的数据作为原始数据，处理得到相应的均值和方差。\nBN有两个优点。\n第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使损失平面更加的平滑，从而加快的收敛速度。\n第二个优点就是缓解了梯度饱和问题（如果使用sigmoid激活函数的话），加快收敛。\nBN的缺点：\n第一个，batch_size较小的时候，效果差。这一点很容易理解。BN的过程，使用 整个batch中样本的均值和方差来模拟全部数据的均值和方差，在batch_size 较小的时候，效果肯定不好。\n第二个缺点就是 BN 在RNN中效果比较差。这一点和第一点原因很类似，不过我单挑出来说。\n首先我们要意识到一点，就是RNN的输入是长度是动态的，就是说每个样本的长度是不一样的。\n举个最简单的例子，比如 batch_size 为10，也就是我有10个样本，其中9个样本长度为5，第10个样本长度为20。\n那么问题来了，前五个单词的均值和方差都可以在这个batch中求出来从而模型真实均值和方差。但是第6个单词到底20个单词怎么办？\n只用这一个样本进行模型的话，不就是回到了第一点，batch太小，导致效果很差。\n第三个缺点就是在测试阶段的问题，分三部分说。\n首先测试的时候，我们可以在队列里拉一个batch进去进行计算，但是也有情况是来一个必须尽快出来一个，也就是batch为1，这个时候均值和方差怎么办？\n这个一般是在训练的时候就把均值和方差保存下来，测试的时候直接用就可以。那么选取效果好的均值和方差就是个问题。\n其次在测试的时候，遇到一个样本长度为1000的样本，在训练的时候最大长度为600，那么后面400个单词的均值和方差在训练数据没碰到过，这个时候怎么办？\n这个问题我们一般是在数据处理的时候就会做截断。\n还有一个问题就是就是训练集和测试集的均值和方差相差比较大，那么训练集的均值和方差就不能很好的反应你测试数据特性，效果就回差。这个时候就和你的数据处理有关系了。\nBN使用场景\n对于使用场景来说，BN在MLP和CNN上使用的效果都比较好，在RNN这种动态文本模型上使用的比较差。至于为啥NLP领域BN效果会差，Layer norm 效果会好，下一个文章会详细聊聊我的理解。\n列一下参考资料：\n模型优化之Batch Normalization - 大师兄的文章 - 知乎 https://zhuanlan.zhihu.com/p/54171297\n这个文章写的很好，推荐，从BN的特点（ICS/梯度饱和），训练，测试以及损失函数平滑都讲了一下。\n李宏毅- Batch Normalization  https://www.bilibili.com/video/av16540598/\n大佬的讲解视频，不解释，推荐\n各种Normalization - Mr.Y的文章 - 知乎 https://zhuanlan.zhihu.com/p/86765356\n这个文章关于BN在CNN中使用的讲解很好，推荐一下。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/BN踩坑记--谈一下Batch Normalization的优缺点和适用场景/"},{"title":"","date":"2024-06-21T03:20:43.783Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.783Z","content":"Transformer面试题全部答案解析\n202007更新–如果对您没有帮助，你点个关闭页面就可以。如果您觉得需要这个东西，不需要您说谢谢，不过至少不太希望换来您的嘲讽。我之前在知乎上分享的是微信文章的界面，但是被知乎认定为含有垃圾广告营销，因为是个小白，不知道问题出在了哪里，改了好几次没改对被禁言了一天。后来想了这个折中的办法，跳转到了这个页面。\n公众号：NLP从入门到放弃\n大家去公众号后台回复 “答案解析” 四个字，获取对应的网盘链接。\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/Transformer面试题全部答案解析合辑/"},{"title":"","date":"2024-06-21T03:20:43.693Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.693Z","content":"\n\ntransformer/bert资源总结\n\n\n\n\n\nTransformer改进之相对位置编码(RPE)\n\n\n\n\n\n\n\n\n\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/transformer-bert资源总结/"},{"title":"","date":"2024-06-21T03:20:43.673Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.673Z","content":"NLP任务中，layer-norm比BatchNorm好在哪里\n本文主要是讲一下，为什么NLP任务中，比如Transformer，使用LayerNorm而不是使用BatchNorm\n这个问题其实很有意思，理解的最核心的点在于：为什么LayerNorm单独对一个样本的所有单词做缩放可以起到效果。\n大家往下慢慢看，我说一下我自己的理解，欢迎大佬拍砖，如果觉得我说的还行，点个在看鼓励一下。\n为啥BN在NLP中效果差\n上一个文章有说 BN的使用场景，不适合 RNN这种动态文本模型，有一个原因是因为batch中的长度不一致，导致有的靠后面的特征的均值和方差不能估算。\n这个问题其实不是个大问题，可以缓解。我们可以在数据处理的时候，使句子长度相近的在一个batch，就可以了。所以这不是为啥NLP不用BN的核心原因。\n回忆一下上个文章中，BN在MLP中的应用。 BN是对每个特征在batch_size上求的均值和方差。记住，是每个特征。比如说身高，比如说体重等等。这些特征都有明确的含义。\n但是我们想象一下，如果BN应用到NLP任务中，对应的是对什么做处理？\n是对每一个单词！\n也就是说，我现在的每一个单词是对应到了MLP中的每一个特征。\n也就是默认了在同一个位置的单词对应的是同一种特征，比如:“我/爱/中国/共产党”和“今天/天气/真/不错”\n如何使用BN，代表着认为 &quot;我&quot;和“今天”是对应的同一个维度特征，这样才可以去做BN。\n大家想一下，这样做BN，会有效果吗？\n不会有效果的，每个单词表达的特征是不一样的，所以按照位置对单词特征进行缩放，是违背直觉的。\nlayner-norm 的特点\nlayner-norm 的特点是什么？layner-norm 做的是针对每一个样本，做特征的缩放。换句话讲，保留了N维度，在C/H/W维度上做缩放。\n也就是，它认为“我/爱/中国/共产党”这四个词在同一个特征之下，所以基于此而做归一化。\n这样做，和BN的区别在于，一句话中的每个单词都可以归到一个名字叫做“语义信息”的一个特征中（我自己瞎起的名字，大家懂就好），也就是说，layner-norm也是在对同一个特征下的元素做归一化，只不过这里不再是对应N（或者说batch size），而是对应的文本长度。\n上面这个解释，有一个细节点，就是，为什么每个单词都可以归到“语义信息”这个特征中。大家这么想，如果让你表达一个句子的语义信息，你怎么做？\n最简单的方法就是词语向量的加权求和来表示句子向量，这一点没问题吧。（当然你也可以自己基于自己的任务去训练语义向量，这里只是说最直觉的办法）\n上面这个方法就是出于每个单词都是语义信息的一部分这个insight。\n引申-为啥BN在CNN可以而在NLP不可以\n但是，我还想问一个问题，CNN中证明BN效果是很好的，NLP中的文本可以类比为图像，为什么BN在图像中效果好，在文本上效果差。\n我是这样理解的。还是回到刚才，BN是对单词做缩放，在NLP中，单词由词向量来表达，本质上是对词向量进行缩放。词向量是什么？是我们学习出来的参数来表示词语语义的参数，不是真实存在的。\n这就是NLP和图像的一个区别，图像的像素是真实存在的，像素中包含固有的信息。比如说，一张图像，最上面的一行像素，可以归为背景这个特征（这里只是为了理解，CNN做BN是基于整个feature map，而不是单独某一行像素）。\n这个理解不确保正确，只是我自己的理解（记得是从一个知乎答案看到的，改天好好找一找）\n简答说一下\n写到这里，我写文章不是为了推导公式，因为这种推导文章太多了，而是想让大家看了我的文章之后再去看这些推导公式能够更加容易理解。\n然后大家有问题的话，私信和我说，我也知道我自己写的哪里有问题，好改进。\n点个在看再走呗，老弟\n列一下参考资料：\n各种Normalization - Mr.Y的文章 - 知乎 https://zhuanlan.zhihu.com/p/86765356\n这个文章关于BN和LN如何应用讲解的比较好，就是CNHW\nNLP中 batch normalization与 layer normalization - 秩法策士的文章 - 知乎 https://zhuanlan.zhihu.com/p/74516930\n这个文章也还行，我在看的时候，看到中间那个图给了我点启发，就是在理解BN的时候，仅仅是在这个时候啊，我们的C，在CNN中是通道数，在理解BN的时候，理解为句子长度，这样”，每个样本通道数为 C，高为 H，宽为 W。对其求均值和方差时，将在 N、H、W上操作，而保留通道 C 的维度。具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道 …… 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值“这句话才比较好理解。\n一般NLP来说，C为1吧。\n模型优化之Layer Normalization - 大师兄的文章 - 知乎 https://zhuanlan.zhihu.com/p/54530247\n推荐一下这个文章，总结了对比实验：”这里我们设置了一组对照试验来对比普通网络，BN以及LN在MLP和RNN上的表现“，我还没细看，之后看。\ntransformer 为什么使用 layer normalization，而不是其他的归一化方法？ - pymars的回答 - 知乎 https://www.zhihu.com/question/395811291/answer/1260290120\n推荐这个答案，很好\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/NLP任务中-layer-norm比BatchNorm好在哪里/"},{"title":"","date":"2024-06-21T03:20:43.733Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.733Z","content":"transformer 资源总结\ntransformer中的positional encoding(位置编码)\nhttps://blog.csdn.net/Flying_sfeng/article/details/100996524\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/transformer资源总结/"},{"title":"","date":"2024-06-21T03:20:43.833Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.833Z","content":"VIT：如何将Transformer更好的应用到CV领域\n大家好，我是DASOU；\n最近因为在做TRM在多模态视频的分类，会写一些TRM在CV中的应用，今天先来讲一下VIT；\n论文名称是：AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\n这个论文看下来，有这么几个重点需要去掌握：\n\n将整张图片转化为多个patches，作为trm的序列输入\n输入的时候需要加入位置编码，三种位置编码：一维，二维，相对位置编码没有太大区别\nTRM可以接受CNN的输出作为输入，作为一种TRM的混合结构，区别于VIT这种无卷积结构\n可能是由于缺乏inductive biases，数据集上直接训练的VIT效果一般，需要先在大数据及上做预训练然后在任务数据上做微调才可以达到不错的效果\nVIT的【CLS】可有可无\npatches重叠与否区别不是特别大；\n\n1. 简单背景介绍\n在CV领域，CNN一直是主流模型；\nTRM的最核心的一点就是自注意力机制，把这点借鉴到CV来说，一个最简单的想法就是我把每个像素当做是一个token，然后作为序列输入；\n那么就是对每个token之间都做了多头注意力机制；假设我们的图像大小是2242241，那么序列长度就是50176，相当于BERT最大长度的512的100倍左右，这个参数量肯定是不能承受的；\n针对这种情况，我们怎么处理呢？这个问题，本质上是去解决随着像素增加，复杂度平方级增长的问题；\n一个改进就是将全局的这种注意力机制改为局部的注意力机制，也就是做token周围几个领域tokens之间的注意力机制；\n还有一种改进是做稀疏注意力，是对注意力做了改进，本质在缓解TRM模型随着长度的增加，Attention部分所占用的内存和计算呈平方比增加的问题。\n这几种改进思路可行，但是实施复杂；\n所以一个比较简单的方法，就是将整个图像化整为零，从一整张图片转化为一个个的patch，也就是一个个的小方块；\n直接看下面这个图：\n\n其实在这里我想插一句，我之前在对图片元素做自注意力机制的时候，是对CNN提取图片的特征图，然后做attention；只不过，VIT这个模型在追求的一个特点就是完全抛弃掉卷积这个操作~~\n2. 具体细节\n2.1 模型架构图\n论文中自带的模型架构图已经足够清晰，我直接搬过来，然后一点点去讲一下：\n\n我们通过形状来了解一下数据的流动情况：\n首先我们有一张图片，形状为:$$HWC$$ ;其中H是高度，W是宽度，C是通道数量；\n然后我们把这个图片转化为一个个的pathes，其中每一个patch的形状是$$P^{2}*C$$ ; P是每个patch正方形的边长；\n然后可以将多个通道压扁，转化为一个一维形状：$$P^{2}.C$$ ;注意，这里就类似变成了一维数组；\n总共有N个patches；\n我们TRM的输入定为维度为$$D$$大小的token，那么我们就需要对每个一维数组$$P^{2}.C$$ 做一个linear映射到$$D$$大小；\n在TRM的输入中，处理token的embedding，其实还有一个是位置编码，VIT使用的就是简单的一维位置嵌入，映射到D维度就够了；这里论文提了一下，因为原始信息是图片，所以尝试了二维编码，但是没有明显提升；\nVIT学习BERT，在最开始加入了CLS符号；\n看到这点我其实疑惑了一下，BERT中加入CLS的一个原因是它预训练的时候使用了NSP，CLS的输出可以作为二分类的任务；但是图片这里显然没有第二张图片，所以加入CLS的解释就变成了想使用元素之间的注意力学习到所有tokens的信息；\n2.2 位置编码消融实验\n还有一点就是位置编码这块，作者做了消融，有四组实验，分别是：没有位置编码，一维位置编码，二维位置编码，以及相对位置编码；\n在位置编码这里，作者还做了另外三组实验，就是只在最开始输入的时候加入位置编码，在每一层加入位置编码同时各自学习，在每一层加入位置编码同时共享这个位置编码；\n实验结果看下面这个图：\n\n结论就是没看出太大的区别，直接用一维的位置编码，同时只是在最开始的时候加入位置编码就可以【从结果看每一层位置编码共享效果会更好一点】\n2.3 是否需要加入【CLS】token\nVIT模型为了最小限度的改变trm架构，依旧沿用了bert中的【CLS】中的这个token；但是就像我在最上面说到的，BERT中加入CLS的一个原因是它预训练的时候使用了NSP，CLS的输出可以作为二分类的任务；但是图片这里显然没有第二张图片，所以可以不加如【CLS】token；\n在整合图片信息的时候，两种方式，一种是使用【CLS】token，另一种就是对所有tokens的输出做一个平均，简称GAP；实验结果证明，两者可以达到的同样的效果，只不过要控制好学习率；\n结果看下面这个图：\n\n3. 预训练以及下游微调\n在上面谈到，VIT在小数据上效果一般，需要做一个预训练再来一个微调，效果还不错；\n这个其实不陌生，TRM在文本上也是，小数据不如lstm，bert在大量文本上预训练，学习到了大量的知识，才横扫了NLP；\n直接看图吧，绝大部分效果还是不错的：\n\n参考：&quot;未来&quot;的经典之作ViT：transformer is all you need! - 小小将的文章 - 知乎 https://zhuanlan.zhihu.com/p/356155277\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/VIT-如何将Transformer更好的应用到CV领域/"},{"title":"","date":"2024-06-21T03:20:43.853Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.853Z","content":"原版Transformer的位置编码究竟有没有包含相对位置信息。\n不涉及到公式推导，面试的时候能大致说出来就可以，很少会让推导，尽最大可能让大家明白\n简单概述\nTransformer 原版的位置编码也就是正余弦函数编码，表达的是绝对位置信息，同时包含相对位置信息。但是经过线性变化，相对位置信息消失。基于此，需要对位置编码进行优化。\n正文\n原版位置编码使用的是正余弦函数，通过三角函数，可以得出一个结论就是：$PE_{pos+k}$可以被$PE_{pos}$线性表示。\n从这一点来说，原版位置编码可以反应一定的相对位置信息。\n接下来，我们来看，经过注意力层，这个相对位置信息还在不在？\n很简单，把词向量和位置向量作为输入，经过注意力层，然后因式分解，得到四个部分，我们重点关注包含两个不同位置编码的公式部分，形式如下：\n$PE_{pos}^{T}W_{q}^{T}W_{k}PE_{pos+k} \\tag{1}$\n我们想要证明，这个公式能不能反应相对位置信息。\n为了解决这个问题，我们化繁为简，先从下面这个公式入手：\n$PE_{pos}^{T}PE_{pos+k} \\tag{2}$\n注意看公式(1)和公式(2)的区别，在中间多了两个矩阵相乘，这两个矩阵，是我们的Q/K矩阵，可以看做是一个线性变化，记住这个细节点。\n经过公式推导，我们很容易知道公式(2)最后的结果只和两个位置的相对位置 $k$ 相关，这个结果是包含相对位置信息。也就是说两个不同位置$PE$的点积的结果可以反映相对距离。\n通过实验我们知道这个结果大小随着相对距离的增大而减小，值得注意的是它并不能反映相对位置的方向，因为他是一个对称的。\n具体的我们可以看下面这个图：\n\n很好，接下来，我们就是证明本来可以反映相对位置信息的公式(2)，在加上中间这个线性变化之后，相对位置信息还在不在。\n直接看效果图：\n\n这个图需要重点看的下面两个，也就是加了线性变化之后，变化趋势从最上面蓝色图标的线变成了下面两条线，也就是趋势已经完全没有了。\n也就是说，实验结果显示，公式(1)的结果随着 k 的变化没有明显的趋势变化，也就是说相对位置信息消失了。\n上面这些内容，估计5分钟左右吧，本来想加上相对位置编码，不过内容也挺多的，下回再发吧。\n同学们，如果觉写的还行，给个在看。\n参考链接：\n一文读懂Transformer模型的位置编码\n这个文章写的不错，主要是给出来正余弦函数表达相对信息的公式推导\n浅谈Transformer模型中的位置表示\n哈工大的SCIR写的文章，不错，从正余弦函数位置信息和相对位置信息和transformerx-l都讲出来了\nTransformer改进之相对位置编码RPE\n这个文章很好，讲了位置编码的几种优化，值得好好看看推导一下公式\n如何优雅地编码文本中的位置信息？三种positioanl encoding方法简述\n夕小瑶的文章，讲了三种位置编码，还可以，没事的时候可以看看\n[相对位置编码一)Relative Position Representatitons RPR - Transformer\n大佬讲了一下相对位置编码，很好，推荐\n相对位置编码(二) Relative Positional Encodings - Transformer-XL\n大佬讲的transformer-xl，推荐\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/原版Transformer的位置编码究竟有没有包含相对位置信息/"},{"title":"","date":"2024-06-21T03:20:43.873Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.873Z","content":"史上最全Transformer面试题\n\nTransformer为何使用多头注意力机制？（为什么不使用一个头）\nTransformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？\n（注意和第一个问题的区别）\nTransformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n在计算attention score的时候如何对padding做mask操作？\n为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）\n大概讲一下Transformer的Encoder模块？\n为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？\n简单介绍一下Transformer的位置编码？有什么意义和优缺点？\n你还了解哪些关于位置编码的技术，各自的优缺点是什么？\n简单讲一下Transformer中的残差结构以及意义。\n为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？\n简答讲一下BatchNorm技术，以及它的优缺点。\n简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？\nEncoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）\nDecoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)\nTransformer的并行化提现在哪个地方？Decoder端可以做并行化吗？\n简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？\nTransformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？\n引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/史上最全Transformer面试题/"},{"title":"","date":"2024-06-21T03:20:43.963Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.963Z","content":"谈一下相对位置编码RPR\n经过线性变化之后，正余弦函数表示的相对位置信息消失，所以需要优化。\n一般来讲，谈到优化，三种比较有名：RPR； Transformer-XL；complex embeddings；\n我在这个文章简单讲一下RPR。\n老样子，不涉及到公式推导，尽量把我的理解讲出来。\nRPR思路\nRPR思路很简单，原始正余弦函数，是在输入的的时候与词向量相加作为输入，在attention丢失相对位置信息.\n改进的话就是不在输入的时候进行位置编码，而是在attention中显示把相对位置信息加入进去。\n如何理解相对位置\n绝对位置编码是在每个位置都对应一个唯一的位置编码信息，RPR把这一部分去掉去学习一个相对位置编码。\n首先我们需要知道相对位置是有方向的的。\n举个例子：”我/爱/中国/共产党“\n”我“对”爱“的相对位置就是 -1， ”中国“对”爱“的相对位置就是 1。\n所以方向不同，对应两个不同的相对位置，在学习的时候，一个距离，也就需要学习两个相对位置编码。\nRPR修改思想\n作者认为在相对位置小于4的时候，attention对相对位置比较敏感，在大于4之后，相对位置不敏感。所以窗口设置为4。\n需要注意的是，窗口设为4，代表的当前位置左边4个，右边也有4个，再加上自己，就是一共9个位置，也就是：\n$[i-4,i-3,i-2,i-1,i,i+1,i+2,i+3,i+4]$\n注解：有方向\n当你的attention进行到哪个单词的时候，你的 $i$ 就对应的是哪个位置。\n还是上面那句话举例子。\n如果此时的输入是“我”，那么用到的相对位置编码就是$[i,i+1,i+2,i+3]$\n如果此时输入的是“爱”，那么这个时候用到的相对位置编码就是$[i-1,i,i+1,i+2]$\n了解了这个，我们再谈一下这个相对位置信息是怎么显示加入进去的。\n这个显示的加入分为两个部分。\n第一个部分是在计算$e_{ij}$的时候，涉及到RPR的一个表征:$a_{ij}^{K}$，表示对 Q/K/V三者中的K做了修改。\n第二个部分就是在计算$z_{i}$的时候，涉及到另一个RPR的表征:$a_{ij}^{V}$，表示对Q/K/V三者中的V做了修改。\n两个部分的修改都是使用加法。\n关于RPR大概就讲这么多吧。其实思路还是比较简单的，总结来说，就是把相对位置信息在attention之中，显势的加入进去，而不是在输入的时候与词向量相加。\n如果觉得对您有点帮助，点个赞再走吧。\n参考资料\ndasou\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/谈一下相对位置编码/"},{"title":"","date":"2024-06-21T03:20:43.953Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.953Z","content":"答案解析(1)—史上最全Transformer面试题：灵魂20问帮你彻底搞定Transformer\n简单介绍\n之前的20个问题的文章在这里：\nhttps://zhuanlan.zhihu.com/p/148656446\n其实这20个问题不是让大家背答案，而是为了帮助大家梳理 transformer的相关知识点，所以你注意看会发现我的问题也是有某种顺序的。\n本文涉及到的代码可以在这里找到：\nhttps://github.com/DA-southampton/NLP_ability\n问题基本上都可以在网上找到答案，所以大家可以先去搜一搜，自己理解一下，我也不会重新把答案回答一遍，而是列出来我看到的比较好的回答，然后加上点自己的注解帮助大家理解，在这里感谢那些大佬回答者，今天整理了其中的五个，剩下的我抽空在整理一下。\n这里我先小声说一下，写这些笔记有两个目的。\n一个是方便大家，好多题目都太散了，没有人归纳一下。\n二个就是方便自己重新复习一遍，所以我也不可能是直接把答案一粘就完事，这对我自己就没啥帮助了。所以没啥别的目的，不是为了博关注粉丝之类的，因为这些如果做不到大V基本没啥用，我也没那时间去经营成为大V，工作忙的要死，就是想要有个一起沟通的渠道而已。\n公众号/知乎/github基本同步更新，大家关注哪一个都可以，不过可能微信链接跳转不方便，知乎编辑不方便，github对有些同学不太方便打开。大家看自己情况关注吧。\n正文\n1.Transformer为何使用多头注意力机制？（为什么不使用一个头）\n答案解析参考这里：为什么Transformer 需要进行 Multi-head Attention？\nhttps://www.zhihu.com/question/341222779\n注解：简单回答就是，多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。其实本质上是论文原作者发现这样效果确实好，我把作者的实验图发在下面：\n\n2.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？\n答案解析参考这里：transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？ - 知乎\nhttps://www.zhihu.com/question/319339652\n注解：简单回答就是，使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。\n3.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n答案解析：为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的效果越显著。更具体的结果，大家可以看一下实验图(从莲子同学那里看到的，专门去看了一下论文)：\n\n4.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n答案解析参考这里：transformer中的attention为什么scaled? - LinT的回答 - 知乎\nhttps://www.zhihu.com/question/339723385/answer/782509914\n注解：针对大佬回答的第二个问题，也就是方差的问题，我简单的写了一个代码验证了一下，不愿意看公式推导的同学直接看代码结果就可以。代码如下:\n123456import numpy as np arr1=np.random.normal(size=(3,1000))arr2=np.random.normal(size=(3,1000))result=np.dot(arr1.T,arr2)arr_var=np.var(result)print(arr_var) #result: 2.9 (基本上就是3，和就是我们设定的维度)\n5.在计算attention score的时候如何对padding做mask操作？\n答案解析：padding位置置为负无穷(一般来说-1000就可以)。对于这一点，涉及到batch_size之类的，具体的大家可以看一下抱抱脸实现的源代码，位置在这里：\nhttps://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720\n这个是最新版，比较老版本的实现地址我也罗列一下，应该没啥区别，我没细看，一直用的老版本的：\nhttps://github.com/DA-southampton/Read_Bert_Code/blob/0605619582f1bcd27144e2d76fac93cb16e44055/bert_read_step_to_step/transformers/modeling_bert.py#L607\n参考链接：\n关于Transformer，面试官们都怎么问？\n写的很好，面试题总结的很好，把整体梳理了一遍。\n关于Transformer的若干问题整理记录 - Adherer的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/82391768\n关于Transformer的若干问题整理记录 - Adherer的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/82391768 和上面是一个文章，在知乎\nTransformer的细节与技巧 - 沧海一栗的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/69697467\n讲了几个代码上的小细节\nNLP预训练模型：从transformer到albert - Serendipity的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/85221503\n大佬主要是大白话讲了一下代码的实现，包括维度的变化\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/答案解析—史上最全Transformer面试题：灵魂20问帮你彻底搞定Transformer/"},{"title":"","date":"2024-06-21T03:20:43.983Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.983Z","content":"谈一谈Decoder模块\n本文主要是谈一些比较容易误解的细节点，说实话，把自己的理解用文字表达出来真是个细致活。\n如果觉得对您有点帮助，帮忙点个在看或者赞。\n一个小小的问题\n我先说一个自己花了点时间才琢磨出来的东西，其实不难，就是当时没转过弯来。\n我们都知道，decoder的交互层，Q矩阵来自本身，K/V矩阵来自整个Encoder层输出。\n但是对于每个单词都会有一个encoder的输出，那么K/V矩阵是用的其中哪个输出计算过来的？\n我这个问题的问法其实是错误的。\n我当时的理解背景是认为这个交互的过程很类似seq2seq的attention，它一般是使用最后一个时刻的隐层输出作为context vector。\n我基于此产生了上面这个问题，这个K/V矩阵应该由哪个位置单词（对比RNN就是哪个时刻的单词）的输出生成。\n后来看了一下代码，才明白自己错在哪里？\nK/V矩阵的计算不是来自于某一个单词的输出，而是所有单词的输出汇总计算K/V矩阵。这个过程和在Encoder中计算K/V矩阵是一样的，只不过放在了交互层，一时没想明白。\n正文\n与Encoder很类似，Decoder同样由完全相同的N个大模块堆叠而成，原论文中N为6。\n每个大的模块分为三部分：多头注意力层，交互层，前馈神经层；每个层内部尾端都含有 Add&amp;Norm。\n和Encoder重复的内容我就跳过了，之前讲过，没看过的同学可以去看那个文章。\n多头自注意力层\n首先谈一下多头自注意力层，这里需要注意的细节点是，需要对当前单词和之后的单词做mask。\n为什么需要mask？\n最常规的解释就是在预测阶段，你的模型看不见当前时刻的输出以及未来时刻单词。\n这句话其实有点绕，如果读的不仔细会让人误解为mask的时候需要把当前时刻的单词也mask掉…(拖出去斩了吧)。\n从代码角度讲，你只需要把当前时刻之后所有单词mask掉就好了。\n我自己对这句话的理解是我们需要确保模型在训练和测试的时候没有GAP。\n举个简单的例子来理解，如果做机器翻译，你需要翻译出来的句子是 “我/爱/吃/苹果”。\n当前时刻是”爱“这个单词作为输入的一部分，另一部分是上一个时刻”我“作为输入的时候的输出值。\n当然在机器翻译中，我们一般使用 teacher forcing加速收敛，所以这里就使用”我“作为当前时刻输入的另一个部分。\n所以这个时候，输入就是”我“的编码信息和”爱“的编码信息（当然还有位置编码）。\n我要预测的是”吃“这个单词。\n如果我们没有mask，模型也是可以运行的，也就说此时”吃“和”苹果“两个词对”爱“这个时刻的输出是有贡献的。\n那么问题来了，测试数据中你根本没有ground truth，你怎么办？\n也就说，训练的时候，你的模型是基于知道这个时刻后面的单词进行的训练，但是测试的时候，做机器翻译，你不知道自己应该翻译出来什么东西。\n这就是问题的核心。\n你训练模型的时候，一部分精力花在了”吃“和”苹果“两个词上，这不就是无用功吗？\n所以，确保模型在训练和测试的时候没有GAP，我们需要mask掉”吃“和”苹果“两个词。\n交互模块\n这一块需要注意的就是之前文章提到的，Q矩阵来自本身，K/V矩阵来自encoder的输出。\n还有一个细节点是，K/V矩阵对应的是来自整个encoder的输出。\n如果看transformer那个经典图的话，初期很容易理解为encoder和decode对应的每一层互相交互，这是不对的。\n是整个输出与decoder做交互。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/谈一谈Decoder模块/"},{"title":"","date":"2024-06-21T03:20:45.243Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.243Z","content":"今天主要聊我在做多模态任务中的六个方面的介绍，如下：\n\n多模态业务简单介绍；\n多模态数据问题；\n如何确保多模态任务的预测速度；\n如何确定多模态任务确实起到了作用；\n多模态中多张图片如何处理；\n交互的时候哪种attention方式更好；\n训练的时候需要注意什么；\n\n1.多模态业务简单介绍；\n之前花了不少时间在多模态这块的落地工作，取得了一定的效果，今天分享一下我的经验；\n首先在调研多模态任务的时候大家可以看一下最近的论文，这两年的多模态任务基本上都在往Transformer上去靠，基本可以分为两种：单流网络和双流网络；\n双流网络就是文本过一个编码器，图片过一个编码器，然后两个编码器的输出进行一个交互；\n单流网络就是文本和图片先concat，然后直接输入到Transformer编码器中，然后输出；\n一般来说，这里的编码器使用的都是TRM结构；\n文本这块，输出的时候得到的是embedding就可以；图片这里，一般来说使用的是Faster-RCNN模型识别出图片中包含的多个物体及其对应的矩形位置信息，把这个作为TRM的输入；\n但是我在真正去做的时候，并没有按照这个思路去做，我是先按照自己的思路做了个baseline，然后有效果，之后再去看论文架构提升模型效果；\n我简单分享一下我的主体思路，文本过的BERT，图像过的Resnet,然后输出的两个表征向量之间做多头注意力，然后接全连接输出logits；\n按照分类，我这个架构应该属于双流网络；\n架构其实很简单，但是在真正去做的时候，真的是比较复杂，有很多细节，我在这里简单的梳理一下，一起探讨；\n2.多模态数据问题；\n多模态一般来说就是双模态数据，我主要接触的是文本+图片；很幸运，我有标注数据~~ 如果没有基于自己场景下的标注数据，还是不太建议强行上多模态任务；\n3.如何确保多模态任务的预测速度；\n为了保证我的预测速度，我不可能所有的case都过多模态网络；所以我做的策略很简答，就是单从文本输出结果置信度不高的而且含有图片信息的case走多模态任务；\n4.如何确定多模态任务确实起到了作用；\n这个问题其实很关键，首先我们当然可以做测试集，验证一下单走文本或者单走图片得到的f1以及做多模态得到的f1，两者一个比较就可以；\n当时确实也这么做了，但是我纠结点在于能不能使用一种可见的方式，告诉大家多模态度确实起到了作用？\n那么一个很有用的方法就是使用attention的可视化；这个方法可以可视化出文本和图片之间确实是有交互的，而且交互的部分是有意义的，比如有的单词就是对图片中的某个部分更加关注；\n5.多张图片如何处理；\n因为我图片过的是Resnet网络，所以输入是多张图片的数量是动态的，这是个问题；\n我们退一步说，按照现在bert多模态预训练中的方法，多张图片完全可以作为transformer中的输入tokens部分；或者把多张图片合并在一起生成一个图片再走正常流程；\n我这边处理的时候需要注意的细节就是resnet输出池化的时候k是个动态的池化就可以；\n6.哪种attention方式更好；\n一般来说做互相之间的交互更好，就是文本对图片做一次attention，图片对文本做一次attention，两者结合来做；\n7.训练的时候需要注意什么；\nbert和resnet网络架构不太一样，训练的时候容易不收敛，需要控制一下不同部分的学习率；\n如上，因为业务的原因，很多东西不能细说，所以我只是大体的介绍了一些自己的经验，希望能对大家有帮助；\n之后我会写一些BERT多模态预训练论文的解读文章，大体是LXMERT，ViLBERT，Unicoder-VL、VisualBERT、VL-VERT、UNITER等等；\n求点赞，求在看，求转发，求一切，爱你们哦~ ~\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/多模态/复盘多模态需要解决的6个问题/"},{"title":"","date":"2024-06-21T03:20:45.263Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.263Z","content":"多模态中各种Fusion骚操作\n大噶好，我是DASOU；\n今天继续写多模态系列文章，对多模态感兴趣的可以看我之前的文章：\n其实对于多模态来说，主要可以从三个部分去掌握它：\n\n如何获取多模态的表示【learning multimodal representations】\n如何做各个模态的融合【fusing multimodal signals at various\nlevels】\n多模态的应用【multimodal applications】\n\n今天我主要放在第二个部分，也就是各个模态的Fusion方式汇总；\nFusion做的事情简单来说就是把不同模态的信息整合为一个信息，得到一个特征向量，然后利用这个特征向量再去做下游任务；\n所以它的任务就是更深的挖掘不同模态信息同时更好的融合进最终的representation;\n我们可以把Fusion分为三种融合方式：\n\n基于简单操作的融合\nAttention-based Fusion\n双线性池化融合\n\n1. 基于简单操作的融合\nSimple Operation-based Fusion 就是说来自不同模态的特征向量可以使用很简单的方式进行整合，比如多个模态的特征向量的拼接，加权和；\n举个简单的例子，比如我们现在做一个图文双模态的分类任务，我们获取了文本特征向量和图片特征向量，那么我们可以把两个特征向量直接拼接，就当做是融合后的向量了；\n如果我认为文本的包含的信息更加的重要，图片包含的信息不是那么重要，我完全可以自定义文本特征向量权重为0.7，图片特征向量权重为0.3，然后两者的向量再concat或者做加权的和；\n其实如果我们自己最开始做一个多模态任务，最先想到的方式就应该是这种基于简单操作的方式；\n但是这个方式存在一个问题，就是两个模态之后没有做足够的交互，两者之间的联系比较弱一点；\n针对这个，我们一般会在得到concat features之后，不会直接去做分类任务，而是再接一个或者几个全连接层，让模型自动的去学习两个模态之间的关系，这样效果会更好；\n这里还有一点需要注意的是，对于concat方式，我们最好是确保文本特征向量和图片特征向量维度是固定的，这样后面接全连接层维度不会出错；\n但是有些时候我们输入的图片数量不固定，那么图片特征向量维度不一定，这个时候操作比较多，举个简单例子可以先做一个max pooling到固定维度再去和文本拼接；\n如果做加权和，我们需要确保文本和图片特征维度是相同的，这个就不多说，很好理解；\n以我自己个人经验来说，在图文多模态分类这个，使用concat这种方式，能比单一的使用文本效果提升不到2个点左右，当然case by case；\n2. Attention-based Fusion\n第一种方式我一般是在任务中作为基线，简单粗暴有提升；之后任务迭代的时候，一般都会往attention上靠一靠；\n因为concat虽然后面加上了全连接层学习两者之间关系，但是在两者的交互上来说还是有点弱的；\n对于attention的操作可以简单分为：1.Image attention；2. Symmetric attention for images and text；3. Attention in a bimodal transformer； 4. Other attention-like mechanisms；\n我详细说一下第三点，就是基于TRM的attention，因为TRM太火了；\n基于TRM的attention这块，从两个类别去理解它，一个是基于TRM的多模态预训练模型，一个是基于TRM的微调模型；\n基于TRM的多模态预训练模型，就是所借助TRM，输入是图片和文本信息，然后做预训练任务，从大量数据中学习到信息，然后得到多模态预训练模型，然后放入到下游任务中去；\n但是这些有个问题，很多人都没有大量的图文平行无监督数据，相反大家一般都有图文平行的标注数据；\n所以我们可以直接借助TRM的结构，直接做下游任务的微调就可以，这一块有个论文是facebook的MMBT；\nMMMBT其实很简单，直接看这个图：\n就是借助bert做初始化，然后图片从resent得到向量输出，一般是三个，然后拼接文本，输入到bert，直接在下游任务做微调；\n在这里我想多说几句，其实还可以直接对文本和图片之间做attention，多头或者单头都可以，其实单头就够了；\n在写代码的时候，我在遇到一个问题，就是文本和图片之间attention的矩阵化，我踩了下坑~~~；\n3. 基于双线性池化的融合办法\n双线性池化也是一个比较受重视的融合方法，不过它的问题就是在于会把n为变成n的平方，复杂度大大提升，后续的改进一般都是在降低复杂度这一块；\n双线性池化最初的操作，就是做向量的外积，获得一个矩阵，然后对矩阵做sum池化，得到特征向量，然后再去做分类；\n如果是在实际业务，大家还是优先考虑前两种吧，双线性池化这个放后一点；\n先写这么多，后续会写一个MMBT论文的解读；\n参考论文：Multimodal Intelligence: Representation Learning, Information Fusion, and Applications\nhttps://arxiv.org/pdf/1911.03977.pdf\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/多模态/多模态中各种Fusion方式汇总/"},{"title":"","date":"2024-06-21T03:20:45.303Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.303Z","content":"多模态资源汇总：\n实战类文章：\nhttps://github.com/DA-southampton/Tech_Aarticle\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/多模态/多模态资源汇总/"},{"title":"","date":"2024-06-21T03:20:45.283Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.283Z","content":"通篇读完这个论文，需要解决如下问题：\n\nViLBERT架构是什么样子的？\nViLBERT预训练任务是什么？\nViLBERT实现细节有哪些？\n\n我之前写了两个多模态基础的文章，没看过的同学可以先看看这两个文章：\n分别是 在解决多模态任务的时候需要重点解决的6个问题 和 如何把BERT的两种预训练任务扩展到多模态数据中去；\n1. ViLBERT架构是什么样子的？\n首先我们来聊第一个问题：ViLBERT架构是什么样子的？\n直接看图：\n\n这个图其实很不错，我简单来概述一下，如下：\n首先ViLBERT包含两个并行的流，上面的那个是图片流，下面那个是文本流；\n每个流是由一些TRM Blocks和  co-attentional TRM layers【Co-TRM】组成；\n需要注意的是TRM Blocks 和Co-TRM 可以是多层的；\n这里面最主要的部分其实就是这个Co-TRM；\n在那个虚线框中，我们可以看到Co-TRM有两个部分，真正的Co-TRM和后连接的TRM；\n首先我们要明确，从图片流前半部分【未交互之前】出来的是一个个图片regions的embeddings；\n从文本流前半部分出来的是一个个文本tokens的embeddings；【需要注意的是文本这有一个L-K X的符号，其实代表的就是构建多层的TRM，在本文就是一个BERT-Base】；\n知道各自流前半部分出来的是什么之后，就到了重头戏上的Co-TRM这个架构，直接来看论文中的图：\n\n其实这个结构很简单，就是在做attention的时候，做一些改动；\n在上面这个图片流，我的Q矩阵来自图片信息，但是我的K和V矩阵来自文本信息；\n在下面这个文本流，我的Q矩阵来自文本信息，但是我的K和V矩阵来自图片信息；\n简单说，就是做了一个在文本条件下的图片的attention和在图片条件下的文本的attention；\n也就是在文本和图片之间做了一个信息的交互；\n这里需要注意的是，在交互之后，各自走自己独立的TRM结构，而并没有拼接在一起走TRM结构；\n我自己在之前的多模态落地讲解文章中有谈到，我的baseline架构和这个很类似，只不过，我是做了双方面的attentinon之后，直接拼接接了任务相关的结构；\n2. ViLBERT预训练任务是什么？\n然后我们再来看ViLBERT预训练任务是什么？\n之前文章谈到，多模态的预训练任务从BERT演化而来，可以分为两类任务：重建任务和匹配任务；\n那么在ViLBERT也是这两类；\n重建任务就是文本重建和图片重建；\n匹配任务是是否匹配；\n需要注意的是重建任务构建的时候并么有保持另一个模态数据保持完整；匹配任务是H_cls和H_img相乘接了一个MLP做分类；\n也是直接来看图：\n\n这么看文本和图片的任务是合在一起训练了，其实从模型架构我们可以看到两个流在最后是各自分支输出的，这点需要注意；\n3. ViLBERT实现细节有哪些？\n实现细节这里其实可说的没有多，主要是ViLBERT本身的预训练和在四个下游任务进行迁移学习；\n在预训练的时候，数据使用的是330万个图像-字幕对；\n这个很有意思，相当于是一种无监督的语料，但是怎么处理文本和字母不相关的问题，因为并不是每时每刻都是相关的，想一下电视剧的情景；所以这种数据噪声估计很严重，需要清理；\n论文使用的数据来自ACL2018论文搞出来的数据，比较干净一点；\n由于担心训练时间，ViLBERT中的BERT这个流使用的是bert-base，后来发现bert-large可能会有更好的表现；\n使用FasterRCNN，通过卡阈值的方式来提取图像中的置信度比较高的候选框【10-36个】，使用 mean-pooled convolutional feature 作为这个候选区域的特征向量；\n其他的:8个TitanX GPUs / batch size of 512 /10 epochs / Adam optimizer / initial learning rates of 1e-4.\n下游任务中的几个任务：Visual Question Answering (VQA)；Grounding Referring Expressions;Caption-Based Image Retrieval;‘Zero-shot’ Caption-Based Image Retrieval;\n做了两个对比实验：\n\n第一个是使用了单流的bert-videobert；没怎么改变bert的架构；\n\n这个其实对照到文本相似度这边，其实属于交互式模型，所以这种模型存在的一个问题是没有办法很好的缓存单个文本或者单个图片的embedding，这样在做一些检索任务的时候就非常的不方面；\n为啥DSSM 架构这么有名，效果是一方面，速度更加的被大家看重；\n\n第二个实验是相同的 ViLBERT架构，但是并没有在我们的图像-字幕数据集中进行预训练；\n\n这个实验是为了 看一下 架构和预训练数据的作用，从而来证明，架构是有用的，预训练也是有用的；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/多模态/多模态之ViLBERT：双流网络，各自为王/"},{"title":"","date":"2024-06-21T03:20:43.913Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:43.913Z","content":"1. 史上最全Transformer面试题\n\nTransformer为何使用多头注意力机制？（为什么不使用一个头）\nTransformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？\n（注意和第一个问题的区别）\nTransformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n在计算attention score的时候如何对padding做mask操作？\n为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）\n大概讲一下Transformer的Encoder模块？\n为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？\n简单介绍一下Transformer的位置编码？有什么意义和优缺点？\n你还了解哪些关于位置编码的技术，各自的优缺点是什么？\n简单讲一下Transformer中的残差结构以及意义。\n为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？\n简答讲一下BatchNorm技术，以及它的优缺点。\n简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？\nEncoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）\nDecoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)\nTransformer的并行化提现在哪个地方？Decoder端可以做并行化吗？\n简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？\nTransformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？\n引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？\n\n\n2. 3分钟从零解读Transformer的Encoder\nTransformer 分为两个部分，encoder 侧 和 decoder 侧。今天，我们聊一下 encoder 侧。这部分由 N 个完全相同的大模块堆叠而成（原论文N=6）。\n这个结构怎么理解？这个构造就需要我们确保每一个模块的输入和输出维度是相同的，在实现代码的时候，我们只需要完成一个模块的代码的构造就可以。\n注解：你可以把这个过程想象成 RNN 竖过来的一个流程，是不是就很好理解（当然这样想只是帮助你理解）。\n其次对于这每一个大的模块，又分为两个模块，分别是多头注意力层和前馈神经网络层。进一步拆分，多头注意力层可以分为注意力层和 Add&amp;Norm 层。前馈神经网络可以分为 Linear 层和 Add&amp;Norm 层。\n多头注意力层，核心点在于 Q/K/V 三个矩阵，其中 Q/K 矩阵生成权重矩阵(经由softmax)，随后和V矩阵得到加权和。\n这个过程重复了 n_heads 次，这个 n_heads 代表的就是头的数目，这里需要注意的是我们需要确保 hidden_size/n_heads 需要为一个整数，不然代码会报错。\nAdd 代表一个残差结构。对于残差结构，可以使得信息前后向传播更加顺畅，缓解了梯度破碎问题。在 NLP 角度来看，残差结构一定程度上促进了 NLP 网络结构向窄而深的方向发展。\n我们可以把 Transformer 和之前的模型对比一下，比如 RNN 模型，一般来说，我们会选择 单层RNN 或者 一个 Bilstm，对于这些比较传统的模型，只是在时间长度上进行了延展，并没有在深度上做的太深。\n所以说，残差结构是有助于网路变深的。\n顺便联想一下 Elmo，使用的是 双层双向lstm，训练起来已经非常慢了，所以对于RNN这种比较传统的模型，做深太难了，GNMT也是用了很多的 tricks 进行加速训练。\nNorm 代表的是 Layer Normalization。为什么这里使用 Layer Normalization，而不是BN，这个后面有文章说，这里直白的回答就是，BN的效果差，所以不用。\n随后多头注意力层的输出经过前馈神经网络。对前馈神经网络，比较简单，我们需要注意的是它分为两个 Linear 层，第一层的激活函数为 Relu，第二层没有使用激活函数。\n最后我们谈一下整个encoder的输入和输出。\n先说输入，分为两个部分：word embedding 和 position encoding\nword embedding 没什么可说的，初始化后跟着训练或者使用word2vec这种已经有的看具体任务的效果。\nposition encoding 这里 transformer 使用的是 正余弦函数进行表达。其实这里进行初始化然后进行训练也是可以的，论文原作者的实验表明效果基本没区别。\n对于 position encoding 表示的绝对位置，这点大家都没异议，那么 position encoding 究竟有没有表达相对位置信息，之后会有个文章专门讲讲这个知识点。\n然后说一下 encoder的输出，感觉很少有人谈到这里。\nencoder 的输出需要注意的细节点在于它需要和 decoder做交互，所以它的输出为 K/V 矩阵，记住这个细节点，Q 矩阵来自decoder模块，K/V矩阵来自encoder。\n写到这里，我估摸这三分钟差不多能看完，现在没有留言功能，有问题大家在公众号对话框发送，我后台能看见。\n能点个在看，老铁们 ！！鞠躬感谢！！\n\n3. 原版Transformer的位置编码究竟有没有包含相对位置信息\n简单概述\nTransformer 原版的位置编码也就是正余弦函数编码，表达的是绝对位置信息，同时包含相对位置信息。但是经过线性变化，相对位置信息消失。基于此，需要对位置编码进行优化。\n正文\n原版位置编码使用的是正余弦函数，通过三角函数，可以得出一个结论就是：$PE_{pos+k}$可以被$PE_{pos}$线性表示。\n从这一点来说，原版位置编码可以反应一定的相对位置信息。\n接下来，我们来看，经过注意力层，这个相对位置信息还在不在？\n很简单，把词向量和位置向量作为输入，经过注意力层，然后因式分解，得到四个部分，我们重点关注包含两个不同位置编码的公式部分，形式如下：\n$PE_{pos}^{T}W_{q}^{T}W_{k}PE_{pos+k} \\tag{1}$\n我们想要证明，这个公式能不能反应相对位置信息。\n为了解决这个问题，我们化繁为简，先从下面这个公式入手：\n$PE_{pos}^{T}PE_{pos+k} \\tag{2}$\n注意看公式(1)和公式(2)的区别，在中间多了两个矩阵相乘，这两个矩阵，是我们的Q/K矩阵，可以看做是一个线性变化，记住这个细节点。\n经过公式推导，我们很容易知道公式(2)最后的结果只和两个位置的相对位置 $k$ 相关，这个结果是包含相对位置信息。也就是说两个不同位置$PE$的点积的结果可以反映相对距离。\n通过实验我们知道这个结果大小随着相对距离的增大而减小，值得注意的是它并不能反映相对位置的方向，因为他是一个对称的。\n具体的我们可以看下面这个图：\n![rela_posi](/Users/zida/Documents/GitHub/NLP_ability/深度学习自然语言处理/T](/links/NLP_ability/深度学习自然语言处理/Transformer/images/rela_posi.png)\n很好，接下来，我们就是证明本来可以反映相对位置信息的公式(2)，在加上中间这个线性变化之后，相对位置信息还在不在。\n直接看效果图：\n![rela_po_none](/Users/zida/Documents/GitHub/NLP_ability/深度学习自然语言处理/T](/links/NLP_ability/深度学习自然语言处理/Transformer/images/rela_po_none.png)\n这个图需要重点看的下面两个，也就是加了线性变化之后，变化趋势从最上面蓝色图标的线变成了下面两条线，也就是趋势已经完全没有了。\n也就是说，实验结果显示，公式(1)的结果随着 k 的变化没有明显的趋势变化，也就是说相对位置信息消失了。\n上面这些内容，估计5分钟左右吧，本来想加上相对位置编码，不过内容也挺多的，下回再发吧。\n同学们，如果觉写的还行，给个在看。\n\n4. 谈一下相对位置编码\n经过线性变化之后，正余弦函数表示的相对位置信息消失，所以需要优化。\n一般来讲，谈到优化，三种比较有名：RPR； Transformer-XL；complex embeddings；\n我在这个文章简单讲一下RPR。\n老样子，不涉及到公式推导，尽量把我的理解讲出来。\nRPR思路\nRPR思路很简单，原始正余弦函数，是在输入的的时候与词向量相加作为输入，在attention丢失相对位置信息，改进的话就是不在输入的时候进行位置编码，而是在attention中显示把相对位置信息加入进去。\n如何理解相对位置\n绝对位置编码是在每个位置都对应一个唯一的位置编码信息，RPR把这一部分去掉去学习一个相对位置编码。\n首先我们需要知道相对位置是有方向的的。\n举个例子：”我/爱/中国/共产党“\n”我“对”爱“的相对位置就是 -1， ”中国“对”爱“的相对位置就是 1。\nRPR修改思想\n作者认为在相对位置小于4的时候，attention对相对位置比较敏感，在大于4之后，相对位置不敏感。所以窗口设置为4。\n需要注意的是，窗口设为4，代表的当前位置左边4个，右边也有4个，再加上自己，就是一共9个位置，也就是：\n$[i-4,i-3,i-2,i-1,i,i+1,i+2,i+3,i+4]$\n当你的attention进行到哪个单词的时候，你的 $i$ 就对应的是哪个位置。\n还是上面那句话举例子。\n如果此时的输入是“我”，那么用到的相对位置编码就是$[i,i+1,i+2,i+3]$\n如果此时输入的是“爱”，那么这个时候用到的相对位置编码就是$[i-1,i,i+1,i+2]$\n了解了这个，我们再谈一下这个相对位置信息是怎么显示加入进去的。\n这个显示的加入分为两个部分。\n第一个部分是在计算$e_{ij}$的时候，涉及到RPR的一个表征:$a_{ij}^{K}$，表示对 Q/K/V三者中的K做了修改。\n第二个部分就是在计算$z_{i}$的时候，涉及到另一个RPR的表征:$a_{ij}^{V}$，表示对Q/K/V三者中的V做了修改。\n两个部分的修改都是使用加法。\n\n5. BN踩坑记–谈一下Batch Normalization的优缺点和适用场景\n这个问题没有定论，很多人都在探索，所以只是聊一下我自己的理解，顺便为讲 layer-norm做个引子。\nBN的理解重点在于它是针对整个Batch中的样本在同一维度特征在做处理。\n在MLP中，比如我们有10行5列数据。5列代表特征，10行代表10个样本。是对第一个特征这一列（对应10个样本）做一次处理，第二个特征（同样是一列）做一次处理，依次类推。\n在CNN中扩展，我们的数据是N·C·H·W。其中N为样本数量也就是batch_size，C为通道数，H为高，W为宽，BN保留C通道数，在N,H,W上做操作。比如说把第一个样本的第一个通道的数据，第二个样本第一个通道的数据…第N个样本第一个通道的数据作为原始数据，处理得到相应的均值和方差。\nBN有两个优点。\n第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使损失平面更加的平滑，从而加快的收敛速度。\n第二个优点就是缓解了梯度饱和问题（如果使用sigmoid激活函数的话），加快收敛。\nBN的缺点：\n第一个，batch_size较小的时候，效果差。这一点很容易理解。BN的过程，使用 整个batch中样本的均值和方差来模拟全部数据的均值和方差，在batch_size 较小的时候，效果肯定不好。\n第二个缺点就是 BN 在RNN中效果比较差。这一点和第一点原因很类似，不过我单挑出来说。\n首先我们要意识到一点，就是RNN的输入是长度是动态的，就是说每个样本的长度是不一样的。\n举个最简单的例子，比如 batch_size 为10，也就是我有10个样本，其中9个样本长度为5，第10个样本长度为20。\n那么问题来了，前五个单词的均值和方差都可以在这个batch中求出来从而模型真实均值和方差。但是第6个单词到底20个单词怎么办？\n只用这一个样本进行模型的话，不就是回到了第一点，batch太小，导致效果很差。\n第三个缺点就是在测试阶段的问题，分三部分说。\n首先测试的时候，我们可以在队列里拉一个batch进去进行计算，但是也有情况是来一个必须尽快出来一个，也就是batch为1，这个时候均值和方差怎么办？\n这个一般是在训练的时候就把均值和方差保存下来，测试的时候直接用就可以。那么选取效果好的均值和方差就是个问题。\n其次在测试的时候，遇到一个样本长度为1000的样本，在训练的时候最大长度为600，那么后面400个单词的均值和方差在训练数据没碰到过，这个时候怎么办？\n这个问题我们一般是在数据处理的时候就会做截断。\n还有一个问题就是就是训练集和测试集的均值和方差相差比较大，那么训练集的均值和方差就不能很好的反应你测试数据特性，效果就回差。这个时候就和你的数据处理有关系了。\nBN使用场景\n对于使用场景来说，BN在MLP和CNN上使用的效果都比较好，在RNN这种动态文本模型上使用的比较差。至于为啥NLP领域BN效果会差，Layer norm 效果会好，下一个文章会详细聊聊我的理解。\n\n6. NLP任务中-layer-norm比BatchNorm好在哪里\n本文主要是讲一下，为什么NLP任务中，比如Transformer，使用LayerNorm而不是使用BatchNorm\n这个问题其实很有意思，理解的最核心的点在于：为什么LayerNorm单独对一个样本的所有单词做缩放可以起到效果。\n大家往下慢慢看，我说一下我自己的理解，欢迎大佬拍砖，如果觉得我说的还行，点个在看鼓励一下。\n为啥BN在NLP中效果差\n上一个文章有说 BN的使用场景，不适合 RNN这种动态文本模型，有一个原因是因为batch中的长度不一致，导致有的靠后面的特征的均值和方差不能估算。\n这个问题其实不是个大问题，可以缓解。我们可以在数据处理的时候，使句子长度相近的在一个batch，就可以了。所以这不是为啥NLP不用BN的核心原因。\n回忆一下上个文章中，BN在MLP中的应用。 BN是对每个特征在batch_size上求的均值和方差。记住，是每个特征。比如说身高，比如说体重等等。这些特征都有明确的含义。\n但是我们想象一下，如果BN应用到NLP任务中，对应的是对什么做处理？\n是对每一个单词！\n也就是说，我现在的每一个单词是对应到了MLP中的每一个特征。\n也就是默认了在同一个位置的单词对应的是同一种特征，比如:“我/爱/中国/共产党”和“今天/天气/真/不错”\n如何使用BN，代表着认为 &quot;我&quot;和“今天”是对应的同一个维度特征，这样才可以去做BN。\n大家想一下，这样做BN，会有效果吗？\n不会有效果的，每个单词表达的特征是不一样的，所以按照位置对单词特征进行缩放，是违背直觉的。\nlayner-norm 的特点\nlayner-norm 的特点是什么？layner-norm 做的是针对每一个样本，做特征的缩放。换句话讲，保留了N维度，在C/H/W维度上做缩放。\n也就是，它认为“我/爱/中国/共产党”这四个词在同一个特征之下，所以基于此而做归一化。\n这样做，和BN的区别在于，一句话中的每个单词都可以归到一个名字叫做“语义信息”的一个特征中（我自己瞎起的名字，大家懂就好），也就是说，layner-norm也是在对同一个特征下的元素做归一化，只不过这里不再是对应N（或者说batch size），而是对应的文本长度。\n上面这个解释，有一个细节点，就是，为什么每个单词都可以归到“语义信息”这个特征中。大家这么想，如果让你表达一个句子的语义信息，你怎么做？\n最简单的方法就是词语向量的加权求和来表示句子向量，这一点没问题吧。（当然你也可以自己基于自己的任务去训练语义向量，这里只是说最直觉的办法）\n上面这个方法就是出于每个单词都是语义信息的一部分这个insight。\n引申-为啥BN在CNN可以而在NLP不可以\n但是，我还想问一个问题，CNN中证明BN效果是很好的，NLP中的文本可以类比为图像，为什么BN在图像中效果好，在文本上效果差。\n我是这样理解的。还是回到刚才，BN是对单词做缩放，在NLP中，单词由词向量来表达，本质上是对词向量进行缩放。词向量是什么？是我们学习出来的参数来表示词语语义的参数，不是真实存在的。\n这就是NLP和图像的一个区别，图像的像素是真实存在的，像素中包含固有的信息。比如说，一张图像，最上面的一行像素，可以归为背景这个特征（这里只是为了理解，CNN做BN是基于整个feature map，而不是单独某一行像素）。\n这个理解不确保正确，只是我自己的理解（记得是从一个知乎答案看到的，改天好好找一找）\n简答说一下\n写到这里，我写文章不是为了推导公式，因为这种推导文章太多了，而是想让大家看了我的文章之后再去看这些推导公式能够更加容易理解。\n然后大家有问题的话，私信和我说，我也知道我自己写的哪里有问题，好改进。\n点个在看再走呗，老弟\n\n7. 谈一谈Decoder模块\n本文主要是谈一些比较容易误解的细节点，说实话，把自己的理解用文字表达出来真是个细致活。\n如果觉得对您有点帮助，帮忙点个在看或者赞。\n一个小小的问题\n我先说一个自己花了点时间才琢磨出来的东西，其实不难，就是当时没转过弯来。\n我们都知道，decoder的交互层，Q矩阵来自本身，K/V矩阵来自整个Encoder层输出。\n但是对于每个单词都会有一个encoder的输出，那么K/V矩阵是用的其中哪个输出计算过来的？\n我这个问题的问法其实是错误的。\n我当时的理解背景是认为这个交互的过程很类似seq2seq的attention，它一般是使用最后一个时刻的隐层输出作为context vector。\n我基于此产生了上面这个问题，这个K/V矩阵应该由哪个位置单词（对比RNN就是哪个时刻的单词）的输出生成。\n后来看了一下代码，才明白自己错在哪里？\nK/V矩阵的计算不是来自于某一个单词的输出，而是所有单词的输出汇总计算K/V矩阵。这个过程和在Encoder中计算K/V矩阵是一样的，只不过放在了交互层，一时没想明白。\n正文\n与Encoder很类似，Decoder同样由完全相同的N个大模块堆叠而成，原论文中N为6。\n每个大的模块分为三部分：多头注意力层，交互层，前馈神经层；每个层内部尾端都含有 Add&amp;Norm。\n和Encoder重复的内容我就跳过了，之前讲过，没看过的同学可以去看那个文章。\n多头自注意力层\n首先谈一下多头自注意力层，这里需要注意的细节点是，需要对当前单词和之后的单词做mask。\n为什么需要mask？\n最常规的解释就是在预测阶段，你的模型看不见当前时刻的输出以及未来时刻单词。\n这句话其实有点绕，如果读的不仔细会让人误解为mask的时候需要把当前时刻的单词也mask掉…(拖出去斩了吧)。\n从代码角度讲，你只需要把当前时刻之后所有单词mask掉就好了。\n我自己对这句话的理解是我们需要确保模型在训练和测试的时候没有GAP。\n举个简单的例子来理解，如果做机器翻译，你需要翻译出来的句子是 “我/爱/吃/苹果”。\n当前时刻是”爱“这个单词作为输入的一部分，另一部分是上一个时刻”我“作为输入的时候的输出值。\n当然在机器翻译中，我们一般使用 teacher forcing加速收敛，所以这里就使用”我“作为当前时刻输入的另一个部分。\n所以这个时候，输入就是”我“的编码信息和”爱“的编码信息（当然还有位置编码）。\n我要预测的是”吃“这个单词。\n如果我们没有mask，模型也是可以运行的，也就说此时”吃“和”苹果“两个词对”爱“这个时刻的输出是有贡献的。\n那么问题来了，测试数据中你根本没有ground truth，你怎么办？\n也就说，训练的时候，你的模型是基于知道这个时刻后面的单词进行的训练，但是测试的时候，做机器翻译，你不知道自己应该翻译出来什么东西。\n这就是问题的核心。\n你训练模型的时候，一部分精力花在了”吃“和”苹果“两个词上，这不就是无用功吗？\n所以，确保模型在训练和测试的时候没有GAP，我们需要mask掉”吃“和”苹果“两个词。\n交互模块\n这一块需要注意的就是之前文章提到的，Q矩阵来自本身，K/V矩阵来自encoder的输出。\n还有一个细节点是，K/V矩阵对应的是来自整个encoder的输出。\n如果看transformer那个经典图的话，初期很容易理解为encoder和decode对应的每一层互相交互，这是不对的。\n是整个输出与decoder做交互。\n\n8. Transformer的并行化\n本文主要谈一下关于 Transformer的并行化。文章比较短，适合大家碎片化阅读。\nDecoder不用多说，没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。\n对于Encoder侧：\n首先，6个大的模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。\n从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。\n当然对于注意力层在做attention的时候会依赖别的时刻的输入，不过这个只需要在计算之前就可以提供。\n然后注意力层和前馈神经层之间是串行，必须先完成注意力层计算再做前馈神经层。\n有点绕，不知道有没有讲清楚。\n简单讲，就是6个encoder之间是串行，每个encoder中的两个子模块之间是串行，子模块自身是可以并行的。\n\n9. 谈一下 Transformer为何使用多头注意力机制？\n答案解析参考这里：为什么Transformer 需要进行 Multi-head Attention？\nhttps://www.zhihu.com/question/341222779\n注解：简单回答就是，多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。其实本质上是论文原作者发现这样效果确实好，我把作者的实验图发在下面：\n![attention_head](/Users/zida/Documents/GitHub/NLP_ability/深度学习自然语言处理/T](/links/NLP_ability/深度学习自然语言处理/Transformer/images/attention_heads.png)\n\n10. Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？\n答案解析参考这里：transformer中为什么使用不同的K 和 Q， 为什么不能使用同一个值？ - 知乎\nhttps://www.zhihu.com/question/319339652\n注解：简单回答就是，使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。\n\n11. Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？\n答案解析：为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的效果越显著。更具体的结果，大家可以看一下实验图(从莲子同学那里看到的，专门去看了一下论文)：\n![attention_methods](/Users/zida/Documents/GitHub/NLP_ability/深度学习自然语言处理/T](/links/NLP_ability/深度学习自然语言处理/Transformer/images/attention_methods.png)\n\n12. 为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解\n答案解析参考这里：transformer中的attention为什么scaled? - LinT的回答 - 知乎\nhttps://www.zhihu.com/question/339723385/answer/782509914\n注解：针对大佬回答的第二个问题，也就是方差的问题，我简单的写了一个代码验证了一下，不愿意看公式推导的同学直接看代码结果就可以。代码如下:\n123456import numpy as np arr1=np.random.normal(size=(3,1000))arr2=np.random.normal(size=(3,1000))result=np.dot(arr1.T,arr2)arr_var=np.var(result)print(arr_var) #result: 2.9 (基本上就是3，和就是我们设定的维度)\n\n13. 计算attention score的时候如何对padding做mask操作？\n答案解析：padding位置置为负无穷(一般来说-1000就可以)。对于这一点，涉及到batch_size之类的，具体的大家可以看一下抱抱脸实现的源代码，点击在这里\n这个是最新版，比较老版本的实现地址我也罗列一下，应该没啥区别，我没细看，一直用的老版本的，点击这里\n\n系列总结\n整个Transformer这一块基本就是讲完了，基本上可以解决之前那个关于transformer面试题百分之八十的题目。\n至于剩下的题目会放在之后别的模块去讲，比如 wordpiece model 会在总结机器翻译知识点的时候写一下，然后 GPT 会在总结词向量知识点的时候写一下。\n欢迎大家关注微信公众号: NLP从入门到放弃\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Transformer/答案合辑/"},{"title":"","date":"2024-06-21T03:20:45.323Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.323Z","content":"大家好，我是DASOU；\n回到2018年BERT刚刚横空出世，如果想快速搞一篇BERT的多模态预训练论文，应该从哪些方面去考虑呢？\n本文讲两个问题，把多模态BERT知识点串起来【绝对原创，至少我还没看到这么讲过的博文】：\n\n如何将MLM和多模态数据融合\n如何将NSP任务和多模态数据融合\n\nBERT中的大部分模块都是已经有的，它最大的作用就是证明了可以通过文本重建的方式从大量的无监督语料中获取到知识；\n那么我们现在思考的问题就是如何从多模态数据中，使用BERT的架构，学习到有用的知识；\nBERT有两个任务，一个是MLM。一个是NSP；\nMLM是做文本重建，NSP是做句间关系；\n1. 如何将MLM和多模态数据融合\nMLM我们需要从三个方面去考虑：\n\nMLM输入形式是什么？\nmask的时候需要注意什么？\n输出形式是什么，损失函数是什么？\n\n在多模态场景下，对MLM任务，需要分为两个方向，一个是对文本的重建，称之为Masked Language Modeling (MLM)，一个是对图像的重建，称之为Masked Region Modeling（MRM）；\n文本这边的MLM很简单，和BERT原始本身没区别，就不赘述了；\n有意思的是图像重建：MRM；\n首先拿到一张图片，要想把这个图片送入到TRM中去，需要的是多个图片tokens；\n有几种方式可以做到这一点，首先第一个就是将图片分为一个个的patch，这个老生常谈了，TRM在CV中的应用大部分都是这种方式；\n还有一种就是使用Faster-RCNN对图片做目标检测，获取到一个个的含有物体的regions，那么这个regions就是可以认为是一个个的tokens；\n这个时候会出现一个问题，我们思考BERT中的文本tokens的输入，不仅仅是embeddings，而且还有position embeddings；\n这是因为TRM中tokens之间是无序的，需要使用position embeddings来标明顺序；\n那么回到图像这里，用什么来标明顺序呢？一般来说使用的是Faster-RCNN中输出的regions的locations信息【5维或者是7维度】；\n仿照文本，我们需要把图片regions的表征和地理位置的表征加起来，由于维度不一致，所以加起来之前需要各自过一个全链接层；\n那么【mask】怎么去操作呢，在操作的时候需要注意什么呢？\n文本这边还是直接使用【mask】符号去mask掉子词就可以；\n那么在图片这边，直接使用全零向量替代掉mask掉的图片regions就可以了；\n这里有一个细节很有意思，在mask的时候我们有两种选择，就是文本和图片是混合mask的或者文本和图片是conditional masking；\n文本和图片是混合的，就是说明我们在mask的时候不区分图片或者文本，随机mask；\n文本和图片是conditional mask，就是说我在mask文本的时候，保持图片是完整的，在mask图片的时候，保持文本是完整的；\n这两方式哪种好呢？\n我们这么来想：\n假如你的句子中存在【苹果】这个单词，而且图片中有【苹果】这个region，那么在mask的时候，会不会存在在mask掉【苹果】这个词汇的时候，同时mask掉了【苹果】这个区域图像呢？\n肯定有概率存在这种情况。\n所以conditional mask一般来说会更好一点。\n我们在来说MLM的第三个问题，输出形式是什么或者说损失函数是什么？\n文本这边就是softmax之后找是哪一个单词，从而进行更新梯度；\n图片这边会更复杂一点，一般来说分为三种形式，这主要是对于一个图片我们可以使用三种方式描述它；\n首先第一种就是使用Faster-RCNN的ROI pooled feature去描述这个图片区域，那么我们就可以使用mask的图片区域的TRM输出的向量接一个全连接打到相同维度，和ROI pooled feature进行一个L2；\n第二个就是，比如说我现在有图片中物体类别有50个类别，那么当前图片区域的输出就可以是一个50个类别软标签（做了softmax的归一化到概率），这样可以和TRM的输出做KL散度；\n第三个是承接第二个，我们可以使用概率最大的那个作为当前区域的类别，也就是得到了一个one-hot，也就是要给硬标签，这个直接做交叉熵就可以；\n2. 多模态数据如何做NSP任务呢？\n其实很简单，NSP任务本质上是做句子间的关系，那么我们只需要类比的做一个图片和文本之间是否匹配的任务就可以了，也就是ITM任务；\nITM本质上是从文本整体和图片整体来做关系，还有的会从字和单个图片区域做关系学习，比如Word-Region Alignment (WRA) ；\n多模态这块有点乱，但是大体上就是按照MLM和NSP任务扩展到多模态数据上，这么理解会更容易一些；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/多模态/如何将多模态数据融入到BERT架构中-多模态BERT的两类预训练任务/"},{"title":"","date":"2024-06-21T03:20:45.333Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.333Z","content":"层次体系的构建\n爱奇艺短视频分类技术解析：https://www.infoq.cn/article/f49e-Gb1xQxh8DttFDgb\n一般来说弹珠模型就可以，爱奇艺这里使用了多任务联合训练的方式\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/多模态/层次体系的构建-多模态解析/"},{"title":"","date":"2024-06-21T03:20:45.353Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.353Z","content":"层次分类体系的必要性-多模态讲解系列(1)\n对文章的详细解读：爱奇艺短视频分类技术解析  https://www.infoq.cn/article/f49e-Gb1xQxh8DttFDgb\n这个文章首先上来就给了一个例子出来：\n这只是一个视频的抽帧，也就是一个图片。算法结果：游戏 - 题材 - 角色扮演，与人工结果一致。\n这句话其实挺重要的。如果我们不看这个图片，只是看这个文本，其实很容易会被认为是属于影视这个类别。但是我们在注意图片这个画质，其实影视一般不会是这种画质。（当然，算法给出属于游戏这个类别，很大概率是基于整个视频，我这里只讲这个图片并不全面，大家理解就可以）\n这里其实就点出来了多模态的一个作用。多模态使用不同类型的数据（文本+图片+视频抽帧），对信息进行一个补充或者说融合，从而获取视频更加全面的语义表达。\n其实这个很容易理解。我之前说过一个更加容易理解的例子。比如我们有一个博文，博文的文本内容是“这个苹果真的是太好了”。如果我们做一个单独的文本算法，我们会对此打上“科技”或者“美食”的标签。\n这个时候，如果有图片，图片内容是“真正的苹果的图片”，那么此博文的类别标签就是“美食”。如果图片内容是“苹果手机的相关内容图片”，那么这个博文很大概率就是会打上是“科技”的标签。\n我上面这个例子，其实更加的容易去理解多模态的含义。\n然后说回来，我们看它这个结果的描述：游戏 - 题材 - 角色扮演。 有没有发现一个特点，它不是单单给出了“游戏”这个标签，还给出了在游戏下面，题材属于“角色扮演”这个子标签。\n业内一般把这个叫做，一级标签/二级标签/三级标签/…\n简单来说，短视频分类体系是一种层次结构，在标签下不停的去细分子类。\n我们可以想一下这样做的好处是什么？举个简单例子，比如你最近准备考公务员，那么对你的一个短期兴趣对应的标签就是“教育”（我自己定的，可能不同公司不同分法）这个一级标签。想一下，\n这个标签有没有精准的表达你的需求？并没有，如果按照“教育”这个标签的内容推荐给你，比如除了公务员的内容，还会大量推给你“计算机培训”这种东西，你很大概率是不感兴趣的。\n所以我们需要对兴趣进行划分。\n如果深入想这个问题，还存在一个问题。\n为什么不直接构建子标签，还需要一级标签？也就是为什么构建标签体系的时候不直接一步到位，还需要一层层的细分？\n这个问题其实有很多原因？比如有历史遗留问题，在一些公司初创的时候，是没有这么多分类的，只能先划分大类。不过在这里，我给一个更加的简单的解释。就拿爱奇艺举例子吧，在头部顶栏，一般会有不同类型，比如电影，综艺，电视剧等等的划分。\n这些就是一级标签。如果不进行一级划分，大家可以想一下，怎么把那么多的细分领域让大家知道？屏幕大小是固定的，细分领域那么多，怎么确保让细节领域被看到？有的app一级领域也很好多，所以顶部栏目可以滑动，或者可以点击一个按钮叫做更多。\n所以基于一个曝光的考虑，一级标签是有必要存在的。\n其实还有一个原因我想说一下。在之前这个文章中提到的，在意图分类的框架中，我们一般是先做\n如果新增一个类别，我们重新训练模型会非常的费力。一级标签基本是固定不动的，所以我们使用一个分类模型就可以了。所以在对一个视频进行分类的时候，使用这种层次分类架构，其实是减少了计算量的。\n然后，重点来了！！！\n这个层次分类架构，大家有没有想到层序softmax类似的感觉。理论上肯定是不等价的，但是从感觉上来说，我自己觉得真的很类似，大家可以思考一下。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/多模态/层次分类体系的必要性-多模态讲解系列/"},{"title":"","date":"2024-06-21T03:20:45.373Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.373Z","content":"文本图像特征表示和融合-多模态讲解系列文章\n接上一个文章聊一下在多模态中文本和图像是如何做到模型特征表示和融合的。\n最直觉的一个思路是，我们分别对文本和图像进行特征建模，然后对特征使用某种方式进行融合。所以就分成两个模块：特征表示模块和特征融合模块。\n特征表示模块\n对于文本特征的表示，我们这么去想：对一个视频，我们能够想到的文本一般是：标题+简介+字幕。\n那么我么如何对文本进行建模呢？之前写一个关于句向量综述的文章，里面有介绍一部分内容。\n\n\n词袋模型（基于统计和基于词向量），这种建模问题在于忽略了词序信息，可以使用n-gram进行缓解。\n\n\n基于任务（CNN/RNN），存在的问题是迁移性较差。如果是分类网络训练出来的CNN表达的句子向量迁移到情感分类效果不会很好。然后我们分开来说，CNN存在一个问题就是对长距离处理的不是很好。\n因为它的本质是重视的n-gram内的语序信息。RNN存在的问题是训练速度慢，这没什么可说的，不能并行是硬伤。\n\n\n还有其他建模方式就不多说了。我们来看爱奇艺的处理方式，一句话简单描述是“采用的是 BOW 和 CNN+Attention 方式完成文本表示的建模”\nBow使用一些人工特征加n-gram缓解自带的问题。CNN使用两个优化，提取信息使用一定步长的pooling，然后基于这个带有文本信息的表达做self-attention。\n对于短视频来说的图像表示是什么？是封面。封面一般是从短视频精选出来的一帧，一定程度可以对文本信息进行补充。\n对图像进行特征的抽取一般是有三种方式：\n\n直接抽取特征\n实现方式：把 ImageNet 预训练的模型作为特征抽取器，将模型的某一层或者某几层特征作为类型标签模型特征提取源。缺点是效果比较差。\n\n一般对应到NLP，大家可以想一下我们直接用Bert抽取出来的词向量做文本分类，效果也比较差。\n\nfinetune+抽取特征\n把 ImageNet 预训练的模型以类型标签为目标进行 FineTune，然后将模型的某一层或者某几层特征作为类型标签模型特征提取源（因训练目标一致，一般选择最后一层即可达到较好的效果）。\n\n大家仔细琢磨一下这个过程。如果我先在要做一个文本分类的任务，想使用LR做一个baseline。那么我的输入可以是这样的，使用bert对我要使用分类数据（注意是和LR一样的训练数据）进行FineTune\n，然后使用这个模型做特征的抽取。\nFineTune的任务和我LR要做的是一样的，那么bert抽取的特具有充足的意义表达，能够很好的迁移过来。\n基于此，大家可以想一下，如果我使用bert做了文本情感分析的FineTune，然后抽取的特征做文本分类，效果会好吗？想一下。\n这还有一个问题，从bert抽取出来的特征，我们需不需要随着模型进行修改？？？\n\n把 ImageNet 预训练的模型嵌入到类型标签的模型当中，让图像的表示和其他特征的表示同时进行训练。\n\n其实这种方式缺点很明显，耗时太大了，有种尾大不掉的感觉。\n爱奇艺选择的第二种，模型选择是 Xception进行特征的抽取。\n接下来我们聊一下文本和图像的特征融合怎么做，也就是文本图像的特征怎么联系在一起？\n我给大家两个思路，第一个就是直接concat，然后接你的各种网络。第二种是两个特征模块做 attention，这个更常见一点。\n爱奇艺还给出其他两种方式CentralNet 和LMF，我没了解过，就不多说了。\n对于attention，一般来说做双向的attention更合适一点。也就是说做一个文本到图像的attention，然后做一个图像到文本的attention，两者再concat，效果会更好。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/多模态/文本和图像特征表示模块详解-多模态讲解系列/"},{"title":"","date":"2024-06-21T03:20:45.463Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.463Z","content":"TextCNN中的卷积核在文本进行处理的时候，是在文本长度上进行卷积。卷积核的大小\n不同，带来的直接后果是这个卷积核每次滑动的时候处理的单词长度不同。\n卷积核大小为2的时候，一次处理2-gram。卷积核大小为3-gram，一次处理三个大小的单词。\n所以卷积核在对文本进行卷积的操作，更像是对在提取文本在n-gram上的特征。卷积核权重的更新\n只是为了能够更好的提取n-gram上的特征。卷积核权重的更新\n大小为2的卷积核提取的是2-gram特征，大小为3的卷积核提取的是3-gram特征，以此类推。\n取不同卷积核大小进行卷积操作的原因，我的理解是可以提取这个句子多个维度不同的信息，使得特征更加的丰富。\n还有一点需要去注意的是，以2-gram为例，每次都是提取两个单词文本，但是如果文本很长，最后两个字和最开始的维度的单词\n联系就很小，唯一的联系就是卷积核的权重是共享的。\n举个例子：\n今天天气不错，适合出去旅游\n在这句话中，如果卷积核大小为2，我们这里不考虑中文分词，那么今天 天天 两个词组中间出了有卷积核权重的联系还有天这个单词的共有性。\n但是今天和旅游两个单词联系性在CNN中并没有体现出来。\n这也就是为什么CNN不适合处理长文本的原因。\n卷积之后，接了一个最大池化。论文中给出的原因是因为输入句子长度不一定，经过卷积之后长度不一定，\n如果直接操作的话，后面的全连接层权重形状不固定，不利于训练。\n其实感觉这一点站不住脚，处理文本的时候，一般会固定长度，阶段长度，不存在卷积之后大小不一定的原因。\n但是如果我们在处理文本的时候，没有截断长度，而是排序然后按照batch中长读补长，是存在上述问题的，所以需要最大池化。\n上面这个原因感觉是最重要的，其实还有一个原因，论文中是说想要获取一个卷积核提取出来特征中的最重要的特征。我的可理解是\n这个原因不太好，因为我直接用所有特征肯定比选取其中一个最重要的效果是好的。\n论文中把一个卷积核抽取特征，然后接一个最大池化的操作，形象的比喻为一个卷积核抽取一个特征。\n有一个人把特点总结的很到位，叫做CNN的卷积核实现了捕捉局部相关性\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/CNN文本分类解读/"},{"title":"","date":"2024-06-21T03:20:45.443Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.443Z","content":"今天分享一个论文Text Classification with Negative Supervision；\n论文思路比较简单，可以用一句话来说明就是：使用负监督+多任务的方式，可以扩大文本表达在输入语句语义相似的情况下的差异性。\n接下来详细说一下。\n1. 语义相似标签不同的问题\n文本分类中存在这样一种情况，有两个句子：\n\n句子A：感冒是个常见的疾病。这句话在标签数据中属于无标签数据。\n句子B: 我得了感冒。这句话在标签数据中数据标签为【感冒】的类别。\n但是我们的文本分类器，把两句话都分为了【感冒】这个类别。\n对于这种现象，可能是由于数据不充足，可能是由于其他原因，结果就是导致两个句子的【CLS】输出向量很接近，之后接一个分类器，分出的类别就是同一个。\n作者想解决的一个问题就是，想要通过一种方式，让模型知道，这两句话语义是不相似的。\n这个方式归到【ClS】这里，就是想要两个句子的【CLS】的输出是区分度大的。\n现在问题落在了了如何度量【CLS】的区分度？\n我读论文的时候第一想法是用KL散度或者交叉熵，然后发现作者使用的是两个【CLS】向量的余弦相似度进行度量。\n模型架构\n先总览一下模型架构：\n\n这个架构其实很容易理解，初看命名Discriminator以为作者用的是对抗网络。。。\n作者使用的是一个很简单的多任务架构，分为了两个任务：\n\nMain Task：主要任务，做常规的文本分类任务\nAuxiliary Task：辅助任务，输入负样本（不同类别的样本），计算余弦下相似度。\n\n总体的损失函数如下：\n\n辅助任务损失函数：\n\n其实比较细节的一个点是辅助任务的输入样本是什么样子的。\n我们的本质是为了度量语义相似性的句子之间的文本表达向量尽可能的大。\n一个很朴素的想法就是，辅助任务中输入的是同类标签的数据，然后同类标签的输出向量和主要任务的输出向量做相似度度量，计算损失。\n但是，想一下这个过程，有没有解决作者最想解决的问题。\n我们会看一下最初的例子，两个句子是虽然有着相近的意思，但是有着不同的标签。所以我们这个关于辅助任务的输入样本的选择是有问题的。\n作者这边选择的是，与主要任务输入样本不同的标签数据作为辅助任务的输入，然后进行相似度的度量。\n当然作者又细分了两种模式：\n\nAAN：辅助任务输入的全部是与主要任务不同的标签数据\nAM：辅助任务包含一个与主要任务相同标签的数据，剩下的是不同标签数据。\n\n结果分析\n直接来看结果分析：\n\n我其实比较感兴趣的是ACE这个为啥不行？按道理交叉熵应该也可以吧。\n作者的解释是，ACE效果不可以，恰恰说明了简单的多任务是不可行的，基于负监督的多任务是很必要的。\n总结\n总结一下从这个论文学到的东西，主要是就是一点，对使用不同标签数据的【CLS】输出向量进行余弦相似度的损失计算（作为多任务的辅助任务），可以提升表达向量的差异化。\n这个思路用在普通的编码器，应该也是适用的，感兴趣的可以试试。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/ACL2020-多任务负监督方式增加CLS表达差异性/"},{"title":"","date":"2024-06-21T03:20:45.803Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.803Z","content":"文本分类\n\n\n文本分类资源总结\n\n\n\n\n\n多分类模型Accuracy, Precision, Recall和F1-score的超级无敌深入探讨\n\n\n\n\n\n\n\n\n\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/README/"},{"title":"","date":"2024-06-21T03:20:45.613Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.613Z","content":"今天分享一个论文LCM；这个论文掌握以下几点，使用LCM模型：\n\n可以捕捉标签与标签之间的关系\n可以捕捉标签和样本之间的关系\n在噪声数据集，效果比LS要好\n\n1. 文本分类普遍存在一个问题\n深度学习模型进行文本分类有一个共性：\n\n\n首先使用一个比较深的模型去做text representation；\n\n\n然后使用一个简单的分类层（比如全连接）去预测标签分布；\n\n\n之后计算预测标签分布和真实one-hot标签向量的交叉熵。\n\n\n这个流程其实是有问题的；\n从标注规则来看，使用one-hot的前提是假设你的数据集中的标签是相互独立的。\n但是这种假设在现实中基本不会有，只是或多或少，有的界限比较清晰，有的不清晰的问题而已；\n还有一个问题从样本来看，如果是单标签分类，同一个样本真实情况下可能对应多个类别。\n比如【今天去公园野炊一下，吃点烧烤呗】；类别可能是【美食】，也可能是【旅游】，也可能是其他的类别。\n这个可能并不明显，我举个最明显的例子：【郭麒麟相声说的是真棒啊，综艺是真好看啊，综艺感真实爆棚了】；\n上面这个例子，你说它的类别是【相声】？【综艺】？【娱乐明星】？\n还有一个问题，就是标注错误的问题。这种情况一般使用标签平滑。\n标签平滑让真实标签不那么极端化，给与标签一定的容错概率。\n但是标签平滑本质上加了一个噪声，并不是真实反映标签的分布情况。\n从这出发，就可以看出下面LCM主要去解决以下问题：\n\n\n标签之间并不相互独立，所以我们需要一种方式能够度量标签之间的关系\n\n\n样本可能对应多个标签，所以我们需要一种方式能够度量样本和每个标签之间的关系\n\n\n标签可能标注错误，所以我们尽量不适用one-hot硬标签，而是使用软化之后的标签。\n\n\n2. LCM-架构图\n先来看架构图\n\n架构图最核心的部分注意看紫色的Similarity Layer层，这一层主要做的是对经过深度学习模型学到的句子表达和label的表达进行相似性度量。\n然后把这个相似性的度量加到one-hot标签中。\n看一下公式就明白了，左半部分比较简单，就不说了，看右半部分：\n\n$f^{L}$是对labels进行encode，得到每个label的表达向量，方便和句子向量做 dot product。\n注意图中的参数$\\alpha$，代表了相似性这个信息对原始标签的影响。\n损失函数使用的是KL散度\n3. 实验结果\n方法有效，就不放实验图了。\n我比较感兴趣的是 label embedding究竟有没有学到相似性，看图：\n\n不同颜色代表将类别根据语义分为不同的组，可以看到同个颜色的labels很大情况下还是挨得很近的。\n说明架构图有半部分的下半部分，也就是那个label encoder确实是有作用的。\n还比较感兴趣的是LCM和标签平滑的对比，看图确实比LS更好一点：\n\n总结\n简单总结一下，\n\nLCM挖掘了标签之间的关系和标签与样本之间的关系\n样本数据如果噪声标签（标注错误），LCM有效\n样本数据的标签如果界限比较模糊（根据语义可以划分为多个组），LCM有效\n\n整个论文最核心的点，我认为是对lables做了编码，从而有机会去和句子编码进行交互，度量相似性。并将整个相似性信息加入到了原始标签中。\n所以网络在训练的时候，学习到的信息更加的丰富。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/LCM-缓解标签不独立以及标注错误的问题/"},{"title":"","date":"2024-06-21T03:20:46.303Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.303Z","content":"在知乎看到这样一个问题【如何将关键词信息融入到文本分类任务】，简单说一下自己的经验，供大家参考；\n首先说，现在基本各组都有自己的关键词词库，构造方法也都基本上相似。\n简单点的就是TF-IDF筛选，复杂的就是构建挖掘特征，关键词二分类模型；\n基于此，大家一般也会加上新词发现+实体挖掘进行候选词库的补充；\n然后我们再来说，如何把关键词信息融入到文本分类任务中去。\n如果说关键词类别未知，这种情况不常见，但是也会有，一般是两种处理方式。\n一种是直接拼接在文本后面，增强信息，很常见。\n举个例子【今天出去旅游吗】，关键词是【旅游】，文本输入就是【今天出去旅游吗旅游】\n另一种是将关键词构造维稀疏特征加入到文本中去，缺点就是维度会比较高；\n如果说关键词类别已知，这种场景比较常见；\n先说个题外话，在挖掘语料的时候，关键词匹配挖掘语料是一个很常见的手段，但是容易造成语料太过简单单一+语料噪声比价大，所以冷启动的情况下，可以用关键词挖掘语料，之后还是上一批人工的标注会好一点；\n关键词类别已知的情况下，也可以使用两种方式来融入到文本分类任务中去；\n第一种就是，把关键词往上抽象化，转为对应的类别，然后作为特征结合文本输入到网络中去；\n第二种，也是我比较常用的就是对文本分类之后，对文本做关键词匹配，对应类别提升分值，简单说加规则，这个手段有点不好控制的地方就是分值的确定。\n但是我为啥爱用呢？最大的原因就是容易在和运营讲道理【撕】的时候获胜，百试不爽~~~\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/关键词信息如何融入到文本分类任务中/"},{"title":"","date":"2024-06-21T03:20:46.353Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.353Z","content":"伪标签，是啥？\n今天分享的论文是 [Pseudo-Label](“The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks”)\n从这个论文，主要是解决三个知识点：\n\n什么是伪标签\n怎么使用伪标签\n伪标签为啥有用\n\n伪标签\n先说第一个问题，假设我们现在有一个文本分类模型（先不用管分类模型是怎么来的以及怎么训练的），以及大量的无标注数据。\n我们现在使用文本分类模型对无标注数据进行预测，挑选softmax之后概率最大的那个类别为当前无标注数据对应的标签。\n因为是无标注数据而且我们模型准确不可能是百分之百，从而导致预测的这个标签我们并不清楚是不是精准，所以我们称之为&quot;伪标签&quot;。\n怎么使用伪标签\n“伪标签”可以帮助模型学习到无标注数据中隐藏的信息。\n我们先来看模型的损失函数是如何定义的：\n\n公式的前半部分针对的是标签数据的损失。我们重点来看后半部分伪标签的损失函数。\n$C$ 是类别数目，$n^{,}$ 是batch数据中伪标签（无标注）数据的数量大小。$y^{,m}$ 是无标注数据的伪标签，$f^{,m}$是无标注数据的输出。$\\alpha(t)$是未标注数据的权重，更新如下：\n\n这个更新公式值得看看，从这里可以看到，在$T_{1}$ steps之前，只是在训练数据上进行训练。随着模型的训练，无标注数据的损失函数权重在慢慢的增加。\n简单来说，就是模型现在标注数据上进行训练，到一定steps之后，开始使用无标签数据的损失函数。\n伪标签为啥有用\n其实，从上面这个损失函数，最好奇的一点就是为什么我加了后半部分的无标签数据的损失之后（也就是在训练的时候使用无标签数据的伪标签计算损失之后），模型的表现会比只是使用标签数据要好。\n损失函数的第二项，利用了熵最小化的思想。\n从形式上来看，它的这个损失是在强迫模型在无标签数据上的输出更加的集中，逼近其中的一个类别，从而使得伪标签数据的熵最小。\n在这个过程中，什么时候加入对伪标签的考量就很重要，因为如果太早的话，模型在训练数据上训练的并不是很好，那么模型在预测数据上的输出置信度其实就很低，误差会慢慢积累变大。\n所以$\\alpha(t)$是一个很重要的部分。\n总结\n伪标签在我的理解中，就是在模型已经训练的还可以的时候，对无标签数据进行预测，我们通过损失函数，让无标签数据逼近其中某一类（其实本质也是在做GT的文本分类）\n想一下，Bert在小样本上进行finetuning之后，我们也可以把它放在无标签数据上直接预测。\n由于Bert强大的能力，这样预测出来的标签置信度是很高的，我们一般可以直接拿这个结果作为冷启动的一部分。\n伪标签数据半监督入门的思想，之后有时间会慢慢深入的分享几个论文。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/半监督入门思想之伪标签/"},{"title":"","date":"2024-06-21T03:20:46.013Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.013Z","content":"今天分享一个论文，UDA，效果惊人：通过UDA，在IMDb文本分类数据集上，使用20个标签数据，相当于使用25000个标签数据。\n先说一个概念，贯穿在整个论文：consistency training\n直译过来就是一致性训练，我自己的理解就是，对于无标注数据，加入噪声，标签不变（或者说数据代表的含义没有发生太大的变化）\nUDA这个论文就是做了一个事情，验证监督学习中的数据增强方式放在半监督中作为一种噪声输入是有效的，是可以提升模型表现能力的。\n1. UDA\n先说有监督情况下的数据增强\n数据增强的目的是通过对示例进行转换而无需更改其标签，从而扩充数据。\n然后说一下半监督学习本质上在解决一个什么问题？\n我的数据中，有标注数据和无标注数据，半监督学习本质上是在从无标注数据上学下一个信息和知识从而使得模型效果更好，更佳的平滑和健壮。\n一般的半监督模式是这样的：\n\n输入数据为$x$，分别计算两个东西，原本的输出分布：$p_{\\theta}(y|x))$，输入数据注入噪声之后的输出分布：$p_{\\theta}(y|x,\\varepsilon)$。\n最小化两个差异之间的度量:$D(p_{\\theta}(y|x)) || p_{\\theta}(y|x,\\varepsilon))$\n\n此过程使模型对噪声不敏感，因此相对于输入（或隐藏）空间的变化更平滑。从另一个角度看，将一致性损失降至最低会逐渐将标签信息从已标记的示例传播到未标记的示例。\n这句话我是这理解的，输入空间越平滑，输入向量发生细微的变化，也就是加入扰动或者说噪声之后，代表的含义没咋变，标签就没咋变。\n之前的一些工作，为了达到上述这点，也就是为了增强一致性，通常采用简单的噪声注入方法，例如将高斯噪声，简单的输入增强添加到未标记噪声的示例中。\n与之相反，这里使用的是将监督学习中的数据增强的方法在这里作为噪声加入到原始数据中。\n在监督学习中，数据增强的做法的一个前提是，增强之后，标签不变。\n半监督中，我们加入噪声的目的是为了，加入噪声之后，原始数据的输出和加入噪声之后的输入向量的度量差异越小越好，越接近越好。\n这两个其实可以很类似，这也是我觉得谷歌在这个论文 中探讨的一点，就是监督学习中的数据增强可不可以作为一种半监督中的噪声扰动。\n先来看损失函数：\n\n重点是后半部分，是在无标签数据上的一致性损失函数。需要注意，后半部分的参数是固定的，从前部分模型直接复制过来的，简单来说就是不参与训练。\n整体架构如下：\n\n2. 训练技巧\n2.1 数据增强\n我以NLP中的任务为例：\n\n回译\n注意些主题分类任务的时候，有些单词很重要，通过TF-IDF来保留这些单词。\n\n这里其实很想说一下这个TF-IDF替换词这个东西，在附录里找到，好好的翻看了一下。\n概括一样是做了两个步骤（不知道我有没有理解准确）：\n\n对于句子中的每个单词计算替换概率\n替换的时候是从语料中抽取单词（简单粗暴），所以还计算了一个语料中每个单词的是不是关键词的概率。如果是关键词，那么就不能要，因为如果替换到当前的句子，可能让当前句子的类别发生改变。\n\n具体的计算公式，大家可以去看一下原论文。\n2.2 Confidence-based masking\n我们发现掩盖当前模型不确定的示例会有所帮助。具体而言，在每个小批量中，仅对分类类别中最高概率大于阈值β的示例计算一致性损失项。将阈值β设定为较高的值。\n简单来说，一致性损失应该针对的是无标签的数据，我们在训练的时候，只是计算那些输出概率高于阈值的样本，其余样本直接抛弃掉。\n2.3 Sharpening Predictions\n如果训练的时候使用了Confidence-based masking，我们可以结合Sharpening Predictions来提升模型的表现。\n什么是Sharpening Predictions？就是通过设定温度参数，改变最后softmax的分布，使它更加的尖锐，也就是熵更小，分布的越集中。\n这个东西蒸馏的时候也有用到，只不过在蒸馏的时候我们需要的是扩大不同类别的相似性，温度参数是大于1比较好的。但是这里我们希望是集中输出的分布，让它的熵更小，所以温度参数应该小于1。\n从另一个角度来说，使用了温度参数之后，阈值大于0.8 的概率应该也会提升，这就让Confidence-based masking变得没有那么难以操作。\n2.4 Domain-relevance Data Filtering\n首先我们要明白的是我们的数据中是含有无标签数据的，那么基于此，我们很容易有这么一个想法：\n既然有无标签数据了，那么为了增大模型的表达能力，能不能我自己从别的地方收集更多无标签数据补充进来，仍然使用同样的流程，这样最终模型效果是不是更好。\n这个想法很朴素，有一个问题就是我们从外部收集的数据是不是和我们已经有的数据同分布的。\n举个简单的例子，如果我们当前的数据的10个类别分类的数据，你从外部收集的数据类别是超出这个10个类别的，那么这样的数据加入进来，是副作用。\n一个很简单的方法就是，我们使用在域内数据上训练的基线模型来推断大型域外数据集中的数据标签，并选择模型最有信心的示例。具体而言，对于每个类别，我们根据分类属于该类别的概率，并选择概率最高的示例。\n这其实本质上也是无监督的一种方式，具体看一下这个文章。\n2.5 TSA\n全称是Training Signal Annealing for Low-data Regime，直译过来是低数据状态的训练信号退火；\n为什么使用这个东西？它本质上是为了解决一个问题就是无标签数据和有标签数据的数量巨大的不平衡问题。\n也就是说标签数据少，无标签数据多，在训练的时候很容易在标签数据上造成过拟合。\nTSA基本思想就是我们首先定义一个阈值，在训练的时候，我们只是使用模型对于当前标注数据的输出置信度低于阈值的样本。\n比如说，阈值如果你定的是0.5，那么在训练的时候，标注数据的输入最大概率如果是0.9，那么这个样本的是不计算在损失内，如果是0.1，那么我们计算损失。\n这个阈值是随着训练不停的增加的，有三种方式，如图：\n\n如果说标注数据很容易学习，或者说标注数据很少，我们使用指数增加。\n因为标注数据少或者容易学习，刚开始很容易过拟合，置信度高的样本占比多，所以我么最开始增长的慢。\n相反，当模型不太可能过拟合时，例如，当我们有大量带标签的示例或模型采用有效的正则化时，对数计划可以很好地发挥作用。\n总的来说，TSA本质上给人的直观感觉更像是在训练初期，压迫模型，不要尽快的学到最好的表现能力（使劲的扯后腿。。。）\n3. 实验结果\n\n有效，不想多分析\n4. 总结\n说一下从这个论文学到的东西。\n\n数据增强在半监督中可以作为噪声输入提升模型表现，文中使用的回译和TF-IDF。其实NLP中监督学习中有效的数据增强方法很多，猜测其他方法也是有效的，可以去尝试。\n上一个文章提到，为了担心模型在无标签数据上表现不好，在最开始，只是使用的标签数据，然后无标签数据才慢慢加进来的。当时使用的是一个$\\alpha(t)$来控制。在这里，为了缓解这个问题，使用的是masking结合Sharpening，Sharpening来控制输出的熵，让它比较尖锐，masking让它舍弃低置信度的样本，防止误差累积。\n第2点注意是针对的一致性损失，也就是无标签数据做的这个操作，对于有标签数据，我们使用TSA，来缓解过拟合，核心思想就是抛弃掉置信度高的样本，压迫模型在初期不要表现的那么好。\n\n半监督这块，我只是在工作中简单涉及到了，所以没有深入的读大量的论文，只是针对我用到的一些东西，看了一些论文。\n文中有不对的内容，欢迎大家拍砖讨论。\n参考资料：\n谷歌惊艳的无监督数据增强方法–Unsupervised Data Augmentation for Consistency Training\nhttps://www.jianshu.com/p/5d4e18b8de04\n半监督学习在金融文本分类上的探索和实践 - 李渔的文章 - 知乎 https://zhuanlan.zhihu.com/p/151021586\nGoogle无监督数据增强方法UDA在文本分类上的实验 - 延陵既智的文章 - 知乎 https://zhuanlan.zhihu.com/p/186211797\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/UDA/"},{"title":"","date":"2024-06-21T03:20:45.403Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:45.403Z","content":"对比学习 学习笔记：\nMoco论文解读细节：\nMoco 论文简单阐述\nMoco是视觉领域使用对比学习一个里程碑的工作；对比学习从2019年开始到现在一直都比较火，；\nMoco作为一个无监督的表征工作，不仅在分类任别务上逼近了有监督的基线模型，在其他任务，检测，分割，人体关键点检测都超越了有监督的预训练模型，也就是imagenet上的预训练模型；\nMoco证明了一点，无监督学习真的可以，我们并不需要大量的标注好的数据；\n什么是对比学习？\n首先说对比学习想要做到一点是什么呢？我们现在有三张图，第一个图是人高兴，第二个图片是人悲伤，第三个图片是狗。\n我们想得到一个一个结果，就是我们不需要知道前两个图片是人这个类别，不需要知道第三个图片是狗这个类别。但是我们能够需知道前两个图片是一个类别，第三张图片是不是一个类别。\n\n换句话说，我们现在把三个图片过一个模型，我们得到三个表征，我们需要让这个三个表征在特征空间中，前两个图片的表征距离比较近，第三个图片和他们的距离比较远。\n\n一句话说，我们希望在特征空间里，同一个类别的物体在特征空间中在相邻的区域，不同类别的物体在特征空间中不相邻的区域。\n在这个过程中，我们需要知道的是，我们并没有用到标签信息，我们不需要第一个和第二个图片是人，第三个是狗。\n但是我们用到了另外一种信息，就是第一个图片和第二个图片是同一个类别，第三个通篇不是同一个类别这个信息。这其实也是一种标签信息。\n不过这种标签信息，我们可以使用一些代理任务，巧妙构造出来的这种信息，而不需要人为的去标注这种标签信息。这些代理任务，会去定义一些规则，这些规则可以去定义哪些图片是相似的，哪些图片是不相似的，从而可以提供一些监督信号给到模型去训练。这个过程其实也是自监督训练的一个过程。\n个体判别代理任务\n一个最经典的代理任务就是：instance discrimination。叫做个体判别\n这个代理任务，就是如果我们有一个没有标注的数据集，里面有n个图片。\n从这个数据集中，我们随机选择一个图片，xi；在这个图片上我们做随机裁剪（或者其他的数据增广操作，我们称之为traansformation）；从而得到另外两张图；\n一个是xi1 一个是xi2；这样我们会得到两个不太一样的照片。但是由于这两个图片是从同一个图片经过某种变化得到的，语义信息不应该发生变化。所以这两个图片就可以称之为正样本，也就是同一个类别的图片。\n这个代理任务，同时认为，这个数据集中剩余的所有图片都是负样本\n\n为什么叫做个体判别呢？因为它认为每个图片自成一个类别，剩余的图片都不是同一个类别。\n（这个粒度其实是很细，你在图片分类的时候是很多照片是同一个类别，其余的照片又分为了很多类别，所以个体判别这个代理任务经过模型训练，表征会很细）\n对于imgenet这数据集来说，如果个体判别任务，就是一千个类别，而是100多万个类别。\n所以个体判别这个代理任务定义了什么是正样本，什么负样本，接下来就很简单了，我们只需要经过模型，然后做一个对比学习的函数去训练模型就可以了。比如说NCEloss\n\n在这个过程中，其实有一个很有意思的点，就是代理任务是多样性的，是很灵活的。只要你能够得到一个判断正样本和负样本的规律，后续的损失函数之类的训练就很常规了。\n比如说在视频领域，同一个视频里的任意两帧是正样本，其他视频里的帧是负样本；NLP中的simcse，你可以通过dropout判断不同句子。\n精读Moco论文\nMomentum Contrast\nMoco这个名字就是来源于前两个单词的前两个字母，就是基于动量的对比学习。\n动量是一种加权移动平均；\n![image-20220813170958764](/Users/zida/Library/Application%20Support/typora-u](…/image-20220813170958764.png)\ny(t-1)是上一个时刻的输出，m是动量超参数，xt是当前时刻的输入。\n说白了，就是不想让我当前时刻的输出只是依赖于我当前时刻的输入，我还希望和什么有关系呢？和之前时刻的输出有关系。动量这个超参数是0-1的一个参数；如果m是趋近于1的一个数，那么我的yt改变是非常缓慢的。\n因为（1-m）是趋近于零的。\nMoco是利用这个动量的特性，去缓慢的更新这个编码器，从而让中间学习到的字典特征尽可能保持的一致（这句话没看懂没关系，一会详细讲）\nMoco摘要部分：\nMoco把对比学习看成了是一个字典查询的东西，他们做了一个动态的字典，这个动态的字典分为两个部分，第一个部分是我们有一个队列，第二个部分是我们有一个移动平均的编码器。\n队列里的样本呢，我们不需要做到梯度回传，所以我们可以往队列里放很多的负样本，从而让字典很大。\n为什么还有一个移动平均的编码器呢，我们是想让字典里的特征尽可能的保持一致。\n在训练过程中，我们发现，如果你有一个很大而且特征比较一致的字典，会让这个无监督的对比学习学的很好。\nMoco从结果来说，在imagenet数据集上，如果采用linear pro去测试，Moco是可以取得和之前最好的无监督方式差不多或者更好的结果；linear pro指的是，我先预训练好一个骨干模型，然后我把这个骨干网络冻住，只取学习最后的全连接层，然后看在不同数据集上的表现结果。这样其实类似于把骨干网络当成了一个特征提取器，只从这里提取特征，这其实和我们使用resne差不多。\nMoco一个很大的卖点，我们学习到的特征，在下游任务上有很好的迁移性，我们看重无监督优点就是它可以从大量无标注上的数据上学习到特征，可以迁移到没有那么多标注数据的任务上。\nMoco在7个下游任务，分割，检测之类的超越之前的有监督预训练模型；举个例子，Moco使用同样的Resnet50，去做无监督，然后和有监督训练的模型去做比较。\n引言部分：\nGPT和BERT，已经证明无监督学习在NLP任务上是行得通的。但是CV领域，有监督预训练还是占据主导地位；\n之前也有很多优秀的无监督工作，但是表现都会比无监督要差，作者认为这是因为CV领域NLP领域不同的原始信号空间。\n对于自然原因来说，他们是离散的信号，也就是原始的信号空间，是有单词组成，或者更细一点，是由单词词缀组成的，所我们可以很容的去建立一个字典，然后让模型去学习特征。那么字典中的每个key就是一个类别，我们可以根据这个类别去学习模型（比如BERT就是最后一个softmax操作吗，不就是分类操作吗）\n但是对于CV领域来讲，完全不一样。CV领域的信号是在一个连续而且高维的空间，它并不像单词那样有很强的的语义信息而且浓缩的非常好，没有那么简洁；所以CV领域并不适合去建立一个字典，去学习模型；如果没有这个字典，无监督就很难去建模。所以在CV领域，出现无监督还不如有监督学习。\n在之前有很多优秀的对比学习工作，都可以归纳为一种字典查询的工作。\n我们之前来看图：\n\n两个编码器，一个是E11，一个是E12；然后我们x1这个图片经过数据增强T1得到的图片X11，然后经过E11这个编码器，得到了图片表征f11;同理，我们这个图片x1，经过数据增强T2，得到的图片x12，然后经过E12这个编码器，得到了f12这个图片。\n我们把X11这个图片叫做archor，瞄点，x12叫做x11的正样本。\n什么是负样本呢？就是图片里剩余的所有的图片都是负样本，那么负样本走哪个编码器呢？走的是E12这个编码器，因为我们正样本和负样本我们都是相对于瞄点来说的，所以正样本和负样本要走同一个编码器，从而让特征的获取过程保持一致性。于是这样负样本x2，x3，x4等等也经过E12得到了真正的负样本表征f2,f3,fn；\n那么我们把f11叫做query，把f12,f2，f3，fn叫做key；\n那么对比学习的过程就是想要在特征空间里，正样本的key和我query近，其余的key离我远。\n我们其实可以把key集合看成字典。那么对比学习的过程，就是想得到一个模型，让query在字典中那个和自己匹配正样本更近。\n如果把对比学习的过程看成一个动态字典的过程，如果想要得到一个比较好的效果，那么字典最好需要满足两个条件，第一个就是字典足够的大，第二个就是在训练的时候尽量保持一致性。\n首先第一个我们在做对比学习的时候，肯定不是一个batch一个batch的去做，所以如果key这个字典足够的大，那么我们从中抽样的可能性组合就很大，那么模型的泛化性就很大。如果字典很少，泛化性就不足，相当于数据来那个不够。\n第二个是保持一致性，是因为我们需要字典中的特征尽可能使用同一个或者相近的编码器进行表征。因为如果不这样做。那么模型可能就学习到和query使用同样的编码器的那个key，导致模型泛化性不足，走了捷径。\n所以Moco要做的就是一句话：在对比学习框架中，提供一个又大又一致的字典；框架图如下：\n\n大字典是怎么做到的：维护一个队列，把每次训练的batch-size和队列大小分离开；具体来说就是这个队列可以很大，但是我们每次更新这个队列，是一点点的更新的，也就是说当我们用一个很小的batchsize的时候，那么我们把现在batch中的特征进入队列，把最老的batch-size的特征抽离队列；那么我们的队列就可以设置的很大，比几万。这样我们用一个GPU也可以很好的训练模型；\n那么一致性是如何做到的？刚才说了，每次都是使用新的编码器更新batch大小的队列特征，除了这个之外的，我们都是使用的之前的编码器得到的，这不就不一致了吗？那么就用动量更新就可以，我们最开始的右边分支的编码是由左边的初始化而来，后续更新使用对右边这个编码器参数进行动量更新，m足够大，保障右边编码器更新的非常缓慢，从公式来说，就是这个图：\n\n可以看到，右边编码器会被之前的k编码，和当前时刻的q编码影响，m足够大，无限接近于1，那么就是可以认为无限被k控制，更新的就会非常缓慢。\n（有个疑问，直接不更新不就可以了吗，不进行梯度回传？）\nMoco只是建立中间模型的一个方式，是很灵活的，可以和很多代理任务结合，这里使用个体判别，之前讲过。\n无监督最大的一个卖点，就是我们的模型在大量无标注的数据集上进行训练之后，我们得到的特征，可以很好的迁移到下游任务中（比如标注数据很少的任务中）；\nMoco结论部分：\nMoco论文在imagenet得到了很好的结果，然后在自己facebook自己数据集是上也得到了很好的结果，但是提升不大，在数据集从100万到10个亿，提升不大，作者认为大规模数据没有被利用起来，可能一个更好的代理任务会有更好的效果。所以作者谈到，除了个体判别这个任务，有没有可能把moco和mask encoded这个任务结合起来，就是类似BERT这种操作，使用mlm自监督的方式去学习。（这不就是MAE模型吗，我之前讲过）；\n这个其实在开头有讲CV和NLP信号空间不一致，直接把bert方式搬过来，可能不太行，具体去看MAE模型；\nMoco相关工作部分：\n一般来说自监督可以有两部分可以去做，一个是在损失函数部分深挖，一个是在代理任务上做文章。\n（注解：自监督学习是无监督学习的一种）\nNCE损失函数把一个超级大的多分类（这个时候softmax是工作不了，计算量太大）转变成一系列的二分类问题，从而让大家可以正常使用softmax，（这个是w2c很类似）\nInfoNCE是NCE的一个变体，如果只\n温度超参数\n在看INfoNCE 的损失函数的时候，首先从softmax看起，这个是softmax的公式：\n\n然后我们加一个-log就是交叉熵损失函数：\n\n这个公式其实可以直接用在对比学习中。\n什么意思呢？\n交叉熵是一个多分类问题的损失函数，一个one-hot向量，和我真实输出做一个损失，目标是为了让真正标签的输出尽可能的大。\n那么有一个问题，如果把这个损失函数，直接套到对比学习中去，那么是什么意义呢？\n比如imagenet100万个图片，那么我当前图片的这个数据增强之后的图片经过编码器1得到了瞄点特征，经过比编码器2得到了正样本，也就是我的groud-turth；\n那么除了我当前这个图片，100万个图片之外的所有图片经过编码器2这个得到的表征都是负样本，也就是会得到这样一个向量：\n1 0 0 0 （1个1,100万-1个0）\n在这个上面我做交叉熵，其实就是可以用在对比学习上。\n但是这样做softamx计算量太大了，其实bert这种模型，也就是几万个类别，没啥问题，几百万太难了。\n这个时候NCE就是一种很好的解决方式，化成一个二分类问题，就是我现在只有两个类别，一个是正常样本，除此之外的都是噪声样本。（计算量没降低下来，这个我待定在看词向量的时候再去看）\n但是这样做不太清楚，所以INFONCE就出来了。\n也是与其你在整个数据集去走loss，不如我抽样一部分去做loss。如果你选取的抽样的这部分很少，那么就没啥意义，不能模拟整个数据集，所以抽样的部分还是要大一点。那么这个字典的大小就很重要，也就是我字典的大小就是我们的分母下方的类别数量；那么这个过程中InfoNCE就把NCE的一系列二分类又转为了多分类。\n\nq就是我query表征，也就是瞄点那个图片特征，k+就是正样本，分母累加那里的K，就是我们的负样本数量，分类累加了K+1，因为K个负样本+一个正样本。\n温度参数T（其实是tao），在蒸馏那里其实我讲过，如果t很大，那么softmax分布会很平滑，看不出区别，就是把所有的负样本一视同仁，导致模型学习没有轻重；如果tao很小，分布会更尖锐，会让模型只关注那个困难的负样本，其实那些负样本很有可能是潜在的正样本，如果模型过度的关注这个困难的负样本，会导致模型很难收敛，或者学号的特征不太好去泛化。\n去除这个温度超参数，InfoNCE本质就是一个交叉熵损失函数，只不过类别和所有样本相比，做了个近似，做了个个随机抽样，就是字典大小。Moco伪代码InfoNCE直接就是用的交叉熵损失函数代码。\n有个细节，为什么使用队列这种数据结构存储字典呢？\n因为先进先出，每次一个batch进来，最老的那个部分batch数据会出去，这部分数据是过时的，从而能够保持队列中的特征尽可能的一致性。\n另一个细节：\n第二个分支不能随着走这一支的样本使用梯度回传进行更新，为什么呢？因为如果这样做了，第二个分支的编码器就更新的太快了，这些特征，我们是放到字典中去的，就会导致特征不一致。\n为什么第二个分支直接就不更新，反而还缓慢更新（我自己理解是不太可以的，因为正样本的定义规则，经过编码器之后语义空间类似，所以是正样本。如果第二个分支一直不变，其实模型在训练的时候就很样本，因为可能到后来，第一个分支和第二个分支编码器差距越来越大，其实是本来是正样本的，损失也很大，就很难训练了。）\n两个贡献：\n一个是很大的字典：计算损失的时候使用多分类，能够很近似整个数据集上做多分类损失\n一个是字典内特征一致性，使用动量更新：\n需要注意的一点：就是infonce损失计算的是整个字典做多分类。minibatch大小和字典大小剥离开，batch可以设置为256，然后进来256个样本，每个样本都需要做一个瞄点，走一遍对比学习的流程。\n动量设置为了0.99，很大了。字典大小是65536\n在Moco之前的工作， 字典和字典特征一致性经常不能同时满足。\n端到端的框架：\n\n端到端的框架就是两个编码器都可以通过梯度回传进行更新，因为xq和xk都是从同一个batch中来的，我们通过一次forward就可以拿到所有样本的特征，我们直接梯度回传就可以了。这要求，我们batc大小要足够的大，那么infonce才能起作用，做到一个近似的全部数据集的多分类。SIMCLR就是这个端到端。这样字典是高度一致的。在这种情况下，batch大小和字典大小是等价的。simclr就是用了8192作为batch大小。\n另一流派，更关注字典的大，然后牺牲一些一致性，就是memory bank；在这个流派只有一个编码器，就是query的编码器，可以进行梯度回传进行更新。对于query这边，是没有一个单独的编码器。\nmemory bank就是把整个数据集的特征，都存到了一起。对于imagenet来说，这里就是128万个特征（作者说到，每个特征128维度，只需要600M的空间，还好。）\n然后每次训练的时候，从memroy bank中随机抽样字典大小就可以了。右边的编码是在线下执行。\n在执行的时候，比如字典是3，那么抽出三个来，左边做一次梯度回传之后，我们把字典中的3个用新的编码器做一个编码，放回到memroy bankl中去。\n（首先，我认为为了保持正样本的定义，肯定得更新样本特征）\n因为你这个更新操作，导致字典内编码特征不一致。\n\nMoco伪代码，讲解的非常好：\n几个中点：\n第一个动量是0.99，字典大小是65536\n第二个是损失函数底下类别是65536+1个=65537，是把所有字典中的都是当成了负样本（这样其实很有可能存在潜在的正样本，不过影响不大 ，一定要注意，这个时候我这次更新的样本特征，还没有放入到字典中去，所以仅仅是可能存在正样本，当前这正样本是一定不在字典中的）。\n参考：https://www.bilibili.com/video/BV1C3411s7t9/?spm_id_from=333.788\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/对比学习/Moco1论文解析/"},{"title":"","date":"2024-06-21T03:20:46.623Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.623Z","content":"文本分类资源总结\nSGM：用序列生成的方法来处理多标签文本分类问题 - SimpleJian的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/58076177\n半监督学习在金融文本分类上的探索和实践\nhttps://mp.weixin.qq.com/s/7EazF26teBSg0_XvWtKdUg\n多分类模型Accuracy, Precision, Recall和F1-score的超级无敌深入探讨 - NaNNN的文章 - 知乎 https://zhuanlan.zhihu.com/p/147663370\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/文本分类资源总结/"},{"title":"","date":"2024-06-21T03:20:46.463Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.463Z","content":"今天分享的论文主要是讲Bert如何在文本分类上获得比较好的效果，比较简单：How to Fine-Tune BERT for Text Classification?：不涉及什么复杂公式，也比较早了，里面很多东西对于当下已经司空见惯，我就直接就分享论文结论，攒个思路。\n1. 如何处理长文本\n我比较感兴趣的是一点是Bert处理长文本的思路。\n首先数据集是IMDB，文本分类任务，超过512个token的12.69%，最大长度为3045；\n1.1 截断方法：\n\n保留头部：保留头部最开始的510个tokens\n保留尾部：保留最后的610个tokens\n头部加尾部：头部128+尾部382\n\n1.2 分层的方法：\n简单来说就是把文本分为 k = L/510个小段落，每个都喂进去Bert，然后得到的K个【CLS】的输出向量，我们对这个K个向量做：\n\nmean pooling\nmax pooling\nself-attention\n\n直接看结果：\n\n看结果，我们知道，头部加尾部会获得更好的结果。\n2. 其他结论\n\nBERT 顶层对于文本分类任务更加有效\n每层适当的逐层降低学习速率，可以提高文本分类效果\n任务内和领域内（和任务内数据分布相似）的进一步预训练可以提升文本分类效果\n\n对于第二点，降低学习率来说，论文中是从顶层到底层逐渐降低，越靠近输出学习率越高，越靠近输入层，学习率越低，这一点还是挺有意思的。\n对于第三点，任务内数据和领域内数据，对提升效果都有用，通用领域基本没啥用，因为Bert本来就是在通用领域训练的。\n还有意思的一点是，并不是在任务内的数据训练的越多step越好，直接看图：\n\n也就是说，在任务领域数据预训练可以提升效果，但是也有注意预训练的步数，不能是过分（有点过拟合的感觉，但是感觉说过拟合有点不准确）。\n3. 总结\n掌握以下几点：\n\n如何处理长文本：head/tail/combine two\n不同层不同学习率提升效果，越靠近输入层学习率应该越低\n领域内和任务内数据进一步预训练提升效果：注意进一步预训练步数控制\n\n参考链接：\n文本 × 分类：让 BERT 适配短句分类任务 - 小莲子的文章 - 知乎 https://zhuanlan.zhihu.com/p/148501319\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/在文本分类上微调Bert/"},{"title":"","date":"2024-06-21T03:20:46.703Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.703Z","content":"DSSM\nhttps://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf\n架构图\n架构图很简单，也有点老了\n\n核心细节点有两个：一个是使用了cosine做了查询和文档的相似度量\n\n第二个就是，softmax\n\n第三个是损失函数，使用最大似然估计，只计算了正样本：\n\n对于DSSM，主要是想提几个小细节，也是我自己的思考，不准确的地方，欢迎拍砖。\n首先，为什么采用(Query,D+,D-1,D-2,D-3)的方式作为输入，而不是采用(Query,D+)；(Query,D-1)；(Query,D-2)；(Query,D-3)；作为单独的pair样本对作为输入；\n这个问题，其实还可以换个问法，为什么DSSM的损失函数，使用的是一个正样本多个负样本归一化之后对正样本求交叉熵，而不是单个pair对作为输入，去求二分类的交叉熵；\n我的理解是，这个其实适合业务场景相关的一个问题；参考下面这个回答的答案：\nDSSM 为什么以一个正样本几个负样本softmax归一化然后正样本交叉熵的方式算loss? - xSeeker的回答 - 知乎 https://www.zhihu.com/question/425436660/answer/1522163398\n我直接截图过来：\n\n本质上，还是在学习一种顺序关系，正样本排在负样本之前\nDSSM在各大公司的实战\n实践DSSM召回模型 - 王多鱼的文章 - 知乎 https://zhuanlan.zhihu.com/p/136253355\n深度语义模型以及在淘宝搜索中的应用:https://developer.aliyun.com/article/422338  写的很好\n百度NLP | 神经网络语义匹配技术：https://www.jiqizhixin.com/articles/2017-06-15-5\n语义匹配 - 乐沐阳的文章 - 知乎 https://zhuanlan.zhihu.com/p/57550660\n损失函数\nDSSM通过推导公式，可以得到最大化似然估计和交叉熵损失函数是一致的。\n【辩难】DSSM 损失函数是 Pointwise Loss 吗？ - xSeeker的文章 - 知乎 https://zhuanlan.zhihu.com/p/322065156\n交叉熵损失函数原理详解：\nhttps://blog.csdn.net/b1055077005/article/details/100152102\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本匹配和文本相似度/DSSM论文-公司实战文章/"},{"title":"","date":"2024-06-21T03:20:46.413Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.413Z","content":"没有标注数据不用怕，只用标签名称就可以文本分类！韩家炜组出品\n对于实际的文本分类需求，没有标注数据是一件很常见的事情。\n针对这种情况，有一个最朴素的思路可以做：\n\n首先，根据对应的标签名称，使用W2C找到对应的相近词\n通过相近词，对文本数据做关键词命中，进而映射到对应的类别\n使用上述的标注数据训练文本分类模型\n使用3步骤的文本分类模型对新数据预测，获得置信度高的文本，之后做半监督。\n\n上面这个思路，非常的简陋，最终的结果也不会很好。实际工作中，需要有大量的规则去补充。\n今天分享的这个论文【Text Classification Using Label Names Only: A Language Model Self-Training Approach】，来自韩家炜组。\n针对上面的场景，这个论文给出了一个思路。\n整个论文读下来，先给个简单的评价，思路不错，但是也并不成熟，不过其中有很多细节点可以让我学习到。\n1. 背景\n人们在对一个文本分类的时候，不会看到任何带标签的标注数据，而只是通过一些关于描述分类类别的单词，就可以做出判断。\n举个例子，人去对文本进行分类的时候，假如文本有一个类别属于计算机。脑海中其实是有先验知识，比如如果句子中出现人工智能，深度学习，NLP等词汇的时候，人们基于此可以很大概率的判断出当前这个文本是属于计算机这个类别。\n随后呢，注意上面只是说的是很大的概率，还会出现苹果属于科技类别，但是不属于水果这个类别，也就是单单第一步还会出现语义歧义的错误，所以人们还会通读一遍句子，根据上下文语义再对句子分类。\n作者类比这个思路，提出了三个步骤：\n（1）找到和标签名称语义相关性较高的词汇；\n（2）查找类别指示性单词并基于这些单词训练单词分类模型\n（3）自训练提升模型\n2. 步骤\n2.1 Category Understanding via Label Name Replacement\n直译过来就是通过标签名称替换理解类别。\n这句话直译过来的话可能不好理解，更好的表述是找到与标签名称语义相关性较高的词汇。\n就像我们上面说的，人们在看到标签名称的时候，会联想到很多与之相关的词汇。\n类别到NLP中，预训练模型其实就相当于模型的先验知识，可以从中知道标签名称的相近词汇。\n我们知道，Bert 训练的时候是这样的：mask掉一部分词汇，然后通过语言模型预测mask部分的输出，计算损失函数；\n现在我当前输入是人工智能（为了方便理解，我们可以认为输入它是一个词作为整体输入），那么输出的时候其实是在整个词汇表上做softmax;\n基于此，从中挑选出概率最大的50个词汇，也就是当前这个位置最有可能出现的50个单词。\n进一步的，因为包含人工智能这个词的肯定不只是一个句子，我们对每个句子中的人工智能做同样的操作，然后都获取对应的前50个词汇。\n最后把这所有的50个词汇累积起来，按照频率从大到小，选取前100个词汇，作为可以替换人工智能这个标签名称的相近词汇。\n这个思路，简单来说，就是从预训练模型Bert获取标签名称的近义词或者更准确的说是获取与标签相关词汇的过程。\n其实，看到这里，我想到了一点，就是这个过程和我们使用GLove或者Word2Vec获取近似词的过程很相似。\n只不过Bert是一个动态的权重，受上下文影响，所以获得结果更加的准确，泛化性也更强。\n在论文，作者也做了实验论证这个道理。\n这一步，我们得到的结果是类似这种:\n\n简单总结一下：\n两个步骤：\n\n找到每个句子中存在的标签名字，使用Bert预训练模型，找到与之最接近的50个单词。\n每个标签中所有单词汇总，按照频率，找出排在前100个单词，作为当前标签名称(Label Name)的类别词汇（category vocabulary/category indicative words ）\n\n2.2 word-level classification via masked category prediction\n这个步骤，简单来说是使用Bert这类的预训练模型在单词这个级别训练分类模型。\n上个步骤中，针对每个Label Name，我们会得到对应的category vocabulary/category indicative words 。\n这个时候一个最简单的办法，就是只要当前的句子出现了category vocabulary中的词汇，我们就认为当前的句子属于相对应的Label Name。\n也就是我们开头说到的关键词命中规则。\n但是这样做是有很大问题的。\n首先，我们知道每个单词的词汇意义是与语境有关系的。一个句子出现苹果这样的单词，你很难武断的认为这个句子是属于科技还是水果。\n其次，我们得到的每个Label Name的category vocabulary都是有数量限制的。有的单词其实也能表达当前Label Name的含义，但是并未包含在category vocabulary中。\n为了缓解这两个问题，作者提出了Masked Category Prediction (MCP)任务；\n简单讲，它分为两个步骤：\n\n针对句子中的每个单词，使用Bert这种预训练模型，找到与之最近接的前50个相关词汇（很类似第一大步骤的第一小步骤）；然后将这50个相关和每个标签的category vocabulary进行比较，当交集超过20个时候，此时这个句子中的这个单词对应的类别就是对应的这个Label Name\n句子经过第一个步骤之后，句子中的部分单词就有了类别。那么mask掉这些单词，然后Bert相对应的每一个单词尾端接一个分类器对当前单词做类别的分类。\n\n整体流程，如下图：\n\n2.3 self-training on unlabeled corpus for generalization\n经过第二个步骤，当前模型仍然存在问题：\n\n有的句子没有被找到有类别的单词，所以这些没有被训练到\n训练到文本分类模型使用的是对应类别单词mask那里的输出，而不是cls。而ClS一般可以看到整个句子的全部信息。\n\n针对这两个问题，作者提出使用全部的无标签数据，进行自训练。\n这一块我自己知识积累的不多，就不多说了。具体的大家可以去看一下论文。\n3. 模型架构总结\n整体的算法流程如下图所示：\n\n4. 实验结果分析\nDatasets使用了四种：AG News;DBPedia;IMDB;Amazon；\n实验效果图如下：\n\nBERT w. simple match情况是这样：句子只要含有标签名称的相近词，就认为当前句子是对应的标签类别，以此进行训练。\nLOTClass w/o. self train是代表LOTClass只走前两步骤，不进行自训练。\n从图中可以看到，如果不进行sefl-training，LOTClass效果在所有数据集上效果也都不错。\n使用了sefl-training之后，LOTClass 可以和半监督和监督模型结果媲美。\n4.1 细节1\n有一个问题，LOTClass这种方法，相当于在使用Bert的情况下，标注了多少数据？\n作者做了一个实验图，如下图：\n\n从效果图可以看到，LOTClass的效果和Bert在每个类别有48个标注数据的情况下训练的效果相当。\n我大概算了一下，AG News有4个类别，每个类别48个，也就是总共192个标注样本。\n4.2 细节2\n这个论文我比较感兴趣的是第一个步骤，获取标签名称的相近词汇。\n针对这个步骤，做两个方面的修改:\n\n\n修改标签词汇：分别使用commerce;economy;business作为label name；标签的名称虽然变化了，但是每个标签名称得到的100个相近词汇表有一半是重复的，另一半的词汇表意思都很类似。\n这说明，这个方法是有鲁棒性的。不会出现，标签名称换了一个相近的名字表示，而得到的词汇表出现了剧烈的抖动。\n\n\n分别使用300维度的Glove和LOTClass获取标签名称相近词汇，GLove得到的词汇非常的贫乏，而LOTClass效果很好，模型具有很好的泛化性;\n\n\n这个思路也给自己赞个思路，获取同义词或者近义词可以使用这种方法。\n5. 简单总结\n说一下自己学到的东西。\n其实看到细节1的时候，LOTClass方法得到的模型表现相当于使用192个标注数据对Bert进行监督训练。\n从这里来看，标注的成本并不大；不过，应该可以使用此方法为半监督积累数据。\n这个方法还不成熟，不过里面有些思路可以积攒。\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本分类/只使用标签名称就可以文本分类/"},{"title":"","date":"2024-06-21T03:20:46.923Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.923Z","content":"最近抽时间把SIMCSE用Pytorch复现了一把，中途涉及到的几个思考点，和大家分享一下：\n注：原作者有开源论文代码，不过有些复杂，大家可以看一下自己魔改一下；\n全文思路如下：\n\nSIMCSE理论介绍以及代码实现的部分细节点\nTextCNN是否可以借鉴SIMCSE的思路，来训练模型从而你获取比较好的Sentence embedding\n是否可以借鉴Dropout数据增强，使用amsoftmax，减少同类距离，增大不同类距离\n\n1. SIMCSE论文理论介绍\n当时读完SIMCSE论文之后，没时间写文章，赶紧发了个朋友圈把思路简单的记录了一下；\n\n感兴趣的朋友加我微信【dasounlp】，互看朋友圈啊，笑；\n论文分为四个部分来讲，对比学习，无监督SIMCSE，有监督SIMCSE，评价指标；\n1.1 对比学习\n对比学习的目的是，是减少同类距离，增大不同类之间的距离，借此获得一个文本或者图片更好的表示向量；\n定义句子对:$D={(x_{i},x_{i}^{+})}{i=1}^{N}$；其中N是一个Batch中句子对样本数量，$x{i},x_{i}^{+}$是语义相似的样本，$h_{i},h_{i}^{+}$分别是$x_{i},x_{i}^{+}$经过编码器Encoder之后得到的表示向量；\n那么对比学习的训练目标就是：\n\n这个公式看着比较唬人，其实本质就是一个多分类softamx的交叉熵损失函数；\n需要注意的是参数 $\\tau$ 是个超参数，$sim(h_{1},h_{2})$是一个相似性度量函数，原论文使用的cosine，其实使用一些其他的相似性函数应该也没问题；\n注意一下分母这里：其实一个batch，比如有N个句子对，那么就有2N个句子，其中正例是1个，负样本应该是总样本数目2N减去样本本身加上样本的正例，也就是2N-2；\n不过，看公式，作者这里用到的是一个batch中的N个样本，也就是使用的是每个句子对中的其中一个；\n关于这个问题，是否使用更多的负样本是不是会获得更好的效果，作者回复说并没有。\n我自己在复现的时候，使用的是2N-1个样本【正例+负例总和】；\n那么在落地到代码的时候，怎么实现这个交叉熵呢？我画了一个简单的图，比如batch是2：\n\n1.2 正例和负例的构建\n上面谈到的整个过程，全程没离开正例和负例；\n在图像中，一个图像经过平移旋转等数据增强的方式，可以看成是生成了图像的正例；\n在文本上，一些常规的数据增强的手段就是删减单词，替换同义词等等；\n文本的数据增强存在的一个问题就是，一个简单的操作可能就会导致语义的改变；\n在无监督的SIMCSE中，正例的构造很有意思，就是通过添加一个Dropout的噪声；\nDropout是在随机失活神经元，每次句子经过网络，失活的神经元是不一致的，导致生成的embedding是不一致的；\n这一点其实大家应该都懂，但是能联想到把这个作为数据增强的一个手段，确实很强。\n在有监督的SIMCSE中，其实是借助了NLI数据集中自带的标签，来构造正例和负例；\n直接来看作者原文中的图吧；\n\n1.3 句子向量评价指标\n句子向量的评价指标这里，用两个东西来量化一下，alignment和Uniformity；\n直接来看图：\n\n2. TextCNN和Dropout的融合\nSIMCSE中，BERT作为Encoder未免太复杂了，这时候按照常规思路，我会去思考可不可以使用简单网络比如textcnn代替bert；\n那么实现方式就可以分为两种：\n一种是我使用textcnn直接作为encoder，然后仿照无监督simcse的训练方式进行训练就可以了；\n第二种方式就是知识蒸馏，无监督simcse训练一个bert的encoder出来之后，使用简单网络textcnn进行学习就可以了；\n我针对第一种方式做了个实验。\n在实验之前，我就没报什么大的希望，只是想亲眼试一下究竟可行不可行；\n为什么没有报太大希望呢，很简单，我自己认为dropout作为一种数据增强的形式，太过简单了，textcnn这种简单网络，不足以学习到其中的差异；\n我在中文的LCQMC和ATEC数据集上做了一个简单的测试，Spearman作为评价指标，结果如下：\n\n之后，我看情况能不能把这部分代码开源出来~~，自己实现也挺简单的；\n3. Amsoftmax的引入\n第三个小思路是这样的，dropout可以看做是一个最小化的文本数据增强的形式。同一个句子，经过encoder，得到的embeding不同，但是语义是相似的，所以可以看做是一个正例；\n进一步的，如果我同一个句子经过多次encoder，比如经过10次，那么我得到的就是10个embedding；\n也就是说，在同一个语义下面，我得到的是10个语义近似但是embedding不同的向量；\n如果我有10万个句子，可以把这个10万个句子当做是10万个类别，每个类别下有10个样本；\n想一下这个感觉，不就是人脸识别的操作吗？\n那么可不可以使用这种方式，得到更好的语义表达呢？\n这个我没做实验，只是一个思路，之后有时间再去做实验，有兴趣的朋友可以做一下；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本匹配和文本相似度/SIMCSE论文解析/"},{"title":"","date":"2024-06-21T03:20:46.683Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.683Z","content":"大家好，我是DASOU；\n因为业务场景常常用到无监督语义匹配，所以一直在关注这方面的进展；\n现在大家都比较熟知的就是：BERT-Flow;BERT-Whitening和SimCSE；\n之前梳理了一下BERT-Whitening的理论和代码，分享给大家，希望有帮助；\n文章大体脉络如下：\n\nBERT-Whitening 公式推导+注解\nPCA和SVD简单梳理\n协方差矩阵的几何意义\n对BERT-Whitening 代码的简单梳理\n\n1. BERT-Whitening 解读\nBERT的输出向量在计算无监督相似度的时候效果很差是一个共识了，具体原因这里不多说，去看我之前这个文章；\n然后一个改进措施就是想要把BERT的输出向量变成高斯分布，也就是让输出向量满足各向同性；\n什么是各向同性呢？就是向量矩阵的协方差矩阵是一个单位矩阵乘以一个常数，换句话说在每个向量维度上方差是一样的；\n现在大家比较熟知的是两种方式：\n一个是bert-flow模型，采用了基于流的生成模型来做这个数据分布的转变；\n第二个是bert-whitening。\n这个文章重点聊一下BERT的白化，也就是第二种。\n它做的事情就是直接将bert的输出向量矩阵变成均值为0，协方差矩阵为单位矩阵；\n补充两个知识点，方便后续大家理解；\n第一个是，协方差矩阵是单位矩阵，说明数据分布是在一个二维的圆上，三维的球上。\n第二个是对于取值确定的矩阵A，经过W=AX变换后，协方差矩阵将变换为$V [ W ] = A V [ X ] A^{T}$\n公式推导如下:\n我们原始的向量矩阵是$X$，变化之后的矩阵是$X_{new}$；\n我们执行的变化是:\n$X_{new} = (X-\\mu)W$\n上面这个操作，是我们想让$X_{new}$的均值为0，协方差矩阵为单位阵；\n我们知道:  $V[X_{new}]=W^{T}V[X]W$\n那么就可以推导出:  $W^{T}V[X]W=E$\n进而可以推导出：$V[X]=(W^{T})^{-1}W^{-1}$\n对$V[X]$做SVD奇异值分解，有:  $V[X]=U\\Sigma V^{T}$\n因为$V[X]$是一个实对称矩阵，有:  $V[X]=U\\Sigma U^{T}$ 这是因为实对称矩阵的$X^{T}X$和$XX^{T}$相等，U和V也就相等\n也就是有：  $(W^{T})^{-1}W^{-1}=U\\Sigma U^{T}$\n求解W就好了：$W^{-1}=\\sqrt(\\Sigma)U^{T}$\n$W=U\\sqrt(\\Sigma)$\n下面我这个解读\n其实这个操作本质上就是PCA的操作，为啥这么说呢？\nPCA是得到协方差矩阵的特征向量，然后挑选出来前k个特征值对应的特征向量，然后做一个转化；这里我们把对PCAde运用停留在得到并使用全部的特征向量.\n其实核心点在于理解：\n对$V[X]$做SVD奇异值分解，有:  $V[X]=U\\Sigma V^{T}$\n因为$V[X]$是一个实对称矩阵，有:  $V[X]=U\\Sigma U^{T}$\n这个是对协方差矩阵做的奇异值分解，如果是PCA的话，就应该是协方差矩阵的特征分解；\n$V[X]=H\\Sigma H^{-1}$\n因为V[X]是要给对称矩阵，所以\n$V[X]=H\\Sigma H^{T}$\n这么一看H和U是等价的，所以之前的操作其实本质就是在做PCA；按道理我们求出来U就做PCA变化就可以了，但是我们最终的结果却是$W=U\\sqrt(\\Sigma)$;\n这是因为PCA之后数据协方差是对角矩阵，并不是一个单位矩阵；\n可以对PCA后的数据做标准化，就可以变成单位矩阵；\n苏剑林这里，直接就是强制等于单位矩阵，所以结果是$W=U\\sqrt(\\Sigma)$;\n太乱了~~\n2. 简单梳理PCA和SVD\n先总体说一下我的觉得最重要的一个知识点：\nSVD是直接对原始矩阵进行奇异值分解，得到左奇异向量，奇异值和右奇异向量；\nPCA是对矩阵的协方差矩阵进行特征分解，得到对应的特征向量和特征值，其中特征向量和SVD中的右奇异值是一个东西(如果我没记错的话~~)；\n2.1 特征分解\n先说一下特征值分解：\n一个方阵A，一般来说可以被对角化为如下式子:\n$$\nA=X\\Sigma X^{-1}\n$$\nX是A特征向量构造的矩阵，$\\Sigma$ 是一个对角阵，也就是只有对角线上有值，同时这个值是A的特征值；\n如果说这个A除了是方阵，还是一个对称阵，那么式子中的X就变成了正交矩阵，我们使用M来表示，可以对角化如下式子:\n$$\nA=M\\Sigma M^{T}\n$$\n有两个变化，一个是变成了正交矩阵，一个是最后面的是转置矩阵符号，而不是逆矩阵符号；\n2.2 PCA\nPCA分为两个步骤：\n第一个步骤是找到一组新的正交基去表示数据，比如我原来是使用n个正交基来表示数据中的向量，我现在找到另外的新的n个正交基来重新表示向量；这n个新的正交基是怎么找到呢？一个比较形象的表述是第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。\n第二个步骤经过上面这个过程，我们会发现，越到后面的基，方差越小，几乎接近于0，也就是说这些后面的基没啥作用。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。这个步骤就是在降维；\n在这里想要说一个细节点，就是经过第一个步骤之后，并不进行第二个步骤，从公式角度就是$Y_{1}=P*X$，而不是$Y_{2}=P_{k}*X$；那么得到的$Y_{1}$的协方差矩阵是一个对角化矩阵，以三维为例子，在空间上的分布是一个椭圆球体；\n这个时候如果协方差矩阵想要变成单位矩阵，就是对向量矩阵做一个标准化就可以了；\n现在有一个问题，上面我们是形象化的描述如何找到这些基，那么从实际出发，如果找到呢？\n我们是这么做的：通过计算数据矩阵的协方差矩阵，然后得到协方差矩阵的特征值特征向量，选择特征值最大(即方差最大)的k个特征所对应的特征向量组成的矩阵。这样就可以将数据矩阵转换到新的空间当中，实现数据特征的降维。\n在这里，需要注意的特征值最大，代表的就是在这个特征值对应的特征向量方向方差最大；\nPCA大体流程：\n\n我自己简单的总结就是，首先对数据进行中心化，然后计算协方差矩阵，然后计算对应的特征值和特征向量等等；\n需要注意的是，第一个步骤之后，如果我们不想去降低维度，那么这个全部的特征向量也可以使用，简单说就是$Y=A*P_{全部}$; 这个操作就是对原始数据做了一个旋转变化，协方差矩阵会变成对角矩阵；\n2.3 SVD分解：\n奇异值分解是一个能适用于任意矩阵的一种分解的方法，对于任意矩阵A总是存在一个奇异值分解：\n$$\nA=U\\Sigma V^{T}\n$$\n假设A是一个$mn$的矩阵，那么得到的U是一个$mm$的方阵，U里面的正交向量被称为左奇异向量。Σ是一个$m*n$的矩阵，Σ除了对角线其它元素都为0，对角线上的元素称为奇异值。\n$V^{T} $是v的转置矩阵，是一个n*n的矩阵，它里面的正交向量被称为右奇异值向量。而且一般来讲，我们会将Σ上的值按从大到小的顺序排列。\n在这里有几个点想要强调一下：\nU这里对应的是$A*A^{T}$ 对应的特征向量；\nV这里对应的是$A^{T}A$对应的特征向量，也就是A矩阵的协方差矩阵对应的特征向量；这一点比较重要，我们在使用PCA降低维度的时候，想要拿到的那个变化矩阵就是这个V（注解，挑选前K个），也就是变化之后为$AV$;\n通过$A=U\\Sigma V^{T}$，我们也可以得到这样一个结果$AV=U\\Sigma$\n所以在降低维度的时候我们这两种都可以；\n具体讲解看这里：\n\nSVD一般流程【也会拿计算协方差矩阵的方法，不是改进的方法】：\n\n2.4 协方差矩阵的几何意义：\n协方差矩阵是一个单位矩阵，数据是分布在一个圆上；\n协方差矩阵是一个对角化矩阵，我们可以将原始数据标准化，这样对应的数据的协方差矩阵就会变成单位矩阵，还可以对原始数据进行平移，移动到原点附近的圆上；\n如果协方差矩阵是一个普通的矩阵，我们可以做PCA【不降低维度的那种】，将其转化为对角化矩阵，之后做标准化，这样协方差矩阵变成了单位矩阵，然后对数据做平移，移动到原点附近；\n3. 梳理BERT白化代码：\n有两个版本的代码，\n一个是苏剑林的Keras版本：https://github.com/bojone/BERT-whitening\n一个是Pytorch版本：https://github.com/autoliuweijie/BERT-whitening-pytorch\n我看了一遍Pytorch版本，主要的细节点我罗列在下面；\n首先就是下载数据和下载一些英文预训练模型。\n之后就是跑代码，分为三种方向：\n第一种就是不使用白化的方式，直接在任务中使用BERT的输出向量；\n第二种是在任务中数据中使用白化方式，也就是在任务数据中计算kernel和bias，然后在任务数据中使用此参数，\n去对BERT系列预训练模型的输出向量做转化；\n第三种是在大数据中，在这里也就是NLI数据，计算相应的kernel和bias，然后在任务数据中使用这个参数，去做对应的转化；\n第三种方式很方便，如果实际工作真的使用bert白化，肯定是我在训练数据中计算出来参数，然后在测试数据中使用这个参数直接去做转化，这样效率最高。\n把测试数据补充进来然后再重新计算对应的参数，感觉总是多了一个步骤，效率不高；\n这就要求我们在大数据中计算参数的时候，确保大数据具有普适性，能够很好的适配任务数据；这样计算出来的参数才有使用的可能；\n在实际运行代码的时候，我只是使用了SICKRelatednessCosin这个任务，整体代码写的相当的不错，大致的过一遍也就可以了；\n最核心的代码是这个：\n12345678910111213141516171819def compute_kernel_bias(vecs, n_components):    &quot;&quot;&quot;计算kernel和bias    最后的变换：y = (x + bias).dot(kernel)    &quot;&quot;&quot;    vecs = np.concatenate(vecs, axis=0)    mu = vecs.mean(axis=0, keepdims=True)    cov = np.cov(vecs.T)    u, s, vh = np.linalg.svd(cov)    W = np.dot(u, np.diag(s**0.5))    W = np.linalg.inv(W.T)    W = W[:, :n_components]    return W, -mudef transform_and_normalize(vecs, kernel, bias):    &quot;&quot;&quot;应用变换，然后标准化    &quot;&quot;&quot;    if not (kernel is None or bias is None):        vecs = (vecs + bias).dot(kernel)    return vecs / (vecs**2).sum(axis=1, keepdims=True)**0.5\n参考：\n苏剑林Bert-whitening：https://kexue.fm/archives/8069\n奇异值分解(SVD)原理 - 鱼遇雨欲语与余的文章 - 知乎 https://zhuanlan.zhihu.com/p/32600280\nPCA 通过 SVD 分解替代协方差矩阵的特征值分解\n协方差矩阵的几何意义：https://blog.csdn.net/nstarLDS/article/details/104874622/\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本匹配和文本相似度/bert白化简单的梳理/"},{"title":"","date":"2024-06-21T03:20:47.243Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.243Z","content":"在学习DSSM的时候，很容易和孪生网络搞混，我这里稍微总结一下自己的思考；\n对于孪生网络，并不是一个网络的名称，而是一种网络的名称；\n它的特点就是encoder参数共享，也就是在对句子或者图像编码的网络权重共享；\n它的网络的输入形式是这样的:：(X1,X2,Y)；X1，X2是两个输入文本，Y是我们的标签数据；\n对于孪生网络来说，一般的损失函数是对比损失函数：Contrastive Loss\n什么是对比损失函数呢？公式如下：\n\n这个公式需要注意两个细节点，一个是d，代表的是距离度量，一个是margin，代表的是一个超参；\n那么我感兴趣的是孪生网路可以不可以使用其他的损失函数？如果可以，又是哪些呢？\n首先当然是可以，只要能够优化都可以；\n\n参考自这里：\nSiamese network 孪生神经网络–一个简单神奇的结构 - mountain blue的文章 - 知乎 https://zhuanlan.zhihu.com/p/35040994\n三种损失函数形式\n我把自己思考的三种损失函数形式总结在这里，之后有问题再回来修改:\n两个向量做consine相似度或者欧氏距离度量函数，然后归一化到0-1，然后做二分类交叉熵损失函数；\n两个向量做cosine相似度或者欧氏距离，带入到对应的对比损失函数\n两个向量做拼接或者其他操作，然后接单个或者多个全连接，然后可以做逻辑回归或者做softmax；\nfassi做向量召回-样本重复和consine度量疑惑点\n我其实一直有一个疑问，在做向量召回的时候，一般的操作就是双塔模型，然后存储对应的样本的向量，存储到fassi中，然后搜索的时候使用找最近的向量就够了；\n这里面我最开始理解的时候有两个疑惑点，一个是如果做到同一个样本不会有多个向量；\n我的误解原因是没有对向量的是如何落地有很好的理解；我开始的理解是模型训练好了之后，进入一个pair（query，d），分别计算向量，然后存储，这样当然会出现同一个d，出现在不同的pair；\n但是，由于之间没有交互，右边这个塔模型参数不变，得到的向量当然也不变，也就是同一个d不会出现多个向量；\n而且在计算的时候，不用输入一个pair对，只需要对右边这个单塔输入去重之后的d就可以；\n第二个问题，为什么在faiss中使用最近的向量可以（先不用计较度量方式）得到相似的向量，而在模型中，我们在得到cosine或者其他度量结果之后，还会再接一个sigmoid；\n优化sigmoid的输出的时候，cosine的值或者其他度量的值也会同等趋势变化，所以可以起到作用；\n然后多说一下，如果用到向量召回，中间还是需要一个度量的值的，需要和faiss对应上，如果接一个MLP，感觉就够呛。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本匹配和文本相似度/聊一下孪生网络和DSSM的混淆点以及向量召回的一个细节/"},{"title":"","date":"2024-06-21T03:20:47.223Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.223Z","content":"[TOC]\n五千字梳理文本相似度判定(无监督+有监督)\n1. 为什么需要文本匹配/文本相似度判定使用场景\n先问一个核心问题，为啥需要文本相似度/文本匹配？换句话说，文本匹配的应用场景有哪些？\n举个例子，比如说海量文本去重。\n在一些社交媒体，某个话题之下，存在大量的营销号的文本，这些文本存在一个特点，内容大同小异，都在说同一个事情。\n我在对这些文本进行处理的时候，本质上只需要处理其中的一条就可以，这样可以极大提高我的处理速度。\n那么问题来了，我如何判定句子a和句子b/c/d/e等等是在说同一个事情？\n再举个例子，老生常谈，搜索场景，你在某度搜索“深度学习如何入门？”，某度如何返回和你这个问题最接近的问题/文章/博客等等内容网页？\n这些本质上都是在做文本相似度的判定或者说文本匹配，只是在不同场景下，有着不同办法不同的特色。\n文本匹配，并不是简简单单的词汇层次的匹配：比如“深度学习如何入门”和“如何入门深度学习”。\n还会有语义方面的匹配考虑，比如说“我想撒尿去哪里啊？”和“我想去卫生间”。这两句话在问答系统中，匹配到标准问题都是“卫生间在哪里”这个问题。\n对文本匹配方法来说，我们一般可以使用两种：无监督和有监督。\n2.无监督文本匹配\n首先我们来谈一下无监督的方法。\n对于无监督的文本匹配，我们需要实时把握两个重点：文本表征和相似函数的度量。\n文本表征指的是我们将文本表示为计算机可以处理的形式，更准确了来说是数字化文本。而这个数字化文本，必须能够表征文本信息，这样才说的通。\n相似函数的度量就是你选择何种函数对文本相似度进行一个判定，比如欧氏距离，余弦距离，Jacard相似度，海明距离等等\n我大概梳理了一下无监督的几种比较典型的方法，，如下所示：\n\n\nTF-IDF/IDF+词向量(word2vec/fasttext/glove)\n\n\nBM25（提前计算IDF矩阵，无需使用词向量）\n\n\nWMD\n\n\nSIF\n\n\nTF-IDF/IDF+词向量比较简单，我就不多说了。我们先来看一下BM25。\n2.1 BM25\n对于BM25，有搜索Query q，分词之后单词 w，候选文档 d。\n掌握BM25，核心要点有三个：分词之后w的权重，w和q的相似性，w和d的相似性。\n对于搜索场景，显而易见的一个问题就是，我们的搜索query和候选文档的长度是不一样甚至差距很大，所以BM25在计算相似性的时候需要对文档长度做一定的处理。\n2.2 WMD\n话说回来，其次我们谈一谈WMD，项目中并没有用到 WMD 这个获取句子向量的方法，所以这里只是对它有一个简单的了解即可，在这里做一个记录，等以后需要用到的时候， 会在这里继续更新，深挖进入。文章末尾会列出相关参考资源，感兴趣的可以看一看。\nWMD Word Mover’s Distance的缩写，翻译为词移距离，WMD距离越大相似度越小，WMD距离越小文本相似度越大。\n理解这个概念，分为两个步骤。\n首先第一步，有两个文档A和B，A中的每个词遍历和B中每个词的距离，挑出A中每个词和B中每个词中最小的距离，最后相加，得到A到B的WMD。\n这个时候，需要明白第一步是存在巨大问题的。什么问题呢？A，B，C三个文档。A全部词和音乐相关。B中一个词和音乐相关，C中一个词和音乐相关，其余词和音乐有点关系，而且定义B和C中和音乐相关的词是同一个词。\n根据我们的直觉，A和B相关性肯定小于A和C的相关性，但是如果按照第一步的算法去做，会得到相关性相等的结果，这是不对的。\n这是因为A中的所有词匹配到的B中的那个和音乐相关的词（遍历取最小），A中所有的词匹配到C中和音乐相关的词（遍历取最小），B和C中和音乐相关的词一样，就导致相关性相等。\n怎么解决这个问题，这就是第二步，让A中的所有词在匹配的时候，不是遍历取最小，而是让每个词匹配到C中的所有词，只不过匹配的权重不同。\n这个权重从两个部分去看：一个是从A看，要符合分配出去的权重等于自身这个词在本文档的权重，一个是从B看，分配到B的某个词的权重要等于这个词在B文档的权重。\n大概理解到这里。之后关于WMD加速（因为复杂度比较高）的内容就没有进一步的去了解\n2.3 SIF\n这个时候，我想到了一个比较有意思的点。我们知道有监督，比如说SiaGRU 这个方法，也是对句子进行编码，只不过这里的编码我们使用的是基于损失函数和标注语料训练出来的编码。\n注意，这句话里我认为是有个重点的，就是句子编码是基于损失函数训练出来的。\n换句话说，不同的损失函数我们训练出来的句子编码肯定是不一样的，效果也就有好有坏。\n换句话说，我们使用不同的损失函数，比如调整正样本对应的损失权重和正常损失函数训练出来的文本编码肯定是不一样的。\n为什么说这么呢？我们想一下，如果我使用SiaGRU训练出来的句子对的编码，我们直接使用cosine进行相似度的度量，效果会怎么样？\n对于这个问题，大家可以自己实践一下。我想提到的一点就是，我们的句子对的编码并不是基于我们的cosine这种相似度量训练出来的，所以不存在完全匹配，所以效果肯定是比不上有监督的。\n我们更近一步的想一个问题。通过实践，我们在使用bert做句子对的相似度的时候会发现一个问题，就是效果可能还没有使用word2vec的效果好。\n我自己猜测这个原因可能就是因为cosine这个函数没有办法表征出bert训练的句子编码信息，因为bert训练过程复杂，不是consien这种简单函数就可以表达的。这是我自己的一个理解。\n然后，我们详细聊一下SIF这个方法。\n我先说一下这个方法最容易出问题的一个地方：数据预处理的时候不要去除停用词等高频词汇\n掌握SIF，就抓住两个核心要点：\n\n\n词向量加权求和\n\n\n句子词向量矩阵减去主成分投影\n\n\n我们先说核心要点1。其实词向量加权求和这个方法并不是特别的陌生，比如等权求和，IDF权重求和，TF-IDF权重求和等等，这些我们平时都会用到。至于用哪一个就看你的数据集上的效果了。\n我们知道word2vec这种词向量是含有语义信息的（在我看来其实它本质上是一种位置信息，相同句子结构下不同词的词向量可能很相似）。所以等权求和或者加权求和更像是在做Pooling（最大池化，平均池化，加权池化）等等，就是从我们各个词语信息中提取中有用的部分。\n因为我们是无监督，这个权重的设定我们没有办法像在CNN处理图片一样，把参数学出来，所以只能人工的去定这个参数。\n说到这里，想要插一句话，上面说到因为无监督我们无法将权重参数学出来。换句话说，如果是有监督，这个参数是可以学出来的，那怎么学呢？\n最简单的一个方法就是全连接，或者说逻辑回归啊，只要有数据，我们就可以使用机器学习，复杂就使用神经网络基于标注数据把这个参数学出来。\n现在没有标注数据，这个参数我们需要人工去定。这个参数需要含有一定的意义，不是我们一拍脑袋就定个参数，它需要加权求和之后使句子编码含有的语义信息可以对下游任务有帮助或者说在下游任务中效果好才可以。\nSIF这里使用的权重函数是这样的：a/a+p(w) p(w)代表的是词在句子中的频次或者频率。用大白话去描述这个公式就是词在语料中出现的越多，对应的权重越小。这么做就减少了高频词的作用。也是我们不需要在数据处理的时候去除高频词的原因。\n想一下这个公式的作用，其实本质上和IDF是很类似的。它并没有涉及到IF这个概念，也就是词在本句话出现的情况。\n我们再来看核心要点2。什么叫主成分？具体的原理不多说，PCA本质上是找到原始数据存在最大方差的那个方向，让原始数据的投影尽可能的分散，从而可以起到降维的作用。\n核心要点2就是减去了矩阵中共有的一部分信息，比如都含有的高频词汇信息等等。比如原始句子是”我爱吃红烧肉“和”我超不爱吃红烧肉“。减去共有信息之后，剩下的词向量矩阵就回对不相似的信息比较敏感。\n总体来说。SIF这个算法还是很不错，很适合做个基线初期上线的。\n3.有监督文本相似度方法\n我主要是讲两个模型：SiamGRU和ESIM模型\n3.1 SiamGRU\n主要是参考了论文：Siamese Recurrent Architectures for Learning Sentence Similarity.\n从三个核心要点来掌握这个模型：\n1.使用神经网络对句子进行编码；\n2.两个句子共享一个神经网络；\n3.对句子编码进行相似性的度量进行训练。\n首先我们来看核心要点1。\n对于这个我觉得应该这么去想。首先，我们考虑一个使用神经网络进行文本分类的场景。我们使用交叉熵损失函数，基于此，训练神经网络。\n在这个过程中，神经网络可以认为依赖了两个东西，一个是标注数据，\n比如属于娱乐领域或者音乐领域等等，我们通过更新神经网络的参数让我们的模型预测结果不停的逼近这个标注数据。\n其次，我们使用损失函数来度量预测结果和真实数据的差距。损失函数在我看来更像是一种解题方法，\n不同的解题方法可能对应不同的结果，有的解题方法得到的结果既高效又准确，有的方法就很差。\n为什么讲上面这些话？神经网络在上面这个过程中究竟起到了什么作用？在上面这个例子中，神经网络就是一个载体，接受文本数据，输出一个文本编码，从而判定属于哪个类别。所以神经网络的输出是带有语义信息。\nSiamGRU的损失函数应该是什么？数据格式是输入一个句子对，标注结果是两个句子是不是表达同一个含义。这个属于0/1的二分类问题，也就是损失函数和二分类问题的损失函数是一样的，基于此损失函数，我们对模型进行训练。\n对于核心要点2，两个句子共享一个神经网络。最大的好处是可以减少参数量。对于这个，我们可以想一下，能不能不共享参数，而是每个句子对应一个自己的Bilstm/GRU等编码器。\n当然可以，我们的神经网络只是对句子进行编码，每个句子对应自己的编码器也可以做到这样的效果，所以没问题。我们再想一下，有没有必要这么做？关于这一点，我并没有去具体的做实验。我只是说一下自己的理解。\n我觉得是没有必要的。首先，如果每个句子都对应自己的编码器，我想到的一个问题是需不需要对训练数据进行一个翻转重新输入的问题。换句话说，&lt;a,b&gt;作为输入，分别对应&lt;encode1,endode2&gt;。\n那么我需不需要&lt;b,a&gt;作为一次输入再训练一次增加泛化性，我认为这个步骤是需要，这样就增加了训练来量。\n简单来说，我们的句子对的输入不应该对句子对的输入顺序敏感，所以共享编码反而很好的解决了这个问题。\n最后，我们来看核心要点3，我觉得这里还是值得注意的。\n这里我认为有两个细节点需要注意。首先，这里做了一个两个句子编码的交互。对的，SiaGRU在我看来是对两个句子编码进行了交互的，只不过是在后期做的交互。\n为什么这么说呢？这里的相似性的度量使用的是曼哈顿距离，也就是对应坐标差值。注意，这里涉及到了对应坐标位置的操作，所以可以看做是一种交互，而且这种差值在模型训练过程中是会随着模型参数的变化而变化的。\n其次一个细节点，还是曼哈顿距离，这里不是一个标量，也就是不是一个和。简单来说就是对应位置相减之后，每个位置对应一个神经元，然后直接接全连接层就可以。\n如果你认为这里是一个标量，就是很大的问题，标量如何接全连接层最终做二分类呢？\n代码实现部分参考：\nhttps://github.com/DA-southampton/NLP_ability/blob/master/深度学习自然语言处理/文本匹配_文本相似度/src/models.py\n3.2 ESIM\n理解ESIM模型最核心的要点就在于两个句子word层次的attentino就可以了。\n区别于SiamGRU 在后期进行两个句子之间的交互，ESIM在模型中期进行了word层级的两个句子之间的交互。这种交互在我看来是一种attention。\n最近在做多模态的东西，其中一部分的attention和这里的很类似，更准确的说attention形式很类似。\n整个ESIM可以分为三个部分。\n首先第一部分是对两个句子的初期编码，这里和SiamGRU没有什么区别，都是使用神经网络对句子进行信息的提取。\n需要注意的一点是在这里，我们编码之后，比如说LSTM，我们是可以得到每一个时刻或者说每一个单词的输出的，这就为交互中用到的Q/K/V提供了基础。\n这里，我们使用LSTM作为编码器，然后得到编码向量，得到的维度是这样的[batch_size,seq_len,embedding_dim]。\n为了下面解释的更加的方便，我们举个简单的例子，句子a，长度为10，句子b，长度为20.\n现在经过初期编码，句子a：[1,10,300],句子b: [1,20,300]\n随后，我们的操作就是进行交互操作，为了简单，我们省略掉第一个维度。它的交互就是矩阵相乘，得到[10,20]一个矩阵。\n这个矩阵需要根据针对a句子横向做概率归一化，针对b句子纵向做概率归一化。\n上面这句话其实就是ESIM的核心要点。它是一个两个item之间互相做attention，简单称之为both attention。\n这样针对句子a我们就获得了attention之后的语境向量，针对句子b我们也获得了attention之后的语境向量，然后各自在做差和点击，然后结果拼接。\n最后我们用获得的向量拼接输入到另一个编码器，输出pool接全连接就可以了，这块没有什么好说的。\n代码参考链接：\nhttps://github.com/DA-southampton/NLP_ability/blob/master/深度学习自然语言处理/文本匹配_文本相似度/src/models.py\n总结\n针对不同的业务场景挑选不同的匹配模型很考验一个工程师的能力，所以需要掌握每个模型的特点和优缺点。\n参考链接：\n参考链接：常见文本相似度计算方法简介 - 李鹏宇的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/88938220\n短文本相似度算法研究 - 刘聪NLP的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/111414376\n现在工业界有哪些比较实用的计算短文本相似度的算法或者模型？ - vincent的回答 - 知乎\nhttps://www.zhihu.com/question/342548427/answer/806986596\n推荐系统中的深度匹配模型 - 辛俊波的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/101136699\n四种计算文本相似度的方法对比 - 论智的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/37104535\n深度文本匹配发展总结 - xiayto的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/40741576\n常见文本相似度计算方法简介 - 李鹏宇的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/88938220\n参考链接：\nhttps://zhuanlan.zhihu.com/p/48188731\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本匹配和文本相似度/五千字全面梳理文本相似度和文本匹配模型/"},{"title":"","date":"2024-06-21T03:20:47.253Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.253Z","content":"RE2 这个名称来源于该网络三个重要部分的合体：Residual vectors；Embedding vectors；Encoded vectors;\n掌握这个论文，最重要的一个细节点就是了解如何将增强残差连接融入到模型之中。\n1.架构图\n先来看架构图，如下：\n\n这个架构图很精简，所以不太容易理解。\n大体上区分可以分为三层。第一层就是输入层，第二个就是中间处理层，第三个就是输出层。\n中间处理层我们可以称之为block，就是画虚线的部分，可以被循环为n次，但是需要注意的是每个block不是共享的，参数是不同的，是独立的，这点需要注意。\n2.增强残差连接\n其实这个论文比较有意思的点就是增强残差连接这里。架构图在这里其实很精简，容易看糊涂，要理解还是要看代码和公式。\n2.1 第一个残差\n首先假设我们的句子长度为$l$，然后对于第n个block（就是第n个虚线框的部分）。\n它的输入和输出分别是:$x^{(n)}=(x_{1}^{(n)},x_{2}^{(n)},…,x_{l}^{(n)})$ 和$o^{(n)}=(o_{1}^{(n)},o_{2}^{(n)},…,o_{l}^{(n)})$;\n首先对一第一个block，也就是$x^{(1)}$，它的输入是embedding层，注意这里仅仅是embedding层；\n对于第二个block，也就是$x^{(2)}$，它的输入是embedding层（就是初始的embedding层）和第一个block的输出$o^{(1)}$拼接在一起；\n紧接着对于n大于2的情况下，也就是对于第三个，第四个等等的block，它的输入形式是这样的;\n\n理解的重点在这里：在每个block的输入，大体可以分为两个部分，第一部分就是初始的embedding层，这个永远不变，第二个部分就是此时block之前的两层的blocks的输出和；这两个部分进行拼接。\n这是第一个体现残差的部分。\n2.2第二个残差\n第二个残差的部分在block内部：\nalignment层之前的输入就有三个部分：第一部分就是embedding，第二部分就是前两层的输出，第三部分就是encoder的输出。\n这点结合着图就很好理解了。\n3.Alignment Layer\nattention这里其实操作比较常规，和ESIM很类似，大家可以去看之前这个文章。\n公式大概如下：\n\n\n这里有一个细节点需要注意，在源码中计算softmax之前，也是做了类似TRM中的缩放，也就是参数，放个代码：\n1234567#核心代码def __init__(self, args, __):        super().__init__()        self.temperature = nn.Parameter(torch.tensor(1 / math.sqrt(args.hidden_size)))def _attention(self, a, b):        return torch.matmul(a, b.transpose(1, 2)) * self.temperature\n4.Fusion Layer\n融合层，就是对attentino之前和之后的特征进行一个融合，具体如下：\n\n三种融合方式分别是直接拼接，算了对位减法然后拼接，算了对位乘法然后拼接。最后是对三个融合结果进行拼接。\n有一个很有意思的点，作者说到减法强调了两句话的不同，而乘法强调了两句话相同的地方。\n5.Prediction Layer\nPooling层之后两个句子分别得到向量表达：$v_{1}$和$v_{2}$\n三个表达方式，各取所需就可以：\n\n\n\n6. 总结\n简单总结一下，这个论文最主要就是掌握残差连接。\n残差体现在模型两个地方，一个是block外，一个是block内；\n对于block，需要了解的是，每一个block的输入是有两部分拼接而成，一个是最初始的embeddding，一个是之前两层的输出和。\n对于block内，需要注意的是Alignment之前，有三个部分的输入一个是最初始的embeddding，一个是之前两层的输出和，还有一个是encoder的输出。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本匹配和文本相似度/阿里RE2-将残差连接和文本匹配模型融合/"},{"title":"","date":"2024-06-21T03:20:47.403Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.403Z","content":"文本纠错\n医疗健康领域的短文本解析探索（三) ----文本纠错\nhttps://mp.weixin.qq.com/s/p9UMvy_VSW5g9IqDDjK6GA\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本纠错/文本纠错资源总结/"},{"title":"","date":"2024-06-21T03:20:51.543Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:51.543Z","content":"机器翻译\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/README/"},{"title":"","date":"2024-06-21T03:20:47.483Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.483Z","content":"bpe论文的我的阅读感受\nNeural Machine Translation of Rare Words with Subword Units\n提出这个算法的直觉是这样的，作者发现翻译一个单词，有时候不需要这个单词的全部信息，可能只需要一部分信息就可以知道大致信息。还有一种可能是翻译这个单词与，可能通过组成\n这个单词的多个小单元信息来翻译就可以了。\n这个方法是为了解决稀少单词（也就是频率在人为规定下的单词）和未登录词没有办法有效翻译的问题。\n这里作者在摘要中提到了一个back-off 字典，就是说在之前翻译模型在处理未登录词汇的时候，处理办法是使用一个词典，把source-target中未登陆词汇一一对应起来，我们在翻译过程中\n如果出现了未登录词汇，直接使用字典中的对应关系进行替换就可以了。但是这样存在一个问题，就是说，最低频率是我们人为规定的，有些时候在调参的时候，这个频率是一个超参，，那么\n我们在准备词典的时候，就是一个动态的长度，这样很不方便，但是如果我们准备所有单词的back-off就得不偿失。还有一个问题是我们不确定source-target对应的关系是一一对应的，可能对应不上，可能对应\n是多种，在不同句子环境中，我们需要选择不同的单词翻译，这也是存在的一个问题。\n基于word-level的模型还存在一个问题，就是不能产生没有看见过的单词，也就是说在翻译端，没有出现在词汇表中的在翻译模型测试的时候是不会出现的。这其实是一个很重要的问题，就是我不能确定\n我的训练语料包含所有情况下的翻译。\n作者在摘要中说明，自己使用了字符级的n-gram和bpe方法，在WMT 15 英文德文翻译中提升1.1，在英文俄罗斯中提升1.3。\n翻译是一个开放词汇表的问题，我们在翻译模型中，一般把翻译模型词汇表限制在30000–50000（基于词）。\n作者通过实验发现，使用subeword模型，比使用大量词汇表的模型和使用back-off模型效果很好更简单。\n我在博客中看到了总结这个论文不错的博客，总结在下面\n通过BPE解决OOV问题----Neural machine Translation of Rare Words with Subword Units\nhttps://blog.csdn.net/weixin_38937984/article/details/101723700\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/bpe-subword论文的我的阅读总结/"},{"title":"","date":"2024-06-21T03:20:51.783Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:51.783Z","content":"假如手上有一个文本分类任务，我们在提升模型效果的时候一般有以下几个思路：\n\n\n增大数据集，同时提升标注质量\n\n\n寻找更多有效的文本特征，比如词性特征，词边界特征等等\n\n\n更换模型，使用更加适合当前任务或者说更加复杂的模型，比如FastText–&gt;TextCNN–Bert\n\n\n…\n之后接触到了知识蒸馏，学习到了简单的神经网络可以从复杂的网路中学习知识，进而提升模型效果。\n之前写个一个文章是TextCNN如何逼近Bert，当时写得比较粗糙，但是比较核心的点已经写出来。\n这个文章脱胎于这个论文：Distilling Task-Specific Knowledge from BERT into Simple Neural Networks\n整个训练过程是这样的：\n\n在标签数据上微调Bert模型\n使用三种方式对无标签数据进行数据增强\nBert模型在无标签数据上进行推理，Lstm模型学习Bert模型的推理结果，使用MSE作为损失函数。\n\n目标函数\n知识蒸馏的目标函数：\n\n一般来说，我们会使用两个部分，一个是硬目标损失函数，一个是软目标损失函数，两者都可以使用交叉熵进行度量。\n在原论文中，作者在计算损失函数的时候只是使用到了软目标，同时这个软目标并不是使用softmax之前的logits进行MSE度量损失，也就是并没有使用带有温度参数T的sotmax进行归一化。\n数据增强\n为了促进有效的知识转移，我们经常需要一个庞大的，未标记的数据集。\n三种数据增强的方式：\n\n\nMasking：使用概率$P_{mask}$随机的替换一个单词为[MASK].\n需要注意的是这里替换之后，Bert模型也会输入这个数据的。从直觉上来讲，这个规则可以阐明每个单词对标签的影响。\n\n\nPOS-guided word replacement.使用概率$P_{pos}$随机替换一个单词为另一个相同POS的单词。这个规则有可能会改变句子的语义信息。\n\n\nn-gram sampling\n\n\n整个流程是这样的：对于每个单词，如果概率p&lt;$p_{mask}$，我们使用第一条规则，如果p&lt;$p_{mask}+p_{pos}$，我们使用第二条规则，两条规则互斥，也就是同一个单词只使用两者之间的一个。当对句子中的每个单词都过了一遍之后，我进行第三条规则，之后把整条句子补充道无标签数据集中。\n知识蒸馏结果图\n效果图：\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/Bert蒸馏到简单网络lstm/"},{"title":"","date":"2024-06-21T03:20:51.773Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:51.773Z","content":"大家好，我是DASOU；\n今天从代码角度深入了解一下知识蒸馏，主要核心部分就是分析一下在知识蒸馏中损失函数是如何实现的；\n之前写过一个关于BERT知识蒸馏的理论的文章，感兴趣的朋友可以去看一下：Bert知识蒸馏系列(一)：什么是知识蒸馏。\n知识蒸馏一个简单的脉络可以这么去梳理：学什么，从哪里学，怎么学？\n学什么：学的是老师的知识，体现在网络的参数上；\n从哪里学：输入层，中间层，输出层；\n怎么学：损失函数度量老师网络和学生网络的差异性；\n从架构上来说，BERT可以蒸馏到简单的TextCNN，LSTM等，也就可以蒸馏到TRM架构模型，比如12层BERT到4层BERT；\n之前工作中用到的是BERT蒸馏到TextCNN；\n最近在往TRM蒸馏靠近，使用的是 Textbrewer 这个库（这个库太强大了）；\n接下来，我从代码的角度来梳理一下知识蒸馏的核心步骤，其实最主要的就是分析一下损失函数那块的代码形式。\n我以一个文本分类的任务为例子，在阅读理解的过程中，最需要注意的一点是数据的流入流出的Shape，这个很重要，在自己写代码的时候，最重要的其实就是这个；\n首先使用的是MNLI任务，也就是一个文本分类任务，三个标签；\n输入为Batch_data：[32,128]—[Batch_size,seq_len];\n老师网络：BERT_base：12层，Hidden_size为768；\n学生网络：BERT_base：4层，Hidden_size为312；\n首先第一个步骤是训练一个老师网络，这个没啥可说。\n其次是初始化学生网络，然后将输入Batch_data流经两个网络；\n在初始化学生网络的时候，之前有的同学问到是如何初始化的一个BERT模型的；\n关于这个，最主要的是修改Config文件那里的层数，由正常的12改为4，然后如果你不是从本地load参数到学生网络，BERT模型的类会自动调用初始化；\n关于代码实现，我之前写过一个文章，大家可以看这里的代码解析，更加的清洗一点：Pytorch代码验证–如何让Bert在finetune小数据集时更“稳”一点；\n然后我们来说数据首先流经学生网络，我们得到两个东西，一个是最后一层【CLS】的输出，此时未经softmax操作，所以是logits，维度为：[32,3]-[batch_size,label_size];\n第二个东西是中间隐层的输出，维度为:[5,32,128,312]，也就是 [隐层数量,batch_size,seq_len,Hidden_size];\n需要注意的是这里的隐层数量是5，因为正常的隐层在模型定义的时候是4，然后这里是加上了embedding层；\n还有一点需要注意的是，在度量学生网络和老师网络隐层差异的时候，这里是度量的seq_len，也就是对每个token的输出都做了操作；\n如果在这里我们想做类似【CLS】的输出的时候，只需要提取最开始的一个[32,312]的向量就可以；不过，一般来说我们不这么做；\n其次流经老师网络，我们同样得到两个东西，一个是最后一层【CLS】的输出，此时未经softmax操作，所以是logits，维度为：[32,3]-[batch_size,label_size];\n第二个东西是中间隐层的输出，维度为:[5,32,128,768]，也就是 [隐层数量,batch_size,seq_len,Hidden_size];\n这里需要注意的是老师网络和学生网络隐层数量不一样，一个是768，一个是312。\n这其实是一个很常见的现象；就是我们的学生网络在减少参数的时候，不仅会变矮，有时候我们也想让它变窄，也就是隐层的输出会发生变化，从768变为312；\n这个维度的变化需要注意两点，首先就是在学生模型初始化的时候，不能套用老师网络的对应层的参数，因为隐层Hidden_size发生了变化。所以一般调用的是BERT自带的初始化方式；\n其次就是在度量学生网络和老师网络差异性的时候，因为矩阵大小不一致，不能直接做MSE。在代码层面上，需要做一个线性映射，才能做MSE。\n而且还需要注意的一点是，由于老师网络已经固定不动了，所以在做映射的时候我们是要对学生网路的312加一个线性层转化到768层，也就是说这个线性层是加在了学生网络；\n整个架构的损失函数可以分为三种：首先对于【CLS】的输出，使用KL散度度量差异；对于隐层输出使用MSE和MMD损失函数进行度量；\n对于损失函数这块的选择，其实我觉得没啥经验可说，只能试一试；\n看了很多论文加上自己的经验，一般来说在最后面使用KL，中间层使用MSE会更好一点；当然有的实验也会在最后一层直接用MSE；玄学。\n在初看代码的时候，MMD这个之前我没接触过，还特意去看了一下，关于理论我就不多说了，一会看代码吧。\n首先对【CLS】的输出，代码如下：\n12345678def kd_ce_loss(logits_S, logits_T, temperature=1):    if isinstance(temperature, torch.Tensor) and temperature.dim() &gt; 0:        temperature = temperature.unsqueeze(-1)    beta_logits_T = logits_T / temperature    beta_logits_S = logits_S / temperature    p_T = F.softmax(beta_logits_T, dim=-1)    loss = -(p_T * F.log_softmax(beta_logits_S, dim=-1)).sum(dim=-1).mean()    return loss\n首先对于 logits_S，就是学生网络的【CLS】的输出，logits_T就是老师网络【CLS】的输出，temperature 在代码中默认参数是1，例子中设置为了8；\n整个代码其实很简单，就是先做Temp的一个转化，注意这里我们对学生网络的输出和老师网络的输出都做了转化，然后做loss计算；\n其次我们来看比较复杂的中间层的度量；\n首先需要掌握一点，就是学生网络和老师网络层之间的对应关系；\n学生网络是4层，老师网络12层，那么在对应的时候，简单的对应关系就是这样的：\n12345layer_T : 0, layer_S : 0,layer_T : 3, layer_S : 1, layer_T : 6, layer_S : 2, layer_T : 9, layer_S : 3,layer_T : 12, layer_S : 4，\n这个对应关系是需要我们认为去设定的，将学生网络的1层对应到老师网络的12层可不可以？当然可以，但是效果不一定好；\n一般来说等间隔的对应上就好；\n这个对应关系其实还有一个用处，就是学生网络在初始化的时候【假如没有变窄，只是变矮，也就是层数变低了】，那么可以从依据这个对应关系把权重copy过来；\n学生网络的隐层输出为：[5,32,128,312],老师网络隐层输出为[5,32,128,768]\n那么在代码实现的时候，需要做一个zip函数把对应层映射过去，然后每一层计算MSE，然后加起来作为损失函数；\n我们来看代码：\n12345678910111213141516171819202122inters_T = &#123;feature: results_T.get(feature,[]) for feature in FEATURES&#125;inters_S = &#123;feature: results_S.get(feature,[]) for feature in FEATURES&#125;for ith,inter_match in enumerate(self.d_config.intermediate_matches):    if type(layer_S) is list and type(layer_T) is list: ## MMD损失函数对应的情况        inter_S = [inters_S[feature][s] for s in layer_S]        inter_T = [inters_T[feature][t] for t in layer_T]        name_S = &#x27;-&#x27;.join(map(str,layer_S))        name_T = &#x27;-&#x27;.join(map(str,layer_T))        if self.projs[ith]: ## 这里失去做学生网络隐层的映射            #inter_T = [self.projs[ith](t) for t in inter_T]            inter_S = [self.projs[ith](s) for s in inter_S]    else:## MSE 损失函数        inter_S = inters_S[feature][layer_S]        inter_T = inters_T[feature][layer_T]        name_S = str(layer_S)        name_T = str(layer_T)        if self.projs[ith]:            inter_S = self.projs[ith](inter_S) # 需要注意的是隐层输出是312，但是老师网络是768，所以这里要做一个linear投影到更高维，方便计算损失函数            intermediate_loss = match_loss(inter_S, inter_T, mask=inputs_mask_S)  ## loss = F.mse_loss(state_S, state_T)    total_loss += intermediate_loss * match_weight\n这个代码里面比如迷糊的是【self.d_config.intermediate_matches】，打印出来发现是这个东西：\n12345678910IntermediateMatch: layer_T : 0, layer_S : 0, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : 3, layer_S : 1, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : 6, layer_S : 2, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : 9, layer_S : 3, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : 12, layer_S : 4, feature : hidden, weight : 1, loss : hidden_mse, proj : [&#x27;linear&#x27;, 312, 768, &#123;&#125;], IntermediateMatch: layer_T : [0, 0], layer_S : [0, 0], feature : hidden, weight : 1, loss : mmd, proj : None, IntermediateMatch: layer_T : [3, 3], layer_S : [1, 1], feature : hidden, weight : 1, loss : mmd, proj : None, IntermediateMatch: layer_T : [6, 6], layer_S : [2, 2], feature : hidden, weight : 1, loss : mmd, proj : None, IntermediateMatch: layer_T : [9, 9], layer_S : [3, 3], feature : hidden, weight : 1, loss : mmd, proj : None, IntermediateMatch: layer_T : [12, 12], layer_S : [4, 4], feature : hidden, weight : 1, loss : mmd, proj : None\n简单说，这个变量存储的就是上面我们谈到的层与层之间的对应关系。前面5行就是MSE损失函数度量，后面那个注意看，层数对应的时候是一个列表，对应的是MMD损失函数；\n我们来看一下MMD损失的代码形式：\n12345678910111213141516def mmd_loss(state_S, state_T, mask=None):    state_S_0 = state_S[0] # (batch_size , length, hidden_dim_S)    state_S_1 = state_S[1] # (batch_size , length, hidden_dim_S)    state_T_0 = state_T[0] # (batch_size , length, hidden_dim_T)    state_T_1 = state_T[1] # (batch_size , length, hidden_dim_T)    if mask isNone:        gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(2)  # (batch_size, length, length)        gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(2)        loss = F.mse_loss(gram_S, gram_T)    else:        mask = mask.to(state_S[0])        valid_count = torch.pow(mask.sum(dim=1), 2).sum()        gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(2)  # (batch_size, length, length)        gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(2)        loss = (F.mse_loss(gram_S, gram_T, reduction=&#x27;none&#x27;) * mask.unsqueeze(-1) * mask.unsqueeze(1)).sum() / valid_count    return loss\n看最重要的代码就可以：\n1234state_S_0 = state_S[0]#  32 128 312 (batch_size , length, hidden_dim_S)state_T_0 = state_T[0] #  32 128 768 (batch_size , length, hidden_dim_T)gram_S = torch.bmm(state_S_0, state_S_1.transpose(1, 2)) / state_S_1.size(2) gram_T = torch.bmm(state_T_0, state_T_1.transpose(1, 2)) / state_T_1.size(2)\n简单说就是现在自己内部计算bmm，然后两个矩阵之间做mse；这里如果我没理解错使用的是一个线性核函数；\n损失函数代码大致就是这样，之后有时间我写个简单的repository，梳理一下整个流程；\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/BERT知识蒸馏代码解析-如何写好损失函数/"},{"title":"","date":"2024-06-21T03:20:52.603Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:52.603Z","content":"PKD  核心点就是不仅仅从Bert（老师网络）的最后一层学习知识去做蒸馏，它还另加了一部分，就是从Bert的中间层去学习。\n简单说，PKD的知识来源有两部分：中间层+最后输出。\n它缓解了之前只用最后softmax输出层的蒸馏方式出现的过拟合而导致泛化能力降低的问题。\n接下来，我们从PKD模型的两个策略说起：PKD-Last 和 PKD-Skip。\n1.PKD-Last and PKD-Skip\nPKD的本质是从中间层学习知识，但是这个中间层如何去定义，就各式各样了。\n比如说，我完全可以定位我只要奇数层，或者我只要偶数层，或者说我只要最中间的两层，等等，不一而足。\n那么作者，主要是使用了这么多想法中的看起来比较合理的两种。\nPKD-Last，就是把中间层定义为老师网络的最后k层。\n这样做是基于老师网络越靠后的层数含有更多更重要的信息。\n这样的想法其实和之前的蒸馏想法很类似，也就是只使用softmax层的输出去做蒸馏。但是从感官来看，有种尾大不掉的感觉，不均衡。\n另一个策略是 就是PKD-Skip，顾名思义，就是每跳几层学习一层。\n这么做是基于老师网络比较底层的层也含有一些重要性信息，这些信息不应该被错过。\n作者在后面的实验中，证明了，PKD-Skip 效果稍微好一点（slightly better）；\n作者认为PKD-Skip抓住了老师网络不同层的多样性信息。而PKD-Last抓住的更多相对来说同质化信息，因为集中在了最后几层。\n2. PKD\n2.1架构图\n两种策略的PKD的架构图如下所示，注意观察图，有个细节很容易忽视掉:\n\n我们注意看这个图，Bert的最后一层（不是那个绿色的输出层）是没有被蒸馏的，这个细节一会会提到。\n2.2 怎么蒸馏中间层\n这个时候，需要解决一个问题：我们怎么蒸馏中间层？\n仔细想一下Bert的架构，假设最大长度是128，那么我们每一层Transformer encoder的输出都应该是128个单元，每个单元是768维度。\n那么在对中间层进行蒸馏的时候，我们需要针对哪一个单元？是针对所有单元还是其中的部分单元？\n首先，我们想一下，正常KD进行蒸馏的时候，我们使用的是[CLS]单元Softmax的输出，进行蒸馏。\n我们可以把这个思想借鉴过来，一来，对所有单元进行蒸馏，计算量太大。二来，[CLS] 不严谨的说，可以看到整个句子的信息。\n为啥说是不严谨的说呢？因为[CLS]是不能代表整个句子的输出信息，这一点我记得Bert中有提到。\n2.3蒸馏层数和学生网络的初始化\n接下来，我想说一个很小的细节点，对比着看上面的模型架构图：\nBert（老师网络）的最后一层 (Layer 12 for BERT-Base) 在蒸馏的时候是不予考虑；\n原因的话，其一可以这么理解，PKD创新点是从中间层学习知识，最后一层不属于中间层。当然这么说有点牵强附会。\n作者的解释是最后一层的隐层输出之后连接的就是Softmax层，而Softmax层的输出已经被KD Loss计算在内了。\n比如说，K=5，那么对于两种PKD的模式，被学习的中间层分别是：\nPKD-Skip: $I_{pt} = {2,4,6,8,10}$;\nPKD-Last: $I_{pt} = {7,8,9,10,11}$\n还有一个细节点需要注意，就是学生网络的初始化方式，直接使用老师网络的前几层去初始化学生网络的参数。\n2.4 损失函数\n首先需要注意的是中间层的损失，作者使用的是MSE损失。如下：\n\n整个模型的损失主要是分为两个部分：KD损失和中间层的损失，如下：\n\n超参数问题：\n\n$T:{5,10,20}$\n$\\alpha:{0.2,0.5,0.7}$\n$LR :{5e-5, 2e-5, 1e-5}$\n$\\beta :{10, 100, 500, 1000} $\n\n3. 实验效果\n实验效果可以总结如下：\n\nPKD确实有效，而且Skip模型比Last效果稍微好一点。\nPKD模型减少了参数量，加快了推理速度，基本是线性关系，毕竟减少了层数\n\n除了这两点，作者还做了一个实验去验证：如果老师网络更大，PKD模型得到的学生网络会表现更好吗？\n这个实验我很感兴趣。\n直接上结果图：\n\nKD情况下，注意不是PKD模型，看#1 和#2，在老师网络增加的情况下，效果有好有坏。这个和训练数据大小有关。\nKD情况下，看#1和#3，在老师网络增加的情况下，学生网络明显变差。\n作者分析是因为，压缩比高了，学生网络获取的信息变少了。\n也就是大网络和小网络本身效果没有差多少，但是学生网络在老师是大网络的情况下压缩比大，学到的信息就少了。\n更有意思的是对比#2和#3，老师是大网络的情况下，学生网络效果差。\n这里刚开始没理解，后来仔细看了一下，注意#2 的学生网络是$Bert_{6}[Base]-KD$，也就是它的初始化是从$Bert_{12}[Base]$来的，占了一半的信息。\n好的，写到这里\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/PKD-Bert基于多层的知识蒸馏方式/"},{"title":"","date":"2024-06-21T03:20:52.633Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:52.633Z","content":"大家好，我是DASOU，今天介绍一下：BERT-of-Theseus\n这个论文我觉得还挺有意思，攒个思路。\n读完这个文章，BERT-of-Theseus 掌握以下两点就可以了：\n\n\n基于模块替换进行压缩\n\n\n除了具体任务的损失函数，没有其他多余损失函数。\n\n\n效果的话，与$Bert-base$相比，$BERT-of-Theseus$：推理速度 $1.94$；模型效果 98%；\n模块替换\n举个例子，比如有一个老师网络是12层的Bert，现在我每隔两层Transformer，替换为学生网络的一层Transformer。那么最后我的学生网络也就变成了6层的小Bert，训练的时候老师网络和学生网络的模块交替训练。\n直接看下面这个架构图：\n\n作者说他是受 Dropout 的启发，仔细想了想还真的挺像的。\n我们来说一下这样做的好处。\n我刚才说每隔老师网络的两层替换为学生网络的一层。很容易就想到PKD里面，有一个是PKD-Skip策略。\n就是每隔几层，学生网络的层去学习老师网络对应层的输出，使用损失函数让两者输出接近，使用的是CLS的输出。\n在这里提一下蒸馏/压缩的基本思想，一个最朴素的想法就是让学生网络和老师网络通过损失函数在输出层尽可能的靠近。\n进一步的，为了提升效果，可以通过损失函数，让学生网络和老师网络在中间层尽可能的靠近，就像PKD这种。\n这个过程最重要的就是在训练的时候需要通过损失函数来让老师网络和学生网络尽可能的接近。\n如果是这样的话，问题就来了，损失函数的选取以及各自损失函数之前的权重就需要好好的选择，这是一个很麻烦的事情。\n然后我们再来看 BERT-of-Theseus，它就没有这个问题。\n它是在训练的时候以概率 $r$ 来从老师网络某一层和学生网络的某一层选择一个出来，放入到训练过程中。\n在这个论文里，老师网络叫做  $predecessor$， 学生网络叫做 $successor$ ；\n训练过程\n对着这个网络架构，我说一下整体训练的过程：\n\n在具体任务数据上训练一个 BERT-base 网络作为 $predecessor$；\n使用 $predecessor$  前六层初始化一个 6层的Bert作为 $successor$ ；\n在具体任务数据上，固定 $predecessor$ 相应权重，以概率$r$（随着steps，线性增加到1），对整个网络（$predecessor$加上$successor$ ）进行整体的训练。\n为了让$successor$  作为一个整体，单独抽离出来$successor$ （其实$r$设置为1就可以了），作为一个单独的个体，在训练数据上继续微调。直至效果不再增加。\n\n简单总结，在训练数据上，老师网络和学生网络共同训练，因为存在概率问题，有的时候是老师网络的部分层加入训练，有的时候是学生网络的部分层加入训练。在这一步训练完成之后，为了保证学生网络作为一个整体（因为在第一步训练的时候大部分情况下学生网络的层都是分开加入训练过程的），在具体任务数据上，对学生网络继续微调，直至效果不再增加。\n结果分析\n不同方法的损失函数\n论文提供了一个不同Bert蒸馏方法使用的损失函数的图，值得一看，见下图：\n\n值得注意的是，这里的 $Finetuning$应该是选取前六层，在具体任务微调的结果。\n效果\n\n整体来说，BERT-of-Theseus 思路很简单，效果也还不错。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/Theseus-模块压缩交替训练/"},{"title":"","date":"2024-06-21T03:20:51.603Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:51.603Z","content":"为什么需要做模型蒸馏？\nBert类模型精读高，但是推理速度慢，模型蒸馏可以在速度和精读之间做一个平衡。\n\n从蒸馏方法\n\n从蒸馏方法来看，一般可以分为三种：\n\n\n参数的共享或者剪枝\n\n\n低秩分解\n\n\n知识蒸馏\n\n\n对于1和2，可以参考一下 Albert。\n而对于知识蒸馏来说，本质是通过一种映射关系，将老师学到的东西映射到或者说传递给学生网络。\n在最开始的时候，一般都会有一种疑问？ 我有训练数据了，训练数据的准确度肯定比你大模型的输出结构准确度高，为什么还需要从老师网络来学习知识？\n我觉得对于这个问题，我在李如的文章看到这样一句话：”好模型的目标不是拟合训练数据，而是学习如何泛化到新的数据“\n我觉得写的很好。对于这个问题，我们这么去想，我们的大模型的输出对于logits不仅仅是类别属于哪一个，还有一个特点就是会给出不同类别之间的一个关系。\n比如说，在预测”今天天气真不错，现在就决定了，出去浪一波，来顿烧烤哦“。\n文本真实标签可能就直接给出了”旅游“这个标签，而我们的模型在概率输出的时候可能会发现”旅游“和”美食“两个标签都还行。\n这就是模型从数据中学习到的一种”暗知识“（好像是这么叫，忘了在哪里看到了）、\n而且还存在一个问题，有些时候是没有那么多训练数据的，需要的是大模型Bert这种给出无监督数据的伪标签作为冷启动也是不错的。\n\n从蒸馏结构\n\n从蒸馏结构来说，我们可以分为两种：\n\n\n从transformer到transformer结构\n\n\n从transformer结构到别的模型（CNN或者lstm结构）\n\n\n我主要是想聊一下 Bert 到 TextCNN模型的蒸馏。\n为啥选择textcnn？最大的原因就是速度快精读还不错。\n论文参考 Distilling Task-Specific Knowledge from BERT into Simple Neural Networks\n对于这个蒸馏，对于我而言，最重要的掌握一个点就是损失函数的设定，别的地方我暂且不考虑。\n对于损失函数，分为两个部分，一个是我当前lstm输出结果和真实标签的交叉熵损失，一个是我的当前lstm输出结果和大模型bert的输出logits的平方损失。\n至于为啥一个是交叉熵一个是平方损失，是因为其实前面的看做分类问题，后面的看做回归问题。当然只是谁更合适的选择问题。\n因为是加权两个部分做损失，我这边选择为都是0.5。\n当然在李如的文章中谈到，可能真实标签这边的权重小一点会更好一点，因为蒸馏本质上还是想多关注bert的输出多一点。\n关于这个论文有一个很好的解释：\n知识蒸馏论文选读（二） - 小禅心的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/89420539\n关于模型蒸馏，我就简单了解到这里，可能之后会花费大量精力看看背的蒸馏方式，放上开源代码：\nbert到lstm的蒸馏\nbert到textcnn/lstm/lkeras/torch\n一个pytorch实现的模型蒸馏库\n罗列一下关于Bert模型蒸馏的文章和博客：\n首先一个讲的比较好的文章就是下面这个文章，比较系统的讲了一遍\nBERT知识蒸馏综述 - 王三火的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/106810758\n还有一个文章讲的比较好的是：BERT 模型蒸馏 Distillation BERT\nhttps://www.jianshu.com/p/ed7942b5207a\n这个文章就是比较系统的对比了Bert的两个蒸馏操作：DistilBERT 和 Distilled BiLSTM  我觉得写得还不错\n从实战的角度来说，我觉得写得很好的就是：BERT 蒸馏在垃圾舆情识别中的探索\nhttps://blog.csdn.net/alitech2017/article/details/107412038\n这个文章是对bert的蒸馏，到textcnn，使用了多种方式并且比较了最终的结果。\n接下来是李如的这个文章，很概括，确实大佬，写得很好：\n【DL】模型蒸馏Distillation - 李如的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/71986772\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/bert2textcnn模型蒸馏/"},{"title":"","date":"2024-06-21T03:20:52.913Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:52.913Z","content":"Bert知识蒸馏系列(一)：什么是知识蒸馏\n全文参考的论文是：Distilling the Knowledge in a Neural Network\n参考的讲解的比较的博文是：\n《Distilling the Knowledge in a Neural Network》知识蒸馏 - musk星辰大海的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/75031938\n这个含有Hiton的PPT介绍\n【经典简读】知识蒸馏(Knowledge Distillation) 经典之作 - 潘小小的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/102038521\n这个把其中的公式推导写的比较明白\n如何理解soft target这一做法？ - YJango的回答 - 知乎\nhttps://www.zhihu.com/question/50519680/answer/136406661\nBert 系列文章\n\n\nBert 模型压缩\n什么是知识蒸馏？知识蒸馏基础概念一览。\n\n\nBert 的后续改进\nAlbert\nRobert\n\n\n什么是蒸馏\n一般来说，为了提高模型效果，我们可以使用两种方式。一种是直接使用复杂模型，比如你原来使用的TextCNN，现在使用Bert。一种是多个简单模型的集成，这种套路在竞赛中非常的常见。\n这两种方法在离线的时候是没有什么问题的，因为不涉及到实时性的要求。但是一旦涉及到到部署模型，线上实时推理，我们需要考虑时延和计算资源，一般需要对模型的复杂度和精度做一个平衡。\n这个时候，我们就可以将我们大模型学到的信息提取精华灌输到到小模型中去，这个过程就是蒸馏。\n什么是知识\n对于一个模型，我们一般关注两个部分：模型架构和模型参数。\n简答的说，我们可以把这两个部分当做是我们模型从数据中学习到的信息或者说是知识（当然主要是参数，因为架构一般来说是训练之前就定下来的）\n但是这两个部分，对于我们来说，属于黑箱，就是我们不知道里面究竟发生了什么事情。\n那么什么东西是我们肉眼可见的呢？从输入向量到输出向量的一个映射关系是可以被我们观测到的。\n简单来说，我输入一个example，你输出是一个什么情况我是可以看到的。\n区别于标签数据格式 [0,0,1,0],模型的输出结果一般是这样的：[0.01,0.01,0.97,0.01]。\n举个比较具象的例子，就是如果我们在做一个图片分类的任务，你的输入图像是一辆宝马，那么模型在宝马这个类别上会有着最大的概率值，与此同时还会把剩余的概率值分给其他的类别。\n这些其他类别的概率值一般都很小，但是仍然存在着一些信息，比如垃圾车的概率就会比胡萝卜的概率更高一些。\n模型的输出结果含有的信息更丰富了，信息熵更大了，我们进一步的可以把这种当成是一种知识，也就是小模型需要从大模型中学习到的经验。\n这个时候我们一般把大模型也就是复杂模型称之为老师网络，小模型也就那我们需要的蒸馏模型称之为学生网络。学生网络通过学习老师网络的输出，进而训练模型，达到比较好的收敛效果。\n为什么知识蒸馏可以获得比较好的效果\n在前面提到过，卡车和胡萝卜都会有概率值的输出，但是卡车的概率会比胡萝卜大，这种信息是很有用的，它定义了一种丰富的数据相似结构。\n上面谈到一个问题，就是不正确的类别概率都比较小，它对交叉熵损失函数的作用非常的低，因为这个概率太接近零了，也就是说，这种相似性存在，但是在损失函数中并没有充分的体现出来。\n第一种就是，使用sofmax之前的值，也就是logits，计算损失函数\n第二种是在计算损失函数的时候，使用温度参数T，温度参数越高，得到的概率值越平缓。通过升高温度T，我们获取“软目标”，进而训练小模型\n其实对于第一种其实是第二种蒸馏方式的的一种特例情况，论文后续有对此进行证明。\n这里的温度参数其实在一定程度上和蒸馏这个名词相呼应，通过升温，提取精华，进而灌输知识。\n带温度参数T的Softmax函数\n软化公式如下：\n\n说一下为什么需要这么一个软化公式。上面我们谈到，通过升温T，我们得到的概率分布会变得比较平缓。\n用上面的例子说就是，宝马被识别为垃圾车的概率比较小，但是通过升温之后，仍然比较小，但是没有那么小（好绕口啊）。\n也就是说，数据中存在的相似性信息通过升温被放大了，这样在计算损失函数的时候，这个相似性才会被更大的注意到，才会对损失函数产生比较大的影响力。\n损失函数\n\n损失函数是软目标损失函数和硬目标损失函数的结合，一般来说，软目标损失函数设置的权重需要大一些效果会更好一点。\n如何训练\n整体的算法示意图如下：\n\n整体的算法示意图如上所示：\n\n首先使用标签数据训练一个正常的大模型\n使用训练好的模型，计算soft targets。\n训练小模型，分为两个步骤，首先小模型使用相同的温度参数得到输出结果和软目标做交叉熵损失，其次小模型使用温度参数为1，和标签数据（也就是硬目标）做交叉损失函数。\n预测的时候，温度参数设置为1，正常预测。\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/什么是知识蒸馏/"},{"title":"","date":"2024-06-21T03:20:52.653Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:52.653Z","content":"大家好，我是DASOU，今天说一下 TinyBert；\nTinyBert 主要掌握两个核心点：\n\n\n提出了对基于 transformer 的模型的蒸馏方式：Transformer distillation；\n\n\n提出了两阶段学习框架：在预训练和具体任务微调阶段都进行了 Transformer distillation（两阶段有略微不同）；\n\n\n下面对这两个核心点进行阐述。\n1. Transformer distillation\n1.1整体架构\n整体架构如下：\n\nBert不严谨的来划分，可以分为三个部分：词向量输入层，中间的TRM层，尾端的预测输出层。\n在这个论文里，作者把词向量输入层 和中间的TRM层统一称之为中间层，大家读的时候需要注意哈。\nBert的不同层代表了学习到了不同的知识，所以针对不同的层，设定不同的损失函数，让学生网络向老师网络靠近，如下：\n\nebedding层的输出\n多头注意力层的注意力矩阵和隐层的输出\n预测层的输出\n\n1.2 Transformer 基础知识：\n注意力层：\n\n多头注意力层：\n\n前馈神经网路：\n\n1.3 Transformer 的蒸馏\n对 Transformer的蒸馏分为两个部分：一个是注意力层矩阵的蒸馏，一个是前馈神经网络输出的蒸馏。\n注意力层矩阵蒸馏的损失函数：\n\n这里注意两个细节点：\n一个是使用的是MSE；\n还有一个是，使用的没有归一化的注意力矩阵，见(1)，而不是softmax之后的。原因是实验证明这样能够更快的收敛而且效果会更好。\n前馈神经网络蒸馏的损失函数\n\n两个细节点：\n第一仍然使用的是MSE.\n第二个细节点是注意，学生网路的隐层输出乘以了一个权重矩阵$w_{h}$，这样的原因是学生网络的隐层维度和老师网络的隐层维度不一定相同。\n所以如果直接计算MSE是不行的，这个权重矩阵也是在训练过程中学习的。\n写到这里提一点，其实这里也可以看出来为什么tinybert的初始化没有采用类似PKD这种，而是使用GD过程进行蒸馏学习。\n因为我们的tinybert 在减少层数的同时也减少了宽度（隐层的输出维度），如果采用PKD这种形式，学生网络的维度和老师网络的维度对不上，是不能初始化的。\n词向量输入层的蒸馏：\n\n预测层输出蒸馏：\n\n1.4 总体蒸馏损失函数\n\n2. 两阶段蒸馏\n2.1 整体架构\n整体架构如图：\n\n2.2 为什么需要GD:\n说一下我自己的理解哈，我觉得有两个原因：\n首先，就是上文说到的，tinybert不仅降低了层数，也降低了维度，所以学生网络和老师网络的维度是不符的，所以PKD这种初始化方式不太行。\n其次，一般来说，比如PKD，学生网络会使用老师网络的部分层进行初始化。这个从直觉上来说，就不太对。\n老师网络12层，学到的是文本的全部信息。学生网络是6层，如果使用老师的12层的前6层进行初始化，这个操作相当于认为这前6层代表了文本的全部信息。\n当然，对于学生网络，还会在具体任务上微调。这里只是说这个初始化方式不太严谨。\nTiny bert的初始化方式很有意思，也是用了蒸馏的方式。\n老师网络是没有经过在具体任务进行过微调的Bert网络，然后在大规模无监督数据集上，进行Transformer distillation。当然这里的蒸馏就没有预测输出层的蒸馏，翻看附录，发现这里只是中间层的蒸馏。\n简单总结一下，这个阶段，使用一个预训练好的Bert（ 尚未微调）进行了3epochs的 distillation；\n2.3 TD：\nTD就是针对具体任务进行蒸馏。\n核心点：先进行中间层（包含embedding层）的蒸馏，再去做输出层的蒸馏。\n老师网络是一个微调好的Bert，学生网络使用GD之后的tinybert，对老师网络进行TD蒸馏。\nTD过程是，先在数据增强之后的数据上进行中间层的蒸馏-10eopchs，learning rate 5e-5；然后预测层的蒸馏3epochs，learning rate 3e-5.\n3. 数据增强\n在具体任务数据上进行微调的时候，进行了数据增强。\n(感觉怪怪的)\n两个细节点：\n\n\n对于 single-piece word 通过Bert找到当前mask词最相近的M个单词；对于 multiple sub-word pieces 使用Glove和Consine找到最相近的M个词\n\n\n通过概率P来决定是否替换当前的词为替换词。\n\n\n对任务数据集中的所有文本数据做上述操作，持续N次。\n\n\n伪代码如下：\n\n4. 实验效果\n其实我最关心的一个点就是，数据增强起到了多大的作用。\n作者确实也做了实验，如下，数据增强作用还是很大的：\n\n我比较想知道的是，在和PKD同等模型架构下，两者的比较，很遗憾，作者好像并没有做类似的实验(或者我没发现)。\n这里的tinybert参数如下：\n\nthe number of layers M=4, the hidden size d 0=312, the feedforward/filter size d 0 i=1200 and the head number h=12.\n\n5. 简单总结\n先说一下，我读完论文学到的东西：\n首先是transformer层蒸馏是如何涉及到的损失函数：\n\n注意力矩阵和前馈神经层使用mse；\n蒸馏的时候注意力矩阵使用未归一化\n维度不同使用权重矩阵进行转化\n\n其次，维度不同导致不能从老师Bert初始化。GD过程为了解决这个问题，直接使用学生网络的架构从老师网络蒸馏一个就可以，这里并不是重新学一个学生网络。\n还有就是数据增强，感觉tinyebert的数据增强还是比较简陋的，也比较牵强，而且是针对英文的方法。\nTD过程，对不同的层的蒸馏是分开进行的，先进行的中间层的蒸馏，然后是进行的输出层的蒸馏，输出层使用的是Soft没有使用hard。\n这个分过程蒸馏很有意思，之前没注意到这个细节点。\n在腾讯的文章中看到这样一句话：\n\n并且实验中，softmax cross-entropy loss 容易发生不收敛的情况，把 softmax 交叉熵改成 MSE, 收敛效果变好，但泛化效果变差。这是因为使用 softmax cross-entropy 需要学到整个概率分布，更难收敛，因为拟合了 teacher BERT 的概率分布，有更强的泛化性。MSE 对极值敏感，收敛的更快，但泛化效果不如前者。\n\n是有道理的，积累一下。\n值得看的一些资料：\n比 Bert 体积更小速度更快的 TinyBERT - 腾讯技术工程的文章 - 知乎 https://zhuanlan.zhihu.com/p/94359189\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/tinybert-全方位蒸馏/"},{"title":"","date":"2024-06-21T03:20:52.983Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:52.983Z","content":"本文是对苏神文章的解读，主要是关于公式推导中省略的部分细节记录了自己的理解，希望能帮助大家更好的理解。\n模型训练的时候，我们会把数据分为训练数据和开发数据。\nLoss变化一般是这样的：训练集损失在不停的降低，开发集先降低随后上升。\n我们一般选择两条线的交叉点（其实也没有交叉），也就是开发数据集开始上升的那个点作为我们的最终模型的选择，这样既可以得到最好的结果，也可以避免过拟合。\n这个论文思路是这样的，当损失函数降低的一定程度（足够小）的时候，改变损失函数为:\n$$\n\\widetilde{J_{\\theta}} =|J_{\\theta}-b|+b\\tag{1}\n$$\n公式 $(1)$ 中 $J_{\\theta}$  为原始的损失函数， $\\widetilde{J_{\\theta}}$ 改变之后的损失函数。\n观察这个公式，其实可以这样去描述：\n\n\n当 $J_{\\theta} \\geq b$ 时，损失函数就是$J_{\\theta}$；\n\n\n当$J_{\\theta} &lt; b$ 的时候，损失函数就是$\\widetilde{J_{\\theta}}=2b - J_{\\theta}$。\n\n\n这个时候，我们想一下梯度下降算法公式，如下:\n$$\n\\theta_{n}=\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})\\tag{2}\n$$\n所以，当损失函数为 $\\widetilde{J_{\\theta}}$ 的时候，符号就会发生变化，这个时候我们就使用就不是梯度下降而是梯度上升算法。也就是说，以$b$ 为临界点，在交替的进行梯度上升和梯度下降算法。\n论文发现，在某些任务上，使用这个方法，开发集上的损失函数会发生二次下降。\n再次说一下，关于这一点，苏剑林给出来相关的数学推导（参考链接放在文章末尾）。不过有个关于泰勒公式的展开跳过了，我简单做了一个补充，帮助自己和大家理解。\n首先如果交替做梯度上升和梯度下降算法，参数更新公式如下所示:\n$$\n\\theta_{n}=\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})\n$$\n$$\n\\theta_{n+1}=\\theta_{n}+ \\alpha\\nabla J(\\theta_{n}) \\tag{3}\n$$\n对此公式上下消参 中，我们可以得到：\n$$\n\\theta_{n+1}= \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})+ \\alpha\\nabla J( \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1}))\\tag{4}\n$$\n对于公式$(4)$ ，重点是对 $ J(\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1}))$ 这个损失函数进行一个剖析化简，这里用到了泰勒公式的展开。\n先回忆一下泰勒公式，这里直接给出一个一阶泰勒公式的展开:\n$$\nJ(\\omega) \\approx   J(\\omega_{0}) + (\\omega - \\omega_{0})*J^{'}(\\omega_{0}) + \\epsilon  \\qquad  \\omega_{0} 和 \\omega 足够接近\\tag{5}\n$$\n注意，这个时候，我们仔细观察公式  $ J(\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1}))$  和 公式$(5)$。\n首先，我们知道的是，$\\alpha\\nabla J(\\theta_{n-1}))$ 是每次参数更新时候的增量，在损失函数足够小的时候，我们每次参数更新的增量可以认定是一个极小值。\n换句话说，$\\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})$可以对应到我们公式(5) 中的$\\omega_{0}$，$\\theta_{n-1}$ 对应的就是公式(5) 中的 $\\omega$\n也就是说，\n$$\nJ( \\theta_{n-1}) \\approx  J( \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})) +\\alpha\\nabla J(\\theta_{n-1})*\\nabla J(\\theta_{n-1})  \\tag{6}\n$$\n这里需要注意的，公式最后面一个$\\nabla J(\\theta_{n-1})$ 的由来。按道理，这里应该是对$J^{'}(\\omega_{0})$进行求导 。但是这里因为$\\omega 和 \\omega_{0}$ 非常的相近，我们直接使用对$J(\\omega)$的求导结果就可以，这一点是个比较重要的细节点。\n基于此，我们可以继续往下推导：\n$$\n\\theta_{n+1}= \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})+ \\alpha\\nabla J( \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1}))\n$$\n$$\n\\approx \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})+ \\alpha\\nabla (J (\\theta_{n-1})- \\alpha\\nabla J(\\theta_{n-1})*\\nabla J(\\theta_{n-1}))\n$$\n$$\n= \\theta_{n-1}- \\alpha\\nabla J(\\theta_{n-1})+ \\alpha\\nabla J (\\theta_{n-1})- \\alpha^{2}\\nabla (\\nabla J(\\theta_{n-1})*\\nabla J(\\theta_{n-1}))\n$$\n$$\n= \\theta_{n-1}- \\alpha^{2}\\nabla ||\\nabla J(\\theta_{n-1})||^{2}  \\tag{7}\n$$\n这里，我还想提一点就是 $||\\nabla J(\\theta_{n-1})||^{2}$ ，它是两个求微分函数的乘积，所以结果是一个带参数的函数，也就是求得一个微分之后，做一个平方，得到的函数，这个时候在参数更新的时候，我们带入相应的值就可以了。\n我们针对这个公式(7)，会发现一个很奇怪的现象，就是参数更新的模式没有发生变化，都是进行了梯度下降（注意开头我们单从损失函数看是认为梯度下降和梯度上升是交替进行的，两个理解其实都没有问题）。\n只是，当前步骤的参数更新不再是取决于上一个步骤，而是取决于上上一个步骤的参数。\n这一点，我是这么理解的。使用普通的损失函数，相当于此时我们站在上一个步骤往山下看。当损失函数非常小的时候，极有可能会陷入局部最小值，并不是全局最优点。此时寻找出来的更新的方向，还是局限于局部最优点。而使用新的损失函数，我们通过公式，最直观的感受就是，是站在了上上一个步骤，是脱离了当前的视线(虽然只是差了一个步骤)，相当于视野变大了，有更大的可能跳出当前的局部最优点，从而寻找到全局最优点。\n我的理解就是这样的，当然苏神给出了另一个解释。大家可以去看一下。我这个文章主要是对他的公式推导中的跳过的泰勒公式的展开做了一个比较详细的阐述，记录下来，方便自己和大家理解。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/论文解读/模型训练需不需要将损失降低为零/"},{"title":"","date":"2024-06-21T03:20:53.043Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.043Z","content":"CBOW和skip-gram相较而言，彼此相对适合哪些场景\n先用一句话来个结论：CBOW比Skip-gram 训练速度快，但是Skip-gram可以得到更好的词向量表达。\n为什么这么说？\n因为我们知道两种优化方式只是对softmax的近似优化，不会影响最终结果，所以这里，我们讨论的时候，为了更加的清晰讲解，不考虑优化的情况。\n使用一句话作为一个例子： “我/永远/爱/中国/共产党”\n先说CBOW，我们想一下，它的情况是使用周围词预测中心词。如果“爱”是中心词，别的是背景词。对于“爱”这个中心词，只是被预测了一次。\n对于Skip-gram，同样，我们的中心词是“爱”，背景词是其他词，对于每一个背景词，我们都需要进行一次预测，每进行一次预测，我们都会更新一次词向量。也就是说，相比CBOW，我们的词向量更新了2k次（假设K为窗口，那么窗口内包含中心词就有2k+1个单词）\n想一下是不是这么回事？Skip-gram被训练的次数更多，那么词向量的表达就会越丰富。\n如果语料库中，我们的的低频词很多，那么使用Skip-gram就会得到更好的低频词的词向量的表达，相应的训练时长就会更多。\n简单来说，我们视线回到一个大小为K的训练窗口（窗口内全部单词为2k+1个），CBOW只是训练一次，Skip-gram 则是训练了2K次。当然是Skip-gram词向量会更加的准确一点，相应的会训练的慢一点。\n欢迎大佬拍砖\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/CBOW和skip-gram相较而言，彼此相对适合哪些场景/"},{"title":"","date":"2024-06-21T03:20:53.073Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.073Z","content":"我先说一个小问题，估计很多人也有疑惑。\n看了很多文章，有的说是fasttext是CBOW的简单变种，有的说是Skip-gram的变种。究竟哪个是对的？\n带着这个问题，我们来聊一聊Fasttext。首先Fasttext涉及到两个论文：\n\n第一个是Bag of Tricks for Efficient TextClassification(201607)。它解决的问题是使用Fasttext进行文本分类\n第二个是Enriching Word Vectors with Subword Information(201607) 。它解决的是使用Fasttext训练词向量。\n\n今天这个文章，主要谈一下Bag of Tricks for Efficient Text Classification 这个论文 ，主要涉及到的就是文本分类的问题。\nFasttext用作文本分类，做到了速度和精读的一个平衡：标准多核CPU情况下，不到十分钟，可以训练超过十亿个单词。不到一分钟，可以对50万个句子在312千个类别中进行分类。\n这么说，其实不太明显，简单算一下。假设每个句子含有20个单词，那么十亿个单词对应就是五千万个句子，换句话讲在多核CPU的条件下，一分钟左右可以训练500万个句子。\n和Bert比较一下，在GPU条件下，8个小时训练300万条数据左右。相比之下Fasttext的这个速度是真的太快了。\n在这个论文中，也就是使用做文本分类的Fasttext，使用的是CBOW的架构。\n注意哦，强调一遍，Fasttext用在文本分类，模型架构使用的是CBOW的变种。（我这句话的意思不是说使用Skip-gram不可以，而是CBOW在理解文本分类的时候更加的容易理解）\n这里和Word2vec的CBOW有两个区别：\n\n第一，使用类别标签替换了中心词。\n第二，使用句子中所有单词作为输入，而不再是单单的针对滑动窗口中的单词。\n\n这两个点如果我们自己考虑，也很容易想到。\n为什么这么说呢？先说第二点。我现在要做的是针对文本进行分类，所以对于我的输入需要转换到整体这个句子来看，才能使对一个句子的特征表达。\n再说第一点，我们知道在Wrod2vec中，我们使用的是中心词作为输出，而且使用了霍夫曼作为输出层。\n非叶子点上的向量为了我的二分类提供计算，叶子节点为整个词汇表中所有词汇的向量。两个向量都会随着模型而训练。\n如果要做分类，我们可以想一下叶子节点和非叶子节点的变化。\n首先叶子节点对应的是所有类别。如果说我们的类别有5000个，那么对应到Word2vec，我们就有着5000个词汇。想一下是不是这么对应。\n非叶子节点其实没什么变化，因为它没有什么实际含义，只是为二分类提供计算。\n在这里还想说一下，word2vec中的叶子节点也就是词向量更新之后我们最后是要的，但是对于fasttext其实不会用到这个，因为我们是对文本进行分类，只需要保存了模型权重在预测的时候可以预测就可以了。\n还想谈一下词向量初始化的问题，模型训练开始的时候，词向量随机初始化就可以，模型训练结束之后，我们在预测阶段直接使用这个词向量就可以（就是随着模型训练而更新的这个词向量）。\n对这个论文还有一个很有意思的点，就是N-gram就是fasttext的模型的输入不仅仅针对的是每个单词，为了加入词序信息，还加入了n-gram信息。\n需要注意的一个细节是，这里的n-gram针对的是word，而不是char。对应到中文，应该对应的是分词之后的词，而不是字。但是我自己认为这么对应过来不太好理解。\n中文的字做n-gram貌似也有词序信息。但是英文的char-level的n-gram很难说针对这个句子提供一个语序信息。大家理解一下就好。\n还有一个问题想说一下，使用了n-gram信息之后，词表肯定是变大了的。你需要的n的内容越多，比如你想要n=1orn=2orn=3等等吧，n的取值范围越大，你的词表越大。\n这就会出现一个问题训练非常缓慢。这点很容易理解，参数越多训练当然越慢。\n针对这个问题，怎么解决呢？使用哈希。\n我举个简单的例子，不一定准确，“我/爱/中国/共产党”，我在更新的时候，把’我’,‘爱’,‘中国’,'共产党’我们都使用同一个参数来代表（这种情况很难遇见，理解一下就好），那么在更新训练参数的时候，我只需要更新一个参数就把这个四个词都更新了，当然会快一点。\n但是会出现一个问题，就是精度的问题。这个过程，不知道大家有咩有想到和albert很类似。哈希这个过程我自己感觉有点共享参数的意思。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/Fasttext解读1/"},{"title":"","date":"2024-06-21T03:20:53.093Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.093Z","content":"这个文章主要是谈一下Enriching Word Vectors with Subword Information 这个论文。\n有了上一个文章的打底，（上一个文章点击这里）这个论文理解起来就比较简单，所以我写的也比较短。\n对于这个论文，我先给出它最核心的部分： 使用负采样的skip-gram的基础上，将每个中心词视为子词的集合，并学习子词的词向量。\n这句话涉及到的一个最关键的部分就是子词subword，也是这个论文的核心。\n举个例子，现在我们的中心词是&quot;where&quot;，设定子词大小为3，那么子词集合分为两个部分，注意是两个部分。\n第一部分形如这样：“&lt;wh”，“whe”，“her”，“ere”，“re&gt;”，第二部分就是特殊子词，也就是整词“”。\n那么对应到模型是，原来我的输入是“where”的词向量，现在在Fasttext就是所有子词的词向量的和。\n注意哦，这里是所有子词，是包含特殊子词，也就是整词的。\n对于背景词，直接使用整词就可以。\n简单来说，就是输出层使用子词（普通子词加上整词），输出层使用整词。\n如果遇到了OOV怎么办？使用普通子词的向量和来表示就可以。\n其实这里的子词，在名字上和上一个文章的ngram很类似，不过，这里使用的是就char的n-gram，缓解的问题并不是语序，而是利用了词序形态的规律。\n对应到中文，其实就是偏旁部首。 我记得阿里好像有发一个关于fasttext的中文版本，训练的就是偏旁部首。大家有兴趣可以去看一看。\n写完了，我对两个文章做个小总结，顺便对文章开头的问题做个回答: fasttext 训练词向量的时候一般是使用Skip-gram模型的变种。在用作文本分类的时候，一般是使用CBOW的变种。\n在这里，我想要强调一下，上一段我说的是一般情况，是为了方便大家了解，并不代表说CBOW架构不能训练词向量，skip-gram不能用作文本分类，需要注意这一点哦。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/Fasttext解读2/"},{"title":"","date":"2024-06-21T03:20:53.283Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.283Z","content":"词向量\nWord2vec\nFasttext\nGlove\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/README/"},{"title":"","date":"2024-06-21T03:20:52.943Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:52.943Z","content":"本文首发公众号：【DASOU】\n涉及到的代码部分，可以去我仓库里找，已经1.2k了：\nDA-southampton/NLP_abilitygithub.com\n文中内容有不同见解大家及时沟通\n目录如下：\n\n知识蒸馏简单介绍\nBert 蒸馏到 BiLSTM\nPKD-BERT\nBERT-of-Theseus\nTinyBert\n\n1. 知识蒸馏简单介绍\n1.1 什么是蒸馏\n一般来说，为了提高模型效果，我们可以使用两种方式。一种是直接使用复杂模型，比如你原来使用的TextCNN，现在使用Bert。一种是多个简单模型的集成，这种套路在竞赛中非常的常见。\n这两种方法在离线的时候是没有什么问题的，因为不涉及到实时性的要求。但是一旦涉及到到部署模型，线上实时推理，我们需要考虑时延和计算资源，一般需要对模型的复杂度和精度做一个平衡。\n这个时候，我们就可以将我们大模型学到的信息提取精华灌输到到小模型中去，这个过程就是蒸馏。\n1.2 什么是知识\n对于一个模型，我们一般关注两个部分：模型架构和模型参数。\n简答的说，我们可以把这两个部分当做是我们模型从数据中学习到的信息或者说是知识（当然主要是参数，因为架构一般来说是训练之前就定下来的）\n但是这两个部分，对于我们来说，属于黑箱，就是我们不知道里面究竟发生了什么事情。\n那么什么东西是我们肉眼可见的呢？从输入向量到输出向量的一个映射关系是可以被我们观测到的。\n简单来说，我输入一个example，你输出是一个什么情况我是可以看到的。\n区别于标签数据格式 [0,0,1,0],模型的输出结果一般是这样的：[0.01,0.01,0.97,0.01]。\n举个比较具象的例子，就是如果我们在做一个图片分类的任务，你的输入图像是一辆宝马，那么模型在宝马这个类别上会有着最大的概率值，与此同时还会把剩余的概率值分给其他的类别。\n这些其他类别的概率值一般都很小，但是仍然存在着一些信息，比如垃圾车的概率就会比胡萝卜的概率更高一些。\n模型的输出结果含有的信息更丰富了，信息熵更大了，我们进一步的可以把这种当成是一种知识，也就是小模型需要从大模型中学习到的经验。\n这个时候我们一般把大模型也就是复杂模型称之为老师网络，小模型也就那我们需要的蒸馏模型称之为学生网络。学生网络通过学习老师网络的输出，进而训练模型，达到比较好的收敛效果。\n1.3 为什么知识蒸馏可以获得比较好的效果\n在前面提到过，卡车和胡萝卜都会有概率值的输出，但是卡车的概率会比胡萝卜大，这种信息是很有用的，它定义了一种丰富的数据相似结构。\n上面谈到一个问题，就是不正确的类别概率都比较小，它对交叉熵损失函数的作用非常的低，因为这个概率太接近零了，也就是说，这种相似性存在，但是在损失函数中并没有充分的体现出来。\n第一种就是，使用sofmax之前的值，也就是logits，计算损失函数\n第二种是在计算损失函数的时候，使用温度参数T，温度参数越高，得到的概率值越平缓。通过升高温度T，我们获取“软目标”，进而训练小模型\n其实对于第一种其实是第二种蒸馏方式的的一种特例情况，论文后续有对此进行证明。\n这里的温度参数其实在一定程度上和蒸馏这个名词相呼应，通过升温，提取精华，进而灌输知识。\n1.4 带温度参数T的Softmax函数\n软化公式如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n说一下为什么需要这么一个软化公式。上面我们谈到，通过升温T，我们得到的概率分布会变得比较平缓。\n用上面的例子说就是，宝马被识别为垃圾车的概率比较小，但是通过升温之后，仍然比较小，但是没有那么小（好绕口啊）。\n也就是说，数据中存在的相似性信息通过升温被放大了，这样在计算损失函数的时候，这个相似性才会被更大的注意到，才会对损失函数产生比较大的影响力。\n1.5 损失函数\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n损失函数是软目标损失函数和硬目标损失函数的结合，一般来说，软目标损失函数设置的权重需要大一些效果会更好一点。\n1.6 如何训练\n整体的算法示意图如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n整体的算法示意图如上所示：\n\n首先使用标签数据训练一个正常的大模型\n使用训练好的模型，计算soft targets。\n训练小模型，分为两个步骤，首先小模型使用相同的温度参数得到输出结果和软目标做交叉熵损失，其次小模型使用温度参数为1，和标签数据（也就是硬目标）做交叉损失函数。\n预测的时候，温度参数设置为1，正常预测。\n\n2. Bert 蒸馏到 BiLSTM\n2.1 简单介绍\n假如手上有一个文本分类任务，我们在提升模型效果的时候一般有以下几个思路：\n\n增大数据集，同时提升标注质量\n寻找更多有效的文本特征，比如词性特征，词边界特征等等\n更换模型，使用更加适合当前任务或者说更加复杂的模型，比如FastText–&gt;TextCNN–Bert\n\n…\n之后接触到了知识蒸馏，学习到了简单的神经网络可以从复杂的网路中学习知识，进而提升模型效果。\n之前写个一个文章是TextCNN如何逼近Bert，当时写得比较粗糙，但是比较核心的点已经写出来。\n这个文章脱胎于这个论文：Distilling Task-Specific Knowledge from BERT into Simple Neural Networks\n整个训练过程是这样的：\n\n在标签数据上微调Bert模型\n使用三种方式对无标签数据进行数据增强\nBert模型在无标签数据上进行推理，Lstm模型学习Bert模型的推理结果，使用MSE作为损失函数。\n\n2.2 目标函数\n知识蒸馏的目标函数：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n一般来说，我们会使用两个部分，一个是硬目标损失函数，一个是软目标损失函数，两者都可以使用交叉熵进行度量。\n在原论文中，作者在计算损失函数的时候只是使用到了软目标，同时这个软目标并不是使用softmax之前的logits进行MSE度量损失，也就是并没有使用带有温度参数T的sotmax进行归一化。\n2.3 数据增强\n为了促进有效的知识转移，我们经常需要一个庞大的，未标记的数据集。\n三种数据增强的方式：\n\nMasking：使用概率随机的替换一个单词为[MASK]. 需要注意的是这里替换之后，Bert模型也会输入这个数据的。从直觉上来讲，这个规则可以阐明每个单词对标签的影响。\nPOS-guided word replacement.使用概率随机替换一个单词为另一个相同POS的单词。这个规则有可能会改变句子的语义信息。\nn-gram sampling\n\n整个流程是这样的：对于每个单词，如果概率p&lt;，我们使用第一条规则，如果p&lt;，我们使用第二条规则，两条规则互斥，也就是同一个单词只使用两者之间的一个。当对句子中的每个单词都过了一遍之后，我进行第三条规则，之后把整条句子补充道无标签数据集中。\n2.4 知识蒸馏结果图\n效果图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n3. PKD-BERT\nPKD  核心点就是不仅仅从Bert（老师网络）的最后一层学习知识去做蒸馏，它还另加了一部分，就是从Bert的中间层去学习。\n简单说，PKD的知识来源有两部分：中间层+最后输出。\n它缓解了之前只用最后softmax输出层的蒸馏方式出现的过拟合而导致泛化能力降低的问题。\n接下来，我们从PKD模型的两个策略说起：PKD-Last 和 PKD-Skip。\n3.1 PKD-Last and PKD-Skip\nPKD的本质是从中间层学习知识，但是这个中间层如何去定义，就各式各样了。\n比如说，我完全可以定位我只要奇数层，或者我只要偶数层，或者说我只要最中间的两层，等等，不一而足。\n那么作者，主要是使用了这么多想法中的看起来比较合理的两种。\nPKD-Last，就是把中间层定义为老师网络的最后k层。\n这样做是基于老师网络越靠后的层数含有更多更重要的信息。\n这样的想法其实和之前的蒸馏想法很类似，也就是只使用softmax层的输出去做蒸馏。但是从感官来看，有种尾大不掉的感觉，不均衡。\n另一个策略是 就是PKD-Skip，顾名思义，就是每跳几层学习一层。\n这么做是基于老师网络比较底层的层也含有一些重要性信息，这些信息不应该被错过。\n作者在后面的实验中，证明了，PKD-Skip 效果稍微好一点（slightly better）；\n作者认为PKD-Skip抓住了老师网络不同层的多样性信息。而PKD-Last抓住的更多相对来说同质化信息，因为集中在了最后几层。\n3.2. PKD\n3. 2.1架构图\n两种策略的PKD的架构图如下所示，注意观察图，有个细节很容易忽视掉:\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n我们注意看这个图，Bert的最后一层（不是那个绿色的输出层）是没有被蒸馏的，这个细节一会会提到。\n3. 2.2 怎么蒸馏中间层\n这个时候，需要解决一个问题：我们怎么蒸馏中间层？\n仔细想一下Bert的架构，假设最大长度是128，那么我们每一层Transformer encoder的输出都应该是128个单元，每个单元是768维度。\n那么在对中间层进行蒸馏的时候，我们需要针对哪一个单元？是针对所有单元还是其中的部分单元？\n首先，我们想一下，正常KD进行蒸馏的时候，我们使用的是[CLS]单元Softmax的输出，进行蒸馏。\n我们可以把这个思想借鉴过来，一来，对所有单元进行蒸馏，计算量太大。二来，[CLS] 不严谨的说，可以看到整个句子的信息。\n为啥说是不严谨的说呢？因为[CLS]是不能代表整个句子的输出信息，这一点我记得Bert中有提到。\n3.2.3蒸馏层数和学生网络的初始化\n接下来，我想说一个很小的细节点，对比着看上面的模型架构图：\nBert（老师网络）的最后一层 (Layer 12 for BERT-Base) 在蒸馏的时候是不予考虑；\n原因的话，其一可以这么理解，PKD创新点是从中间层学习知识，最后一层不属于中间层。当然这么说有点牵强附会。\n作者的解释是最后一层的隐层输出之后连接的就是Softmax层，而Softmax层的输出已经被KD Loss计算在内了。\n比如说，K=5，那么对于两种PKD的模式，被学习的中间层分别是：\nPKD-Skip: ;\nPKD-Last:\n还有一个细节点需要注意，就是学生网络的初始化方式，直接使用老师网络的前几层去初始化学生网络的参数。\n3.2.4 损失函数\n首先需要注意的是中间层的损失，作者使用的是MSE损失。如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n整个模型的损失主要是分为两个部分：KD损失和中间层的损失，如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n3.3. 实验效果\n实验效果可以总结如下：\n\nPKD确实有效，而且Skip模型比Last效果稍微好一点。\nPKD模型减少了参数量，加快了推理速度，基本是线性关系，毕竟减少了层数\n\n除了这两点，作者还做了一个实验去验证：如果老师网络更大，PKD模型得到的学生网络会表现更好吗？\n这个实验我很感兴趣。\n直接上结果图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\nKD情况下，注意不是PKD模型，看#1 和#2，在老师网络增加的情况下，效果有好有坏。这个和训练数据大小有关。\nKD情况下，看#1和#3，在老师网络增加的情况下，学生网络明显变差。\n作者分析是因为，压缩比高了，学生网络获取的信息变少了。\n也就是大网络和小网络本身效果没有差多少，但是学生网络在老师是大网络的情况下压缩比大，学到的信息就少了。\n更有意思的是对比#2和#3，老师是大网络的情况下，学生网络效果差。\n这里刚开始没理解，后来仔细看了一下，注意#2 的学生网络是，也就是它的初始化是从来的，占了一半的信息。\n好的，写到这里\n4. BERT-of-Theseus\n大家好，我是DASOU，今天介绍一下：BERT-of-Theseus\n这个论文我觉得还挺有意思，攒个思路。\n读完这个文章，BERT-of-Theseus 掌握以下两点就可以了：\n\n基于模块替换进行压缩\n除了具体任务的损失函数，没有其他多余损失函数。\n\n效果的话，与相比，：推理速度 ；模型效果 98%；\n4.1 模块替换\n举个例子，比如有一个老师网络是12层的Bert，现在我每隔两层Transformer，替换为学生网络的一层Transformer。那么最后我的学生网络也就变成了6层的小Bert，训练的时候老师网络和学生网络的模块交替训练。\n直接看下面这个架构图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n作者说他是受 Dropout 的启发，仔细想了想还真的挺像的。\n我们来说一下这样做的好处。\n我刚才说每隔老师网络的两层替换为学生网络的一层。很容易就想到PKD里面，有一个是PKD-Skip策略。\n就是每隔几层，学生网络的层去学习老师网络对应层的输出，使用损失函数让两者输出接近，使用的是CLS的输出。\n在这里提一下蒸馏/压缩的基本思想，一个最朴素的想法就是让学生网络和老师网络通过损失函数在输出层尽可能的靠近。\n进一步的，为了提升效果，可以通过损失函数，让学生网络和老师网络在中间层尽可能的靠近，就像PKD这种。\n这个过程最重要的就是在训练的时候需要通过损失函数来让老师网络和学生网络尽可能的接近。\n如果是这样的话，问题就来了，损失函数的选取以及各自损失函数之前的权重就需要好好的选择，这是一个很麻烦的事情。\n然后我们再来看 BERT-of-Theseus，它就没有这个问题。\n它是在训练的时候以概率  来从老师网络某一层和学生网络的某一层选择一个出来，放入到训练过程中。\n在这个论文里，老师网络叫做  ， 学生网络叫做  ；\n4.2 训练过程\n对着这个网络架构，我说一下整体训练的过程：\n\n在具体任务数据上训练一个 BERT-base 网络作为 ；\n使用   前六层初始化一个 6层的Bert作为  ；\n在具体任务数据上，固定  相应权重，以概率（随着steps，线性增加到1），对整个网络（加上 ）进行整体的训练。\n为了让  作为一个整体，单独抽离出来 （其实设置为1就可以了），作为一个单独的个体，在训练数据上继续微调。直至效果不再增加。\n\n简单总结，在训练数据上，老师网络和学生网络共同训练，因为存在概率问题，有的时候是老师网络的部分层加入训练，有的时候是学生网络的部分层加入训练。在这一步训练完成之后，为了保证学生网络作为一个整体（因为在第一步训练的时候大部分情况下学生网络的层都是分开加入训练过程的），在具体任务数据上，对学生网络继续微调，直至效果不再增加。\n4.3 结果分析\n不同方法的损失函数\n论文提供了一个不同Bert蒸馏方法使用的损失函数的图，值得一看，见下图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n值得注意的是，这里的 应该是选取前六层，在具体任务微调的结果。\n效果\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n整体来说，BERT-of-Theseus 思路很简单，效果也还不错。\n5. Tiny-Bert\n大家好，我是DASOU，今天说一下 TinyBert；\nTinyBert 主要掌握两个核心点：\n\n提出了对基于 transformer 的模型的蒸馏方式：Transformer distillation；\n提出了两阶段学习框架：在预训练和具体任务微调阶段都进行了 Transformer distillation（两阶段有略微不同）；\n\n下面对这两个核心点进行阐述。\n5.1. Transformer distillation\n5.1.1整体架构\n整体架构如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\nBert不严谨的来划分，可以分为三个部分：词向量输入层，中间的TRM层，尾端的预测输出层。\n在这个论文里，作者把词向量输入层 和中间的TRM层统一称之为中间层，大家读的时候需要注意哈。\nBert的不同层代表了学习到了不同的知识，所以针对不同的层，设定不同的损失函数，让学生网络向老师网络靠近，如下：\n\nebedding层的输出\n多头注意力层的注意力矩阵和隐层的输出\n预测层的输出\n\n5.1.2 Transformer 基础知识：\n注意力层：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n多头注意力层：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n前馈神经网路：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.1.3 Transformer 的蒸馏\n对 Transformer的蒸馏分为两个部分：一个是注意力层矩阵的蒸馏，一个是前馈神经网络输出的蒸馏。\n注意力层矩阵蒸馏的损失函数：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n这里注意两个细节点：\n一个是使用的是MSE；\n还有一个是，使用的没有归一化的注意力矩阵，见(1)，而不是softmax之后的。原因是实验证明这样能够更快的收敛而且效果会更好。\n前馈神经网络蒸馏的损失函数\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n两个细节点：\n第一仍然使用的是MSE.\n第二个细节点是注意，学生网路的隐层输出乘以了一个权重矩阵，这样的原因是学生网络的隐层维度和老师网络的隐层维度不一定相同。\n所以如果直接计算MSE是不行的，这个权重矩阵也是在训练过程中学习的。\n写到这里提一点，其实这里也可以看出来为什么tinybert的初始化没有采用类似PKD这种，而是使用GD过程进行蒸馏学习。\n因为我们的tinybert 在减少层数的同时也减少了宽度（隐层的输出维度），如果采用PKD这种形式，学生网络的维度和老师网络的维度对不上，是不能初始化的。\n词向量输入层的蒸馏：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n预测层输出蒸馏：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.1.4 总体蒸馏损失函数\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.2. 两阶段蒸馏\n5.2.1 整体架构\n整体架构如图：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.2.2 为什么需要GD:\n说一下我自己的理解哈，我觉得有两个原因：\n首先，就是上文说到的，tinybert不仅降低了层数，也降低了维度，所以学生网络和老师网络的维度是不符的，所以PKD这种初始化方式不太行。\n其次，一般来说，比如PKD，学生网络会使用老师网络的部分层进行初始化。这个从直觉上来说，就不太对。\n老师网络12层，学到的是文本的全部信息。学生网络是6层，如果使用老师的12层的前6层进行初始化，这个操作相当于认为这前6层代表了文本的全部信息。\n当然，对于学生网络，还会在具体任务上微调。这里只是说这个初始化方式不太严谨。\nTiny bert的初始化方式很有意思，也是用了蒸馏的方式。\n老师网络是没有经过在具体任务进行过微调的Bert网络，然后在大规模无监督数据集上，进行Transformer distillation。当然这里的蒸馏就没有预测输出层的蒸馏，翻看附录，发现这里只是中间层的蒸馏。\n简单总结一下，这个阶段，使用一个预训练好的Bert（ 尚未微调）进行了3epochs的 distillation；\n5.2.3 TD：\nTD就是针对具体任务进行蒸馏。\n核心点：先进行中间层（包含embedding层）的蒸馏，再去做输出层的蒸馏。\n老师网络是一个微调好的Bert，学生网络使用GD之后的tinybert，对老师网络进行TD蒸馏。\nTD过程是，先在数据增强之后的数据上进行中间层的蒸馏-10eopchs，learning rate 5e-5；然后预测层的蒸馏3epochs，learning rate 3e-5.\n5.3. 数据增强\n在具体任务数据上进行微调的时候，进行了数据增强。\n(感觉怪怪的)\n两个细节点：\n\n对于 single-piece word 通过Bert找到当前mask词最相近的M个单词；对于 multiple sub-word pieces 使用Glove和Consine找到最相近的M个词\n通过概率P来决定是否替换当前的词为替换词。\n对任务数据集中的所有文本数据做上述操作，持续N次。\n\n伪代码如下：\n\n编辑切换为居中\n添加图片注释，不超过 140 字（可选）\n5.4. 实验效果\n其实我最关心的一个点就是，数据增强起到了多大的作用。\n作者确实也做了实验，如下，数据增强作用还是很大的：\n\n编辑\n添加图片注释，不超过 140 字（可选）\n我比较想知道的是，在和PKD同等模型架构下，两者的比较，很遗憾，作者好像并没有做类似的实验(或者我没发现)。\n这里的tinybert参数如下：\n\nthe number of layers M=4, the hidden size d 0=312, the feedforward/filter size d 0 i=1200 and the head number h=12.\n\n5.5. 简单总结\n先说一下，我读完论文学到的东西：\n首先是transformer层蒸馏是如何涉及到的损失函数：\n\n注意力矩阵和前馈神经层使用mse；\n蒸馏的时候注意力矩阵使用未归一化\n维度不同使用权重矩阵进行转化\n\n其次，维度不同导致不能从老师Bert初始化。GD过程为了解决这个问题，直接使用学生网络的架构从老师网络蒸馏一个就可以，这里并不是重新学一个学生网络。\n还有就是数据增强，感觉tinyebert的数据增强还是比较简陋的，也比较牵强，而且是针对英文的方法。\nTD过程，对不同的层的蒸馏是分开进行的，先进行的中间层的蒸馏，然后是进行的输出层的蒸馏，输出层使用的是Soft没有使用hard。\n这个分过程蒸馏很有意思，之前没注意到这个细节点。\n在腾讯的文章中看到这样一句话：\n\n并且实验中，softmax cross-entropy loss 容易发生不收敛的情况，把 softmax 交叉熵改成 MSE, 收敛效果变好，但泛化效果变差。这是因为使用 softmax cross-entropy 需要学到整个概率分布，更难收敛，因为拟合了 teacher BERT 的概率分布，有更强的泛化性。MSE 对极值敏感，收敛的更快，但泛化效果不如前者。\n\n是有道理的，积累一下。\n值得看的一些资料：\n比 Bert 体积更小速度更快的 TinyBERT - 腾讯技术工程的文章 - 知乎 https://zhuanlan.zhihu.com/p/94359189\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/模型蒸馏/知识蒸馏综述万字长文/"},{"title":"","date":"2024-06-21T03:20:53.313Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.313Z","content":"Word2vec为什么需要二次采样？\n说到 Word2vec 的采样，首先会想起来的是负采样，属于对Word2vec的一个近似训练方法。\n其实它还涉及到一个采样方法，就是subsampling，中文叫做二次采样。\n用最简单的一句话描述二次采样就是，对文本中的每个单词会有一定概率删除掉，这个概率是和词频有关，越高频的词越有概率被删掉。\n二次采样的公式如下所示：\n\n注意: t为超参数，分母 f(w) 为单词w的词频与总词数之比\n首先说一下，我们需要对文本数据进行二次采样？\n举个简单例子，“他/是/个/优秀/的/学生”。如果此时中心词为&quot;学生&quot;，背景词为&quot;的&quot;。\n那么，我们的背景词对于我们这个中心词其实是没有什么作用的，并没有什么语义信息上的补充。\n但是像“的”这种高频词，出现的机会还很大，所以对于这一句话信息是存在冗余的。\n也就是说，在一个背景窗口中，一个词和较低频词同时出现比和较高频词同时出现对训练词嵌入模型更有益。\n举个生活中的例子，现实生活中自律优秀的人比较少，堕落不努力人的人比较多，当然是优秀的人出现在我们身边会对我们自身的成长更加的有益。\n所以我们的想法就是减少和堕落的人出现的次数，远离他们，让优秀的人出现在我们生活中的概率上升。\n那么二次采样之后文本数据变成了什么样子？\n还是上面那句话，“他/是/个/优秀/的/学生”，在这个时候，就变成了“他/是/个/优秀/学生”。也就是说高频词“的”在我们的训练数据中消失了。\n当然这个消失正如上文所说，是一个概率，可能在之后的另一个句子中，它还是存在的，只不过它出现在文本中的词频肯定是降低了的。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/Word2vec为什么需要二次采样？/"},{"title":"","date":"2024-06-21T03:20:53.343Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.343Z","content":"Word2vec模型究竟是如何获得词向量的?\n问大家一个问题：Word2vec模型是如何获得词向量的?\n很多文章在解释的时候，会说对一个词通过One-hot编码，然后通过隐层训练，得到的输入到隐层的矩阵就对应的词表词向量。\n我不能说这么解释是不对的，但是我认为是不准确的。\n在前面文章也说过了，Word2vec是不涉及到隐层的，CBOW有投影层，只是简单的求和平均，Skip-gram没有投影层，就是中心词接了一个霍夫曼树。\n所以，很多文章涉及到的隐层的权重矩阵也就无从谈起。\n在此情况下，词向量是怎么来的？\n从源码的角度来看，我们是对每个词都初始化了一个词向量作为输入，这个词向量是会随着模型训练而更新的，词向量的维度就是我们想要的维度，比如说200维。\n以Skip-gram为例，我们的输入的中心词的词向量其实不是One-hot编码，而是随机初始化的一个词向量，它会随着模型训练而更新。\n需要注意的一个细节点是，每个单词都会对应两个词向量，一个是作为中心词的时候的词向量，一个是作为背景词的时候的词向量。大家一般选择第一种。\n这一点需要注意区别Glove中的中心词向量和背景词向量。Glove中的中心词向量和背景词向量从理论上来说是等价的，只不过由于初始化的不同，最终结果会略有不同。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/Word2vec模型究竟是如何获得词向量的/"},{"title":"","date":"2024-06-21T03:20:53.353Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.353Z","content":"Word2vec的负采样\n负采样的特点\n首先对基于负采样的技术，我们更新的权重只是采样集合，减少了训练量，同时效果上来说，中心词一般来说只和上下文有关，更新其他词的权重并不重要，所以在降低计算量的同时，效果并没有变差。\n负采样具体实施细节\n我自己的总结就是创建两个线段，第一个线段切开词表大小的份数，每个份数的长度和频率正比。\n第二个线段均分M个，然后随机取整数，整数落在第二个线段那里，然后取第一个线段对应的词，如果碰到是自己，那么就跳过。\n欢迎拍砖\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/Word2vec的负采样/"},{"title":"","date":"2024-06-21T03:20:53.373Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.373Z","content":"Word2vec训练参数的选定?\n首先根据具体任务，选一个领域相似的语料，在这个条件下，语料越大越好。然后下载一个 word2vec 的新版（14年9月更新），语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型。最后记得设置迭代次数为三五十次，维度至少选 50，就可以了。（引自 《How to Generate a Good Word Embedding》）\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/Word2vec训练参数的选定/"},{"title":"","date":"2024-06-21T03:20:53.303Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.303Z","content":"总结不易，请大力点赞，感谢\n上一个文章，Word2vec-负采样/霍夫曼之后模型是否等价-绝对干货是字节的面试真题，建议朋友们多看几遍，有问题及时沟通。\n私下有几个朋友看完之后还是有点懵，又问了一下具体细节。基于此，我重新写了一个简短的文章，希望能让大家明白，大家可以结合上一个文章看。\n我们再看一下题目：W2V经过霍夫曼或者负采样之后，模型与原模型相比，是等价的还是相似的？\n首先，我们要明确，这里的原模型指的是什么？原模型就是我们的没有经过优化的W2V（当然我们也说过它是一个工具不是一个模型）。\n也就是只是使用Skip-gram模型或者CBOW模型而没有进行优化的原始版本。对于这个原始版本，是在最后一层进行了Softmax。\n我们的目标函数中，最核心的一个部分就是在给定中心词的条件下生成正确背景词的概率，我们要最大化这个东西，公式如下：\n\n仔细看，在分母涉及到了一个V，这里的V就是我们的词典大小。也就是说，为了计算这个条件概率，我们需要对整个词典进行操作，复杂度就是O(|V|)\n所以，负采样和霍夫曼就是针对这一个计算开销大的地方进行了优化。当然W2V为了减少计算量，还是去掉了隐层。比如CBOW直接是输入向量求和平均然后接霍夫曼树。比如Skip-gram直接是中心词的词向量接霍夫曼树。\n这不是我这个文章的重点，就不细细展开了。\n我们先说负采样。负采样的本质在于生成K个噪声。它的本质是基于中心词生成正确的背景词概率为1，生成噪声词概率为0，这个是我们的优化方向。公式如下：\n\n仔细看这个公式，V已经消失，取而代之的是K，也就是我们的噪声词的数量，换句话讲，我们的复杂度被K这个大小限制住了，降低为了O(|K|)\n然后我们再来看层序Softmax。它的核心本质是在一条路径上不停的做二分类，概率连乘就会得到我们的条件概率。公式如下：\n\n注意看，这个公式中，V也已经消失了，被霍夫曼树中到达背景词的路径限制住了，这也就是上个文章中说到的，复杂度变成了二叉树的高度: O(log|V|)\n既然只是针对的部分节点，那么与原始版本相比，当然是近似。\n简单的总结一下：\n其实可以这样理解，以跳字模型为例，条件概率是中心词生成背景词的概率，也就是我们优化函数中最核心的部分。没有使用优化的，分母涉及到全部词汇，训练开销大。负采样近似训练，把复杂度限制在了k个噪声词，层序softmax也属于近似训练，在它的条件概率中，不断的二分类，涉及到的是能够达到背景词的那个路径上的非叶子结点，也就是没涉及到其他节点，这一点和负采样很类似，都是从全部词汇降低复杂度，只不过负采样是被k限制，层序是被路径编码限制(0,1,1,1,0)这种限制住。\n不知道大家有没有注意到，负采样和霍夫曼都是讲Softmax转化为二分类的问题从而降低了复杂度。负采样是针对是不是背景词做二分类，霍夫曼是在对是不是正确路径上的节点做二分类。这么说有点不严谨，但是意思就是这么个意思，大家理解一下。\n总结不易，请大力点赞，感谢\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/word2vec两种优化方式的联系和区别/"},{"title":"","date":"2024-06-21T03:20:53.383Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.383Z","content":"微信公众号：NLP从入门到放弃\n\n有没有使用自己的数据训练过Word2vec，详细说一下过程。包括但是不限于：语料如何获取，清理以及语料的大小，超参数的选择及其原因，词表以及维度大小，训练时长等等细节点。\nWord2vec模型是如何获得词向量的？聊一聊你对词嵌入的理解？如何理解分布式假设？\n如何评估训练出来的词向量的好坏\nWord2vec模型如何做到增量训练\n大致聊一下 word2vec这个模型的细节，包括但不限于：两种模型以及两种优化方法（大致聊一下就可以，下面会详细问）\n解释一下 hierarchical softmax 的流程(CBOW and Skip-gram)\n基于6，可以展开问一下模型如何获取输入层，有没有隐层，输出层是什么情况。\n基于6，可以展开问输出层为何选择霍夫曼树，它有什么优点，为何不选择其他的二叉树\n基于6，可以问该模型的复杂度是多少，目标函数分别是什么，如何做到更新梯度（尤其是如何更新输入向量的梯度）\n基于6，可以展开问一下 hierarchical softmax 这个模型 有什么缺点\n聊一下负采样模型优点（为什么使用负采样技术）\n如何对输入进行负采样（负采样的具体实施细节是什么）\n负采样模型对应的目标函数分别是什么（CBOW and Skip-gram）\nCBOW和skip-gram相较而言，彼此相对适合哪些场景\n有没有使用Word2vec计算过句子的相似度，效果如何，有什么细节可以分享出来\n详细聊一下Glove细节，它是如何进行训练的？有什么优点？什么场景下适合使用？与Word2vec相比，有什么区别（比如损失函数）？\n详细聊一下Fasttext细节，每一层都代表了什么？它与Wod2vec的区别在哪里？什么情况下适合使用Fasttext这个模型？\nELMO的原理是什么？以及它的两个阶段分别如何应用？（第一阶段如何预训练，第二阶段如何在下游任务使用）\nELMO的损失函数是什么？它是一个双向语言模型吗？为什么？\nELMO的优缺点分别是什么？为什么可以做到一词多义的效果？\n\n本面试题词向量资源参考：\nword2vec、glove、cove、fastext以及elmo对于知识表达有什么优劣？ - 霍华德的回答 - 知乎\nhttps://www.zhihu.com/question/292482891/answer/492247284\n面试题：Word2Vec中为什么使用负采样？ - 七月在线 七仔的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/66088781\n关于word2vec，我有话要说 - 张云的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/29364112\nword2vec（二）：面试！考点！都在这里 - 我的土歪客的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/133025678\nnlp中的词向量对比：word2vec/glove/fastText/elmo/GPT/bert - JayLou娄杰的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/56382372\n史上最全词向量讲解（LSA/word2vec/Glove/FastText/ELMo/BERT） - 韦伟的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/75391062\nword2vec详解（CBOW，skip-gram，负采样，分层Softmax） - 孙孙的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/53425736\nWord2Vec详解-公式推导以及代码 - link-web的文章 - 知乎\nhttps://zhuanlan.zhihu.com/p/86445394\n关于ELMo，面试官们都怎么问：https://cloud.tencent.com/developer/article/1594557\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/史上最全词向量面试题梳理/"},{"title":"","date":"2024-06-21T03:20:53.423Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.423Z","content":"本文大概需要阅读 5.25 分钟\n大概用三篇文章好好谈一下Word2vec，这篇主要是关于 Word2vec 的两种模型。\n先问大家个问题：一个词通过Word2vec训练之后，可以得到几个词向量？\n如果您有点懵，说明我写对了，您慢慢看下去，码字不易，请多多点赞，让更多人看到，谢谢。\n词向量一般被认为是一个词的特征向量，也就是说可以代表一个词的含义。一个中文词，比如&quot;中国&quot;这个词，是很难被神经网络直接作为输入，我们需要将词向量化（或者说数字化），才能更好的喂进神经网络。\n这个过程很类似于计算机在处理我们输入的文字的时候，会转化为二进制进行处理。\n只不过这个二进制表达规则我们会自己人为规定，而词向量这个向量表达根据不同的方式会有着不同的结果。想一下，两者是不是很类似。\n谈到向量表达单词，一般首先想到的都是One-Hot编码。但是它是有缺点的。我这里谈两个缺点。\n首先是维度灾难：如果我们有1万个单词，那么你的One-hot去表示每个单词的的时候，会转变为一个1万维度的向量。\n那么在计算的时候会带来巨大的不便，而且向量矩阵极其稀疏，占据了太多不必要的内存。当然对于维度灾难，我们一般可以使用PCA等降维手段来缓解。\n其次是语义表达不足。这一点很简单，”娱乐“与”八卦“两个词，通过One-hot编码得到的向量，计算Consine得到相似度为0。\n这显然是不合适的。One-Hot编码表示出来的词向量是两两正交的，从余弦相似度的角度来看，是互不相关的，所以 One-Hot 不能很好的表达词语的相似性。\n这个时候，我们再来看 Word2vec。首先，要明确一点，Word2vec 不是一个模型，而是一个工具。这个点虽然无伤大雅，但是每每看到有的文章说它是个模型，就有点…\nWord2vec 从整体上理解，就是包含两个模型（CBOW and Skip-gram）和两个高效的优化训练方式（负采样和层序softmax）\n这个文章主要谈一下两种模型。\n先假定我们的窗口大小为2，也就是说中心词前面有两个词，后面有两个词，比如&quot;我/永远/爱/中国/共产党&quot;，抽象出来就是：\nW_{t-2}，W_{t-1}，W_{t}，W_{t+1}，W_{t+2}\n对于Skip-gram，中文我们叫跳字模型，使用中心词预测背景词。也就是用&quot;爱&quot;去预测其他的四个词。\n对于CBOW，中文我们叫连续词袋模型，使用背景词预测中心词，也就是用&quot;我&quot;，“永远”，“中国”，“共产党” 去预测&quot;爱&quot;。\nCBOW的模型架构，从整体上，我们可以分为三层：输入层，投影层，和输出层。\n对于输入层，对应的是窗口中的单词，也就是例子中&quot;我&quot;，“永远”，“中国”，“共产党” 四个词的词向量，在投影层，将四个词的词向量进行相加求平均，输出层在没有优化的前提下，维度为词表大小，随后做 Softmax即可。\nSkip-gram模型架构很类似，只不过可以省去投影层，因为输入为中心词，是一个单词的词向量，无从谈起求和与平均了。\n接下来，我们来详细谈一下Skip-gram。\n对于一个神经网络模型，我们需要确定我们的优化目标或者说目标函数是什么？\n对于Skip-gram，其实很容易理解，就是我要最大化在给出中心词的条件下背景词出现的概率，公式如下\n\\prod_{t=1}^T \\ \\prod_{-m\\leq j \\leq m,j\\neq 0}  P(w^{t+j}|w^{t})\n这个公式对应着两个连乘，第一个连乘是对应 T 个输入样本，也就是说我们文本中的每个单词都要做中心词。第二个连乘代表着给定一个中心词的情况下，窗口中的单词出现的概率，内含相互独立的假设。\n在这里，有一个细节点想要提醒大家，在词汇表中的每个单词，都是对应两个词向量的，一个是在作为中心词的时候的中心词向量，一个是在作为背景词时候的背景词向量。\n优化目标函数的时候，为了减少复杂度（也就是忽略 T），我们可以使用随机梯度下降，针对每一个样本我们都更新一次参数。\n通过公式推导（略），我们能够观察到一个特点，就是每次更新参数，都会涉及到词典中的全部词汇，这是因为我们在做 Softmax 的时候，是分母是针对所有词汇的操作。\n这样的操作的复杂度是 O(|V|)，其中|V|就是我们词汇表的大小。CBOW 其实是很类似的情况，每次更新都会涉及到我们的全部词汇。\n于是，我们就需要对此进行优化，使用了两种近似的训练方式来高效的训练Word2vec，降低训练的复杂度。\n下一个文章谈一下优化方式。\n如果觉得写的还行，帮忙点个赞或者在看，让更多人关注到我吧，感谢。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/聊一下Word2vec-模型篇/"},{"title":"","date":"2024-06-21T03:20:53.413Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.413Z","content":"本文大概需要阅读 4.75 分钟\n先问大家两个问题，看能不能解答\n\n\nGlove 中词向量的表达是使用的中心词向量还是背景词向量还是有其他方法？\n\n\n能不能分别用一句话概括出Glove和Fasttext 的核心要点？\n\n\n先来谈Glove。中文全称 Global Vectors for Word Representation。它做的事情概括出来就是：基于全局语料，获得词频统计，学习词语表征。\n我们从语料之中，学习到X共现词频矩阵，词频矩阵中的每个元素$x_{ij}$，代表的是词\n$x_{j}$出现在$x_{i}$的环境中的次数。注意，对于共现词频矩阵来说，它是一个对称矩阵。\n这一点非常的重要，也很容易理解，词A出现在词B周围的次数肯定是等价于词B出现在词A周围的次数的。\n类比于Word2vec，对于词$x_{i}$，就是中心词，对于词$x_{j}$也就是背景词。\n理论上，一个词作为中心词向量和一个词作为背景学到的两种向量应该是完全相同的。\n但是现实中，由于我们初始化的不同，所以我们最终学习到的两种词向量是有些许不同。\n为了增加模型的鲁棒性，在Glove中，使用两个词向量的和作为我们一个词的词向量的表达。\n这一点是区别于Word2vec，对于Word2vec，中心词向量和背景词向量是不等价的，我们一般使用中心词向量代表一个词最终的语义表达。\nGlove 论文中的推导过程其实不是很严谨，大致流程就是从大语料中发现了一个规律，即条件概率的比值可以比较直观的表达出词与词之间的关系。\n随后可以构建词向量函数去拟合条件概率的比值。基于此一步步的进行延伸推导，在我看了就是在基于一些假设，寻找出一种可能性的存在。\n在这里就不细说，直接给出Glove的损失函数：\n$\\sum_{i \\in V} \\sum_{j \\in V} h(x_{ij})(u_{j}^Tv_{i}+b_{i}+b_{j}-log(x_{ij}))^2$\n详细讲一下这个损失函数，它是一个平方损失函数，很值得琢磨。\n我把它分为了三个部分，权重函数h(x),词向量表达是$u_{j}^Tv_{i}+b_{i}+b_{j}$，共现词频的对数 $log(x_{ij})$\n从这里，我插一句我的理解，就是GLove基于的是无标签的语料，属于无监督的训练过程，但是从损失函数看，我觉得可以认为它是一个有监督的过程。\n标签就是$log(x_{ij})$，这个是我们从语料之中计算出来的，换句话说，在模型训练之前，我们可以对语料的计算，得到相应的标签数据，所以我自己认为这个可以看做是一个有监督的过程。\n我们使用一句话去描述这个损失函数可以这么去说：随着模型不停的优化，词向量的表达在不断的拟合共现词频的对数。\nh(x)是权重函数，代表的含义是表达一个词对的重要性，在值域[0,1]上单调递增。直观上理解就是一对词语共现的词频越多，那么它的重要性也就越大。\n论文中给出的函数是这样的，在x&lt;c(比如c=100)的情况下，h(x)=(x/c)^\\alpha (\\alpha=0.75)，在其余情况下，h(x)=1。也就是重要度不能无限增大，给了一个上限。\n我们看这个损失函数，有一个很有意思的点，就是h(0)=0。想一下，这个代表着什么？\n也就是说，当我们一对词的共现词频为0的时候，损失函数是为0，换句话讲，我们的损失函数的复杂度是和共现词频矩阵中的非零元素是线性关系。\n所以在训练的时候，我们只需要对非零元素采样，使用随机梯度就可以对词向量和两个偏置项进行学习更新。\n这里还有一个细节点，需要注意，偏置项是不可以省略的。为什么呢？因为如果去掉，在公式推导中的对称性假设就不满足，感兴趣的同学可以自己推导一系。\n参考链接：\n视频：\nhttps://www.bilibili.com/video/BV154411S7Tf?p=17\n文档：http://zh.d2l.ai/chapter_natural-language-processing/glove.html\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/聊一下Glove/"},{"title":"","date":"2024-06-21T03:20:53.463Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.463Z","content":"Word2vec 涉及到两种优化方式，一种是负采样，一种是层序Softmax\n先谈一下负采样，以跳字模型为例。中心词生成背景词可以由两个相互独立事件的联合组成来近似（引自李沐大神的讲解）。\n第一个事件是，中心词和背景词同时出现在窗口中。第二个事件是，中心词和K个噪声词不同时出现在窗口数据中，其中噪声词由噪声分布随机生成。\n这里我们就可以知道上一个文章开头说到的，负采样是一种等价操作还是近似操作？我们在第二个事件中，使用了K个噪声词。但是实际上呢？应该远远大于K。\n还是那个例子，句子为&quot;我/永远/爱/中国/共产党&quot;，中心词为’爱’，我们在选择噪声词的时候，选择了K个，但是实际上，在词汇表中，排除掉’我’，‘永远’，‘中国’，‘共产党’ 这四个词汇的其他词都可以算做我的噪声词，然而为了减少复杂度，我只选择了其中的K个，所以当然应该是近似了。\n接下来，我们看层序Softmax。\n层序Softmax 对应的就是在输出层使用一个霍夫曼树，代替了原本在输出层统一进行的softmax。\n首先，我们需要了解霍夫曼树在这里是如何构建的。\n简单讲，霍夫曼树是一个二叉树，以语料中出现过的词当做叶子节点，以各词在语料中出现的次数当做权值进行构造。其中叶子节点有N个，就是词典的大小，非叶子节点有N-1个（包括根节点）。\n比如说我的所有文章中，“共产党”这个词出现了 100次，是最大的，那么根节点的左分支（或者右分支）就对应着”共产党“这个词，另一个分支做与根节点相同的操作，找到排除”共产党“这个词之外的所有词中最大的词，比如”中国“作为其中的左分支（或者右分支），以此类推，一个霍夫曼树就成功构建。\n霍夫曼树中，我们需要注意的是，每个非叶子节点对应一个向量，每个叶子节点对应一个向量。两种向量都会随着模型的训练进行更新。\n其中叶子节点的向量就是我们的词向量，而非叶子节点上的向量就是没有什么实际含义，它的作用就是帮助我们计算模型在霍夫曼树上不断的进行二分类时候的概率。\n以上面那句话为例，我们现在中心词为‘爱’，然后，我要预测背景词‘中国’。首先我们要确定的是我的叶子节点是包含所有单词的，也就是包含了我这个简单句子的五个单词（不考虑前期数据清洗低频率词的情况）。\n也就是说，在这个霍夫曼树上，有且仅有一条路径，让我从根节点出发，经过多次判断（也就是说走过了多个非叶子节点），最终走到了“中国”这个叶子节点，对应的概率就是每个节点概率的连乘。\n然后这个时候，我们想一下霍夫曼树是不是一种近似？\n当然，我们每更新一个词向量，只是涉及到了可以到达叶子节点的这一条路径上节点。所以复杂度就是树的高度，也就是 O(log|V|)\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/聊一下Word2vec-训练优化篇/"},{"title":"","date":"2024-06-21T03:20:46.723Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:46.723Z","content":"\nBERT推理速度慢，导致落地困难；找到效果不错，推理速度快的模型是一个方向，ESIM是一个很好的选择；\n\nESIM 推理速度快，效果不错，堪称文本匹配的利器；\n对于ESIM，重点掌握就一点：是两个句子之间究竟是如何交互的.\n0 整体架构\n先来看整体结构是什么样子：\n\n对于这个架构，我们主要是看左边这个就可以；\n可以看到，从架构上来看，这个模型大概可以范围四层：最底层是一个双向LSTM，作为句子的的编码器，随后是一个交互层，然后又是一个双向LSTM，最后是一个输出层。\n原论文中，也是将ESIM分为四个部分，Input Encoding，Local Inference Modeling， Inference Composition和Prediction，我们一个个说。\n1. Input Encoding\n先假设，我现在有两个句子：\n$a=(a_{1},a_{2},…,a_{l_{a}})$ 和$b=(b_{1},b_{2},…,b_{l_{b}})$；\n我要判断它是否表达同样的意思：0或者1；\n首先第一步是Input Encoding ，这是一个常规操作，就是tokens的embeddings接BiLSTM；注意，我们是两个句子都进入到这同一个BiLSTM中，而不是进入到两个；\n公式如下：\n\n作者同时也测试了使用GRU对句子进行编码，但是结果在NLI任务上效果并不好；不过我认为，在实际工作中，两者都可尝试，毕竟玄学。\n2. Local Inference Modeling\n首先这里回顾一下符号：$ \\bar{a_{i}} $是句子a在i时刻的是输出，$\\bar{b_{j}}$是句子b在j时刻的输出；\n那么我们使用如下公式计算两个单词输出之间的交互：\n\n举个很简单的例子，比如说隐层维度为256，那么$ \\bar{a_{i}}=[1,256] $，$ \\bar{b_{j}}=[1,256] $\n那么相乘之后，就是维度[1,1]的值；\n这只是两个单词输出之间的交互，我们知道a和b句子长度是不一样的（当然也可能一样）；\n这里我们假设a长度为10，b长度为20；\n那么经过（11）的计算，我们会得到一个[10,20]的矩阵，用来描述两个句子之间不同单词之间的交互。\n核心点就是在于对于这个[10,20]的矩阵，如何对它进行操作，公式如下：\n\n一定要注意看这里的$\\widetilde a_{i}$，我们得到的是[10,20]的矩阵，然后对每一行做softmax操作，得到相似度，然后乘以$b_{j}$。\n（12）和（13）本质上是对这个[10,20]的矩阵分别做了按照行的相似度和按照列的相似度；\n有点难理解，还是举个例子（这里的例子就不举长度为10和20了，简单点）。\na：【我今天去吃饭了】\nb：【他为什么还在学习】\na中的【我】依次对【他为什么还在学习】中的每个单词做乘法，就是得到了$e_{ij}$；然后softmax之后，每个相似度对应乘以【他为什么还在学习】的每个单词输出encoding从而得到加权和，作为$\\widetilde a_{i}$\n之后就是对特征进行拼接，分为两种，对位相减和对位相乘：\n\n3. inference composition和Prediction\n这一步也是常规操作，就是把$m_{a}和m_{b}$输入到第二层的BiLSTM，并把输出做最大池化和平均池化，然后拼接特征，然后输出到全连接，得到结果：\n\n4. 总结\n最核心的点还是在于理解如从两个句子单词之间的交互矩阵获得两个句子之间的交互结果；\n也就是需要对单词之间的交互矩阵，比如[10,20]，分别按照行做softmax和列做softmax；\n这个思想其实在后期很多模型中都有用到，值得思考。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本匹配和文本相似度/ESIM/"},{"title":"","date":"2024-06-21T03:20:53.443Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.443Z","content":"本文大概需要阅读 2.75 分钟\n以Q&amp;A的形式，聊一聊Word2vec的细节点\nWord2vec训练参数的选定?\n首先根据具体任务，选一个领域相似的语料，在这个条件下，语料越大越好。然后下载一个 word2vec 的新版（14年9月更新），语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型。最后记得设置迭代次数为三五十次，维度至少选 50，就可以了。（引自 《How to Generate a Good Word Embedding》）\nWord2vec模型是如何获得词向量的?\n很多文章在解释的时候，会说对一个词通过One-hot编码，然后通过隐层训练，得到的矩阵就对应的词表词向量。这个我觉得是不准确的。\n首先，在前面文章也说过了，Word2vec是不涉及到隐层的，CBOW有投影层，只是简单的求和平均，Skip-gram没有投影层，就是中心词接了一个霍夫曼树。\n在此情况下，词向量是怎么来的？首先，要明确一点，以Skip-gram为例，我们的输入的中心词的词向量其实不是One-hot编码，而是随机初始化的一个词向量，它会随着模型训练而更新。\n需要注意的一个细节点是，每个单词都会对应两个词向量，一个是作为中心词的时候的词向量，一个是作为背景词的时候的词向量。大家一般选择第一种。\nCBOW和skip-gram相较而言，彼此相对适合哪些场景\nCBOW比Skip-gram 训练速度快，但是Skip-gram可以得到更好的词向量表达。\n为什么这么说？Skip-gram 是中心词预测背景词，假如我们有K个背景词，那么对于一个中心词来说，就是做了K次训练，那么词向量就回得到更加充分的训练。\n而对于CBOW，我们是背景词预测中心词，相当于只是做了一次训练。想一下是不是这么回事？\n简单来说，我们视线回到一个大小为K的训练窗口（窗口内全部单词为2k+1个），CBOW只是训练一次，Skip-gram 则是训练了2K次。当然是Skip-gram词向量会更加的准确一点，相应的会训练的慢一点。\n负采样的特点\n首先对基于负采样的技术，我们更新的权重只是采样集合，减少了训练量，同时效果上来说，中心词一般来说只和上下文有关，更新其他词的权重并不重要，所以在降低计算量的同时，效果并没有变差。\n负采样具体实施细节\n就是创建两个线段，第一个线段切开词表大小的份数，每个份数的长度和频率正比。\n第二个线段均分M个，然后随机取整数，整数落在第二个线段哪里，然后取第一个线段对应的词，如果碰到是自己，那么就跳过。\n在代码实现中，第一个线段的长度不仅仅是频率，而是一个3/4的幂次方，第二个线段切分为10的8次方个段数\n如果觉得对您有所帮助，帮忙点个在看或者赞，谢谢!\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/聊一下Word2vec-细节篇/"},{"title":"","date":"2024-06-21T03:20:53.493Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.493Z","content":"词向量资源总结\n一文总结词向量的计算、评估与优化\nhttps://mp.weixin.qq.com/s/RXY5A4gcx60BN6bFdeccKQ\n比较系统的介绍了常见的生成词向量的神经网络模型有NNLM模型,C&amp;W模型,CBOW模型和Skip-gram模型。不错\n深入浅出词嵌入技术\nhttps://mp.weixin.qq.com/s/UE7ClHu7kiY_HXoJrZ0CwA\nword2vec bert gpt的简单介绍\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/词向量资源总结/"},{"title":"","date":"2023-11-03T23:00:09.296Z","date_formatted":{"ll":"Nov 3, 2023","L":"11/03/2023","MM-DD":"11-03"},"updated":"2023-11-03T15:00:09.296Z","content":"\n\n\n\n\n\nEphesus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","plink":"http://www.ephesus.top/files/rl/omega/JSOmega/JSomega/"},{"title":"","date":"2024-06-21T03:48:20.755Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:20.755Z","content":"ESIM\n\n导入数据\n\n1.1 train_data = LCQMC_Dataset(train_file, vocab_file, max_length)\n通过LCQMC_Dataset 导入数据，在LCQMC_Dataset 中，做了这几件事情：\n1.1.1 p, h, self.label = load_sentences(LCQMC_file)\n读取数据文件，切分数据\n1.1.2 word2idx, _, _ = load_vocab(vocab_file)\n读取字典文件，从而获得 word2idx, idx2word, vocab\n1.1.3 self.p_list, self.p_lengths, self.h_list, self.h_lengths = word_index(p, h, word2idx, max_char_len)\n将数据转化为对应的数值，并且根据max_char_len进行切分或者截断\n1.1.4\nself.p_list = torch.from_numpy(self.p_list).type(torch.long)\nself.h_list = torch.from_numpy(self.h_list).type(torch.long)\n转化为对应的torch tensor\n1.2  数据batch化\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n按照batch进行读取。是torch的代码，我就不看了\n1.3 读取embedding文件\nembeddings = load_embeddings(embeddings_file)\n注意pad全为零\n\n\nmodel构建\nmodel = ESIM(hidden_size, embeddings=embeddings, dropout=dropout, num_classes=num_classes, device=device).to(device)\n\n\n为了方便，我把ESIM整体流程重点是attention的部分抽离了出来\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/src/ESIM-attention/ESIM代码解读/"},{"title":"","date":"2024-06-20T19:48:21.575Z","updated":"2024-06-20T19:48:21.575Z","content":"{\"models_root\":\"./available_models\",\"models\":[{\"id\":100,\"model\":\"model_0.pt\",\"timeout\":600,\"on_timeout\":\"to_cpu\",\"load\":true,\"opt\":{\"gpu\":0,\"beam_size\":5},\"tokenizer\":{\"type\":\"sentencepiece\",\"model\":\"wmtenfr.model\"}},{\"model\":\"model_0.light.pt\",\"timeout\":-1,\"on_timeout\":\"unload\",\"model_root\":\"../other_models\",\"opt\":{\"batch_size\":1,\"beam_size\":10}}]}"},{"title":"","date":"2024-06-20T19:48:21.705Z","updated":"2024-06-20T19:48:21.705Z","content":"{\"data\":\"data/cnndm/CNNDM\",\"save_model\":\"models/cnndm\",\"save_checkpoint_steps\":10000,\"keep_checkpoint\":10,\"seed\":3435,\"train_steps\":100000,\"valid_steps\":10000,\"report_every\":100,\"encoder_type\":\"brnn\",\"word_vec_size\":128,\"rnn_size\":512,\"layers\":1,\"optim\":\"adagrad\",\"learning_rate\":0.15,\"adagrad_accumulator_init\":0.1,\"max_grad_norm\":2,\"batch_size\":16,\"dropout\":0,\"copy_attn\":\"true\",\"global_attention\":\"mlp\",\"reuse_copy_attn\":\"true\",\"bridge\":\"true\",\"world_size\":2,\"gpu_ranks\":[0,1]}"},{"title":"","date":"2024-06-20T19:48:21.735Z","updated":"2024-06-20T19:48:21.735Z","content":"{\"data\":\"exp/dataset.de-en\",\"save_model\":\"exp/model.de-en\",\"save_checkpoint_steps\":10000,\"keep_checkpoint\":10,\"seed\":3435,\"train_steps\":500000,\"valid_steps\":10000,\"warmup_steps\":8000,\"report_every\":100,\"decoder_type\":\"transformer\",\"encoder_type\":\"transformer\",\"word_vec_size\":512,\"rnn_size\":512,\"layers\":6,\"transformer_ff\":2048,\"heads\":8,\"accum_count\":8,\"optim\":\"adam\",\"adam_beta1\":0.9,\"adam_beta2\":0.998,\"decay_method\":\"noam\",\"learning_rate\":2,\"max_grad_norm\":0,\"batch_size\":4096,\"batch_type\":\"tokens\",\"normalization\":\"tokens\",\"dropout\":0.1,\"label_smoothing\":0.1,\"max_generator_batches\":2,\"param_init\":0,\"param_init_glorot\":\"true\",\"position_encoding\":\"true\",\"world_size\":1,\"gpu_ranks\":[0]}"},{"title":"","date":"2024-06-20T19:48:21.785Z","updated":"2024-06-20T19:48:21.785Z","content":"{\"data\":\"exp/dataset.de-en\",\"save_model\":\"exp/model.de-en\",\"save_checkpoint_steps\":10000,\"keep_checkpoint\":10,\"seed\":3435,\"train_steps\":200000,\"valid_steps\":10000,\"warmup_steps\":8000,\"report_every\":100,\"decoder_type\":\"transformer\",\"encoder_type\":\"transformer\",\"word_vec_size\":512,\"rnn_size\":512,\"layers\":6,\"transformer_ff\":2048,\"heads\":8,\"accum_count\":2,\"optim\":\"adam\",\"adam_beta1\":0.9,\"adam_beta2\":0.998,\"decay_method\":\"noam\",\"learning_rate\":2,\"max_grad_norm\":0,\"batch_size\":4096,\"batch_type\":\"tokens\",\"normalization\":\"tokens\",\"dropout\":0.1,\"label_smoothing\":0.1,\"max_generator_batches\":2,\"param_init\":0,\"param_init_glorot\":\"true\",\"position_encoding\":\"true\",\"world_size\":4,\"gpu_ranks\":[0,1,2,3]}"},{"title":"","date":"2024-06-21T03:48:28.485Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:28.485Z","content":"This directly contains scripts and tools adopted from other open source projects such as Apache Joshua and Moses Decoder.\nTODO: credit the authors and resolve license issues (if any)\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/tools/README/"},{"title":"","date":"2024-06-21T03:20:47.683Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.683Z","content":"Notes on versioning\n[Unreleased]\nFixes and improvements\n1.0.0.rc1 (2019-10-01)\n\nFix Apex / FP16 training (Apex new API is buggy)\nMultithread preprocessing way faster (Thanks François Hernandez)\nPip Installation v1.0.0.rc1 (thanks Paul Tardy)\n\n0.9.2 (2019-09-04)\n\nSwitch to Pytorch 1.2\nPre/post processing on the translation server\noption to remove the FFN layer in AAN + AAN optimization (faster)\nCoverage loss (per Abisee paper 2017) implementation\nVideo Captioning task: Thanks Dylan Flaute!\nToken batch at inference\nSmall fixes and add-ons\n\n0.9.1 (2019-06-13)\n\nNew mechanism for MultiGPU training “1 batch producer / multi batch consumers”\nresulting in big memory saving when handling huge datasets\nNew APEX AMP (mixed precision) API\nOption to overwrite shards when preprocessing\nSmall fixes and add-ons\n\n0.9.0 (2019-05-16)\n\nFaster vocab building when processing shards (no reloading)\nNew dataweighting feature\nNew dropout scheduler.\nSmall fixes and add-ons\n\n0.8.2 (2019-02-16)\n\nUpdate documentation and Library example\nRevamp args\nBug fixes, save moving average in FP32\nAllow FP32 inference for FP16 models\n\n0.8.1 (2019-02-12)\n\nUpdate documentation\nRandom sampling scores fixes\nBug fixes\n\n0.8.0 (2019-02-09)\n\nMany fixes and code cleaning thanks @flauted, @guillaumekln\nDatasets code refactor (thanks @flauted) you need to r-preeprocess datasets\n\nNew features\n\nFP16 Support: Experimental, using Apex, Checkpoints may break in future version.\nContinuous exponential moving average (thanks @francoishernandez, and Marian)\nRelative positions encoding (thanks @francoishernanndez, and Google T2T)\nDeprecate the old beam search, fast batched beam search supports all options\n\n0.7.2 (2019-01-31)\n\nMany fixes and code cleaning thanks @bpopeters, @flauted, @guillaumekln\n\nNew features\n\nMultilevel fields for better handling of text featuer embeddinggs.\n\n0.7.1 (2019-01-24)\n\nMany fixes and code refactoring thanks @bpopeters, @flauted, @guillaumekln\n\nNew features\n\nRandom sampling thanks @daphnei\nEnable sharding for huge files at translation\n\n0.7.0 (2019-01-02)\n\nMany fixes and code refactoring thanks @benopeters\nMigrated to Pytorch 1.0\n\n0.6.0 (2018-11-28)\n\nMany fixes and code improvements\nNew: Ability to load a yml config file. See examples in config folder.\n\n0.5.0 (2018-10-24)\n\nFixed advance n_best beam in translate_batch_fast\nFixed remove valid set vocab from total vocab\nNew: Ability to reset optimizer when using train_from\nNew: create_vocabulary tool + fix when loading existing vocab.\n\n0.4.1 (2018-10-11)\n\nFixed preprocessing files names, cleaning intermediary files.\n\n0.4.0 (2018-10-08)\n\n\nFixed Speech2Text training (thanks Yuntian)\n\n\nRemoved -max_shard_size, replaced by -shard_size = number of examples in a shard.\nDefault value = 1M which works fine in most Text dataset cases. (will avoid Ram OOM in most cases)\n\n\n0.3.0 (2018-09-27)\n\n\nNow requires Pytorch 0.4.1\n\n\nMulti-node Multi-GPU with Torch Distributed\nNew options are:\n-master_ip: ip address of the master node\n-master_port: port number of th emaster node\n-world_size = total number of processes to be run (total GPUs accross all nodes)\n-gpu_ranks = list of indices of processes accross all nodes\n\n\ngpuid is deprecated\nSee examples in https://github.com/OpenNMT/OpenNMT-py/blob/master/docs/source/FAQ.md\n\n\nFixes to img2text now working\n\n\nNew sharding based on number of examples\n\n\nFixes to avoid 0.4.1 deprecated functions.\n\n\n0.2.1 (2018-08-31)\nFixes and improvements\n\nFirst compatibility steps with Pytorch 0.4.1 (non breaking)\nFix TranslationServer (when various request try to load the same model at the same time)\nFix StopIteration error (python 3.7)\n\nNew features\n\nEnsemble at inference (thanks @Waino)\n\n0.2 (2018-08-28)\nimprovements\n\nCompatibility fixes with Pytorch 0.4 / Torchtext 0.3\nMulti-GPU based on Torch Distributed\nAverage Attention Network (AAN) for the Transformer (thanks @francoishernandez )\nNew fast beam search (see -fast in translate.py) (thanks @guillaumekln)\nSparse attention / sparsemax (thanks to @bpopeters)\nRefactoring of many parts of the code base:\n\n\nchange from -epoch to -train_steps -valid_steps (see opts.py)\nreorg of the logic train =&gt; train_multi / train_single =&gt; trainer\n\n\nMany fixes / improvements in the translationserver (thanks @pltrdy @francoishernandez)\nfix BPTT\n\n0.1 (2018-06-08)\nFirst and Last Release using Pytorch 0.3.x\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/CHANGELOG/"},{"title":"","date":"2024-06-21T03:20:47.813Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.813Z","content":"Contributors\nOpenNMT-py is a community developed project and we love developer contributions.\nGuidelines\nBefore sending a PR, please do this checklist first:\n\nPlease run onmt/tests/pull_request_chk.sh and fix any errors. When adding new functionality, also add tests to this script. Included checks:\n\nflake8 check for coding style;\nunittest;\ncontinuous integration tests listed in .travis.yml.\n\n\nWhen adding/modifying class constructor, please make the arguments as same naming style as its superclass in PyTorch.\nIf your change is based on a paper, please include a clear comment and reference in the code (more on that below).\n\nDocstrings\nAbove all, try to follow the Google docstring format\n(Napoleon example,\nGoogle styleguide).\nThis makes it easy to include your contributions in the Sphinx documentation. And, do feel free\nto autodoc your contributions in the API .rst files in the docs/source folder! If you do, check that\nyour additions look right.\n12345cd docs# install some dependencies if necessary:# recommonmark, sphinx_rtd_theme, sphinxcontrib-bibtexmake htmlfirefox build/html/main.html  # or your browser of choice\nSome particular advice:\n\n\nTry to follow Python 3 typing module conventions when documenting types.\n\nException: use “or” instead of unions for more readability\nFor external types, use the full “import name”. Common abbreviations (e.g. np) are acceptable.\nFor torch.Tensor types, the torch. is optional.\nPlease don’t use tics like (`str`) or rst directives like (:obj:`str`). Napoleon handles types\nvery well without additional help, so avoid the clutter.\n\n\n\nGoogle docstrings don’t support multiple returns.\nFor multiple returns, the following works well with Sphinx and is still very readable.\n12345678910111213141516def foo(a, b):    &quot;&quot;&quot;This is my docstring.    Args:        a (object): Something.        b (class): Another thing.    Returns:        (object, class):        * a: Something or rather with a long          description that spills over.        * b: And another thing.    &quot;&quot;&quot;    return a, b\n\n\nWhen citing a paper, avoid directly linking in the docstring! Add a Bibtex entry to docs/source/refs.bib.\nE.g., to cite “Attention Is All You Need”, visit arXiv, choose the\nbibtext link, search docs/source/refs.bib\nusing CTRL-F for DBLP:journals/corr/VaswaniSPUJGKP17, and if you do not find it then copy-paste the\ncitation into refs.bib. Then, in your docstring, use :cite:`DBLP:journals/corr/VaswaniSPUJGKP17` .\n\nHowever, a link is better than nothing.\n\n\n\nPlease document tensor shapes. Prefer the format\nb, c)`` ```. This style is easy to read, allows using ``x`` for multplication, and is common1234567891011121314  (PyTorch uses a few variations on the parentheses format, AllenNLP uses exactly this format, Fairseq uses  the parentheses format with single ticks).    - Again, a different style is better than no shape documentation.- Please avoid unnecessary space characters, try to capitalize, and try to punctuate.  For multi-line docstrings, add a blank line after the closing ``&quot;&quot;&quot;``.  Don&#x27;t use a blank line before the closing quotes.  ``&quot;&quot;&quot; not this &quot;&quot;&quot;`` ``&quot;&quot;&quot;This.&quot;&quot;&quot;``  ```python  &quot;&quot;&quot;      Not this.  &quot;&quot;&quot;\n1&quot;&quot;&quot;This.&quot;&quot;&quot;\nThis note is the least important. Focus on content first, but remember that consistent docs look good.\n\n\nBe sensible about the first line. Generally, one stand-alone summary line (per the Google guidelines) is good.\nSometimes, it’s better to cut directly to the args or an extended description. It’s always acceptable to have a\n“trailing” citation.\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/CONTRIBUTING/"},{"title":"","date":"2024-06-21T03:20:44.093Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:44.093Z","content":"中文分词中基于词典的正向最大匹配和逆向最大匹配\n正向最大匹配和逆向最大匹配步骤类似，只是方向不同，我以正向匹配为例，先用一句话去总结它：\n在做整个正向成词的过程中，我们做了两个步骤，首先按照字典最大长度进行对原始文本进行切分，然后逐渐去掉右边一个单字，去查看剩余文本在字典是否存在，依次迭代。\n上面这句话只看不太好理解，我来个简单的例子，如下：\n我要被切分的句子是这样的：”今天天气真不错啊“\n我的字典是这样的：[今天,天天,天气,真不错,不错,啊,哈哈哈哈哈]\n对于字典这一块，最粗糙的就是存成列表，优化一点可以存成字典树，这里简化一点，我们存成列表。\n我字典的最大长度是 “哈哈哈哈哈”，为5\n所以我第一次正向匹配就是：\n今天天气真 # 取原始文本前五个单词，查看是否存在于字典，否，删除底部\n今天天气 # 查看是否存在于字典，否，删除底部\n今天天 # 查看是否存在于字典，否，删除底部\n今天 #匹配到字典中的“天气”这个词\n第二次正向匹配是这样的：\n天气真不错啊 # 因为”今天“已经被匹配到了，所以我们不在考虑，取剩余文本的前五个单词，查看是否存在于字典，否，删除底部\n天气真不错 #查看是否存在于字典，否，删除底部\n天气真不 #查看是否存在于字典，否，删除底部\n天气真 #查看是否存在于字典，否，删除底部\n天气 #匹配到字典中的“天气“这个单词\n第三次正向匹配的过程：\n真不错啊 # 剩余文本不够5个，我们取小，取4个，查看是否存在于字典，否，删除底部\n真不错 # 匹配到”真不错“ 这个单词\n第四次正向匹配的过程：\n啊 # 字典中没有与之相关的单词，由于长度已经为1，直接单独成词就可以\n在做整个正向成词的过程中，我们做了两个步骤，首先按照字典最大长度进行对原始文本进行切分（需要比对最大长度和文本的长度，如果文本长度不够的话，就取文本长度，总之取小。比如第三次正向匹配”真不错啊“这剩余的四个字就不够5个），\n然后逐渐去掉右边一个单字，去查看剩余文本在字典是否存在，依次迭代。\n其实逆向匹配是很类似的过程，只不过方向变了，需要注意的是我们始终删除的是底部单词：\n第一次逆向匹配：\n气真不错啊 # 查看是否存在于字典，否，删除底部\n真不错啊 # 查看是否存在于字典，否，删除底部\n不错啊 # 查看是否存在于字典，否，删除底部\n错啊 # 查看是否存在于字典，否，删除底部\n啊 # 字典中没有与之相关的单词，由于长度已经为1，直接单独成词就可以\n…\n…\n…\n双向最大匹配算法就是两种方法都切一遍，从中选择一种比较好的，标准就是：大颗粒度词越多越好，非词典词和单字词越少越好.\n对于代码的实现，我记得是好久之前从网上down下来的，具体来源忘了，不过都大同小异，自己写也没啥问题。\n我在这里啰嗦的讲一下大致思路，如果您觉得比较简单，或者只想看代码，跳过就可以：\n基本思路是这样的，我有一个存储我词典的列表，以词典中最大长度为基线顺序对原始文本进行切分，迭代查看当前切分词是否在词典，在就算一个词，不在的话，当前词长度减一，就是往前缩小一个词，继续进行上述活动。直至长度为1，是最后的一个迭代条件。\n在写代码的时候，我自己觉得从两个方面来掌握，一个是从小方面，怎么讲，就是比如说我的字典最大的长度是5个单词，我在5个单词迭代的去找有没有在字典的中的词，这是一个while循环。\n还有一个方面是大的方面，就是我现在5个单词迭代完了，比如找到了一个长度为2的在字典中的词（需要注意的是如果没有在字典中，那么长度就是1的单字就可以加进去了），然后我要做的就是把这两个单词之后的字段作为输入，再重复上面这个过程，这个是大的方面，是另一个While循环\n12345678910111213141516171819202122232425262728## 正向最大匹配算法def cut_words(split_sentence,words_dic):    #统计词典中最长的词    max_length = max(len(word) for word in words_dic)    sentence = split_sentence.strip() ## 简单清理一下    #统计序列长度    words_length = len(sentence) ## 在第二个循环的时候，我需要不停的和字典最大长度比较，取最小值作为基线    #存储切分好的词语    cut_word_list = []    while words_length &gt; 0: ## 第二个循环，找到一个之后，循环的去找下一个符合要求的        max_cut_length = min(max_length, words_length)        subSentence = sentence[0 : max_cut_length]        while max_cut_length &gt; 0: ## 第一个循环，迭代找到符号字典的            if subSentence in words_dic:                cut_word_list.append(subSentence)                break            elif max_cut_length == 1:                cut_word_list.append(subSentence)                break            else:                max_cut_length = max_cut_length -1                subSentence = subSentence[0:max_cut_length]        sentence = sentence[max_cut_length:]        words_length = words_length - max_cut_length    return cut_word_listinput_str=&quot;今天天气真不错啊，适合出去旅游&quot;bmm_word_list = cut_words(input_str, words_dic)print(bmm_word_list)\n123456789101112131415161718192021222324252627##逆向最大匹配def cut_words(raw_sentence,words_dic):    #统计词典中词的最长长度    max_length = max(len(word) for word in words_dic)    sentence = raw_sentence.strip()    #统计序列长度    words_length = len(sentence)    #存储切分出来的词语    cut_word_list = []    #判断是否需要继续切词    while words_length &gt; 0:        max_cut_length = min(max_length, words_length)        subSentence = sentence[-max_cut_length:]        while max_cut_length &gt; 0:            if subSentence in words_dic:                cut_word_list.append(subSentence)                break            elif max_cut_length == 1:                cut_word_list.append(subSentence)                break            else:                max_cut_length = max_cut_length -1                subSentence = subSentence[-max_cut_length:]        sentence = sentence[0:-max_cut_length]        words_length = words_length -max_cut_length    cut_word_list.reverse()    return  cut_word_list\n参考链接：\n中文分词中的正向最大匹配与逆向最大匹配：https://blog.csdn.net/chengzheng_hit/article/details/54752673\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/关键词提取/中文分词/基于词典的正向最大匹配和逆向最大匹配中文分词/"},{"title":"","date":"2024-06-21T03:20:48.673Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.673Z","content":"MIT License\nCopyright © 2017-Present OpenNMT\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the “Software”), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/LICENSE/"},{"title":"","date":"2024-06-21T03:20:50.633Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:50.633Z","content":"机器翻译竞赛-唱园杯-Pytorch代码-Baseline\n几天前看到一个机器翻译竞赛-唱园杯，奖金60万，真是吓了一跳。\n不过我不是冲奖金，因为这么高奖金可以想一下竞争程度。我本意想要积累一下中英文翻译数据，后来发现是编码之后的数据…\n有点失望，就没有然后了。所以就没有花太多时间去做这个东西，简单跑了一个baselines。\n官方评测指标简单粗暴，一个句子有一个单词翻译错了就pass。这个比赛数据量不小，迭代20万步，目测需要一周。所以现在排行榜的分数都很低，大佬们估计在等后期发力吧。\n没时间打比赛，一些相关代码也不想浪费掉，就分享给大家，希望对您有所帮助。\nBaseline代码很简单，就是用 OpenNMT-py 这个库做的机器翻译，不过中文关于这个库的资料很少，当初啃这个库也是一点点看的源代码，细节还挺多的。\n默默吐槽一句代码组织架构有点乱，有些地方真的是让人摸不到头脑…\n我也用这个文章做一个简单的 OpenNMT-py 的教程。如果是参加竞赛的话，后期可能需要修改源代码，所以建议大家不用安装 OpenNMT-py 库，而是直接下载源代码，方便修改。\n首先，使用环境如下，大家照此下载就可以:\ntorchtext==0.4\nOpenNMT-py==1.0\npython==3.5\ncuda==9.0\n如果 OpenNMT-py==1.0 这个版本大家找不到，直接来我github上下载下来用就可以。\n数据预处理\n在 /data 目录下，需要包含四个文件，分别是 src-train.txt  src-val.txt  tgt-train.txt  tgt-val.txt\n假如我们是中文翻译成英文，那么我们的 src-train.txt 和 src-val.txt 就是中文文件， tgt-train.txt  和 tgt-val.txt 就是英文文件。\n其中文件内容格式为每行为一句文本，以空格进行分割。\n对于唱园杯的数据，我们需要对其进行一些简单的修改，以满足上面的要求，我这边给出一个简单的处理代码，以字为单位，代码文件名称为「process_ori_data.py」：\n12345678910111213141516171819202122232425262728293031323334353637file=open(&#x27;train_data.csv&#x27;,&#x27;r&#x27;)lines=file.readlines()src_train=open(&#x27;src-train.txt&#x27;,&#x27;w&#x27;)tgt_train=open(&#x27;tgt-train.txt&#x27;,&#x27;w&#x27;)src_val=open(&#x27;src-val.txt&#x27;,&#x27;w&#x27;)tgt_val=open(&#x27;tgt-val.txt&#x27;,&#x27;w&#x27;)chinese_lists=[]english_lists=[]index=0for line in lines:    if index ==0:        index+=1        continue    line=line.strip().split(&#x27;,&#x27;)    chinese=line[1].strip().split(&#x27;_&#x27;)    english=line[2].strip().split(&#x27;_&#x27;)    chinese_lists.append(&#x27; &#x27;.join(chinese))    english_lists.append(&#x27; &#x27;.join(english))    index+=1assert len(chinese_lists)==len(english_lists)split_num=int(0.85*index)for num in range(len(english_lists)):    if num&lt;=split_num:        src_train.write(chinese_lists[num]+&#x27;\\n&#x27;)        tgt_train.write(english_lists[num]+&#x27;\\n&#x27;)    else:        src_val.write(chinese_lists[num]+&#x27;\\n&#x27;)        tgt_val.write(english_lists[num]+&#x27;\\n&#x27;)src_train.close()tgt_train.close()src_val.close()tgt_val.close()\n在对原始数据进行处理之后，我们还需要进一步处理，代码如下：\n1nohup python preprocess.py -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/data  -src_seq_length 500 -tgt_seq_length 500 &gt;preposs_datalog &amp;\n在这里需要提一个很重要的小细节，就是 src_seq_length 参数 和tgt_seq_length 参数的设定问题。默认这里是50。它的含义是如果句子长度小于50，不会被读入dataset！！！因为唱园杯的数据普遍比较长，所以你如果这里保持默认的话，会出现你只处理了一小部分原始数据的问题。\n具体这个数值你设定为多少，看你自己具体情况。因为唱园杯在数据说明中说到已经去掉了特殊字符等，所以我就全部保留了。\n模型进行预测\n直接使用 Transformer 进行训练。Opennmt使用特定参数复现了 Transformer 的效果，这里我们直接套用就可以。\n123456789nohup python train.py -data data/data -save_model data-model \\        -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \\        -encoder_type transformer -decoder_type transformer -position_encoding \\        -train_steps 200000  -max_generator_batches 2 -dropout 0.1 \\        -batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2 \\        -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 \\        -max_grad_norm 0 -param_init 0  -param_init_glorot \\        -label_smoothing 0.1 -valid_steps 10000 -save_checkpoint_steps 10000 \\        -world_size 4 -gpu_ranks 0 1 2 3  &amp;\n预测\n在预测之前，我们需要看一下测试数据，发现是双向预测，所以我们需要将上面的数据颠倒过来再来一次，训练另一个模型即可。\n按道理也可以使用全部数据（颠倒混合），这样训练一个模型就可以，不过我没试过，不知道效果如何，感兴趣的可以试一试。\n预测代码如下：\n1python  translate.py  -model demo-model_200000.pt -src data/src-test.txt -output pred.txt \n优化思路\n因为是编码之后的数据，所有常规的优化没啥用，这里简单提两个：\n\n\n使用全部数据（训练数据和测试数据）训练Word2vec/Glove/Bert 等，然后作为输入，从而加入先验信息\n\n\n如果不想自己训练，可以使用词频对应到编码之后的数据，得到一个大致的结果，从而可以使用我们正常的word2vec/glove/bert\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/README/"},{"title":"","date":"2024-06-20T19:20:48.563Z","updated":"2024-06-20T19:20:48.563Z","content":"{\"env\":\"pytorch-0.4\",\"machine\":\"cpu\"}"},{"title":"","date":"2024-06-21T03:20:53.483Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:53.483Z","content":"[TOC]\n1. 灵魂20问帮你彻底搞定词向量\n\n有没有使用自己的数据训练过Word2vec，详细说一下过程。包括但是不限于：语料如何获取，清理以及语料的大小，超参数的选择及其原因，词表以及维度大小，训练时长等等细节点。\nWord2vec模型是如何获得词向量的？聊一聊你对词嵌入的理解？如何理解分布式假设？\n如何评估训练出来的词向量的好坏\nWord2vec模型如何做到增量训练\n大致聊一下 word2vec这个模型的细节，包括但不限于：两种模型以及两种优化方法（大致聊一下就可以，下面会详细问）\n解释一下 hierarchical softmax 的流程(CBOW and Skip-gram)\n基于6，可以展开问一下模型如何获取输入层，有没有隐层，输出层是什么情况。\n基于6，可以展开问输出层为何选择霍夫曼树，它有什么优点，为何不选择其他的二叉树\n基于6，可以问该模型的复杂度是多少，目标函数分别是什么，如何做到更新梯度（尤其是如何更新输入向量的梯度）\n基于6，可以展开问一下 hierarchical softmax 这个模型 有什么缺点\n聊一下负采样模型优点（为什么使用负采样技术）\n如何对输入进行负采样（负采样的具体实施细节是什么）\n负采样模型对应的目标函数分别是什么（CBOW and Skip-gram）\nCBOW和skip-gram相较而言，彼此相对适合哪些场景\n有没有使用Word2vec计算过句子的相似度，效果如何，有什么细节可以分享出来\n详细聊一下Glove细节，它是如何进行训练的？有什么优点？什么场景下适合使用？与Word2vec相比，有什么区别（比如损失函数）？\n详细聊一下Fasttext细节，每一层都代表了什么？它与Wod2vec的区别在哪里？什么情况下适合使用Fasttext这个模型？\nELMO的原理是什么？以及它的两个阶段分别如何应用？（第一阶段如何预训练，第二阶段如何在下游任务使用）\nELMO的损失函数是什么？它是一个双向语言模型吗？为什么？\nELMO的优缺点分别是什么？为什么可以做到一词多义的效果？\n\n2. W2C模型篇–一个词通过Word2vec训练之后，可以得到几个词向量?\n先问大家个问题：一个词通过Word2vec训练之后，可以得到几个词向量？\n如果您有点懵，说明我写对了，您慢慢看下去，码字不易，请多多点赞，让更多人看到，谢谢。\n词向量一般被认为是一个词的特征向量，也就是说可以代表一个词的含义。一个中文词，比如&quot;中国&quot;这个词，是很难被神经网络直接作为输入，我们需要将词向量化（或者说数字化），才能更好的喂进神经网络。\n这个过程很类似于计算机在处理我们输入的文字的时候，会转化为二进制进行处理。\n只不过这个二进制表达规则我们会自己人为规定，而词向量这个向量表达根据不同的方式会有着不同的结果。想一下，两者是不是很类似。\n谈到向量表达单词，一般首先想到的都是One-Hot编码。但是它是有缺点的。我这里谈两个缺点。\n首先是维度灾难：如果我们有1万个单词，那么你的One-hot去表示每个单词的的时候，会转变为一个1万维度的向量。\n那么在计算的时候会带来巨大的不便，而且向量矩阵极其稀疏，占据了太多不必要的内存。当然对于维度灾难，我们一般可以使用PCA等降维手段来缓解。\n其次是语义表达不足。这一点很简单，”娱乐“与”八卦“两个词，通过One-hot编码得到的向量，计算Consine得到相似度为0。\n这显然是不合适的。One-Hot编码表示出来的词向量是两两正交的，从余弦相似度的角度来看，是互不相关的，所以 One-Hot 不能很好的表达词语的相似性。\n这个时候，我们再来看 Word2vec。首先，要明确一点，Word2vec 不是一个模型，而是一个工具。这个点虽然无伤大雅，但是每每看到有的文章说它是个模型，就有点…\nWord2vec 从整体上理解，就是包含两个模型（CBOW and Skip-gram）和两个高效的优化训练方式（负采样和层序softmax）\n这个文章主要谈一下两种模型。\n先假定我们的窗口大小为2，也就是说中心词前面有两个词，后面有两个词，比如&quot;我/永远/爱/中国/共产党&quot;，抽象出来就是：\n$W_{t-2}，W_{t-1}，W_{t}，W_{t+1}，W_{t+2}$\n对于Skip-gram，中文我们叫跳字模型，使用中心词预测背景词。也就是用&quot;爱&quot;去预测其他的四个词。\n对于CBOW，中文我们叫连续词袋模型，使用背景词预测中心词，也就是用&quot;我&quot;，“永远”，“中国”，“共产党” 去预测&quot;爱&quot;。\nCBOW的模型架构，从整体上，我们可以分为三层：输入层，投影层，和输出层。\n对于输入层，对应的是窗口中的单词，也就是例子中&quot;我&quot;，“永远”，“中国”，“共产党” 四个词的词向量，在投影层，将四个词的词向量进行相加求平均，输出层在没有优化的前提下，维度为词表大小，随后做 Softmax即可。\nSkip-gram模型架构很类似，只不过可以省去投影层，因为输入为中心词，是一个单词的词向量，无从谈起求和与平均了。\n接下来，我们来详细谈一下Skip-gram。\n对于一个神经网络模型，我们需要确定我们的优化目标或者说目标函数是什么？\n对于Skip-gram，其实很容易理解，就是我要最大化在给出中心词的条件下背景词出现的概率，公式如下\n$\\prod_{t=1}^T \\ \\prod_{-m\\leq j \\leq m,j\\neq 0} P(w^{t+j}|w^{t})$\n这个公式对应着两个连乘，第一个连乘是对应 T 个输入样本，也就是说我们文本中的每个单词都要做中心词。第二个连乘代表着给定一个中心词的情况下，窗口中的单词出现的概率，内含相互独立的假设。\n在这里，有一个细节点想要提醒大家，在词汇表中的每个单词，都是对应两个词向量的，一个是在作为中心词的时候的中心词向量，一个是在作为背景词时候的背景词向量。\n优化目标函数的时候，为了减少复杂度（也就是忽略 T），我们可以使用随机梯度下降，针对每一个样本我们都更新一次参数。\n通过公式推导（略），我们能够观察到一个特点，就是每次更新参数，都会涉及到词典中的全部词汇，这是因为我们在做 Softmax 的时候，是分母是针对所有词汇的操作。\n这样的操作的复杂度是 O(|V|)，其中|V|就是我们词汇表的大小。CBOW 其实是很类似的情况，每次更新都会涉及到我们的全部词汇。\n于是，我们就需要对此进行优化，使用了两种近似的训练方式来高效的训练Word2vec，降低训练的复杂度。\n3. W2C优化方式篇\nWord2vec 涉及到两种优化方式，一种是负采样，一种是层序Softmax\n先谈一下负采样，以跳字模型为例。中心词生成背景词可以由两个相互独立事件的联合组成来近似（引自李沐大神的讲解）。\n第一个事件是，中心词和背景词同时出现在窗口中。第二个事件是，中心词和K个噪声词不同时出现在窗口数据中，其中噪声词由噪声分布随机生成。\n这里我们就可以知道上一个文章开头说到的，负采样是一种等价操作还是近似操作？我们在第二个事件中，使用了K个噪声词。但是实际上呢？应该远远大于K。\n还是那个例子，句子为&quot;我/永远/爱/中国/共产党&quot;，中心词为’爱’，我们在选择噪声词的时候，选择了K个，但是实际上，在词汇表中，排除掉’我’，‘永远’，‘中国’，‘共产党’ 这四个词汇的其他词都可以算做我的噪声词，然而为了减少复杂度，我只选择了其中的K个，所以当然应该是近似了。\n接下来，我们看层序Softmax。\n层序Softmax 对应的就是在输出层使用一个霍夫曼树，代替了原本在输出层统一进行的softmax。\n首先，我们需要了解霍夫曼树在这里是如何构建的。\n简单讲，霍夫曼树是一个二叉树，以语料中出现过的词当做叶子节点，以各词在语料中出现的次数当做权值进行构造。其中叶子节点有N个，就是词典的大小，非叶子节点有N-1个（包括根节点）。\n比如说我的所有文章中，“共产党”这个词出现了 100次，是最大的，那么根节点的左分支（或者右分支）就对应着”共产党“这个词，另一个分支做与根节点相同的操作，找到排除”共产党“这个词之外的所有词中最大的词，比如”中国“作为其中的左分支（或者右分支），以此类推，一个霍夫曼树就成功构建。\n霍夫曼树中，我们需要注意的是，每个非叶子节点对应一个向量，每个叶子节点对应一个向量。两种向量都会随着模型的训练进行更新。\n其中叶子节点的向量就是我们的词向量，而非叶子节点上的向量就是没有什么实际含义，它的作用就是帮助我们计算模型在霍夫曼树上不断的进行二分类时候的概率。\n以上面那句话为例，我们现在中心词为‘爱’，然后，我要预测背景词‘中国’。首先我们要确定的是我的叶子节点是包含所有单词的，也就是包含了我这个简单句子的五个单词（不考虑前期数据清洗低频率词的情况）。\n也就是说，在这个霍夫曼树上，有且仅有一条路径，让我从根节点出发，经过多次判断（也就是说走过了多个非叶子节点），最终走到了“中国”这个叶子节点，对应的概率就是每个节点概率的连乘。\n然后这个时候，我们想一下霍夫曼树是不是一种近似？\n当然，我们每更新一个词向量，只是涉及到了可以到达叶子节点的这一条路径上节点。所以复杂度就是树的高度，也就是 O(log|V|)\n4. W2C-负采样/霍夫曼之后模型是否等价\n私下有几个朋友看完之后还是有点懵，又问了一下具体细节。基于此，我重新写了一个简短的文章，希望能让大家明白，大家可以结合上一个文章看。\n我们再看一下题目：W2V经过霍夫曼或者负采样之后，模型与原模型相比，是等价的还是相似的？\n首先，我们要明确，这里的原模型指的是什么？原模型就是我们的没有经过优化的W2V（当然我们也说过它是一个工具不是一个模型）。\n也就是只是使用Skip-gram模型或者CBOW模型而没有进行优化的原始版本。对于这个原始版本，是在最后一层进行了Softmax。\n我们的目标函数中，最核心的一个部分就是在给定中心词的条件下生成正确背景词的概率，我们要最大化这个东西，公式如下：\n![条件概率_中心词生成背景词](https://github.com/DA-southampton/NLP_ability/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%9](https://github.com/DA-southampton/NLP_ability/blob/master/深度学习](…/images/条件概率_中心词生成背景词.png)\n仔细看，在分母涉及到了一个V，这里的V就是我们的词典大小。也就是说，为了计算这个条件概率，我们需要对整个词典进行操作，复杂度就是O(|V|)\n所以，负采样和霍夫曼就是针对这一个计算开销大的地方进行了优化。当然W2V为了减少计算量，还是去掉了隐层。比如CBOW直接是输入向量求和平均然后接霍夫曼树。比如Skip-gram直接是中心词的词向量接霍夫曼树。\n这不是我这个文章的重点，就不细细展开了。\n我们先说负采样。负采样的本质在于生成K个噪声。它的本质是基于中心词生成正确的背景词概率为1，生成噪声词概率为0，这个是我们的优化方向。公式如下：\n![负采样_条件概率_中心词生成背景词](https://github.com/DA-southampton/NLP_ability/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%9](https://github.com/DA-southampton/NLP_ability/blob/master/深度学习](…/images/负采样_条件概率_中心词生成背景词.png)\n仔细看这个公式，V已经消失，取而代之的是K，也就是我们的噪声词的数量，换句话讲，我们的复杂度被K这个大小限制住了，降低为了O(|K|)\n然后我们再来看层序Softmax。它的核心本质是在一条路径上不停的做二分类，概率连乘就会得到我们的条件概率。公式如下：\n![层序softmax_条件概率](https://github.com/DA-southampton/NLP_ability/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%9](https://github.com/DA-southampton/NLP_ability/blob/master/深度学习](…/images/层序softmax_条件概率.png)\n注意看，这个公式中，V也已经消失了，被霍夫曼树中到达背景词的路径限制住了，这也就是上个文章中说到的，复杂度变成了二叉树的高度: O(log|V|)\n既然只是针对的部分节点，那么与原始版本相比，当然是近似。\n简单的总结一下：\n其实可以这样理解，以跳字模型为例，条件概率是中心词生成背景词的概率，也就是我们优化函数中最核心的部分。没有使用优化的，分母涉及到全部词汇，训练开销大。负采样近似训练，把复杂度限制在了k个噪声词，层序softmax也属于近似训练，在它的条件概率中，不断的二分类，涉及到的是能够达到背景词的那个路径上的非叶子结点，也就是没涉及到其他节点，这一点和负采样很类似，都是从全部词汇降低复杂度，只不过负采样是被k限制，层序是被路径编码限制(0,1,1,1,0)这种限制住。\n不知道大家有没有注意到，负采样和霍夫曼都是讲Softmax转化为二分类的问题从而降低了复杂度。负采样是针对是不是背景词做二分类，霍夫曼是在对是不是正确路径上的节点做二分类。这么说有点不严谨，但是意思就是这么个意思，大家理解一下。\n5. Word2vec训练参数的选定?\n首先根据具体任务，选一个领域相似的语料，在这个条件下，语料越大越好。然后下载一个 word2vec 的新版（14年9月更新），语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型。最后记得设置迭代次数为三五十次，维度至少选 50，就可以了。（引自 《How to Generate a Good Word Embedding》）\n6. W2C为什么需要二次采样？\n说到 Word2vec 的采样，首先会想起来的是负采样，属于对Word2vec的一个近似训练方法。\n其实它还涉及到一个采样方法，就是subsampling，中文叫做二次采样。 用最简单的一句话描述二次采样就是，对文本中的每个单词会有一定概率删除掉，这个概率是和词频有关，越高频的词越有概率被删掉。 二次采样的公式如下所示：\n![二次采样](https://github.com/DA-southampton/NLP_ability/raw/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/%E8%AF%8D%E5%90%9](https://github.com/DA-southampton/NLP_ability/blob/master/深度学习](…/images/二次采样.png)\n注意: t为超参数，分母 f(w) 为单词w的词频与总词数之比\n首先说一下，我们需要对文本数据进行二次采样？ 举个简单例子，“他/是/个/优秀/的/学生”。如果此时中心词为&quot;学生&quot;，背景词为&quot;的&quot;。\n那么，我们的背景词对于我们这个中心词其实是没有什么作用的，并没有什么语义信息上的补充。\n但是像“的”这种高频词，出现的机会还很大，所以对于这一句话信息是存在冗余的。 也就是说，在一个背景窗口中，一个词和较低频词同时出现比和较高频词同时出现对训练词嵌入模型更有益。\n举个生活中的例子，现实生活中自律优秀的人比较少，堕落不努力人的人比较多，当然是优秀的人出现在我们身边会对我们自身的成长更加的有益。\n所以我们的想法就是减少和堕落的人出现的次数，远离他们，让优秀的人出现在我们生活中的概率上升。\n那么二次采样之后文本数据变成了什么样子？\n还是上面那句话，“他/是/个/优秀/的/学生”，在这个时候，就变成了“他/是/个/优秀/学生”。也就是说高频词“的”在我们的训练数据中消失了。\n当然这个消失正如上文所说，是一个概率，可能在之后的另一个句子中，它还是存在的，只不过它出现在文本中的词频肯定是降低了的。\n7. Word2vec的负采样\n负采样的特点\n首先对基于负采样的技术，我们更新的权重只是采样集合，减少了训练量，同时效果上来说，中心词一般来说只和上下文有关，更新其他词的权重并不重要，所以在降低计算量的同时，效果并没有变差。\n负采样具体实施细节\n我自己的总结就是创建两个线段，第一个线段切开词表大小的份数，每个份数的长度和频率正比。\n第二个线段均分M个，然后随机取整数，整数落在第二个线段那里，然后取第一个线段对应的词，如果碰到是自己，那么就跳过。\n欢迎拍砖\n8. W2C模型究竟是如何获得词向量的\n问大家一个问题：Word2vec模型是如何获得词向量的?\n很多文章在解释的时候，会说对一个词通过One-hot编码，然后通过隐层训练，得到的输入到隐层的矩阵就对应的词表词向量。\n我不能说这么解释是不对的，但是我认为是不准确的。\n在前面文章也说过了，Word2vec是不涉及到隐层的，CBOW有投影层，只是简单的求和平均，Skip-gram没有投影层，就是中心词接了一个霍夫曼树。\n所以，很多文章涉及到的隐层的权重矩阵也就无从谈起。\n在此情况下，词向量是怎么来的？\n从源码的角度来看，我们是对每个词都初始化了一个词向量作为输入，这个词向量是会随着模型训练而更新的，词向量的维度就是我们想要的维度，比如说200维。\n以Skip-gram为例，我们的输入的中心词的词向量其实不是One-hot编码，而是随机初始化的一个词向量，它会随着模型训练而更新。\n需要注意的一个细节点是，每个单词都会对应两个词向量，一个是作为中心词的时候的词向量，一个是作为背景词的时候的词向量。大家一般选择第一种。\n这一点需要注意区别Glove中的中心词向量和背景词向量。Glove中的中心词向量和背景词向量从理论上来说是等价的，只不过由于初始化的不同，最终结果会略有不同。\n9. CBOW和skip-gram相较而言，彼此相对适合哪些场景\n先用一句话来个结论：CBOW比Skip-gram 训练速度快，但是Skip-gram可以得到更好的词向量表达。\n为什么这么说？\n因为我们知道两种优化方式只是对softmax的近似优化，不会影响最终结果，所以这里，我们讨论的时候，为了更加的清晰讲解，不考虑优化的情况。\n使用一句话作为一个例子： “我/永远/爱/中国/共产党”\n先说CBOW，我们想一下，它的情况是使用周围词预测中心词。如果“爱”是中心词，别的是背景词。对于“爱”这个中心词，只是被预测了一次。\n对于Skip-gram，同样，我们的中心词是“爱”，背景词是其他词，对于每一个背景词，我们都需要进行一次预测，每进行一次预测，我们都会更新一次词向量。也就是说，相比CBOW，我们的词向量更新了2k次（假设K为窗口，那么窗口内包含中心词就有2k+1个单词）\n想一下是不是这么回事？Skip-gram被训练的次数更多，那么词向量的表达就会越丰富。\n如果语料库中，我们的的低频词很多，那么使用Skip-gram就会得到更好的低频词的词向量的表达，相应的训练时长就会更多。\n简单来说，我们视线回到一个大小为K的训练窗口（窗口内全部单词为2k+1个），CBOW只是训练一次，Skip-gram 则是训练了2K次。当然是Skip-gram词向量会更加的准确一点，相应的会训练的慢一点。\n欢迎大佬拍砖\n10. Fasttext解读-文本分类\n我先说一个小问题，估计很多人也有疑惑。\n看了很多文章，有的说是fasttext是CBOW的简单变种，有的说是Skip-gram的变种。究竟哪个是对的？\n带着这个问题，我们来聊一聊Fasttext。首先Fasttext涉及到两个论文：\n\n第一个是Bag of Tricks for Efficient TextClassification(201607)。它解决的问题是使用Fasttext进行文本分类\n第二个是Enriching Word Vectors with Subword Information(201607) 。它解决的是使用Fasttext训练词向量。\n\n今天这个文章，主要谈一下Bag of Tricks for Efficient Text Classification 这个论文 ，主要涉及到的就是文本分类的问题。\nFasttext用作文本分类，做到了速度和精读的一个平衡：标准多核CPU情况下，不到十分钟，可以训练超过十亿个单词。不到一分钟，可以对50万个句子在312千个类别中进行分类。\n这么说，其实不太明显，简单算一下。假设每个句子含有20个单词，那么十亿个单词对应就是五千万个句子，换句话讲在多核CPU的条件下，一分钟左右可以训练500万个句子。\n和Bert比较一下，在GPU条件下，8个小时训练300万条数据左右。相比之下Fasttext的这个速度是真的太快了。\n在这个论文中，也就是使用做文本分类的Fasttext，使用的是CBOW的架构。\n注意哦，强调一遍，Fasttext用在文本分类，模型架构使用的是CBOW的变种。（我这句话的意思不是说使用Skip-gram不可以，而是CBOW在理解文本分类的时候更加的容易理解）\n这里和Word2vec的CBOW有两个区别：\n\n第一，使用类别标签替换了中心词。\n第二，使用句子中所有单词作为输入，而不再是单单的针对滑动窗口中的单词。\n\n这两个点如果我们自己考虑，也很容易想到。\n为什么这么说呢？先说第二点。我现在要做的是针对文本进行分类，所以对于我的输入需要转换到整体这个句子来看，才能使对一个句子的特征表达。\n再说第一点，我们知道在Wrod2vec中，我们使用的是中心词作为输出，而且使用了霍夫曼作为输出层。\n非叶子点上的向量为了我的二分类提供计算，叶子节点为整个词汇表中所有词汇的向量。两个向量都会随着模型而训练。\n如果要做分类，我们可以想一下叶子节点和非叶子节点的变化。\n首先叶子节点对应的是所有类别。如果说我们的类别有5000个，那么对应到Word2vec，我们就有着5000个词汇。想一下是不是这么对应。\n非叶子节点其实没什么变化，因为它没有什么实际含义，只是为二分类提供计算。\n在这里还想说一下，word2vec中的叶子节点也就是词向量更新之后我们最后是要的，但是对于fasttext其实不会用到这个，因为我们是对文本进行分类，只需要保存了模型权重在预测的时候可以预测就可以了。\n还想谈一下词向量初始化的问题，模型训练开始的时候，词向量随机初始化就可以，模型训练结束之后，我们在预测阶段直接使用这个词向量就可以（就是随着模型训练而更新的这个词向量）。\n对这个论文还有一个很有意思的点，就是N-gram就是fasttext的模型的输入不仅仅针对的是每个单词，为了加入词序信息，还加入了n-gram信息。\n需要注意的一个细节是，这里的n-gram针对的是word，而不是char。对应到中文，应该对应的是分词之后的词，而不是字。但是我自己认为这么对应过来不太好理解。\n中文的字做n-gram貌似也有词序信息。但是英文的char-level的n-gram很难说针对这个句子提供一个语序信息。大家理解一下就好。\n还有一个问题想说一下，使用了n-gram信息之后，词表肯定是变大了的。你需要的n的内容越多，比如你想要n=1orn=2orn=3等等吧，n的取值范围越大，你的词表越大。\n这就会出现一个问题训练非常缓慢。这点很容易理解，参数越多训练当然越慢。\n针对这个问题，怎么解决呢？使用哈希。\n我举个简单的例子，不一定准确，“我/爱/中国/共产党”，我在更新的时候，把’我’,‘爱’,‘中国’,'共产党’我们都使用同一个参数来代表（这种情况很难遇见，理解一下就好），那么在更新训练参数的时候，我只需要更新一个参数就把这个四个词都更新了，当然会快一点。\n但是会出现一个问题，就是精度的问题。这个过程，不知道大家有咩有想到和albert很类似。哈希这个过程我自己感觉有点共享参数的意思。\n11. Fasttext解读-获取词向量\n这个文章主要是谈一下Enriching Word Vectors with Subword Information 这个论文。\n有了上一个文章的打底，（上一个文章点击这里）这个论文理解起来就比较简单，所以我写的也比较短。\n对于这个论文，我先给出它最核心的部分： 使用负采样的skip-gram的基础上，将每个中心词视为子词的集合，并学习子词的词向量。\n这句话涉及到的一个最关键的部分就是子词subword，也是这个论文的核心。\n举个例子，现在我们的中心词是&quot;where&quot;，设定子词大小为3，那么子词集合分为两个部分，注意是两个部分。\n第一部分形如这样：“&lt;wh”，“whe”，“her”，“ere”，“re&gt;”，第二部分就是特殊子词，也就是整词“”。\n那么对应到模型是，原来我的输入是“where”的词向量，现在在Fasttext就是所有子词的词向量的和。\n注意哦，这里是所有子词，是包含特殊子词，也就是整词的。\n对于背景词，直接使用整词就可以。\n简单来说，就是输出层使用子词（普通子词加上整词），输出层使用整词。\n如果遇到了OOV怎么办？使用普通子词的向量和来表示就可以。\n其实这里的子词，在名字上和上一个文章的ngram很类似，不过，这里使用的是就char的n-gram，缓解的问题并不是语序，而是利用了词序形态的规律。\n对应到中文，其实就是偏旁部首。 我记得阿里好像有发一个关于fasttext的中文版本，训练的就是偏旁部首。大家有兴趣可以去看一看。\n写完了，我对两个文章做个小总结，顺便对文章开头的问题做个回答: fasttext 训练词向量的时候一般是使用Skip-gram模型的变种。在用作文本分类的时候，一般是使用CBOW的变种。\n在这里，我想要强调一下，上一段我说的是一般情况，是为了方便大家了解，并不代表说CBOW架构不能训练词向量，skip-gram不能用作文本分类，需要注意这一点哦。\n12. Glove细节详解解读\n本文大概需要阅读 4.75 分钟 先问大家两个问题，看能不能解答\n\nGlove 中词向量的表达是使用的中心词向量还是背景词向量还是有其他方法？\n能不能分别用一句话概括出Glove和Fasttext 的核心要点？\n\n先来谈Glove。中文全称 Global Vectors for Word Representation。它做的事情概括出来就是：基于全局语料，获得词频统计，学习词语表征。\n我们从语料之中，学习到X共现词频矩阵，词频矩阵中的每个元素$x_{ij}$，代表的是词\n$x_{j}$出现在$x_{i}$的环境中的次数。注意，对于共现词频矩阵来说，它是一个对称矩阵。\n这一点非常的重要，也很容易理解，词A出现在词B周围的次数肯定是等价于词B出现在词A周围的次数的。\n类比于Word2vec，对于词$x_{i}$，就是中心词，对于词$x_{j}$也就是背景词。\n理论上，一个词作为中心词向量和一个词作为背景学到的两种向量应该是完全相同的。\n但是现实中，由于我们初始化的不同，所以我们最终学习到的两种词向量是有些许不同。\n为了增加模型的鲁棒性，在Glove中，使用两个词向量的和作为我们一个词的词向量的表达。\n这一点是区别于Word2vec，对于Word2vec，中心词向量和背景词向量是不等价的，我们一般使用中心词向量代表一个词最终的语义表达。\nGlove 论文中的推导过程其实不是很严谨，大致流程就是从大语料中发现了一个规律，即条件概率的比值可以比较直观的表达出词与词之间的关系。\n随后可以构建词向量函数去拟合条件概率的比值。基于此一步步的进行延伸推导，在我看了就是在基于一些假设，寻找出一种可能性的存在。\n在这里就不细说，直接给出Glove的损失函数：\n$\\sum_{i \\in V} \\sum_{j \\in V} h(x_{ij})(u_{j}^Tv_{i}+b_{i}+b_{j}-log(x_{ij}))^2$\n详细讲一下这个损失函数，它是一个平方损失函数，很值得琢磨。\n我把它分为了三个部分，权重函数h(x),词向量表达是$u_{j}^Tv_{i}+b_{i}+b_{j}$，共现词频的对数 $log(x_{ij})$\n从这里，我插一句我的理解，就是GLove基于的是无标签的语料，属于无监督的训练过程，但是从损失函数看，我觉得可以认为它是一个有监督的过程。\n标签就是$log(x_{ij})$，这个是我们从语料之中计算出来的，换句话说，在模型训练之前，我们可以对语料的计算，得到相应的标签数据，所以我自己认为这个可以看做是一个有监督的过程。\n我们使用一句话去描述这个损失函数可以这么去说：随着模型不停的优化，词向量的表达在不断的拟合共现词频的对数。\nh(x)是权重函数，代表的含义是表达一个词对的重要性，在值域[0,1]上单调递增。直观上理解就是一对词语共现的词频越多，那么它的重要性也就越大。\n论文中给出的函数是这样的，在x&lt;c(比如c=100)的情况下，h(x)=(x/c)^\\alpha (\\alpha=0.75)，在其余情况下，h(x)=1。也就是重要度不能无限增大，给了一个上限。\n我们看这个损失函数，有一个很有意思的点，就是h(0)=0。想一下，这个代表着什么？ 也就是说，当我们一对词的共现词频为0的时候，损失函数是为0，换句话讲，我们的损失函数的复杂度是和共现词频矩阵中的非零元素是线性关系。\n所以在训练的时候，我们只需要对非零元素采样，使用随机梯度就可以对词向量和两个偏置项进行学习更新。\n这里还有一个细节点，需要注意，偏置项是不可以省略的。为什么呢？因为如果去掉，在公式推导中的对称性假设就不满足，感兴趣的同学可以自己推导一系。\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/词向量/词向量/"},{"title":"","date":"2024-06-21T03:20:50.643Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:50.643Z","content":"OpenNMT-py: Open-Source Neural Machine Translation\n\n\nThis is a Pytorch\nport of OpenNMT,\nan open-source (MIT) neural machine translation system. It is designed to be research friendly to try out new ideas in translation, summary, image-to-text, morphology, and many other domains. Some companies have proven the code to be production ready.\nWe love contributions. Please consult the Issues page for any Contributions Welcome tagged post.\n\nBefore raising an issue, make sure you read the requirements and the documentation examples.\nUnless there is a bug, please use the Forum or Gitter to ask questions.\nTable of Contents\n\nFull Documentation\nRequirements\nFeatures\nQuickstart\nRun on FloydHub\nAcknowledgements\nCitation\n\nRequirements\nInstall OpenNMT-py from pip:\n1pip install OpenNMT-py\nor from the sources:\n123git clone https://github.com/OpenNMT/OpenNMT-py.gitcd OpenNMT-pypython setup.py install\nNote: If you have MemoryError in the install try to use pip with --no-cache-dir.\n(Optionnal) some advanced features (e.g. working audio, image or pretrained models) requires extra packages, you can install it with:\n1pip install -r requirements.opt.txt\nNote:\n\nsome features require Python 3.5 and after (eg: Distributed multigpu, entmax)\nwe currently only support PyTorch 1.2 (should work with 1.1)\n\nFeatures\n\nSeq2Seq models (encoder-decoder) with multiple RNN cells (lstm/gru) and attention (dotprod/mlp) types\nTransformer models\nCopy and Coverage Attention\nPretrained Embeddings\nSource word features\nImage-to-text processing\nSpeech-to-text processing\nTensorBoard logging\nMulti-GPU training\nData preprocessing\nInference (translation) with batching and beam search\nInference time loss functions.\n[Conv2Conv convolution model]\nSRU “RNNs faster than CNN” paper\nMixed-precision training with APEX, optimized on Tensor Cores\n\nQuickstart\nFull Documentation\nStep 1: Preprocess the data\n1onmt_preprocess -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\nWe will be working with some example data in data/ folder.\nThe data consists of parallel source (src) and target (tgt) data containing one sentence per line with tokens separated by a space:\n\nsrc-train.txt\ntgt-train.txt\nsrc-val.txt\ntgt-val.txt\n\nValidation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.\nAfter running the preprocessing, the following files are generated:\n\ndemo.train.pt: serialized PyTorch file containing training data\ndemo.valid.pt: serialized PyTorch file containing validation data\ndemo.vocab.pt: serialized PyTorch file containing vocabulary data\n\nInternally the system never touches the words themselves, but uses these indices.\nStep 2: Train the model\n1onmt_train -data data/demo -save_model demo-model\nThe main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder.\nIf you want to train on GPU, you need to set, as an example:\nCUDA_VISIBLE_DEVICES=1,3\n-world_size 2 -gpu_ranks 0 1 to use (say) GPU 1 and 3 on this node only.\nTo know more about distributed training on single or multi nodes, read the FAQ section.\nStep 3: Translate\n1onmt_translate -model demo-model_acc_XX.XX_ppl_XXX.XX_eX.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose\nNow you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into pred.txt.\n!!! note “Note”\nThe predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for translation or summarization.\nAlternative: Run on FloydHub\n\nClick this button to open a Workspace on FloydHub for training/testing your code.\nPretrained embeddings (e.g. GloVe)\nPlease see the FAQ: How to use GloVe pre-trained embeddings in OpenNMT-py\nPretrained Models\nThe following pretrained models can be downloaded and used with translate.py.\nhttp://opennmt.net/Models-py/\nAcknowledgements\nOpenNMT-py is run as a collaborative open-source project.\nThe original code was written by Adam Lerer (NYC) to reproduce OpenNMT-Lua using Pytorch.\nMajor contributors are:\nSasha Rush (Cambridge, MA)\nVincent Nguyen (Ubiqus)\nBen Peters (Lisbon)\nSebastian Gehrmann (Harvard NLP)\nYuntian Deng (Harvard NLP)\nGuillaume Klein (Systran)\nPaul Tardy (Ubiqus / Lium)\nFrançois Hernandez (Ubiqus)\nJianyu Zhan (Shanghai)\n[Dylan Flaute](http://github.com/flauted (University of Dayton)\nand more !\nOpentNMT-py belongs to the OpenNMT project along with OpenNMT-Lua and OpenNMT-tf.\nCitation\nOpenNMT: Neural Machine Translation Toolkit\nOpenNMT technical report\n123456789101112@inproceedings&#123;opennmt,  author    = &#123;Guillaume Klein and               Yoon Kim and               Yuntian Deng and               Jean Senellart and               Alexander M. Rush&#125;,  title     = &#123;Open&#123;NMT&#125;: Open-Source Toolkit for Neural Machine Translation&#125;,  booktitle = &#123;Proc. ACL&#125;,  year      = &#123;2017&#125;,  url       = &#123;https://doi.org/10.18653/v1/P17-4012&#125;,  doi       = &#123;10.18653/v1/P17-4012&#125;&#125;\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/README_old/"},{"title":"","date":"2024-06-21T03:48:22.385Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:22.385Z","content":"Library\nFor this example, we will assume that we have run preprocess to\ncreate our datasets. For instance\n\nonmt_preprocess -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/data -src_vocab_size 10000 -tgt_vocab_size 10000\n\n1234567import torchimport torch.nn as nnimport onmtimport onmt.inputtersimport onmt.modulesimport onmt.utils\nWe begin by loading in the vocabulary for the model of interest. This will let us check vocab size and to get the special ids for padding.\n123456789vocab_fields = torch.load(&quot;data/data.vocab.pt&quot;)src_text_field = vocab_fields[&quot;src&quot;].base_fieldsrc_vocab = src_text_field.vocabsrc_padding = src_vocab.stoi[src_text_field.pad_token]tgt_text_field = vocab_fields[&#x27;tgt&#x27;].base_fieldtgt_vocab = tgt_text_field.vocabtgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]\nNext we specify the core model itself. Here we will build a small model with an encoder and an attention based input feeding decoder. Both models will be RNNs and the encoder will be bidirectional\n1234567891011121314151617181920212223242526272829emb_size = 100rnn_size = 500# Specify the core model.encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),                                             word_padding_idx=src_padding)encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,                                   rnn_type=&quot;LSTM&quot;, bidirectional=True,                                   embeddings=encoder_embeddings)decoder_embeddings = onmt.modules.Embeddings(emb_size, len(tgt_vocab),                                             word_padding_idx=tgt_padding)decoder = onmt.decoders.decoder.InputFeedRNNDecoder(    hidden_size=rnn_size, num_layers=1, bidirectional_encoder=True,     rnn_type=&quot;LSTM&quot;, embeddings=decoder_embeddings)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;model = onmt.models.model.NMTModel(encoder, decoder)model.to(device)# Specify the tgt word generator and loss computation modulemodel.generator = nn.Sequential(    nn.Linear(rnn_size, len(tgt_vocab)),    nn.LogSoftmax(dim=-1)).to(device)loss = onmt.utils.loss.NMTLossCompute(    criterion=nn.NLLLoss(ignore_index=tgt_padding, reduction=&quot;sum&quot;),    generator=model.generator)\nNow we set up the optimizer. Our wrapper around a core torch optim class handles learning rate updates and gradient normalization automatically.\n1234lr = 1torch_optimizer = torch.optim.SGD(model.parameters(), lr=lr)optim = onmt.utils.optimizers.Optimizer(    torch_optimizer, learning_rate=lr, max_grad_norm=2)\nNow we load the data from disk with the associated vocab fields. To iterate through the data itself we use a wrapper around a torchtext iterator class. We specify one for both the training and test data.\n123456789101112131415161718192021# Load some datafrom itertools import chaintrain_data_file = &quot;data/data.train.0.pt&quot;valid_data_file = &quot;data/data.valid.0.pt&quot;train_iter = onmt.inputters.inputter.DatasetLazyIter(dataset_paths=[train_data_file],                                                     fields=vocab_fields,                                                     batch_size=50,                                                     batch_size_multiple=1,                                                     batch_size_fn=None,                                                     device=device,                                                     is_train=True,                                                     repeat=True)valid_iter = onmt.inputters.inputter.DatasetLazyIter(dataset_paths=[valid_data_file],                                                     fields=vocab_fields,                                                     batch_size=10,                                                     batch_size_multiple=1,                                                     batch_size_fn=None,                                                     device=device,                                                     is_train=False,                                                     repeat=False)\nFinally we train. Keeping track of the output requires a report manager.\n1234567891011report_manager = onmt.utils.ReportMgr(    report_every=50, start_time=None, tensorboard_writer=None)trainer = onmt.Trainer(model=model,                       train_loss=loss,                       valid_loss=loss,                       optim=optim,                       report_manager=report_manager)trainer.train(train_iter=train_iter,              train_steps=400,              valid_iter=valid_iter,              valid_steps=200)\n1234567891011121314151617[2019-02-15 16:34:17,475 INFO] Start training loop and validate every 200 steps...[2019-02-15 16:34:17,601 INFO] Loading dataset from data/data.train.0.pt, number of examples: 10000[2019-02-15 16:35:43,873 INFO] Step 50/  400; acc:  11.54; ppl: 1714.07; xent: 7.45; lr: 1.00000; 662/656 tok/s;     86 sec[2019-02-15 16:37:05,965 INFO] Step 100/  400; acc:  13.75; ppl: 534.80; xent: 6.28; lr: 1.00000; 675/671 tok/s;    168 sec[2019-02-15 16:38:31,289 INFO] Step 150/  400; acc:  15.02; ppl: 439.96; xent: 6.09; lr: 1.00000; 675/668 tok/s;    254 sec[2019-02-15 16:39:56,715 INFO] Step 200/  400; acc:  16.08; ppl: 357.62; xent: 5.88; lr: 1.00000; 642/647 tok/s;    339 sec[2019-02-15 16:39:56,811 INFO] Loading dataset from data/data.valid.0.pt, number of examples: 3000[2019-02-15 16:41:13,415 INFO] Validation perplexity: 208.73[2019-02-15 16:41:13,415 INFO] Validation accuracy: 23.3507[2019-02-15 16:41:13,567 INFO] Loading dataset from data/data.train.0.pt, number of examples: 10000[2019-02-15 16:42:41,562 INFO] Step 250/  400; acc:  17.07; ppl: 310.41; xent: 5.74; lr: 1.00000; 347/344 tok/s;    504 sec[2019-02-15 16:44:04,899 INFO] Step 300/  400; acc:  19.17; ppl: 262.81; xent: 5.57; lr: 1.00000; 665/661 tok/s;    587 sec[2019-02-15 16:45:33,653 INFO] Step 350/  400; acc:  19.38; ppl: 244.81; xent: 5.50; lr: 1.00000; 649/642 tok/s;    676 sec[2019-02-15 16:47:06,141 INFO] Step 400/  400; acc:  20.44; ppl: 214.75; xent: 5.37; lr: 1.00000; 593/598 tok/s;    769 sec[2019-02-15 16:47:06,265 INFO] Loading dataset from data/data.valid.0.pt, number of examples: 3000[2019-02-15 16:48:27,328 INFO] Validation perplexity: 150.277[2019-02-15 16:48:27,328 INFO] Validation accuracy: 24.2132\nTo use the model, we need to load up the translation functions. A Translator object requires the vocab fields, readers for source and target and a global scorer.\n1234567891011121314151617181920212223242526import onmt.translatesrc_reader = onmt.inputters.str2reader[&quot;text&quot;]tgt_reader = onmt.inputters.str2reader[&quot;text&quot;]scorer = onmt.translate.GNMTGlobalScorer(alpha=0.7,                                          beta=0.,                                          length_penalty=&quot;avg&quot;,                                          coverage_penalty=&quot;none&quot;)gpu = 0 if torch.cuda.is_available() else -1translator = onmt.translate.Translator(model=model,                                        fields=vocab_fields,                                        src_reader=src_reader,                                        tgt_reader=tgt_reader,                                        global_scorer=scorer,                                       gpu=gpu)builder = onmt.translate.TranslationBuilder(data=torch.load(valid_data_file),                                             fields=vocab_fields)for batch in valid_iter:    trans_batch = translator.translate_batch(        batch=batch, src_vocabs=[src_vocab],        attn_debug=False)    translations = builder.from_batch(trans_batch)    for trans in translations:        print(trans.log(0))\n[2019-02-15 16:48:27,419 INFO] Loading dataset from data/data.valid.0.pt, number of examples: 3000\n\n\nSENT 0: ['Parliament', 'Does', 'Not', 'Support', 'Amendment', 'Freeing', 'Tymoshenko']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.0983\n\n\nSENT 0: ['Today', ',', 'the', 'Ukraine', 'parliament', 'dismissed', ',', 'within', 'the', 'Code', 'of', 'Criminal', 'Procedure', 'amendment', ',', 'the', 'motion', 'to', 'revoke', 'an', 'article', 'based', 'on', 'which', 'the', 'opposition', 'leader', ',', 'Yulia', 'Tymoshenko', ',', 'was', 'sentenced', '.']\nPRED 0: &lt;unk&gt; ist das &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5950\n\n\nSENT 0: ['The', 'amendment', 'that', 'would', 'lead', 'to', 'freeing', 'the', 'imprisoned', 'former', 'Prime', 'Minister', 'was', 'revoked', 'during', 'second', 'reading', 'of', 'the', 'proposal', 'for', 'mitigation', 'of', 'sentences', 'for', 'economic', 'offences', '.']\nPRED 0: Es gibt es das &lt;unk&gt; der &lt;unk&gt; für &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5128\n\n\nSENT 0: ['In', 'October', ',', 'Tymoshenko', 'was', 'sentenced', 'to', 'seven', 'years', 'in', 'prison', 'for', 'entering', 'into', 'what', 'was', 'reported', 'to', 'be', 'a', 'disadvantageous', 'gas', 'deal', 'with', 'Russia', '.']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5578\n\n\nSENT 0: ['The', 'verdict', 'is', 'not', 'yet', 'final;', 'the', 'court', 'will', 'hear', 'Tymoshenko', '&amp;apos;s', 'appeal', 'in', 'December', '.']\nPRED 0: &lt;unk&gt; ist nicht &lt;unk&gt; .\nPRED SCORE: -0.9623\n\n\nSENT 0: ['Tymoshenko', 'claims', 'the', 'verdict', 'is', 'a', 'political', 'revenge', 'of', 'the', 'regime;', 'in', 'the', 'West', ',', 'the', 'trial', 'has', 'also', 'evoked', 'suspicion', 'of', 'being', 'biased', '.']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -0.8703\n\n\nSENT 0: ['The', 'proposal', 'to', 'remove', 'Article', '365', 'from', 'the', 'Code', 'of', 'Criminal', 'Procedure', ',', 'upon', 'which', 'the', 'former', 'Prime', 'Minister', 'was', 'sentenced', ',', 'was', 'supported', 'by', '147', 'members', 'of', 'parliament', '.']\nPRED 0: &lt;unk&gt; Sie sich mit &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.4778\n\n\nSENT 0: ['Its', 'ratification', 'would', 'require', '226', 'votes', '.']\nPRED 0: &lt;unk&gt; Sie sich &lt;unk&gt; .\nPRED SCORE: -1.3341\n\n\nSENT 0: ['Libya', '&amp;apos;s', 'Victory']\nPRED 0: &lt;unk&gt; Sie die &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5192\n\n\nSENT 0: ['The', 'story', 'of', 'Libya', '&amp;apos;s', 'liberation', ',', 'or', 'rebellion', ',', 'already', 'has', 'its', 'defeated', '.']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.2772\n\n...\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/Library/"},{"title":"","date":"2024-06-21T03:48:23.135Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:23.135Z","content":"Summarization\nNote: The process and results below are presented in our paper Bottom-Up Abstractive Summarization. Please consider citing it if you follow these instructions.\n1234567@inproceedings&#123;gehrmann2018bottom,  title=&#123;Bottom-Up Abstractive Summarization&#125;,  author=&#123;Gehrmann, Sebastian and Deng, Yuntian and Rush, Alexander&#125;,  booktitle=&#123;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing&#125;,  pages=&#123;4098--4109&#125;,  year=&#123;2018&#125;&#125;\nThis document describes how to replicate summarization experiments on the CNN-DM and gigaword datasets using OpenNMT-py.\nIn the following, we assume access to a tokenized form of the corpus split into train/valid/test set. You can find the data here.\nAn example article-title pair from Gigaword should look like this:\nInput\naustralia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .\nOutput\naustralian current account deficit narrows sharply\nPreprocessing the data\nSince we are using copy-attention [1] in the model, we need to preprocess the dataset such that source and target are aligned and use the same dictionary. This is achieved by using the options dynamic_dict and share_vocab.\nWe additionally turn off truncation of the source to ensure that inputs longer than 50 words are not truncated.\nFor CNN-DM we follow See et al. [2] and additionally truncate the source length at 400 tokens and the target at 100. We also note that in CNN-DM, we found models to work better if the target surrounds sentences with tags such that a sentence looks like &lt;t&gt; w1 w2 w3 . &lt;/t&gt;. If you use this formatting, you can remove the tags after the inference step with the commands sed -i 's/ &lt;\\/t&gt;//g' FILE.txt and sed -i 's/&lt;t&gt; //g' FILE.txt.\nCommand used:\n(1) CNN-DM\n123456789101112onmt_preprocess -train_src data/cnndm/train.txt.src \\                -train_tgt data/cnndm/train.txt.tgt.tagged \\                -valid_src data/cnndm/val.txt.src \\                -valid_tgt data/cnndm/val.txt.tgt.tagged \\                -save_data data/cnndm/CNNDM \\                -src_seq_length 10000 \\                -tgt_seq_length 10000 \\                -src_seq_length_trunc 400 \\                -tgt_seq_length_trunc 100 \\                -dynamic_dict \\                -share_vocab \\                -shard_size 100000\n(2) Gigaword\n123456789onmt_preprocess -train_src data/giga/train.article.txt \\                -train_tgt data/giga/train.title.txt \\                -valid_src data/giga/valid.article.txt \\                -valid_tgt data/giga/valid.title.txt \\                -save_data data/giga/GIGA \\                -src_seq_length 10000 \\                -dynamic_dict \\                -share_vocab \\                -shard_size 100000\nTraining\nThe training procedure described in this section for the most part follows parameter choices and implementation similar to that of See et al. [2]. We describe notable options in the following list:\n\ncopy_attn: This is the most important option, since it allows the model to copy words from the source.\nglobal_attention mlp: This makes the model use the  attention mechanism introduced by Bahdanau et al. [3] instead of that by Luong et al. [4] (global_attention dot).\nshare_embeddings: This shares the word embeddings between encoder and decoder. This option drastically decreases the number of parameters a model has to learn. We did not find this option to helpful, but you can try it out by adding it to the command below.\nreuse_copy_attn: This option reuses the standard attention as copy attention. Without this, the model learns an additional attention that is only used for copying.\ncopy_loss_by_seqlength: This modifies the loss to divide the loss of a sequence by the number of tokens in it. In practice, we found this to generate longer sequences during inference. However, this effect can also be achieved by using penalties during decoding.\nbridge: This is an additional layer that uses the final hidden state of the encoder as input and computes an initial hidden state for the decoder. Without this, the decoder is initialized with the final hidden state of the encoder directly.\noptim adagrad: Adagrad outperforms SGD when coupled with the following option.\nadagrad_accumulator_init 0.1: PyTorch does not initialize the accumulator in adagrad with any values. To match the optimization algorithm with the Tensorflow version, this option needs to be added.\n\nWe are using using a 128-dimensional word-embedding, and 512-dimensional 1 layer LSTM. On the encoder side, we use a bidirectional LSTM (brnn), which means that the 512 dimensions are split into 256 dimensions per direction.\nWe additionally set the maximum norm of the gradient to 2, and renormalize if the gradient norm exceeds this value and do not use any dropout.\ncommands used:\n(1) CNN-DM\n12345678910111213141516171819202122onmt_train -save_model models/cnndm \\           -data data/cnndm/CNNDM \\           -copy_attn \\           -global_attention mlp \\           -word_vec_size 128 \\           -rnn_size 512 \\           -layers 1 \\           -encoder_type brnn \\           -train_steps 200000 \\           -max_grad_norm 2 \\           -dropout 0. \\           -batch_size 16 \\           -valid_batch_size 16 \\           -optim adagrad \\           -learning_rate 0.15 \\           -adagrad_accumulator_init 0.1 \\           -reuse_copy_attn \\           -copy_loss_by_seqlength \\           -bridge \\           -seed 777 \\           -world_size 2 \\           -gpu_ranks 0 1\n(2) CNN-DM Transformer\nThe following script trains the transformer model on CNN-DM\n12345678910111213141516171819202122232425262728onmt_train -data data/cnndm/CNNDM \\           -save_model models/cnndm \\           -layers 4 \\           -rnn_size 512 \\           -word_vec_size 512 \\           -max_grad_norm 0 \\           -optim adam \\           -encoder_type transformer \\           -decoder_type transformer \\           -position_encoding \\           -dropout 0\\.2 \\           -param_init 0 \\           -warmup_steps 8000 \\           -learning_rate 2 \\           -decay_method noam \\           -label_smoothing 0.1 \\           -adam_beta2 0.998 \\           -batch_size 4096 \\           -batch_type tokens \\           -normalization tokens \\           -max_generator_batches 2 \\           -train_steps 200000 \\           -accum_count 4 \\           -share_embeddings \\           -copy_attn \\           -param_init_glorot \\           -world_size 2 \\           -gpu_ranks 0 1\n(3) Gigaword\nGigaword can be trained equivalently. As a baseline, we show a model trained with the following command:\n12345onmt_train -data data/giga/GIGA \\           -save_model models/giga \\           -copy_attn \\           -reuse_copy_attn \\           -train_steps 200000\nInference\nDuring inference, we use beam-search with a beam-size of 10. We also added specific penalties that we can use during decoding, described in the following.\n\nstepwise_penalty: Applies penalty at every step\ncoverage_penalty summary: Uses a penalty that prevents repeated attention to the same source word\nbeta 5: Parameter for the Coverage Penalty\nlength_penalty wu: Uses the Length Penalty by Wu et al.\nalpha 0.8: Parameter for the Length Penalty.\nblock_ngram_repeat 3: Prevent the model from repeating trigrams.\nignore_when_blocking &quot;.&quot; &quot;&lt;/t&gt;&quot; &quot;&lt;t&gt;&quot;: Allow the model to repeat trigrams with the sentence boundary tokens.\n\ncommands used:\n(1) CNN-DM\n12345678910111213141516onmt_translate -gpu X \\               -batch_size 20 \\               -beam_size 10 \\               -model models/cnndm... \\               -src data/cnndm/test.txt.src \\               -output testout/cnndm.out \\               -min_length 35 \\               -verbose \\               -stepwise_penalty \\               -coverage_penalty summary \\               -beta 5 \\               -length_penalty wu \\               -alpha 0.9 \\               -verbose \\               -block_ngram_repeat 3 \\               -ignore_when_blocking &quot;.&quot; &quot;&lt;/t&gt;&quot; &quot;&lt;t&gt;&quot;\nEvaluation\nCNN-DM\nTo evaluate the ROUGE scores on CNN-DM, we extended the pyrouge wrapper with additional evaluations such as the amount of repeated n-grams (typically found in models with copy attention), found here. The repository includes a sub-repo called pyrouge. Make sure to clone the code with the git clone --recurse-submodules https://github.com/sebastianGehrmann/rouge-baselines command to check this out as well and follow the installation instructions on the pyrouge repository before calling this script.\nThe installation instructions can be found here. Note that on MacOS, we found that the pointer to your perl installation in line 1 of pyrouge/RELEASE-1.5.5/ROUGE-1.5.5.pl might be different from the one you have installed. A simple fix is to change this line to #!/usr/local/bin/perl -w if it fails.\nIt can be run with the following command:\n1python baseline.py -s testout/cnndm.out -t data/cnndm/test.txt.tgt.tagged -m sent_tag_verbatim -r\nThe sent_tag_verbatim option strips &lt;t&gt; and &lt;/t&gt; tags around sentences - when a sentence previously was &lt;t&gt; w w w w . &lt;/t&gt;, it becomes w w w w ..\nGigaword\nFor evaluation of large test sets such as Gigaword, we use the a parallel python wrapper around ROUGE, found here.\ncommand used:\nfiles2rouge giga.out test.title.txt --verbose\nScores and Models\nCNN-DM\n\n\nModel Type\nModel\nR1 R\nR1 P\nR1 F\nR2 R\nR2 P\nR2 F\nRL R\nRL P\nRL F\n\n\n\n\nPointer-Generator + Coverage [2]\nlink\n39.05\n43.02\n39.53\n17.16\n18.77\n17.28\n35.98\n39.56\n36.38\n\n\nPointer-Generator [2]\nlink\n37.76\n37.60\n36.44\n16.31\n16.12\n15.66\n34.66\n34.46\n33.42\n\n\nOpenNMT BRNN  (1 layer, emb 128, hid 512)\nlink\n40.90\n40.20\n39.02\n17.91\n17.99\n17.25\n37.76\n37.18\n36.05\n\n\nOpenNMT BRNN  (1 layer, emb 128, hid 512, shared embeddings)\nlink\n38.59\n40.60\n37.97\n16.75\n17.93\n16.59\n35.67\n37.60\n35.13\n\n\nOpenNMT BRNN (2 layer, emb 256, hid 1024)\nlink\n40.41\n40.94\n39.12\n17.76\n18.38\n17.35\n37.27\n37.83\n36.12\n\n\nOpenNMT Transformer\nlink\n40.31\n41.09\n39.25\n17.97\n18.46\n17.54\n37.41\n38.18\n36.45\n\n\nGigaword\n\n\nModel Type\nModel\nR1 R\nR1 P\nR1 F\nR2 R\nR2 P\nR2 F\nRL R\nRL P\nRL F\n\n\n\n\nOpenNMT, no penalties\nlink\n?\n?\n35.51\n?\n?\n17.35\n?\n?\n33.17\n\n\nReferences\n[1] Vinyals, O., Fortunato, M. and Jaitly, N., 2015. Pointer Network. NIPS\n[2] See, A., Liu, P.J. and Manning, C.D., 2017. Get To The Point: Summarization with Pointer-Generator Networks. ACL\n[3] Bahdanau, D., Cho, K. and Bengio, Y., 2014. Neural machine translation by jointly learning to align and translate. ICLR\n[4] Luong, M.T., Pham, H. and Manning, C.D., 2015. Effective approaches to attention-based neural machine translation. EMNLP\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/Summarization/"},{"title":"","date":"2024-06-21T03:48:22.125Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:22.125Z","content":"Translation\nThe example below uses the Moses tokenizer (http://www.statmt.org/moses/) to prepare the data and the moses BLEU script for evaluation. This example if for training for the WMT’16 Multimodal Translation task (http://www.statmt.org/wmt16/multimodal-task.html).\nStep 0. Download the data.\n1234mkdir -p data/multi30kwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz &amp;&amp;  tar -xf training.tar.gz -C data/multi30k &amp;&amp; rm training.tar.gzwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz &amp;&amp; tar -xf validation.tar.gz -C data/multi30k &amp;&amp; rm validation.tar.gzwget http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_test2016.tar.gz &amp;&amp; tar -xf mmt_task1_test2016.tar.gz -C data/multi30k &amp;&amp; rm mmt_task1_test2016.tar.gz\nStep 1. Preprocess the data.\n123for l in en de; do for f in data/multi30k/*.$l; do if [[ &quot;$f&quot; != *&quot;test&quot;* ]]; then sed -i &quot;$ d&quot; $f; fi;  done; donefor l in en de; do for f in data/multi30k/*.$l; do perl tools/tokenizer.perl -a -no-escape -l $l -q  &lt; $f &gt; $f.atok; done; doneonmt_preprocess -train_src data/multi30k/train.en.atok -train_tgt data/multi30k/train.de.atok -valid_src data/multi30k/val.en.atok -valid_tgt data/multi30k/val.de.atok -save_data data/multi30k.atok.low -lower\nStep 2. Train the model.\n1onmt_train -data data/multi30k.atok.low -save_model multi30k_model -gpu_ranks 0\nStep 3. Translate sentences.\n1onmt_translate -gpu 0 -model multi30k_model_*_e13.pt -src data/multi30k/test2016.en.atok -tgt data/multi30k/test2016.de.atok -replace_unk -verbose -output multi30k.test.pred.atok\nAnd evaluate\n1perl tools/multi-bleu.perl data/multi30k/test2016.de.atok &lt; multi30k.test.pred.atok\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/extended/"},{"title":"","date":"2024-06-21T03:48:22.175Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:22.175Z","content":"Image to Text\nA deep learning-based approach to learning the image-to-text conversion, built on top of the OpenNMT system. It is completely data-driven, hence can be used for a variety of image-to-text problems, such as image captioning, optical character recognition and LaTeX decompilation.\nTake LaTeX decompilation as an example, given a formula image:\n\nThe goal is to infer the LaTeX source that can be compiled to such an image:\n1d s _ &#123; 1 1 &#125; ^ &#123; 2 &#125; = d x ^ &#123; + &#125; d x ^ &#123; - &#125; + l _ &#123; p &#125; ^ &#123; 9 &#125; \\frac &#123; p _ &#123; - &#125; &#125; &#123; r ^ &#123; 7 &#125; &#125; \\delta ( x ^ &#123; - &#125; ) d x ^ &#123; - &#125; d x ^ &#123; - &#125; + d x _ &#123; 1 &#125; ^ &#123; 2 &#125; + \\; \\cdots \\; + d x _ &#123; 9 &#125; ^ &#123; 2 &#125; \nThe paper [What You Get Is What You See: A Visual Markup Decompiler] provides more technical details of this model.\nDependencies\n\ntorchvision: conda install torchvision\nPillow: pip install Pillow\n\nQuick Start\nTo get started, we provide a toy Math-to-LaTex example. We assume that the working directory is OpenNMT-py throughout this document.\nIm2Text consists of four commands:\n\nDownload the data.\n\n1wget -O data/im2text.tgz http://lstm.seas.harvard.edu/latex/im2text_small.tgz; tar zxf data/im2text.tgz -C data/\n\nPreprocess the data.\n\n123456789onmt_preprocess -data_type img \\                -src_dir da](../images/ \\                -train_src data/im2text/src-train.txt \\                -train_tgt data/im2text/tgt-train.txt -valid_src data/im2text/src-val.txt \\                -valid_tgt data/im2text/tgt-val.txt -save_data data/im2text/demo \\                -tgt_seq_length 150 \\                -tgt_words_min_frequency 2 \\                -shard_size 500 \\                -image_channel_size 1\n\nTrain the model.\n\n12345678910onmt_train -model_type img \\           -data data/im2text/demo \\           -save_model demo-model \\           -gpu_ranks 0 \\           -batch_size 20 \\           -max_grad_norm 20 \\           -learning_rate 0.1 \\           -word_vec_size 80 \\           -encoder_type brnn \\           -image_channel_size 1\n\nTranslate the images.\n\n123456789onmt_translate -data_type img \\               -model demo-model_acc_x_ppl_x_e13.pt \\               -src_dir da](../images \\               -src data/im2text/src-test.txt \\               -output pred.txt \\               -max_length 150 \\               -beam_size 5 \\               -gpu 0 \\               -verbose\nThe above dataset is sampled from the im2latex-100k-dataset. We provide a trained model [link] on this dataset.\nOptions\n\n\n-src_dir: The directory containing the images.\n\n\n-train_tgt: The file storing the tokenized labels, one label per line. It shall look like:\n\n\n1234&lt;label0_token0&gt; &lt;label0_token1&gt; ... &lt;label0_tokenN0&gt;&lt;label1_token0&gt; &lt;label1_token1&gt; ... &lt;label1_tokenN1&gt;&lt;label2_token0&gt; &lt;label2_token1&gt; ... &lt;label2_tokenN2&gt;...\n\n-train_src: The file storing the paths of the images (relative to src_dir).\n\n1234&lt;image0_path&gt;&lt;image1_path&gt;&lt;image2_path&gt;...\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/im2text/"},{"title":"","date":"2024-06-21T03:48:22.215Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:22.215Z","content":"… toctree::\n:maxdepth: 2\nindex.md\nquickstart.md\nextended.md\nThis portal provides a detailled documentation of the OpenNMT toolkit. It describes how to use the PyTorch project and how it works.\nInstallation\n1. Install PyTorch\n2. Clone the OpenNMT-py repository:\n12git clone https://github.com/OpenNMT/OpenNMT-pycd OpenNMT-py\n3. Install required libraries\n1pip install -r requirements.txt\nAnd you are ready to go! Take a look at the quickstart to familiarize yourself with the main training workflow.\nAlternatively you can use Docker to install with nvidia-docker. The main Dockerfile is included\nin the root directory.\nCitation\nWhen using OpenNMT for research please cite our\nOpenNMT technical report\n123456789101112@inproceedings&#123;opennmt,  author    = &#123;Guillaume Klein and               Yoon Kim and               Yuntian Deng and               Jean Senellart and               Alexander M. Rush&#125;,  title     = &#123;OpenNMT: Open-Source Toolkit for Neural Machine Translation&#125;,  booktitle = &#123;Proc. ACL&#125;,  year      = &#123;2017&#125;,  url       = &#123;https://doi.org/10.18653/v1/P17-4012&#125;,  doi       = &#123;10.18653/v1/P17-4012&#125;&#125;\nAdditional resources\nYou can find additional help or tutorials in the following resources:\n\nGitter channel\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/"},{"title":"","date":"2024-06-21T03:48:22.475Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:22.475Z","content":"Overview\nThis portal provides a detailed documentation of the OpenNMT toolkit. It describes how to use the PyTorch project and how it works.\nInstallation\nInstall from pip:\nInstall OpenNMT-py from pip:\n1pip install OpenNMT-py\nor from the sources:\n123git clone https://github.com/OpenNMT/OpenNMT-py.gitcd OpenNMT-pypython setup.py install\n(Optionnal) some advanced features (e.g. working audio, image or pretrained models) requires extra packages, you can install it with:\n1pip install -r requirements.opt.txt\nAnd you are ready to go! Take a look at the quickstart to familiarize yourself with the main training workflow.\nAlternatively you can use Docker to install with nvidia-docker. The main Dockerfile is included\nin the root directory.\nCitation\nWhen using OpenNMT for research please cite our\nOpenNMT technical report\n123456789101112@inproceedings&#123;opennmt,  author    = &#123;Guillaume Klein and               Yoon Kim and               Yuntian Deng and               Jean Senellart and               Alexander M. Rush&#125;,  title     = &#123;OpenNMT: Open-Source Toolkit for Neural Machine Translation&#125;,  booktitle = &#123;Proc. ACL&#125;,  year      = &#123;2017&#125;,  url       = &#123;https://doi.org/10.18653/v1/P17-4012&#125;,  doi       = &#123;10.18653/v1/P17-4012&#125;&#125;\nAdditional resources\nYou can find additional help or tutorials in the following resources:\n\n\nGitter channel\n\n\nForum\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/main/"},{"title":"","date":"2024-06-21T03:48:22.985Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:22.985Z","content":"Quickstart\nStep 1: Preprocess the data\n1onmt_preprocess -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\nWe will be working with some example data in data/ folder.\nThe data consists of parallel source (src) and target (tgt) data containing one sentence per line with tokens separated by a space:\n\nsrc-train.txt\ntgt-train.txt\nsrc-val.txt\ntgt-val.txt\n\nValidation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.\n1234$ head -n 3 data/src-train.txtIt is not acceptable that , with the help of the national bureaucracies , Parliament &amp;apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .Federal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .&amp;quot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .\nStep 2: Train the model\n1onmt_train -data data/demo -save_model demo-model\nThe main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder.\nIf you want to train on GPU, you need to set, as an example:\nCUDA_VISIBLE_DEVICES=1,3\n-world_size 2 -gpu_ranks 0 1 to use (say) GPU 1 and 3 on this node only.\nTo know more about distributed training on single or multi nodes, read the FAQ section.\nStep 3: Translate\n1onmt_translate -model demo-model_XYZ.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose\nNow you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into pred.txt.\nNote:\nThe predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for translation or summarization.\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/quickstart/"},{"title":"","date":"2024-06-21T03:48:22.155Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:22.155Z","content":"FAQ\nHow do I use Pretrained embeddings (e.g. GloVe)?\nUsing vocabularies from OpenNMT-py preprocessing outputs, embeddings_to_torch.py to generate encoder and decoder embeddings initialized with GloVe’s values.\nthe script is a slightly modified version of ylhsieh’s one2.\nUsage:\n123456embeddings_to_torch.py [-h] [-emb_file_both EMB_FILE_BOTH]                       [-emb_file_enc EMB_FILE_ENC]                       [-emb_file_dec EMB_FILE_DEC] -output_file                       OUTPUT_FILE -dict_file DICT_FILE [-verbose]                       [-skip_lines SKIP_LINES]                       [-type &#123;GloVe,word2vec&#125;]\nRun embeddings_to_torch.py -h for more usagecomplete info.\nExample\n\nget GloVe files:\n\n123mkdir &quot;glove_dir&quot;wget http://nlp.stanford.edu/data/glove.6B.zipunzip glove.6B.zip -d &quot;glove_dir&quot;\n\nprepare data:\n\n123456onmt_preprocess \\-train_src data/train.src.txt \\-train_tgt data/train.tgt.txt \\-valid_src data/valid.src.txt \\-valid_tgt data/valid.tgt.txt \\-save_data data/data\n\nprepare embeddings:\n\n123./tools/embeddings_to_torch.py -emb_file_both &quot;glove_dir/glove.6B.100d.txt&quot; \\-dict_file &quot;data/data.vocab.pt&quot; \\-output_file &quot;data/embeddings&quot;\n\ntrain using pre-trained embeddings:\n\n12345678onmt_train -save_model data/model \\           -batch_size 64 \\           -layers 2 \\           -rnn_size 200 \\           -word_vec_size 100 \\           -pre_word_vecs_enc &quot;data/embeddings.enc.pt&quot; \\           -pre_word_vecs_dec &quot;data/embeddings.dec.pt&quot; \\           -data data/data\nHow do I use the Transformer model?\nThe transformer model is very sensitive to hyperparameters. To run it\neffectively you need to set a bunch of different options that mimic the Google\nsetup. We have confirmed the following command can replicate their WMT results.\n123456789python  train.py -data /tmp/de2/data -save_model /tmp/extra \\        -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \\        -encoder_type transformer -decoder_type transformer -position_encoding \\        -train_steps 200000  -max_generator_batches 2 -dropout 0.1 \\        -batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2 \\        -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 \\        -max_grad_norm 0 -param_init 0  -param_init_glorot \\        -label_smoothing 0.1 -valid_steps 10000 -save_checkpoint_steps 10000 \\        -world_size 4 -gpu_ranks 0 1 2 3\nHere are what each of the parameters mean:\n\nparam_init_glorot -param_init 0: correct initialization of parameters\nposition_encoding: add sinusoidal position encoding to each embedding\noptim adam, decay_method noam, warmup_steps 8000: use special learning rate.\nbatch_type tokens, normalization tokens, accum_count 4: batch and normalize based on number of tokens and not sentences. Compute gradients based on four batches.\nlabel_smoothing 0.1: use label smoothing loss.\n\nDo you support multi-gpu?\nFirst you need to make sure you export CUDA_VISIBLE_DEVICES=0,1,2,3.\nIf you want to use GPU id 1 and 3 of your OS, you will need to export CUDA_VISIBLE_DEVICES=1,3\nBoth -world_size and -gpu_ranks need to be set. E.g. -world_size 4 -gpu_ranks 0 1 2 3 will use 4 GPU on this node only.\nIf you want to use 2 nodes with 2 GPU each, you need to set -master_ip and -master_port, and\n\n-world_size 4 -gpu_ranks 0 1: on the first node\n-world_size 4 -gpu_ranks 2 3: on the second node\n-accum_count 2: This will accumulate over 2 batches before updating parameters.\n\nif you use a regular network card (1 Gbps) then we suggest to use a higher -accum_count to minimize the inter-node communication.\nNote:\nWhen training on several GPUs, you can’t have them in ‘Exclusive’ compute mode (nvidia-smi -c 3).\nThe multi-gpu setup relies on a Producer/Consumer setup. This setup means there will be 2&lt;n_gpu&gt; + 1 processes spawned, with 2 processes per GPU, one for model training and one (Consumer) that hosts a Queue of batches that will be processed next. The additional process is the Producer, creating batches and sending them to the Consumers. This setup is beneficial for both wall time and memory, since it loads data shards ‘in advance’, and does not require to load it for each GPU process.\nHow can I ensemble Models at inference?\nYou can specify several models in the translate.py command line: -model model1_seed1 model2_seed2\nBear in mind that your models must share the same target vocabulary.\nHow can I weight different corpora at training?\nPreprocessing\nWe introduced -train_ids which is a list of IDs that will be given to the preprocessed shards.\nE.g. we have two corpora : parallel.en and  parallel.de + from_backtranslation.en from_backtranslation.de, we can pass the following in the preprocess.py command:\n123456...-train_src parallel.en from_backtranslation.en \\-train_tgt parallel.de from_backtranslation.de \\-train_ids A B \\-save_data my_data \\...\nand it will dump my_data.train_A.X.pt based on parallel.en//parallel.de and my_data.train_B.X.pt based on from_backtranslation.en//from_backtranslation.de.\nTraining\nWe introduced -data_ids based on the same principle as above, as well as -data_weights, which is the list of the weight each corpus should have.\nE.g.\n12345...-data my_data \\-data_ids A B \\-data_weights 1 7 \\...\nwill mean that we’ll look for my_data.train_A.*.pt and my_data.train_B.*.pt, and that when building batches, we’ll take 1 example from corpus A, then 7 examples from corpus B, and so on.\nWarning: This means that we’ll load as many shards as we have -data_ids, in order to produce batches containing data from every corpus. It may be a good idea to reduce the -shard_size at preprocessing.\nCan I get word alignment while translating?\nRaw alignments from averaging Transformer attention heads\nCurrently, we support producing word alignment while translating for Transformer based models. Using -report_align when calling translate.py will output the inferred alignments in Pharaoh format. Those alignments are computed from an argmax on the average of the attention heads of the second to last decoder layer. The resulting alignment src-tgt (Pharaoh) will be pasted to the translation sentence, separated by |||.\nNote: The second to last default behaviour was empirically determined. It is not the same as the paper (they take the penultimate layer), probably because of light differences in the architecture.\n\nalignments use the standard “Pharaoh format”, where a pair i-j indicates the ith word of source language is aligned to jth word of target language.\nExample: {‘src’: ‘das stimmt nicht !’; ‘output’: ‘that is not true ! ||| 0-0 0-1 1-2 2-3 1-4 1-5 3-6’}\nUsing the-tgt option when calling translate.py, we output alignments between the source and the gold target rather than the inferred target, assuming we’re doing evaluation.\nTo convert subword alignments to word alignments, or symetrize bidirectional alignments, please refer to the lilt scripts.\n\nSupervised learning on a specific head\nThe quality of output alignments can be further improved by providing reference alignments while training. This will invoke multi-task learning on translation and alignment. This is an implementation based on the paper Jointly Learning to Align and Translate with Transformer Models.\nThe data need to be preprocessed with the reference alignments in order to learn the supervised task.\nWhen calling preprocess.py, add:\n\n--train_align &lt;path&gt;: path(s) to the training alignments in Pharaoh format\n--valid_align &lt;path&gt;: path to the validation set alignments in Pharaoh format (optional).\nThe reference alignment file(s) could be generated by GIZA++ or fast_align.\n\nNote: There should be no blank lines in the alignment files provided.\nOptions to learn such alignments are:\n\n-lambda_align: set the value &gt; 0.0 to enable joint align training, the paper suggests 0.05;\n-alignment_layer: indicate the index of the decoder layer;\n-alignment_heads:  number of alignment heads for the alignment task - should be set to 1 for the supervised task, and preferably kept to default (or same as num_heads) for the average task;\n-full_context_alignment: do full context decoder pass (no future mask) when computing alignments. This will slow down the training (~12% in terms of tok/s) but will be beneficial to generate better alignment.\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/FAQ/"},{"title":"","date":"2024-06-21T03:20:47.153Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.153Z","content":"ESIM\n\n导入数据\n\n1.1 train_data = LCQMC_Dataset(train_file, vocab_file, max_length)\n通过LCQMC_Dataset 导入数据，在LCQMC_Dataset 中，做了这几件事情：\n1.1.1 p, h, self.label = load_sentences(LCQMC_file)\n读取数据文件，切分数据\n1.1.2 word2idx, _, _ = load_vocab(vocab_file)\n读取字典文件，从而获得 word2idx, idx2word, vocab\n1.1.3 self.p_list, self.p_lengths, self.h_list, self.h_lengths = word_index(p, h, word2idx, max_char_len)\n将数据转化为对应的数值，并且根据max_char_len进行切分或者截断\n1.1.4\nself.p_list = torch.from_numpy(self.p_list).type(torch.long)\nself.h_list = torch.from_numpy(self.h_list).type(torch.long)\n转化为对应的torch tensor\n1.2  数据batch化\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n按照batch进行读取。是torch的代码，我就不看了\n1.3 读取embedding文件\nembeddings = load_embeddings(embeddings_file)\n注意pad全为零\n\n\nmodel构建\nmodel = ESIM(hidden_size, embeddings=embeddings, dropout=dropout, num_classes=num_classes, device=device).to(device)\n\n\n为了方便，我把ESIM整体流程重点是attention的部分抽离了出来\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/文本匹配和文本相似度/src/ESIM-attention/ESIM代码解读/"},{"title":"","date":"2024-06-20T19:20:47.653Z","updated":"2024-06-20T19:20:47.653Z","content":"{\"models_root\":\"./available_models\",\"models\":[{\"id\":100,\"model\":\"model_0.pt\",\"timeout\":600,\"on_timeout\":\"to_cpu\",\"load\":true,\"opt\":{\"gpu\":0,\"beam_size\":5},\"tokenizer\":{\"type\":\"sentencepiece\",\"model\":\"wmtenfr.model\"}},{\"model\":\"model_0.light.pt\",\"timeout\":-1,\"on_timeout\":\"unload\",\"model_root\":\"../other_models\",\"opt\":{\"batch_size\":1,\"beam_size\":10}}]}"},{"title":"","date":"2024-06-21T03:48:23.095Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:23.095Z","content":"Speech to Text\nA deep learning-based approach to learning the speech-to-text conversion, built on top of the OpenNMT system.\nGiven raw audio, we first apply short-time Fourier transform (STFT), then apply Convolutional Neural Networks to get the source features. Based on this source representation, we use an LSTM decoder with attention to produce the text character by character.\nDependencies\n\ntorchaudio: sudo apt-get install -y sox libsox-dev libsox-fmt-all; pip install git+https://github.com/pytorch/audio\nlibrosa: pip install librosa\n\nQuick Start\nTo get started, we provide a toy speech-to-text example. We assume that the working directory is OpenNMT-py throughout this document.\n\nDownload the data.\n\n1wget -O data/speech.tgz http://lstm.seas.harvard.edu/latex/speech.tgz; tar zxf data/speech.tgz -C data/\n\nPreprocess the data.\n\n1onmt_preprocess -data_type audio -src_dir data/speech/an4_dataset -train_src data/speech/src-train.txt -train_tgt data/speech/tgt-train.txt -valid_src data/speech/src-val.txt -valid_tgt data/speech/tgt-val.txt -shard_size 300 -save_data data/speech/demo\n\nTrain the model.\n\n1onmt_train -model_type audio -enc_rnn_size 512 -dec_rnn_size 512 -audio_enc_pooling 1,1,2,2 -dropout 0 -enc_layers 4 -dec_layers 1 -rnn_type LSTM -data data/speech/demo -save_model demo-model -global_attention mlp -gpu_ranks 0 -batch_size 8 -optim adam -max_grad_norm 100 -learning_rate 0.0003 -learning_rate_decay 0.8 -train_steps 100000\n\nTranslate the speechs.\n\n1onmt_translate -data_type audio -model demo-model_acc_x_ppl_x_e13.pt -src_dir data/speech/an4_dataset -src data/speech/src-val.txt -output pred.txt -gpu 0 -verbose\nOptions\n\n\n-src_dir: The directory containing the audio files.\n\n\n-train_tgt: The file storing the tokenized labels, one label per line. It shall look like:\n\n\n1234&lt;label0_token0&gt; &lt;label0_token1&gt; ... &lt;label0_tokenN0&gt;&lt;label1_token0&gt; &lt;label1_token1&gt; ... &lt;label1_tokenN1&gt;&lt;label2_token0&gt; &lt;label2_token1&gt; ... &lt;label2_tokenN2&gt;...\n\n-train_src: The file storing the paths of the audio files (relative to src_dir).\n\n1234&lt;speech0_path&gt;&lt;speech1_path&gt;&lt;speech2_path&gt;...\n\nsample_rate: Sample rate. Default: 16000.\nwindow_size: Window size for spectrogram in seconds. Default: 0.02.\nwindow_stride: Window stride for spectrogram in seconds. Default: 0.01.\nwindow: Window type for spectrogram generation. Default: hamming.\n\nAcknowledgement\nOur preprocessing and CNN encoder is adapted from deepspeech.pytorch.\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/speech2text/"},{"title":"","date":"2024-06-20T19:20:47.743Z","updated":"2024-06-20T19:20:47.743Z","content":"{\"data\":\"exp/dataset.de-en\",\"save_model\":\"exp/model.de-en\",\"save_checkpoint_steps\":10000,\"keep_checkpoint\":10,\"seed\":3435,\"train_steps\":500000,\"valid_steps\":10000,\"warmup_steps\":8000,\"report_every\":100,\"decoder_type\":\"transformer\",\"encoder_type\":\"transformer\",\"word_vec_size\":512,\"rnn_size\":512,\"layers\":6,\"transformer_ff\":2048,\"heads\":8,\"accum_count\":8,\"optim\":\"adam\",\"adam_beta1\":0.9,\"adam_beta2\":0.998,\"decay_method\":\"noam\",\"learning_rate\":2,\"max_grad_norm\":0,\"batch_size\":4096,\"batch_type\":\"tokens\",\"normalization\":\"tokens\",\"dropout\":0.1,\"label_smoothing\":0.1,\"max_generator_batches\":2,\"param_init\":0,\"param_init_glorot\":\"true\",\"position_encoding\":\"true\",\"world_size\":1,\"gpu_ranks\":[0]}"},{"title":"","date":"2024-06-20T19:20:47.733Z","updated":"2024-06-20T19:20:47.733Z","content":"{\"data\":\"data/cnndm/CNNDM\",\"save_model\":\"models/cnndm\",\"save_checkpoint_steps\":10000,\"keep_checkpoint\":10,\"seed\":3435,\"train_steps\":100000,\"valid_steps\":10000,\"report_every\":100,\"encoder_type\":\"brnn\",\"word_vec_size\":128,\"rnn_size\":512,\"layers\":1,\"optim\":\"adagrad\",\"learning_rate\":0.15,\"adagrad_accumulator_init\":0.1,\"max_grad_norm\":2,\"batch_size\":16,\"dropout\":0,\"copy_attn\":\"true\",\"global_attention\":\"mlp\",\"reuse_copy_attn\":\"true\",\"bridge\":\"true\",\"world_size\":2,\"gpu_ranks\":[0,1]}"},{"title":"","date":"2024-06-20T19:20:47.763Z","updated":"2024-06-20T19:20:47.763Z","content":"{\"data\":\"exp/dataset.de-en\",\"save_model\":\"exp/model.de-en\",\"save_checkpoint_steps\":10000,\"keep_checkpoint\":10,\"seed\":3435,\"train_steps\":200000,\"valid_steps\":10000,\"warmup_steps\":8000,\"report_every\":100,\"decoder_type\":\"transformer\",\"encoder_type\":\"transformer\",\"word_vec_size\":512,\"rnn_size\":512,\"layers\":6,\"transformer_ff\":2048,\"heads\":8,\"accum_count\":2,\"optim\":\"adam\",\"adam_beta1\":0.9,\"adam_beta2\":0.998,\"decay_method\":\"noam\",\"learning_rate\":2,\"max_grad_norm\":0,\"batch_size\":4096,\"batch_type\":\"tokens\",\"normalization\":\"tokens\",\"dropout\":0.1,\"label_smoothing\":0.1,\"max_generator_batches\":2,\"param_init\":0,\"param_init_glorot\":\"true\",\"position_encoding\":\"true\",\"world_size\":4,\"gpu_ranks\":[0,1,2,3]}"},{"title":"","date":"2024-06-21T03:20:51.403Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:51.403Z","content":"This directly contains scripts and tools adopted from other open source projects such as Apache Joshua and Moses Decoder.\nTODO: credit the authors and resolve license issues (if any)\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/tools/README/"},{"title":"","date":"2024-06-21T03:20:47.943Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.943Z","content":"Contributors\nOpenNMT-py is a community developed project and we love developer contributions.\nGuidelines\nBefore sending a PR, please do this checklist first:\n\nPlease run tools/pull_request_chk.sh and fix any errors. When adding new functionality, also add tests to this script. Included checks:\n\nflake8 check for coding style;\nunittest;\ncontinuous integration tests listed in .travis.yml.\n\n\nWhen adding/modifying class constructor, please make the arguments as same naming style as its superclass in PyTorch.\nIf your change is based on a paper, please include a clear comment and reference in the code (more on that below).\n\nDocstrings\nAbove all, try to follow the Google docstring format\n(Napoleon example,\nGoogle styleguide).\nThis makes it easy to include your contributions in the Sphinx documentation. And, do feel free\nto autodoc your contributions in the API .rst files in the docs/source folder! If you do, check that\nyour additions look right.\n12345cd docs# install some dependencies if necessary:# recommonmark, sphinx_rtd_theme, sphinxcontrib-bibtexmake htmlfirefox build/html/main.html  # or your browser of choice\nSome particular advice:\n\n\nTry to follow Python 3 typing module conventions when documenting types.\n\nException: use “or” instead of unions for more readability\nFor external types, use the full “import name”. Common abbreviations (e.g. np) are acceptable.\nFor torch.Tensor types, the torch. is optional.\nPlease don’t use tics like (`str`) or rst directives like (:obj:`str`). Napoleon handles types\nvery well without additional help, so avoid the clutter.\n\n\n\nGoogle docstrings don’t support multiple returns.\nFor multiple returns, the following works well with Sphinx and is still very readable.\n12345678910111213141516def foo(a, b):    &quot;&quot;&quot;This is my docstring.        Args:        a (object): Something.        b (class): Another thing.      Returns:        (object, class):              * a: Something or rather with a long          description that spills over.        * b: And another thing.    &quot;&quot;&quot;      return a, b\n\n\nWhen citing a paper, avoid directly linking in the docstring! Add a Bibtex entry to docs/source/refs.bib.\nE.g., to cite “Attention Is All You Need”, visit arXiv, choose the\nbibtext link, search docs/source/refs.bib\nusing CTRL-F for DBLP:journals/corr/VaswaniSPUJGKP17, and if you do not find it then copy-paste the\ncitation into refs.bib. Then, in your docstring, use :cite:`DBLP:journals/corr/VaswaniSPUJGKP17` .\n\nHowever, a link is better than nothing.\n\n\n\nPlease document tensor shapes. Prefer the format\nb, c)`` ```. This style is easy to read, allows using ``x`` for multplication, and is common1234567891011121314  (PyTorch uses a few variations on the parentheses format, AllenNLP uses exactly this format, Fairseq uses  the parentheses format with single ticks).    - Again, a different style is better than no shape documentation.- Please avoid unnecessary space characters, try to capitalize, and try to punctuate.      For multi-line docstrings, add a blank line after the closing ``&quot;&quot;&quot;``.  Don&#x27;t use a blank line before the closing quotes.    ``&quot;&quot;&quot; not this &quot;&quot;&quot;`` ``&quot;&quot;&quot;This.&quot;&quot;&quot;``    ```python  &quot;&quot;&quot;      Not this.  &quot;&quot;&quot;\n1&quot;&quot;&quot;This.&quot;&quot;&quot;\nThis note is the least important. Focus on content first, but remember that consistent docs look good.\n\n\nBe sensible about the first line. Generally, one stand-alone summary line (per the Google guidelines) is good.\nSometimes, it’s better to cut directly to the args or an extended description. It’s always acceptable to have a\n“trailing” citation.\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/CONTRIBUTING/"},{"title":"","date":"2024-06-21T03:20:48.003Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.003Z","content":"FAQ\nHow do I use Pretrained embeddings (e.g. GloVe)?\nUsing vocabularies from OpenNMT-py preprocessing outputs, embeddings_to_torch.py to generate encoder and decoder embeddings initialized with GloVe’s values.\nthe script is a slightly modified version of ylhsieh’s one2.\nUsage:\n123456embeddings_to_torch.py [-h] [-emb_file_both EMB_FILE_BOTH]                       [-emb_file_enc EMB_FILE_ENC]                       [-emb_file_dec EMB_FILE_DEC] -output_file                       OUTPUT_FILE -dict_file DICT_FILE [-verbose]                       [-skip_lines SKIP_LINES]                       [-type &#123;GloVe,word2vec&#125;]\nRun embeddings_to_torch.py -h for more usagecomplete info.\nExample\n\nget GloVe files:\n\n123mkdir &quot;glove_dir&quot;wget http://nlp.stanford.edu/data/glove.6B.zipunzip glove.6B.zip -d &quot;glove_dir&quot;\n\nprepare data:\n\n123456onmt_preprocess \\-train_src data/train.src.txt \\-train_tgt data/train.tgt.txt \\-valid_src data/valid.src.txt \\-valid_tgt data/valid.tgt.txt \\-save_data data/data\n\nprepare embeddings:\n\n123./tools/embeddings_to_torch.py -emb_file_both &quot;glove_dir/glove.6B.100d.txt&quot; \\-dict_file &quot;data/data.vocab.pt&quot; \\-output_file &quot;data/embeddings&quot;\n\ntrain using pre-trained embeddings:\n\n12345678onmt_train -save_model data/model \\           -batch_size 64 \\           -layers 2 \\           -rnn_size 200 \\           -word_vec_size 100 \\           -pre_word_vecs_enc &quot;data/embeddings.enc.pt&quot; \\           -pre_word_vecs_dec &quot;data/embeddings.dec.pt&quot; \\           -data data/data\nHow do I use the Transformer model?\nThe transformer model is very sensitive to hyperparameters. To run it\neffectively you need to set a bunch of different options that mimic the Google\nsetup. We have confirmed the following command can replicate their WMT results.\n123456789python  train.py -data /tmp/de2/data -save_model /tmp/extra \\        -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \\        -encoder_type transformer -decoder_type transformer -position_encoding \\        -train_steps 200000  -max_generator_batches 2 -dropout 0.1 \\        -batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2 \\        -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 \\        -max_grad_norm 0 -param_init 0  -param_init_glorot \\        -label_smoothing 0.1 -valid_steps 10000 -save_checkpoint_steps 10000 \\        -world_size 4 -gpu_ranks 0 1 2 3\nHere are what each of the parameters mean:\n\nparam_init_glorot -param_init 0: correct initialization of parameters\nposition_encoding: add sinusoidal position encoding to each embedding\noptim adam, decay_method noam, warmup_steps 8000: use special learning rate.\nbatch_type tokens, normalization tokens, accum_count 4: batch and normalize based on number of tokens and not sentences. Compute gradients based on four batches.\nlabel_smoothing 0.1: use label smoothing loss.\n\nDo you support multi-gpu?\nFirst you need to make sure you export CUDA_VISIBLE_DEVICES=0,1,2,3.\nIf you want to use GPU id 1 and 3 of your OS, you will need to export CUDA_VISIBLE_DEVICES=1,3\nBoth -world_size and -gpu_ranks need to be set. E.g. -world_size 4 -gpu_ranks 0 1 2 3 will use 4 GPU on this node only.\nIf you want to use 2 nodes with 2 GPU each, you need to set -master_ip and -master_port, and\n\n-world_size 4 -gpu_ranks 0 1: on the first node\n-world_size 4 -gpu_ranks 2 3: on the second node\n-accum_count 2: This will accumulate over 2 batches before updating parameters.\n\nif you use a regular network card (1 Gbps) then we suggest to use a higher -accum_count to minimize the inter-node communication.\nNote:\nWhen training on several GPUs, you can’t have them in ‘Exclusive’ compute mode (nvidia-smi -c 3).\nThe multi-gpu setup relies on a Producer/Consumer setup. This setup means there will be 2&lt;n_gpu&gt; + 1 processes spawned, with 2 processes per GPU, one for model training and one (Consumer) that hosts a Queue of batches that will be processed next. The additional process is the Producer, creating batches and sending them to the Consumers. This setup is beneficial for both wall time and memory, since it loads data shards ‘in advance’, and does not require to load it for each GPU process.\nHow can I ensemble Models at inference?\nYou can specify several models in the translate.py command line: -model model1_seed1 model2_seed2\nBear in mind that your models must share the same target vocabulary.\nHow can I weight different corpora at training?\nPreprocessing\nWe introduced -train_ids which is a list of IDs that will be given to the preprocessed shards.\nE.g. we have two corpora : parallel.en and  parallel.de + from_backtranslation.en from_backtranslation.de, we can pass the following in the preprocess.py command:\n123456...-train_src parallel.en from_backtranslation.en \\-train_tgt parallel.de from_backtranslation.de \\-train_ids A B \\-save_data my_data \\...\nand it will dump my_data.train_A.X.pt based on parallel.en//parallel.de and my_data.train_B.X.pt based on from_backtranslation.en//from_backtranslation.de.\nTraining\nWe introduced -data_ids based on the same principle as above, as well as -data_weights, which is the list of the weight each corpus should have.\nE.g.\n12345...-data my_data \\-data_ids A B \\-data_weights 1 7 \\...\nwill mean that we’ll look for my_data.train_A.*.pt and my_data.train_B.*.pt, and that when building batches, we’ll take 1 example from corpus A, then 7 examples from corpus B, and so on.\nWarning: This means that we’ll load as many shards as we have -data_ids, in order to produce batches containing data from every corpus. It may be a good idea to reduce the -shard_size at preprocessing.\nCan I get word alignment while translating?\nRaw alignments from averaging Transformer attention heads\nCurrently, we support producing word alignment while translating for Transformer based models. Using -report_align when calling translate.py will output the inferred alignments in Pharaoh format. Those alignments are computed from an argmax on the average of the attention heads of the second to last decoder layer. The resulting alignment src-tgt (Pharaoh) will be pasted to the translation sentence, separated by |||.\nNote: The second to last default behaviour was empirically determined. It is not the same as the paper (they take the penultimate layer), probably because of light differences in the architecture.\n\nalignments use the standard “Pharaoh format”, where a pair i-j indicates the ith word of source language is aligned to jth word of target language.\nExample: {‘src’: ‘das stimmt nicht !’; ‘output’: ‘that is not true ! ||| 0-0 0-1 1-2 2-3 1-4 1-5 3-6’}\nUsing the-tgt option when calling translate.py, we output alignments between the source and the gold target rather than the inferred target, assuming we’re doing evaluation.\nTo convert subword alignments to word alignments, or symetrize bidirectional alignments, please refer to the lilt scripts.\n\nSupervised learning on a specific head\nThe quality of output alignments can be further improved by providing reference alignments while training. This will invoke multi-task learning on translation and alignment. This is an implementation based on the paper Jointly Learning to Align and Translate with Transformer Models.\nThe data need to be preprocessed with the reference alignments in order to learn the supervised task.\nWhen calling preprocess.py, add:\n\n--train_align &lt;path&gt;: path(s) to the training alignments in Pharaoh format\n--valid_align &lt;path&gt;: path to the validation set alignments in Pharaoh format (optional).\nThe reference alignment file(s) could be generated by GIZA++ or fast_align.\n\nNote: There should be no blank lines in the alignment files provided.\nOptions to learn such alignments are:\n\n-lambda_align: set the value &gt; 0.0 to enable joint align training, the paper suggests 0.05;\n-alignment_layer: indicate the index of the decoder layer;\n-alignment_heads:  number of alignment heads for the alignment task - should be set to 1 for the supervised task, and preferably kept to default (or same as num_heads) for the average task;\n-full_context_alignment: do full context decoder pass (no future mask) when computing alignments. This will slow down the training (~12% in terms of tok/s) but will be beneficial to generate better alignment.\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/FAQ/"},{"title":"","date":"2024-06-21T03:48:22.065Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:22.065Z","content":"Contributors\nOpenNMT-py is a community developed project and we love developer contributions.\nGuidelines\nBefore sending a PR, please do this checklist first:\n\nPlease run tools/pull_request_chk.sh and fix any errors. When adding new functionality, also add tests to this script. Included checks:\n\nflake8 check for coding style;\nunittest;\ncontinuous integration tests listed in .travis.yml.\n\n\nWhen adding/modifying class constructor, please make the arguments as same naming style as its superclass in PyTorch.\nIf your change is based on a paper, please include a clear comment and reference in the code (more on that below).\n\nDocstrings\nAbove all, try to follow the Google docstring format\n(Napoleon example,\nGoogle styleguide).\nThis makes it easy to include your contributions in the Sphinx documentation. And, do feel free\nto autodoc your contributions in the API .rst files in the docs/source folder! If you do, check that\nyour additions look right.\n12345cd docs# install some dependencies if necessary:# recommonmark, sphinx_rtd_theme, sphinxcontrib-bibtexmake htmlfirefox build/html/main.html  # or your browser of choice\nSome particular advice:\n\n\nTry to follow Python 3 typing module conventions when documenting types.\n\nException: use “or” instead of unions for more readability\nFor external types, use the full “import name”. Common abbreviations (e.g. np) are acceptable.\nFor torch.Tensor types, the torch. is optional.\nPlease don’t use tics like (`str`) or rst directives like (:obj:`str`). Napoleon handles types\nvery well without additional help, so avoid the clutter.\n\n\n\nGoogle docstrings don’t support multiple returns.\nFor multiple returns, the following works well with Sphinx and is still very readable.\n12345678910111213141516def foo(a, b):    &quot;&quot;&quot;This is my docstring.        Args:        a (object): Something.        b (class): Another thing.      Returns:        (object, class):              * a: Something or rather with a long          description that spills over.        * b: And another thing.    &quot;&quot;&quot;      return a, b\n\n\nWhen citing a paper, avoid directly linking in the docstring! Add a Bibtex entry to docs/source/refs.bib.\nE.g., to cite “Attention Is All You Need”, visit arXiv, choose the\nbibtext link, search docs/source/refs.bib\nusing CTRL-F for DBLP:journals/corr/VaswaniSPUJGKP17, and if you do not find it then copy-paste the\ncitation into refs.bib. Then, in your docstring, use :cite:`DBLP:journals/corr/VaswaniSPUJGKP17` .\n\nHowever, a link is better than nothing.\n\n\n\nPlease document tensor shapes. Prefer the format\nb, c)`` ```. This style is easy to read, allows using ``x`` for multplication, and is common1234567891011121314  (PyTorch uses a few variations on the parentheses format, AllenNLP uses exactly this format, Fairseq uses  the parentheses format with single ticks).    - Again, a different style is better than no shape documentation.- Please avoid unnecessary space characters, try to capitalize, and try to punctuate.      For multi-line docstrings, add a blank line after the closing ``&quot;&quot;&quot;``.  Don&#x27;t use a blank line before the closing quotes.    ``&quot;&quot;&quot; not this &quot;&quot;&quot;`` ``&quot;&quot;&quot;This.&quot;&quot;&quot;``    ```python  &quot;&quot;&quot;      Not this.  &quot;&quot;&quot;\n1&quot;&quot;&quot;This.&quot;&quot;&quot;\nThis note is the least important. Focus on content first, but remember that consistent docs look good.\n\n\nBe sensible about the first line. Generally, one stand-alone summary line (per the Google guidelines) is good.\nSometimes, it’s better to cut directly to the args or an extended description. It’s always acceptable to have a\n“trailing” citation.\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/CONTRIBUTING/"},{"title":"","date":"2024-06-21T03:20:48.133Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.133Z","content":"Library\nFor this example, we will assume that we have run preprocess to\ncreate our datasets. For instance\n\nonmt_preprocess -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/data -src_vocab_size 10000 -tgt_vocab_size 10000\n\n1234567import torchimport torch.nn as nnimport onmtimport onmt.inputtersimport onmt.modulesimport onmt.utils\nWe begin by loading in the vocabulary for the model of interest. This will let us check vocab size and to get the special ids for padding.\n123456789vocab_fields = torch.load(&quot;data/data.vocab.pt&quot;)src_text_field = vocab_fields[&quot;src&quot;].base_fieldsrc_vocab = src_text_field.vocabsrc_padding = src_vocab.stoi[src_text_field.pad_token]tgt_text_field = vocab_fields[&#x27;tgt&#x27;].base_fieldtgt_vocab = tgt_text_field.vocabtgt_padding = tgt_vocab.stoi[tgt_text_field.pad_token]\nNext we specify the core model itself. Here we will build a small model with an encoder and an attention based input feeding decoder. Both models will be RNNs and the encoder will be bidirectional\n1234567891011121314151617181920212223242526272829emb_size = 100rnn_size = 500# Specify the core model.encoder_embeddings = onmt.modules.Embeddings(emb_size, len(src_vocab),                                             word_padding_idx=src_padding)encoder = onmt.encoders.RNNEncoder(hidden_size=rnn_size, num_layers=1,                                   rnn_type=&quot;LSTM&quot;, bidirectional=True,                                   embeddings=encoder_embeddings)decoder_embeddings = onmt.modules.Embeddings(emb_size, len(tgt_vocab),                                             word_padding_idx=tgt_padding)decoder = onmt.decoders.decoder.InputFeedRNNDecoder(    hidden_size=rnn_size, num_layers=1, bidirectional_encoder=True,     rnn_type=&quot;LSTM&quot;, embeddings=decoder_embeddings)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;model = onmt.models.model.NMTModel(encoder, decoder)model.to(device)# Specify the tgt word generator and loss computation modulemodel.generator = nn.Sequential(    nn.Linear(rnn_size, len(tgt_vocab)),    nn.LogSoftmax(dim=-1)).to(device)loss = onmt.utils.loss.NMTLossCompute(    criterion=nn.NLLLoss(ignore_index=tgt_padding, reduction=&quot;sum&quot;),    generator=model.generator)\nNow we set up the optimizer. Our wrapper around a core torch optim class handles learning rate updates and gradient normalization automatically.\n1234lr = 1torch_optimizer = torch.optim.SGD(model.parameters(), lr=lr)optim = onmt.utils.optimizers.Optimizer(    torch_optimizer, learning_rate=lr, max_grad_norm=2)\nNow we load the data from disk with the associated vocab fields. To iterate through the data itself we use a wrapper around a torchtext iterator class. We specify one for both the training and test data.\n123456789101112131415161718192021# Load some datafrom itertools import chaintrain_data_file = &quot;data/data.train.0.pt&quot;valid_data_file = &quot;data/data.valid.0.pt&quot;train_iter = onmt.inputters.inputter.DatasetLazyIter(dataset_paths=[train_data_file],                                                     fields=vocab_fields,                                                     batch_size=50,                                                     batch_size_multiple=1,                                                     batch_size_fn=None,                                                     device=device,                                                     is_train=True,                                                     repeat=True)valid_iter = onmt.inputters.inputter.DatasetLazyIter(dataset_paths=[valid_data_file],                                                     fields=vocab_fields,                                                     batch_size=10,                                                     batch_size_multiple=1,                                                     batch_size_fn=None,                                                     device=device,                                                     is_train=False,                                                     repeat=False)\nFinally we train. Keeping track of the output requires a report manager.\n1234567891011report_manager = onmt.utils.ReportMgr(    report_every=50, start_time=None, tensorboard_writer=None)trainer = onmt.Trainer(model=model,                       train_loss=loss,                       valid_loss=loss,                       optim=optim,                       report_manager=report_manager)trainer.train(train_iter=train_iter,              train_steps=400,              valid_iter=valid_iter,              valid_steps=200)\n1234567891011121314151617[2019-02-15 16:34:17,475 INFO] Start training loop and validate every 200 steps...[2019-02-15 16:34:17,601 INFO] Loading dataset from data/data.train.0.pt, number of examples: 10000[2019-02-15 16:35:43,873 INFO] Step 50/  400; acc:  11.54; ppl: 1714.07; xent: 7.45; lr: 1.00000; 662/656 tok/s;     86 sec[2019-02-15 16:37:05,965 INFO] Step 100/  400; acc:  13.75; ppl: 534.80; xent: 6.28; lr: 1.00000; 675/671 tok/s;    168 sec[2019-02-15 16:38:31,289 INFO] Step 150/  400; acc:  15.02; ppl: 439.96; xent: 6.09; lr: 1.00000; 675/668 tok/s;    254 sec[2019-02-15 16:39:56,715 INFO] Step 200/  400; acc:  16.08; ppl: 357.62; xent: 5.88; lr: 1.00000; 642/647 tok/s;    339 sec[2019-02-15 16:39:56,811 INFO] Loading dataset from data/data.valid.0.pt, number of examples: 3000[2019-02-15 16:41:13,415 INFO] Validation perplexity: 208.73[2019-02-15 16:41:13,415 INFO] Validation accuracy: 23.3507[2019-02-15 16:41:13,567 INFO] Loading dataset from data/data.train.0.pt, number of examples: 10000[2019-02-15 16:42:41,562 INFO] Step 250/  400; acc:  17.07; ppl: 310.41; xent: 5.74; lr: 1.00000; 347/344 tok/s;    504 sec[2019-02-15 16:44:04,899 INFO] Step 300/  400; acc:  19.17; ppl: 262.81; xent: 5.57; lr: 1.00000; 665/661 tok/s;    587 sec[2019-02-15 16:45:33,653 INFO] Step 350/  400; acc:  19.38; ppl: 244.81; xent: 5.50; lr: 1.00000; 649/642 tok/s;    676 sec[2019-02-15 16:47:06,141 INFO] Step 400/  400; acc:  20.44; ppl: 214.75; xent: 5.37; lr: 1.00000; 593/598 tok/s;    769 sec[2019-02-15 16:47:06,265 INFO] Loading dataset from data/data.valid.0.pt, number of examples: 3000[2019-02-15 16:48:27,328 INFO] Validation perplexity: 150.277[2019-02-15 16:48:27,328 INFO] Validation accuracy: 24.2132\nTo use the model, we need to load up the translation functions. A Translator object requires the vocab fields, readers for source and target and a global scorer.\n1234567891011121314151617181920212223242526import onmt.translatesrc_reader = onmt.inputters.str2reader[&quot;text&quot;]tgt_reader = onmt.inputters.str2reader[&quot;text&quot;]scorer = onmt.translate.GNMTGlobalScorer(alpha=0.7,                                          beta=0.,                                          length_penalty=&quot;avg&quot;,                                          coverage_penalty=&quot;none&quot;)gpu = 0 if torch.cuda.is_available() else -1translator = onmt.translate.Translator(model=model,                                        fields=vocab_fields,                                        src_reader=src_reader,                                        tgt_reader=tgt_reader,                                        global_scorer=scorer,                                       gpu=gpu)builder = onmt.translate.TranslationBuilder(data=torch.load(valid_data_file),                                             fields=vocab_fields)for batch in valid_iter:    trans_batch = translator.translate_batch(        batch=batch, src_vocabs=[src_vocab],        attn_debug=False)    translations = builder.from_batch(trans_batch)    for trans in translations:        print(trans.log(0))\n[2019-02-15 16:48:27,419 INFO] Loading dataset from data/data.valid.0.pt, number of examples: 3000\n\n\nSENT 0: ['Parliament', 'Does', 'Not', 'Support', 'Amendment', 'Freeing', 'Tymoshenko']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.0983\n\n\nSENT 0: ['Today', ',', 'the', 'Ukraine', 'parliament', 'dismissed', ',', 'within', 'the', 'Code', 'of', 'Criminal', 'Procedure', 'amendment', ',', 'the', 'motion', 'to', 'revoke', 'an', 'article', 'based', 'on', 'which', 'the', 'opposition', 'leader', ',', 'Yulia', 'Tymoshenko', ',', 'was', 'sentenced', '.']\nPRED 0: &lt;unk&gt; ist das &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5950\n\n\nSENT 0: ['The', 'amendment', 'that', 'would', 'lead', 'to', 'freeing', 'the', 'imprisoned', 'former', 'Prime', 'Minister', 'was', 'revoked', 'during', 'second', 'reading', 'of', 'the', 'proposal', 'for', 'mitigation', 'of', 'sentences', 'for', 'economic', 'offences', '.']\nPRED 0: Es gibt es das &lt;unk&gt; der &lt;unk&gt; für &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5128\n\n\nSENT 0: ['In', 'October', ',', 'Tymoshenko', 'was', 'sentenced', 'to', 'seven', 'years', 'in', 'prison', 'for', 'entering', 'into', 'what', 'was', 'reported', 'to', 'be', 'a', 'disadvantageous', 'gas', 'deal', 'with', 'Russia', '.']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5578\n\n\nSENT 0: ['The', 'verdict', 'is', 'not', 'yet', 'final;', 'the', 'court', 'will', 'hear', 'Tymoshenko', '&amp;apos;s', 'appeal', 'in', 'December', '.']\nPRED 0: &lt;unk&gt; ist nicht &lt;unk&gt; .\nPRED SCORE: -0.9623\n\n\nSENT 0: ['Tymoshenko', 'claims', 'the', 'verdict', 'is', 'a', 'political', 'revenge', 'of', 'the', 'regime;', 'in', 'the', 'West', ',', 'the', 'trial', 'has', 'also', 'evoked', 'suspicion', 'of', 'being', 'biased', '.']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -0.8703\n\n\nSENT 0: ['The', 'proposal', 'to', 'remove', 'Article', '365', 'from', 'the', 'Code', 'of', 'Criminal', 'Procedure', ',', 'upon', 'which', 'the', 'former', 'Prime', 'Minister', 'was', 'sentenced', ',', 'was', 'supported', 'by', '147', 'members', 'of', 'parliament', '.']\nPRED 0: &lt;unk&gt; Sie sich mit &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.4778\n\n\nSENT 0: ['Its', 'ratification', 'would', 'require', '226', 'votes', '.']\nPRED 0: &lt;unk&gt; Sie sich &lt;unk&gt; .\nPRED SCORE: -1.3341\n\n\nSENT 0: ['Libya', '&amp;apos;s', 'Victory']\nPRED 0: &lt;unk&gt; Sie die &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.5192\n\n\nSENT 0: ['The', 'story', 'of', 'Libya', '&amp;apos;s', 'liberation', ',', 'or', 'rebellion', ',', 'already', 'has', 'its', 'defeated', '.']\nPRED 0: &lt;unk&gt; ist ein &lt;unk&gt; &lt;unk&gt; .\nPRED SCORE: -1.2772\n\n...\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/Library/"},{"title":"","date":"2024-06-19T22:08:57.436Z","date_formatted":{"ll":"Jun 19, 2024","L":"06/19/2024","MM-DD":"06-19"},"updated":"2024-06-19T14:08:57.436Z","content":"记一场ToMENet中的死亡\n注意看，这名玩家叫做刺猬，他目前被困在了一款游戏中，而且仅有三次死亡的机会……\n本次涉及到的游戏为ToMENet，不少玩家可能会对其“精神续作”ToME4（马基埃亚尔的传说）较为熟悉，尽管两者最初由同一作者创建，但内部分道扬镳后，它们都有了自己的开发团队，相似之处也只剩下个名字罢了。\nToMENet最主要，也是最为罕见的特点在于它是多人联机的在线Roguelike，玩家需要下载好客户端并连接至在线服务器以进行游戏；虽然底层框架仍是回合制，但为了多人游戏，回合处理是自动且迅速的，某种意义上已经算即时制游戏了，或许将它的类型归至MMO RPG更合适。\n既然是在线联机，那自然也有一些别的真人玩家，对，整个情况差不多就是著名动漫《刀剑神域》里的SAO事件，不过由于这是我们自己架设的私服，大家互相认识，关系也都比较好，起码不用担心恶意PK和抢夺装备的问题。\n\n故事背景以J.R.R.的中土世界为基础，游戏中所有城镇与大多数地牢都源自于此，而玩家的最终目标是摧毁堕落之神米尔寇，祂更广为人知的名字叫黑暗之主，魔苟斯（Morgoth）。\n依照传统Roguelike的惯例，字符画面相当简洁抽象，@表示一名玩家，那些方框前的数字表示不同种类的商店入口，+号则表示功能性建筑的大门，像是“马松屋（Mathom-house）”博物馆；在LotR的设定里“马松”是一种夏尔地区的方言，霍比特人用它指代没有用处但又舍不得丢弃的物品，玩家可以通过往马松屋里捐赠些烂得出奇的诅咒装备，或是已被识别的负面道具，让其他未能鉴定的角色过目后避开危险。\n玩家出生的新手村是位于夏尔东部的布理镇（Bree），商店布局大体上继承于各种band系的前辈游戏，交易则需要在对应的类型中进行，没法把冒险中捡来的长枪短剑卖给魔法店，也别指望在药水店里蹲到出售的法术手稿；镇里还有一家酒馆，多半便是著名的跃马客栈，作为大伙的主要碰头区域，如果不担心小毛贼的话可以跟同伴一起存些常用的探险物资于此。\n\n角色创建方面自由度相当之高，除了老掉牙的龙裔、高等精灵、霍比特人之类，甚至还能选择恩特（Ent）和迈雅（Maia）这些十分具有特色的种族，虽然靠后稀有的种族通常更加强力，但对应的代价也更为沉重；以迈雅为例，他们拥有卓越的初始属性和红外视觉，能够感应善良与邪恶生物，看穿隐形并存在隐匿加成，永远从商店中享受最佳优惠，不需要世俗食物维持生命；50级后还将获得漂浮能力、霜电光环、AC加成以及绝大多数抗性等等，更为重要的除了迈雅以外没有人能够使用星界法术。\n代价则是高达400%的经验惩罚，其他种族可能三四十级了迈雅还在二十多级晃悠，此前还必须完成一项试炼，杀死黑暗仆从以选择开悟，或者杀死持烛者（Candlebearer）踏上腐败之路，但消灭两者或未能击杀任何一方都将导致无处容身，角色会在抵达二十级时被直接抹杀；除此之外，能使用星界法术几乎让迈雅与法系职业绑定，可升级缓慢无法快速点出各学派的强力技能，而没有魔法盾的法师又极为脆弱，前中期死亡率极高。\n\n至于职业我曾在上个角色玩过一阵子Mimic（模仿者），算是作为“封测组”的经验吧，该职业允许玩家变成他们杀死过的敌人，且能使用对应形态的所有技能，比如变成寒冰巨魔获得力量补正以增加BpR（每回合近战攻击次数）且免疫冰霜伤害，变成红龙使用火焰吐息和飞行能力，智力足够甚至可以在变形后随意使用法师类敌人的魔法，还无需法典。\n麻烦的地方在于不是所有怪物都具有人形肢体，玩家变形后很容易损失装备槽，毕竟你没法指望一条蠕虫什么的能穿戴盔甲并挥舞长剑；另外模仿者需要杀死足够的敌人才能学习对应形态，具体数量等同于对方等级，特殊boss与精英怪无法被模仿，这意味着能变形的目标都必然比当前角色弱，实在是有些鸡肋。\n哦，还有一件事，千万别在陆地上变形成大白鲨之类的海洋生物，否则就会和某个倒霉的德鲁伊一样，没人想把变成鱼干的同伴抬到神庙去复活。\n\n一番苦思冥想，为了生存能力我还是按惯例车了一只平平无奇的矮子战士，坚韧但行动略为缓慢，免疫失明，在挖掘和使用斧头上存在加成；考虑到后来的主要工作是叮叮当当地凿石头，或许叫矮人矿工更为准确。\n角色创建完就被扔到了酒馆里，身上揣着几百金币和些最为简陋的装备，可惜不会有古怪巫师来你家门口做奇怪的记号，还硬要拉着你踏上冒险之旅，因为玩家压根就是个买不起房的穷鬼；沿着林间小道离开镇上，周边一带几乎全是房区，除了西面不知道为什么有座该死的火山，开荒时期无数萌新曾误入并被烫死在了里面。\n\n房产的平均价格并非让人望而却步，十余格的小房子只需要几万金币就能置办下来，初期玩家咬咬牙也能负担得起，然而房屋的主要作用仅是储存物品，以阻止内部掉落物被系统刷新后消失，是否要将宝贵的积蓄投入到这上面而不是用来提升实力，相当值得斟酌。\n得益于中洲优秀的社会福利制度，每片房区中都有免费供所有人使用的公共房间，通常还比那些廉价的小房子大，再次凸显了购房的华而不实；不过它们的访问权限不可更改，储存贵重物品的安全性得不到保障，也无法被设置成玩家店铺用于摆摊，就只有全员都是熟人的情况下能大大咧咧地当作仓库用了。\n在酒馆爆满后我们往这些公房里扔了许多暂时用不上，又不舍得卖的珍贵马松，介于里面总是乱糟糟的一团，大伙后来干脆形象地将其简称为了“公厕”。\n\n既然已经出门转了一圈，那说不定已经有牺牲者掉湖里或者火山熔岩里了，脆皮法师可能在街上挂个机都会被小混混袭击致死，眼下不得不面对本游戏最为严峻的问题——永久死亡。\n正常模式下一名角色只有三次灵体出窍的机会，且没有任何手段增加，人物生命值归零后会掉落身上携带的物品，变成没有形体但是能够穿墙的鬼魂，此时可以选择放弃所有装备飘到神庙复活，也可以选择在原地等待他人救援；需要注意，一旦鬼魂再被杀死，下场将与三次复活机会用完的情况一样：永远的从这个世界上消逝，虽然屏幕前的玩家不会被烧毁大脑，但将失去至今积攒的一切，无论是苦练的等级，还是精挑细选的武器盔甲，乃至名下的财富和房产，全部都将在死亡后化作虚无。\n纵使允许再建角色重新开始，服务器后来也被他们改成了99命，但死亡应当有其本身的意义，所以我决定玩完最后的这名3命角色就结束这款游戏。\n\n前往真正的地城之前，通常都建议先去镇子外面的训练塔先提升一下等级，在训练塔中生命值归零不会死亡，而是被治疗好再踢出塔楼，代价为损失部分经验和金币；尽管安全，但训练塔一共只有三层，最强的敌人无非是农夫马戈特（幼年弗罗多经常去他田里偷蘑菇）养的几条大狗，稍微升两级经验就不够看了，龟缩其中也不是长久之计。\n古冢岗（Barrow-downs）是离布理镇最近的地下区域，虽然叫这名字，但里面以普通敌人为主，浅层见不到什么妖魔鬼怪，是多数玩家的初期探索地点；所处深度以英尺表示，每下去一层会降低50ft，而该区域的最终boss尸妖王（Wight-King）则在-1750ft守候。\n出发时记得检查是否买好了口粮，火把和灯油有没有备齐，有闲钱最好携带一些保命措施，比如瞬移卷轴和轻伤药水；探索过程倒没有什么好提的，对于近战职业而言，由于即时进行的自动战斗，整个操作还被简化了不少，但对于需要手动施法的各种术士来说，就相当考验手速……以及网络延迟。\n没错，这毕竟还是款在线网游，网络问题是永远绕不开的一道坎，遇上传输波动轻则卡顿，重则断线失联；某些惊险时刻反应过来后，哪怕慢上一次按键输入都有可能导致生命危险，要是掉线再等人登录回来，角色早就一命归西了。\n\n也是band系的传统设定，ToMENet采用了刷新式地城，已存在的楼层如果没有玩家就会自动消失，再有人进入则重新生成，这样保证了同一地点就算被反复探索，也一直能有怪物和宝藏可供掠夺；不过玩家的鬼魂无法捡起物品，一旦离开死亡地点，掉落的装备和财物将全部消失，高等级角色陨落倒是会让那层暂不刷新，问题在于复活后不光等级降低，还只能使用备用装备，想在时限内报仇并取回遗物相当困难。\n绝大多数时候摇人帮忙是更为可靠的办法，我们将其称为“创伤小组”，主要任务便是替人收尸，标准救援流程为带张复活卷轴下去，杀光全部敌人清场，再把死者从阴间拽回来；万一预算不够或者情况紧急，没法复活鬼魂就帮人收拾些最为值钱的物品背到地表，但危险程度要是过于离谱，那么创伤小组也无能为力，因为救助人员很可能折在半路上。\n在我正式决定开始permadeath前，有过两次印象深刻的团灭经历，一次是我被奇形怪状的法师爆杀后，言静姐姐跑来帮我捡尸，那时大伙貌似都比较穷，还用不起返程卷轴，只能一层层走回去，结果在倒数第二层不知怎么的碰到一大群杂种兽人（Snaga），虽然我靠灵体穿墙提前发现并给出了预警，但最终没起到什么作用，也许是盔甲太沉，静姐的法师背不动，总之牺牲人数+1。\n随后赶来的ham干脆利落地解决了那群杂碎，刚嘲讽完对面，没想到还有个精英怪，转眼间被反杀，牺牲人数再+1，这个故事告诉我们，在确定周围绝对安全之前，千万别放松警惕去打字聊天或者截图留念；后面又陆陆续续地来了几位群友，但除了一只没找到路的翡翠鸟幸存，其他顺利抵达现场的创伤小组全员覆没。\n\n\n另外一次团灭是在古冢岗的-300ft，有几位组团下去的群友感受到了special feeling，该信息通常表示本层存在精英怪携带神器；事发之初我并不在场，不过根据复盘来看他们应该是发现了宝库（vault），这是种特殊的建筑结构，里面一般会有不少怪物和好东西，chacha阅读了侦测卷轴，他们肯定被其中的财富所诱惑，然后凿开了宝库墙壁，也同时打开了通向死亡的大门。\n后来发生了什么没有详细记载，只知道包括创伤小组在内，没有任何活人返回地面，尸体的死状都惨不忍睹，未能成功回收哪怕一件遗物；经统计，本次一共五名成员阵亡，作为事故元凶的那件神器披风，不光因为等级要求无法捡起以查看具体信息，还让我们倒贴了另外一件宝物，真是好奇心害死猹。\n\n\n已经提到了就解释下神器（Artifact）的定义，简单来讲是一类非常强大的装备，通常独一无二有自己的名称，不过并非专指跟神明或神话故事有关的东西，只要足够强，来源是什么都无所谓；ToMENet的神器分为两种，一种由高级词条叠加后随机生成，比如混沌之锤，防御者板甲，还有一种是开发团队根据LotR世界观预先设计的真神器（true-artifact），像是统御众戒的至尊戒。\n它们的词条和各项加成都碾压同级的good和excellent装备，但两者的主要区别在于同一真神器只能存在一件，而随机神器理论上能同时随机出两件相同或差不多的，虽然概率多半比买彩票中头奖还小；举个例子，假如有人得到了安督利尔，那么在这把剑消失之前，其他人不管刷怪开宝库有多么卖力，都不可能掉出第二把圣剑。\n好坏参半的是，无论真神器还是随机神器，都会过期，每件神器的大致使用期限为现实中的三十多天，就算物主没有死外面或者把东西卖给系统商人，时间一到神器也会自己消失得无影无踪；算是防止高阶装备通货膨胀，以及有效阻止囤积的手段，不过角色死亡速度通常比装备过期速度快就是了。\n顺带一提，游戏里真有彩票卷轴，极小概率读完后暴富，我们开出过最高的是六等奖500金币，没中奖就只有一张吐舌头的废纸:-P\n\n\n刚进入地城寻宝的那些日子，大家基本上都是有啥捡啥，再多再烂的垃圾也要统统扛回镇里卖掉，白板装备只能卖个几十金币，基础伤害稍微好点的重型武器也常常只能卖个两百出头；药水和卷轴还好说，即便不认得，卖给商店一次就能自动识别并在下次遇到时认出，但其他物品需要鉴定，要么找法师队友蹭鉴定术，要么得去购买一次性的卷轴或者昂贵的鉴定法杖，这让本就不多的收入更是大打折扣。\n法师要攒钱买各种法术手稿，战士也得凑齐自己的重甲，这些东西动辄上千金币，还没算强化费用，那时候为了节俭，走夜路连油灯都舍不得买，举着个廉价火把满地牢找灯捡；但当我们在店门口蹲97一张的打折卷轴时，翡翠鸟捡到了一双靴子，尽管不属于神器，这双靴子却能够卖出足足12330金币的高价。\n12k，也就是一万多金币，该数额在前期足够武装出三四名重装战士，或者让法师学会好几种新法术，再添点存款甚至能买栋小房子；这对纠结于吃生命体征维持餐还是买食物口粮更省钱的我们来说，完全是跨越阶级的一记重击，持靴人翡翠鸟瞬间成为中洲首富，俯视芸芸众生，仿佛成了天上的皇帝XD\n\n\n当事人后来并没有把靴子卖掉，因为它不但提供毒素抗性还能让使用者完全免疫麻痹，这些都是相当实用且不常见的词条，另外不知道是不是果蝠身体的特质，这靴子还为吸血鬼翡翠鸟增加了巨额AC（盔甲等级，可视作物理防御值，在本游戏中数值越高越好）。\n果蝠身体是开局可选择的特质之一，角色将无法使用任何武器，但是近战攻击能够造成hp吸收的效果，自带飞行能力并获得大量移速加成，专门查了一下发现果蝠不能穿鞋子和手套，那可能是人形+武术等技能给予轻甲的AC加成；虽然听起来很适合搭配吸血鬼种族使用，然而吸血鬼可以在20级时主动变成缺点更少的蝙蝠，实际略为吃亏。\n吸血鬼自然是惧怕阳光和神圣，擅长黑暗系魔法，能够夜视并抵抗邪术，食物方面虽然不用吸血，但必须靠不断击杀活物来维持自身存在；十分有趣的一点在于吸血鬼和地狱骑士无视多数负面词条，作为死者显然不会受到幽魂装备的生命汲取，甚至还能逆转一些Heavily cursed的诅咒装备，从而让其成为正面效果，考虑到吸血鬼免疫黑息，他们甚至能毫无顾虑的使用魔古尔（Morgul）的戒灵武器。\n理论上吸血鬼暴露在日光之下就会被烧成渣，但是我们在这里隆重介绍一款产品：ToMENet裹尸布™，有了它，您可以享受到360°全方位无死角防护，保护脆弱的肌肤不受烈日侵害，最先进的自适应缠绕功能保证无论是人形生物还是娇小果蝠都能完美贴合，丝毫不影响您迅捷灵巧的动作，还在犹豫什么？立刻前往最近的神庙或者黑市订购吧，只要您的长相没有恶心到店主（堕落迈雅也可以用自己的古神之姿把老板吓到给你折扣），不到1000金币即可拿下这条破布！\n下面请欣赏非常好笑的吸血鬼笑话一则👇\n\n新裹尸布用上的比预料中快，除了埋头往地底钻，自然也有人对广阔的中土世界感兴趣，最早是冒险者ham穿越森林希望绕西面的火山一圈再回来，没想到在离镇子一步之遥的山边被巨人敲了一闷棍，当场死亡。\n而在后来，可怜的翡翠鸟希望一路向西南方向飞行，穿过海洋以抵达坐标原点，起初顺顺利利还能在水面上捕鱼玩，结果飞到半路被巨型乌贼喷射的酸液融化成了渣，渔夫秒变落汤蝙蝠，还好附近有座孤岛，灵魂龟缩在岛中央避免了魂飞魄散的下场；ham的新果蝠角色飞去营救，结果刚到目标地点就被两口喷死，但这次不是因为酸液，而是乌贼释放了强劲闪电，一发伤害高达175，作为参考，我这样一名health点满的二十几级重装战士也才400出头的血量。\n巨型乌贼的这几口酸电喷吐根本不是我们这个阶段能硬抗的，如果假设ham受到的伤害经过抗性减免，那原先的数据只会更加离谱；我当时由于没有飞行能力，仅能在后方帮忙跑腿买个裹尸布，移动时猛冲过头给翡翠鸟从屋檐下挤出来差点被阳光烧成灰，罪过罪过。\n\n\n俩果蝠复活之后觉得忍一时越想越气，退一步越想越亏，收拾了点新的“陪葬品”又重新上路要再找大鱿鱼一决高下，结果显而易见，复活后临时准备的道具根本不是对手，两只果蝠彻底葬身大海，再也没回来；装备太烂是一方面，最大的问题在于敌人等级太高，巨型乌贼整整41级，生命骰150d10，吐息计算为hp/3，那么预期伤害在250左右，非常纯粹的数值碾压。\n十来级的果蝠这么干，完全是电子虫找通关战争（能把Nethack杀穿的狠角色）打架，言静姐姐：“战争上限才30级”。\n\n\n外面的世界很危险，但我从开始permadeath起就十分谨慎，显示深度的数字会用颜色表明当前区域的经验倍率，白色是正常，黄色是降低，灰色则是怪物等级太低无法获取任何经验，某种意义上也可以当作危险指示器来使用；我主要在白色与黄色区域的交界处进行采矿作业，下楼时感受到special feeling尚可找楼层头目放手一搏，但如果是challenge或者dangerous around那就只有立马离开保命。\n某些极为罕见的特殊装备可以显示隐藏的矿脉，但考虑到宝物侦测卷轴才二十多金币一张，探矿成本几乎能忽略，反倒得注意ToMENet中的卷轴都相当沉，背多了很容易超重，一张卷轴足有半磅，疑似高档羊皮纸配了卷轴筒；挖矿，或者说挖金币的核心要点在于叠加倍率，隐藏矿脉比裸露矿脉更值钱，每十级digging技能都有机会产生额外的金币掉落，而运气属性可以增加掉落金币的数额，最后也是最重要一点，深度越危险，矿物就越值钱。\n挖掘速度是次要的，但能够大为节省时间，这方面主要取决于工具，镐头铲子什么的本质上算武器的一种，伤害和命中的强化等级都将影响挖掘速度，其次是物品重量，重型挖掘道具的效率自然比小铁锹强（感谢chacha送的pick of digging）。\n相较于找boss杀再捡宝贝去卖，挖矿的安全系数要高很多，毕竟活着才是最重要的，具体收入也不会落后太远，中期一镐下去爆四五千都是很稀松平常的事，另外金币不占背包空间和负重，跟再背些装备回去补贴家用并不冲突。\n\n高贵的符文法师甚至挖岩石墙都有小概率得到符文，除了施法自用以外，这东西的售价接近10k一枚，相当值钱；玩家也可能碰到些奇怪的大块金属和零部件，我曾经在古冢岗挖出过大块的铜（超级沉），后来又在魔多挖出过机械腿，这些东西可以制作某种十分有趣的造物——魔像。\n构造一尊魔像需要一块大型特殊材料，两条机械腿，几张命令卷轴和魔像创造卷轴，魔像胳膊是可选的，最多允许添加四条，但不加的话魔像无法攻击敌人；另外还可以在物品上刻写{@G}，让它们融入材料，并根据适合的词条为魔像提供特质。\n魔像的强度主要取决于材料类型，最高级的精金魔像足有210点AC，10d150的生命骰，基础伤害将和胳膊的强化等级相乘再算得实际输出，总之也低不到哪去；所需卷轴很容易在黑市买到，但大块材料与四肢就很难获取了，我个人要求不高，只想要个硅胶魔像放家里自用罢了（并不存在）。\n\n回顾了一下初期冒险，最为惨重的经济损失跟咕噜有关，事情发生于果蝠惨案前，翡翠鸟捡到靴子的不久后；咕噜，原名Smeagol，正是跟巴金斯玩猜谜游戏顺便送至尊戒的那位，在本游戏中以精英怪的形式出现，尽管能将障碍物视若无物，它倒没有清空我的钱包顺便再偷几件装备走，事实恰恰相反，刚从树丛中现身就被我几斧头当场砍死，掉落了一枚戒指和一条非常不起眼的长鞭。\n小心翼翼地收好戒指，返程路上大家都在猜测它会不会是至尊魔戒，我一边鬼叫着“my precious!”一边处理着包中的其他垃圾，顺手把那条没鉴定的长鞭以20金币的价格卖了，下一秒我在商店里看到了它后面跟着的一长串售价，足足五位数，53k，五万三千金币。\n时间在那一瞬间仿佛停止了流逝，我整个人呆若木鸡，周围的一切事物都变得陌生遥远起来，我口干舌燥汗流浃背，心脏几乎停拍，反反复复数着那个数字，希望是太缺钱导致自己眼花，可不容争辩的事实如山一般压得让人喘不过气，沉默良久，认清现实的我当场发出了尖锐的爆鸣：\n五万三千金币的鞭子啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊————\n\n我成功刷新了chacha以20金币卖出保护披风的记录，荣登中洲第一大冤种的宝座，彻底失心疯了，自那以后逢人就傻笑着问对方有没有见过自己53k的鞭子；至于那枚戒指，不过是个很常见也很便宜的潜行戒指而已，给隐身饰品提鞋都不配。\n出售后显示的物品真名为Chaotic Whip of the Thunderlords，混沌的雷霆主宰之鞭，并且使用等级高达29级；当时我由于红温太久，被店主踢出店导致没有截到详细的物品描述，找了张群友的后期装备截图作为代替，但图中的雷霆主宰匕首只要求17级，还少了一项Chaotic前缀，该前缀通常能提供chaos和confusion的抗性，跟这比起来那条鞭子约莫着只好不坏。\n\n最惨重的损失回忆完了，接下来自然该提一下赚得盆满钵满的那趟，当时我只打算非常普通的回地下挖挖矿，选择的层数并不深，应该在750ft左右，意想不到的是刚返程完毕就收到了special feeling的提示，然而没等我评估风险，一名boss从旁边直接突我脸上了，好像还带着个精英跟班。\n结果我赢得莫名其妙，因为boss罗宾汉（Robin Hood）是弓箭手，贴脸冲上来跟重装战士肉搏完全是嫌死得不够快，射箭和放置陷阱等技能统统没有见到，三两下被我砍翻了，另外那只叫Borshin的精英怪好像是坨血肉混合物，也就多撑了几回合，之后当场变成了肉酱，想细细地给它切成臊子都没机会。\n罗宾汉掉了把长弓，后来经鉴定是一件真神器，名为巴德之弓，对，就是长湖镇那位射杀史矛革的河谷之王，真好奇汉丁顿伯爵怎么会有他的长弓；虽然我非常缺少远程攻击手段，也很想用这把对龙宝具射爆那群喜欢瞬移的魔法师，但身上的重甲严重影响射击精度，而且职业与种族让这名角色的箭术潜力极其之差，甚至还不如丢回旋镖有天赋，它并不适合由我使用，考虑到队里当时也没射手，最终还是打算将其卖掉。\n要是装备上的话倒能增加些敏捷和幸运，另外还能让使用者免疫麻痹，可当时我已经有麻痹免疫的防具了，敏捷对战士的重要程度也没那么高，而幸运？加上两点的回报可能还不如卖了弓的零头，尽管委托了魅力点得比较高的静姐帮忙，但这把弓的拾取等级太高，其他人没一个能捡起来；最终我这名魅力10的矮子，在戴上了一件CHR +3的护符后，将巴德之弓卖出了近90k金币的售价，托它的福，我才发现店主会随机更换并且带有种族歧视，同族之间的优惠程度是最高的，如果两族关系不好，玩家的回收金额会被大幅克扣。\n没想到的是精灵和矮人的关系还挺好，精灵给出的报价比人类都要高出四千多金币，或许这是第一纪元吧，矮人还没杀害辛葛抢走带有宝钻的项链。\n\n\n随着不断深入，宝藏增多的同时，怪物也在变得更加致命，各种攻击手段层出不穷，毒素、麻痹、失明、混乱、恐惧、幻觉等等，还有各式各样的元素和魔法伤害，此时抗性（Resistance）就成了很重要的一类保命属性。\n最为头疼的是法师玩家们，他们的法典很容易很容易被火焰烧毁，同时非常怕水怕酸，独特的心灵工匠（Mindcrafter）倒是不用纸质法典，但他们的法术晶体受到电击也会损坏，而失去施法媒介的法师无异于待宰羔羊，另外高阶魔法非常昂贵，已添加的手稿不能从法典上撕下来，万一整本书被烧了那大致等于倾家荡产。\n元素伤害在拥有抗性后变为1/3，拥有双重抗性则会变为1/9，但物品仍然有概率被元素摧毁，装备与装备之间的抗性不能叠加，双重抗性通常需要饮用药水或者激活某些特殊道具，但这些效果大多只是暂时的，还可能遇上消耗品用完和道具失窃的情况；想要稳定持久且保护物品不被破坏，那就必须获得对应元素的免疫（Immune），最简单的方法是变形成有免疫特质的怪物，比如骷髅和僵尸免疫剧毒，地狱猎犬免疫火焰之类的，不具备相关能力的角色可以找模仿者为其制作变形戒指，当然，防具能带有极为罕见的免疫词条自然是最好的。\n\n\n在缺乏免疫和抗性的情况下自救也是一门学问，致命伤治疗药水能够治愈混乱、失明、以及大多数割伤，似乎还能够缓解短暂眩晕但无法解除麻痹，也许是因为不可行动状态下没办法喝药，十分合理。\n中毒与麻痹是萌新冒险者的主要死因，如果不幸被毒虫咬伤或者触发了涂有毒液的陷阱，那么需要服下化解毒素药水或者精灵口粮兰巴斯，不知道精灵是改进了配方还是在里面下了药，ToMENet中的兰巴斯ProMaxUltraPlus版能够解除非常多状态，祛除疾病、中毒、以及幻觉统统不在话下，当然也能消除饥饿，但这东西带有圣洁属性，对吸血鬼等种族来说跟生吞火炭没啥区别。\n在备齐基础抗性的中期，主要威胁来自混乱与幻觉，混乱将让角色失去控制，表现为分不清移动方向并朝空气胡乱攻击，导致单方面疯狂挨打，雪上加霜的是混乱状态下不可阅读卷轴，这将让角色失去传送或回城的能力，很容易被活活揍死而毫无逃跑机会；虽然治疗药水可以解除混乱，但请保证之后能立即脱身，否则可能像某个忘记携带节育卷轴的冒险者，刚喝完药又混乱，被几百条繁殖出虫子硬控到死。\n幻觉的致命性稍低一些，视野中的所有生物会不断变幻外形，从鼠人到戒灵，从泰坦到青蛙，一切都在闪烁扭曲，幸好外观是虚假的，不会改变怪物本体仍然是坨几刀就死的致幻地衣，问题是你也有可能把巨龙看成蠕虫而忽略，把队友看成水怪然后一发火球术给人炸上天；幻觉的持续时间非常长，而且抗性和善后手段极难获得，我几乎就没见过抵抗幻觉的装备，不像隔壁混乱，只要花几万就能轻松买到抗性戒指，再加几万还能买到同时抵抗chaos和混乱的戒指。\n\n兰巴斯虽然能消除幻觉，但它难以获取，普通商店没有这种精灵食物出售，想要批发估摸着得前往精灵国度，神秘的罗斯洛立安；除此之外也存在一些代替方案，比如让牧师给中幻觉的队友来发祷告，又或者吃上一朵神奇的蘑菇。\n蘑菇大体上是把双刃剑，没有鉴定的情况下乱吃可能会中毒失明，但如果能分辨好坏，那么它可以被视作重量和药效都轻一些的药水；有一种Cure Paranoia的蘑菇可以治愈恐惧和幻觉，还有种叫做Unmagic的蘑菇会移除一切现有状态，尽管是万能药但它也将同时消除正面增益，类似于完全净化自身。\n城镇附近的田里说不定会有蘑菇能摘，地城里也不时能采到一些野生蘑菇，但更为稳妥的方案是寻找随机在特殊楼层中出现的蘑菇商店，等待货物刷新顺便托队友送点钱就能轻松购入大量所需品种；比较有意思的是致幻菇，虽然会引起幻觉，但它也能回复一定的魔法值，这可比几千金币一瓶的魔力药水便宜太多了，大啖蘑菇几十朵，饭后再来口解除幻觉，完美（然而会扣珍贵的理智）。\n\n被某些敌人或法术击中将损失经验和能力值，主要威胁在于战斗时的猝不及防，好在玩家不会永久失去它们，可以去神庙处蹲出售的对应药剂，大约几百金币一瓶，经验掉了就喝等级恢复药水，STR掉了就喝力量恢复药水，诸如此类。\n部分像是那兹古尔（Nazgul）之类的高阶不死者，能够腐朽玩家的非神器级近战武器，另外要是没有Slay Undead or Evil的词条，那将无法对其造成任何伤害；最为恐怖的一点是它们能让角色感染“黑息”，这是一种缓慢消耗经验和属性值的负面状态，它不光无法随时间流逝而自然消散，还会在玩家间互相传染，持续蔓延，必须服用罕见的Sprig of Athelas（阿塞拉斯的嫩枝）才能治疗，刚铎人们将其称之为王叶草，正是阿拉贡拿去救人的那种。\n曾遇到过一次非常惊险的情况，当时好像是带着两位群友的新号下去练级，德鲁伊靠毒雾攻击，结果遇上了一大群无视毒素的不死族，当事人冲上去就是揍，结果被一顿乱啃，撤回来之后还感染了疾病，我和另外一名法师清完敌人只能眼睁睁的看着他血量狂掉，想丢药但来不及，好在后面剩一丝血苟住了。\n德鲁伊恢复完刚上楼梯，法师和我在水边又遇到了一大群魔鬼鱼还是什么来着，远距离疯狂吸收经验，并且给我们叠上了混乱和幻觉，场面一度十分惊险，幸亏没有非常强大的高伤害怪物在附近，我们有足够的时间处理并后撤，最终成功相继返回了地表，一看经验条，被活生生从16级吸到了7级。\n\n后期还有更多更恶心的攻击类型，像是Chaos，Nether，Plasma，Life Drain，Disenchantment……以及Nexus，这玩意轻则将人驱逐出地城，背包里塞满垃圾，重则永久交换玩家的两种属性值，战士当场虚弱，法师当场降智，唯一的解决办法只有再去挨打尝试换回来。\n随后的一次冒险就没那么好运了，德鲁伊离队休整，我和言静姐姐准备再返古冢岗，倒也怪我，忘记解释清不要直接返回，否则没有退路非常容易陷入危险；可打完字扭头回来人已经直接阅读了返程卷轴，在地城上方这样会做会直接前往曾经抵达过的最深层，想要回到指定层数必须在卷轴上刻写{@R}，然后使用/rec指令。\n我赶忙查看玩家信息，指定层数阅读了卷轴，接着暗暗祈祷能在意外之前赶上并且传送位置不要相距太远，不过愿望只实现了一半，我抵达时确实在静姐的法师附近，但她已经被一名帕拉丁残忍咒杀，更让人如坠冰窟的是，通知栏中紧接着弹出了鬼魂被杀的消息，这意味着不可复活的彻底死亡。\n本来灵体出窍后可以立马疯狂上楼逃命，但是显然也会让原先那层被刷新掉，所以静姐为了保护装备并未离开本层，结果碰上了同样能穿墙的幽灵战士，不幸牺牲。\n\n解决那名帕拉丁只需要几斧头，但生命在指尖逝去的绝望与无能为力却是久久不能忘怀的。\nVault：“nice night, expect for the ending”\nHedgehog: “got the staff?”\nVault: “is beginner’s kit too, no need”\nVault: “id call it a day, won’t play it till all exams done”\nHedgehog: “hope cya, but i may die before（”\n\n死亡flag立完了，这种凄美的月下道别，感觉我不死上个几次都对不起剧情发展，但当天悲伤过度的我并没有再次返回深处，之后更是将谨慎与稳妥作为了探索的第一准则，仔细地管理着身上的所有保命道具。\n创伤小组那边依旧业务繁忙，从拯救大兵ham发现自己忘记带灯，再到全员出动前往-1350ft勇战三头恶龙，这应该是规模最大的一次行动，所有冒险者几乎倾巢而出，前仆后继地空降填线，最终以折损数名30级中坚玩家为代价，惨痛地取得了胜利；我也在现场，可惜是负责事后打扫战场的那批人，错过了见证史诗级大战的机会。\n\n\n本次收获颇为丰富，光是真神器就有整整三件，其中最为突出的又非埃兰迪尔之星莫属，这是一颗能够用于照明的精灵宝石，为北方王国阿尔诺的王权象征；遗憾之处在于它并不是瓦尔坦封圣的那几颗精灵宝钻之一，不然可能会珍贵过头了点，我起初还把它和凯兰崔尔的水晶瓶（精灵女王送弗罗多的手电筒）搞混了，后来去查才分清星光瓶里储存的是埃雅仁迪尔之星的光芒。\n这颗宝石可以提供失明抗性，免疫恐惧，看穿隐形，抵抗生命汲取，甚至能增加玩家的行动速度；此外它将产生半径为5的光照且永远不需要添加燃料，该照明范围是我们常用油灯的2.5倍，然而还没完，每隔几十回合便可将它激活，当作魔法地图显示周边一大片区域的详细情况，简直就是一双没消耗的天眼。\n不知道算不算乐极生悲，由于任何神器都不能放置在酒馆或家中，所以那场战斗的核心战利品都必须由活人随身保管，另外考虑到拾取等级限制，能拿起来的也就那么几人，没法随便找个小号存着；结果……揣着所有神器的那只仓鼠信心爆棚，一路高歌猛进，随后不负众望地暴毙在了-1750ft，古冢岗底层。\n\n\n大伙自然是又组织了几场声势浩大的救援活动，奈何位置实在太深太危险，主力队伍又于上一场恶战中元气大伤，除了有个倒霉蛋死在-1600ft送了人头之外，我们暂时未能取得任何进展，几位新冒险者甚至因为陷阱剑和其他玩意在浅层死了又死。\n三龙之战尚未开始的前夕我正在忙于收拾行李，准备将初始城镇周边调查一圈，虽然每座据点都是提前设计好的，不过城镇分布与整个地理环境由随机生成决定，探索世界仍是不可或缺的一环；最先发现的是古代森林，这属于地表无建筑的的野外地城，但由于服务器管理员作弊开全图剧透，这里实际上已经被几名群友简单探索过了，没有什么意义。\n初次远行时走得步履维艰，主要问题出在各种障碍物上，缺少漂浮道具也不会飞就无法穿越森林，得手动一棵棵地伐树取道；即便会飞也不能翻过高山，一定要拥有攀爬工具或者点够Climbing技能，否则只能绕远路，深水区没学游泳技能也可硬闯，但角色在溺水状态下会飞速扣血，想要跨越大型水体，飞行依然是不容忽视一道门槛。\n\n硬趟浑水的时候我曾出现过无法移动的情况，不清楚是网络卡顿还是按键失灵，当时角色的hp疯狂下降，身上大量药剂和卷轴也因为浸水而不断损坏；幸亏提前设置的道具快捷键仍然能用，我玩命地把治疗药水往嘴里灌，第一反应是盔甲超重得脱，但想着之前既然能走进来这么远，那应该不是这方面的问题；况且我一旦停止给药，卸个装备的功夫估计够淹死两遍。\n事发仓促，结束的也挺唐突，在我差点要对自己使用传送法杖的刹那，角色忽然又能行动了，我赶紧冲上岸边然后使劲咳嗽（肺部活动为当事人脑补）；大难不死，心有余悸，刺激程度不亚于腐败湖粪坑蝶泳，说来离奇，在水里快淹死的时候居然可以喝药续命。\n\n\n之后托群友花整整两万四千金币，代购到了漂浮戒指，但海洋仍然是我这名矮子的噩梦，这里不光充斥着大片大片的食人鱼，还有剧毒的箱水母，残暴凶猛的杀人鲸和大白鲨，能远距离发射穿甲水弹的枪鱼（Gunfish），更流传着喷吐闪电的克拉肯传说。\n即使在岸边也能冒出一大群水鬼，那架势如同潮水涌出，随时准备吞没路过的无辜冒险者；虽然危险万分，为了绘制地图而在深夜飞越大洋，依旧犹如家常便饭，但正因如此，见到能够歇脚的小岛时才会发自内心的感到欢欣与喜悦。\n\n要让我回忆最惊心动魄的一次出海经历，那毫无疑问是在探索镇子西北方向的那趟，给我造成了严重的精神损伤（物理层面上），几乎真的患上了海洋恐惧症，对比之下，前阵子差点淹死简直是小儿科。\n当时我正沿着海岸线徐徐前进，碰巧发现了不少水族战士，于是利用他们无法踏上陆地的特性，轻松在岸边大杀特杀，可当我边战斗边检查满地掉落物的时候，没注意到几只海马魔法师混在其他怪物里凑了过来；角色进入射程的一瞬间，铺天盖地的元素箭矢就飞了过来，伤害最高的可以一发带走我将近三分之一的hp，还附赠眩晕和摧毁物品的效果，我立刻启动外置血条，开始猛喝速度与治疗药水，然后拼尽全力将敌人斩于斧下。\n虽然是魔法师，但从手感上讲，这些海马的生命值惊人的高，丝毫不逊色于陆地上某些老练的战士，另外不知道是法术列表中空缺，还是因为其战斗风格本就倾向于近身作战，它们没有使用传送或闪现拉开距离，反倒冲上来贴脸施法，甚至还会咬人；得亏是群近战法师，在消耗了许多药剂后，我惊险地获得了胜利，毕竟肉搏方面爷才是专家，它们要采取风筝战术，那说不定我真得落荒而逃。\n正当我洋洋得意，甚至打算截个图吐槽一下走路上被海马揍了的时候，一股诡异的灵能直冲我的意识袭来，大脑仿佛要爆炸了一般，认知被扭曲还导致眼前出现了幻觉；虽然来不及弄清具体情况，可意识到理智降低后，我毫不犹豫地使用了传送法杖脱战，然后光速逃离了那片区域，直到确认周围安全，才惊出一身冷汗。\n\n\n从战斗记录来看，我是遭到了邪教徒的精神攻击，虽然hp没有丝毫减少，可理智值（Sanity）已经下滑至危险程度，两发心灵冲击将其蒸发了约40%，如果说生命值耗尽等于肉体毁坏，那么彻底丧失理智则是灵魂层面上被完全抹去。\n无论角色是3命还是99命，一旦SN归零，都将不可复活的彻底死去，而更恐怖的一点在于理智值不会自然恢复，虽然有药水能够增加，但理智药剂极为罕见，任何商店都没法订货和买到，只能依靠地城拾取或是用空瓶从喷泉舀水，瓶子会随机装满一种药水，而其中就有小概率roll到理智药剂。\n理智状况最初以非常模糊的形式进行描述，在health技能足够高后才可以显示出百分比，但具体数值取决于感知（WIS）属性，同样是100%，人类牧师的理智值肯定比巨魔战士要高得多，面对不可名状之物时也会更加坚韧。\n\n\n不存在任何道具能够抵御精神污染，哪怕神器也不例外，但有一些办法可以降低传入的精神伤害：\n首先是变形成不死者，僵尸骷髅什么的由于没有脑子，可以拥有T1级别（数字越小越弱）的心灵防护，其次是吸血鬼种族，防护等级为T2，而如果想要拥有效果最好的T3级减伤，那得选择心灵工匠职业并将角色练到Level 40，而其他职业需要花费宝贵的技能点数将Hereticism和Traumaturgy两项技能同时加到45级，部分近战职业没有这方面的技能树，尚未开始努力就已遗憾离场。\n另外玩家要是因为hp过低而死，那么变成鬼魂状态时会自带T3级别的心灵防护，危急关头又还有复活次数，说不定自杀还真是个解决办法，如果来得及的话……\n\n不过事已至此，木已成舟，除了想办法恢复理智之外也没什么可做的，哦，或许可以痛批一下中土邪教组织猖獗的问题，这帮疯子还完全不传教，想加入都没机会，正常人没跑掉的全给送去见了他们的真主。\n外出是不敢再出远门了，地城深处似乎有听说过邪教成员出没的传闻，同样不敢继续深入，我整个人草木皆兵，大部分时间都缩在角落里痴呆地流口水，感受着脑海中闪回的记忆场景，眼前不断冒出各种幻象。\n当然，被迫宅在镇上算是个等待商店刷新的好机会，虽然大部分商人售卖的都是基础物品，但偶尔会出现些物美价廉的高级货，比如带’*'的强力卷轴，就算运气差，趁打折期间批发点常用的消耗物资也总是有用的；攒下一定积蓄后，黑市就成了我们的主要消费场所，听名字便知道价格宰人，但里面能买到真正好用，且对于同级玩家来说难以获取的装备，不像其他商店的“占位符”，开局凑合穿一下就换掉了。\n\n黑市中比较值的一笔交易是我花40k左右的金币买到了反射护符，忘记有无折扣，当时的花销水平完全支撑得起，而且反射词条极其稀有，尽管不如隔壁Nethack那样几乎无视所有远程魔法，它多少可以够弹开一些射弹和元素箭矢，而且光面板提供15点AC就相当于一枚优质的保护戒指，对我这种常常看着远程敌人无能狂怒的战士来讲，无比实用。\n非常血亏的交易则是初期花将近25k购入了一柄白板双头战斧，虽然基础伤害骰面很高，但由于太重，我那名矮人拿起时BpR降低，导致伤害还不如原来的旧斧头，如果转手卖给商店又回收不了多少资金，真是花钱买教训，我后来专门把它扔到杂物间当成了预防冲动消费的警钟。\n有些迷惑人的是，黑市里边经常会卖些很贵又没啥用的怪东西，比如各种皇冠，什么词条都没有，AC也差到几乎可以忽略，唯一的特性疑似是比较值钱，买这玩意或许只能满足一下使用者的role play或者换装需求，但这种游戏里角色就是个@符号，根本不会有人在乎你头上带了顶秘银皇冠还是摆着一缸金鱼。\n\n在我因病休假逛黑市期间，可怜的仓鼠鬼魂还在古冢岗底层飘着，当事人后来用小号车了名巨魔战士，慢慢杀了下去，虽然最终成功抵达目的地，但角色实力不足以回收遗物，再次送命，底下躺着的神器倒又多了一件。\n人美心善的言静姐姐也新建了一名叫Doll的牧师，在训练塔里猛猛刷级，高等级的牧师和超能力者都有办法恢复其他玩家的理智，不过施法成功率极低，而且耗蓝奇高无比，估摸着只有在城镇等安全区域能够进行操作；另外不知道是不是在16级前夕死太多次，给静姐造成了心理创伤，之后很长一段时间都只能看到她蜗在训练塔，要不是等级高到一定程度就没经验了，估计可以练成十里坡剑神。\n如此执着于16级是因为我曾弄到过一根萨鲁曼密探的魔法长杖，鉴定之后没想到还是随机神器，自然就给了队伍中唯一的法师，可那玩意有使用等级需求，法师前期又十分脆弱，静姐就不断挣扎在需求线附近；倒不是我不想babysitting，而是ToMENet有组队限制，两名玩家的等级差距超过8级就无法在同一楼层中共享经验，除了复活之外也找不到能够永久降级的方法，痛苦万分。\n经历了漫长的练级，又戴上一堆增加属性的饰品，甚至靠拿法杖增加蓝量后，静姐的牧师终于达到了施放Faithful Focus的最低条件，此时她的主属性WIS为18/80，作为参考，我的力量也不过18/60；但即便这样，祷告的失败率依然高达91%以上，放一发就得耗尽魔力，需要更换回蓝饰品加速恢复，在不知道经历了多少次失败以后，中洲首例精神病治疗案例出现了！我的理智值从61%增加到76%，随着另外两次成功，SN终于重返100%，彻底归复常人！\n这样一来，就完全摆脱了幻觉与潜在的死亡风险，终于能再次踏上广阔的新天地了，多谢言静姐姐！\n\n\n恢复理智后无论瞧见什么都感到赏心悦目，从未如此觉得活着是种奢侈的享受，哪怕仅是坐在屋里，闲看一下游戏的天气和时间变化，都让人无比放松，安心，或许只有死亡的残酷才能凸显出生命的美好与可贵。\n以及，虽然铺张浪费，我在很早之前就购入了自己的两间房屋，一栋主体与一处杂物间，刚好挨在一起，花了35k左右的金币，大约是古冢岗1000ft两趟的收入；实际使用面积为20格，我非常想把中间的隔墙打通，可以再多两格空间并连通两室，遗憾的是城镇墙体无法挖掘，只好作罢。\n这些空间主要拿来储存较为重要，又必须分场合携带的道具，比如倾销垃圾用的魅力护符，挖矿时佩戴的幸运饰品，还有飞越森林海洋的漂浮戒指等等，另外我自己常用的几种卷轴和药水也都囤积了“至死量”——到死之前应该用不完。\n世界危机四伏，所有冒险者得把脑袋别在裤腰带上，过着刀头舐血的日子，刚刚还在嬉笑的同伴可能下一秒就会变成冰冷的尸体，这种严峻的环境下能有一处安全稳定，充满补给物资的容身之所，夫复何求；抛开各种复杂的访问权限，或是将房屋设置成店铺向其他玩家兜售物品，部分花里胡哨的装修小功能反而更加深得我心，像是能将有颜色的药水作为涂料，粉刷屋子外墙，不小心画错了还可以用清水洗掉痕迹，十分具有生活气息。\n\n如果势力庞大，发展到大后期需要公会馆和活动场地，当然也有更加奢华的选择，比如离城镇较远的超大型豪宅和带有护城河的城堡，虽然本质上仍是个大号方框，但它们的售价可以轻松突破千万级别，这么一看隔壁SAO血盟骑士团的总部塔才要10亿珂尔真是太便宜了。\n不过这方面就不得不提到一项令人头疼的问题，无论城堡还是小茅屋，其所有者一旦彻底死亡，房产与里面存储的全部物品都将直接蒸发，也不能写遗嘱什么的留给其他玩家。\n想避免这类情况有两种选择，第一，任命一名完全不离开安全区的法人角色，将资金交由他进行购置房产，再接着将房屋的访问权限设为队内共享，这样当事人出现意外也不会影响到固定资产；第二，则是组建玩家公会，然后再将房屋变更为公会财产，即便原主和公会长双双殒命，也不会导致公会解散或是资产消失，取得公会钥匙的成员将自动晋升为新任公会长；然而成本方面，创建公会需要一名30级以上的角色和整整2000k的费用，你没看错，需要花费整整两百万金币，仅仅为了注册，不过嘛，都想着买小城堡了，那这点钱应该不会放在眼里吧。\n\n地城里有几率遇到丢钱陷阱，部分敌人也拥有盗窃能力，最离谱的是有些魔法能让你的金币造反，跳出钱包变成活化怪物然后狠狠地揍你，如果不想遭遇意外，那么还是不要腰缠万贯地冲进地城深处为好。\n虽然我个人很喜欢把货币扔在屋里，然后欣赏那种积玉堆金的景色，但要是不想每次都得回屋取钱，商人行会很乐意为您提供存款服务，并在户主死亡后侵吞其所有财产，不需要手续费，但也别想获得利息；另外有便捷的快递代发可供选择，能将物品或金钱寄给其他玩家，不过服务费不便宜，收货人则可以在任意一处行会站点取到包裹。\nham曾经给我寄过一盏不用添加燃料的矮人灯，整个收货体验还是比较奇妙的，但必须前往商人行会的站点仍有些不方便，而且东西越珍贵收费越高；可如果队伍里有使用念力的高阶法师和超能力者，那么可以让收货方open mind，建立起精神联系后直接隔空传送物品；万一没带光源直接前往地下深处是件很危险的事，不仅看不到怪物与周遭地形，种族缺少夜视能力甚至无法在黑暗中阅读返回地面的卷轴，只得困在下方等待营救，这时要是能有人隔空递根火把可就帮大忙了。\n\n之前的探索到也不全是白费功夫，地图的绘制上多少取得了一些进展，ToMENet的世界由区块组成，这些小区域互不干涉，但玩家可以穿过边界抵达相邻区块；即使角色正在被猎豹追杀，一旦跑到其他区域，哪怕怪物与你仅有一步之遥，它们也无法越过区域边界，只能望尘莫及，然而在隔壁会遭遇什么可就不好说……\n矮人30级时将获得翻越一切高山的能力，在此之前也不得不像其他人一样使用攀爬工具，或是在狭窄的岩石缝隙中穿行，而这正是最为惊险的地方；在跨越边界时无法看见对面的落脚点，如果对面是山体，那么角色会被“挤压”到最近的空旷处，可当玩家想回去时，游戏又没这么好心了，必须翻过那块山体才能回去；虽然通常可以绕路从其他连通边界的地方回去，但要是遇上死胡同或是怪物占道，那就必须读卷轴回城（希望包里还剩几张没用完），位于火山的话，那角色脚底下很可能有一池岩浆，在返程卷轴生效前多半会先被活活烧成渣。\n\n前文曾提到过，每个服务器的游戏世界都是随机创造的，所以也没法按照LotR的地图直奔邪黑塔，先简单介绍一下我们的周边环境：\n布理镇的北面是片难以穿越的林子，显示为绿色的#，越过树林则会见到海洋，我就是在那边的海岸线附近遭遇了邪教徒，镇子西侧生成了一座火山（萌新吞噬者），想不到用什么恶毒的语言来攻击这玩意，后续经探索，里面除了岩浆和乱石之外别无他物，并非恐怖的末日火山；自东北向西南有一条大河，或许正是著名的烈酒河，跨过河流可以见到古代森林与一大片沙漠，那里还有机率刮沙尘暴。\n\n我们发现的首座其他城镇是“隐匿之城”——刚多林（Gondolin），至此可以确定游戏内的时间是在第一纪元了，当时我正在北部原野往东探索，走着走着突然发现了不少房屋与菜地，随即反应过来这是房区，那说明周围必定能够找到城镇，一番地毯式搜索后果然发现了刚多林，可惜我溜进来的位置是后门，否则从雄伟的城墙正门漫步至王之广场肯定更加震撼。\n刚多林里的商店全是基础类型，没有高级版的变体，不过数量上更多一些，食物店有整整3家，药水店也有2家，此外存在大量布理镇没有的服务点，像是草药治疗、变形恢复、物品研究、全背包鉴定……都相当实用；我们后面经常背一些神器来这里鉴别，因为某些进阶词条无法用普通的鉴定卷轴揭示，而*identify scroll*很难在基础卷轴店里见到，从黑市中花高价买又太过吃亏，到头来还是跑一趟刚多林实惠。\n\n\n有些让人难以理解的是，跟刚多林配套的地城居然是魔多，大概想表达对抗关系吧，我起初看名字还猜索伦在下面等人，最次也得是炎魔之首或者格劳龙，后来查了才发现这地方居然没有固定的关底boss。\n可这不代表魔多是好捏的软柿子，光是下去的第一层就有35级的强度，要知道在击败魔苟斯前，玩家能达到的等级上限才50级；发现这地方之后有几名群友兴冲冲地来练级，打算变强再把古冢岗底下那几堆神器捞回来，结果不幸遭遇了克苏鲁神话中的外神，当场被秒杀，祂吐一口邪垢的伤害都有515，比我满血开狂暴还高；再后来还听说有人遭遇了近七十级的天使，完全不是一个层次的生物，简直让人怀疑人生。\n\n\n埃兰迪尔之星和其余的几件神器，随着底层那只鬼魂的网络掉线，最终还是没能挽救回来；我之后去过-1750ft，虽然神器早八百年前就给刷新掉了，那一次的经历倒也算得上是创伤小组的光荣事迹。\n起因是ham忘记带灯又直接返回了古代森林的深处，如前文所述，这时要是有人用念力直接远程塞个光源就完事了，遗憾的是我们没人会这种操作，所以只能由创伤小组下去送灯送干粮。\n一路上遇到了三名头目：yeek king、东来者首领罗甘（Lorgan）、以及乌姆巴尔海盗的领袖之一，安加麦提（Angamaite of Umbar），这群人就算加上手底下的杂兵，对我来说也不是啥很难缠的对手，除了最后一位的名字把我吓得不轻，老眼昏花差点看成了乌顿（Udun）的安格玛巫王（Angmar），要不是没来得及跑就给对面砍死了，差点闹出大乌龙。\n\n罗甘给我掉了个能放火球术的护手，清理一些很弱又追不上的东西非常非常好用，比如野生果蝠和法师学徒，这些怪物几乎没威胁，但极其喜欢在玩家周围瞎晃悠，果蝠完全不攻击但一直远距离跟着，法师则时不时放个魔法恶心一下人，你冲过去他又瞬移到旁边继续骚扰。\n玩家附近有敌怪就无法快速移动，只能缓慢地一格一格走，由于速度不够追不上，通常只能靠进入其他楼层或是穿越区块边界来甩掉这些玩意，超级浪费时间，光是想起来就血压高，千人血书请求灭绝果蝠（1/1000）。\n尊贵的远程职业吐口痰就能把他们弄死，但智力为⑨的战士只能干瞪眼，除非你愿意喝瓶几千金币的速度药水就为了砍一只野生果蝠；自从有了火球护手，再也没被乱七八糟的玩意烦过，虽然伤害不高，放完一发还得等cd，但它能无限使用而且没有智力门槛，更没有蓝耗和施法失败率，真是爱死这护手了，赐名“炎拳”。\n\n好装备见多了也绝对不能放松警惕，去找ham的路上就遇见了几件烂到出奇的垃圾，比如这把魔古尔的Tomahawk（一种原始战斧），攻击和命中修正都是负数，拿上之后会让玩家感染恐怖的黑息，降低运气和最高生命上限，同时激怒周围的怪物，导致更加容易遭受围殴；不光如此，它属于严重诅咒的物品，必须用带&quot;*&quot;号的高级解咒卷轴才能从手中卸下，由于它还能重复诅咒自己，且在未解咒时无法丢弃，不小心捡起来的话又得斥巨资再到黑市买张卷轴。\n然而这玩意或许可以考虑给吸血鬼用来逆转，但还有一种无法攻击的Nothingness词缀，那是真正彻头彻尾的垃圾，吸血鬼都用不了，而且光放在包里就会妨碍玩家强化别的装备，让升级变成降级，所以出门在外一定要时刻记得鉴定。\n\n扯远了，把重点转回ham这边，成功救援之后我们回镇上补给了一趟，然后决定顺带把森林打穿，本想着两个人应该很轻松，可返回原位后刷新的地形是一大片空地，四处都是敌人，更不妙的是我们并没有传送到一起，不得不各自为战。\n我当时旁边围着一群猎犬，其中还有只不断狂吠的精英怪，这叫声引来了附近一座vault里的巨量怪物，右面还有几队不知道啥品种的半兽人也赶了过来；光是这些也还好，我借助地形仅需同时面对四五名左右的敌人，盔甲较厚也吃不了多少伤害，甚至有空发消息呼叫增援，问题在于未知法师远距离降低了我的理智，杀出重围后理智值仅剩73%。\n几名远处的法师被ham杀掉后我以为危机解除，便不打算撤退，返回了右下角的vault准备收拾些战利品，在靠近时不知道哪里有株反魔真菌一直在释放干扰立场，由于我根本不用魔法，没有丝毫心灵负担地就找了过去；结果眼中突然天旋地转开始出现幻觉，原来之前攻击角色理智的不是什么普通法师，而是一名高等级的敌对心灵工匠，并且在我突围后他没有跟上来被消灭，意识到这一点我立即对自己使用了传送法杖逃命，但仍然吃了两发灵能冲击，理智值下滑至了危险的25%，整个人离踏进鬼门关只差半步。\n\n\n好在我们命不该绝，虽然一个精神残疾一个血药喝完，最终还是成功的撤了回来，多亏之前舀了大量理智药剂存在酒馆，我灌上几口又重回巅峰，否则还得等牧师睡醒；尽管出师不利，但两名活阎王显然不肯放弃，毕竟也没遇见什么较为离谱的怪物，直接带上了更多的补给道具就下去报仇。\n清点着包里的理智药剂，我干脆绑定了快捷键，大不了硬抗精神伤害也要上去把那帮杀千刀的心灵系术士给剁了；阅读返程卷轴，回到森林深处，使用魔法地图，喝速度药水开狂暴，大杀四方一气呵成，然后……然后就没了，我们把整个楼层全探索完了都未能找到下去的楼梯，也没有再刷新出会施放精神攻击的敌怪。\n仔细一查，发现这层已经是森林的最底部了，而我们一直以为在倒数第二层，白折腾半天，甚至还攒着不少打算到最后关头再用的消耗品，像是启蒙和抗性药水之类的，就这样，我们稀里糊涂地完成了古代森林的攻略。\n\n并不过瘾的两名战狂准备回古冢岗一雪前耻，大致上是见到楼梯就下，直奔最底层，一路上顺风顺水，甚至没有精英怪和楼层头目拦路；可抵达-1750ft之后却受到了大群地狱猎犬的热烈欢迎，虽然它们的吐息喷我并不怎么疼，但背包里的各种药水却因为元素伤害而不断损坏，9瓶宝贵的理智药剂碎了5瓶，治疗和速度药水更是损失惨重，吓得我赶紧跑上楼把剩下的那4瓶塞给了队友。\n被底部一巴掌扇回来，我们还是决定在-1700ft逛逛得了，至少不用担心可能冒出来的尸妖王，一顿slash&amp;loot之后，我们发现了个闪光的字母E，ham先冲上去开怪，但不知道踩了陷阱还是直接被当场暴打，血量蹭蹭往下掉，几番周旋后药水还耗尽了；幸亏我们配合默契，他往旁边跑我往前面冲，成功把怪物拦在路口，那一瞬间简直泪流满面，我这名战士终于有前排站桩的机会了，之前经常是果蝠身体的队友飞太快，我在后面跟不上，或是对可怜的脆皮法师照顾不周，一个疏忽就只剩尸体。\n那个闪光字母是头火焰树人，也不知道它怎么没给自己烧干净，反正现在由我们送它上路，比较麻烦的地方在于攻击附加火焰属性，甚至站旁边都会被它的烈焰光环烧到，好在我有一件火抗披风，还可以激活能力提供双重抗性，打我跟挠痒痒似的；然而就像前文提到过，纵使双重抗性也无法阻止物品被破坏，我包里的药水劈里啪啦碎的像是放爆竹，卷轴也跟烧纸钱一样全成了灰。\n\n另外据受害人ham所述，这玩意还能降低玩家的属性，他几乎全属性全黄了，光是买各种对应的恢复药剂就得花几千金币，我的话比较幸运，之前的yeek king掉了件brigandine armour，提供各种属性维持，还加2点移动速度，我最后只损失了微乎其微的力量，完全无伤大雅。\n本来打完火焰树人就想让队友先回地表补给，结果我继续清怪的时候ham跟了一步，不小心踩到陷阱，刚恢复的血量又蹭蹭狂掉，果蝠身体虽然闪避高，速度也快，但hp上限不高，非常头疼于必定命中的陷阱；当时那叫一个惊险，我连忙回防让他赶紧recall，旁边源源不断有怪物涌来，还潜藏着成堆的陷阱，万幸，卷轴生效速度比死神的脚步快，ham成功撤退，不过这种你先走我殿后的感觉简直燃爆了。\n对准怪群大开Taunt拉仇恨！You shall not pass！\n\n这种光荣事迹就应该记在创伤小组的历史上，配享群太庙；不过总的来讲，最终收获是：被陷阱扎到半死的蝙蝠一只，烤得外酥里嫩的矮子一名。\n地上那些垃圾我顶着陷阱收拾了一下，反正重甲可以肆无忌惮，除了几个戒指以外没啥能看的，火焰树人甚至什么都不掉，还真烧了个一干二净；当天的冒险之旅到返程完就结束了，我们后来一起下过魔多，那条魔像腿就是两个人在魔多的时候挖出来的，ham负责冲在前面开无双，我由于负重高就在后面捡垃圾顺便挖矿。\n在魔多也遇上过危急情况，不过是有惊无险的那种，当时我不小心踩到了活板门，然后直接掉到下方楼层的boss脸上，他叫乌法斯特（Ulfast），是东来者族长的儿子，不强，比较轻松的干掉了，印象中没有给好东西；但刚摔下楼又看到闪光头目的时候真冷汗下来了，毕竟来不及仔细看是谁，就结果而言，只在前两层溜达，felling又正常的话，应该不至于撞见古神什么的，深处就不好说了。\n\n\n风险越大回报也就越大，魔多虽然不是啥好地方，但各种板甲龙皮甲随处可见，像Kolla这种高级袍子，哪怕是无前缀的基础款也自带一堆修正，卖掉能值50k金币；地板上甚至可以捡到野生的属性药水，喝下去永久增加角色的对应属性，放黑市里要卖近十万一瓶。\n实力足够的话，在这也能爽快练级，不死族和亡灵较少，经常能见到大群的半兽人与巨魔，砍翻一屏幕的o相当解压，德鲁伊和法师放毒更是一死死一片，由于第一层就足够危险，非常适合stair-dance，指的是在楼梯口不断上下刷新地形，挑选最近最稳妥的怪物进行清理，一旦感觉不对便可立马上楼回地表；但或许值得注意，我在这遇到过几乎整层都是岩浆的地形，即使能够飞行，在岩浆上方仍然会遭受伤害，多半需要有火焰免疫才能安全经过。\n\n\n再后来的那些日子里，我主要都在为绘制地图而探索世界——\n旅途总体上是平淡乏味的，听着音乐穿过大片大片的旷野，飞越海洋，翻过高山，作为大自然的回报，旅行者也能欣赏到一些美景，像是林中小湖、沙漠绿洲、海洋中的灰森林岛屿等等，尤其在遭受过精神创伤后，这些美丽的事物格外令人治愈。\n\n\n第二座被发现的城镇是LotR中著名的白城，但此时它还不叫米那斯提力斯，而叫米那斯阿诺尔（Minas Anor），有些混乱的是，ToMENet中已经出现了魔古尔武器等描述文本，而白城的姊妹城市米那斯伊希尔（Minas Ithil）实际上要在第三纪元才被索伦攻占，变成戒灵要塞，改名米那斯魔古尔（Minas Morgul），也许游戏里是什么平行时空吧，否则我们不可能在见到这些的同时看到刚多林，还有精灵矮人一家亲之类的。\n白城展现于世人面前的过程也历经艰辛，它被一座座火山和高峰环绕的严丝合缝，房区在火山外围，我第一趟费了好大劲，几次重度烫伤都没能穿过那些高耸的尖峰，直到30级获得种族攀岩能力才最终得以如愿。\n从正门进入，观赏完城内美景后，我去执政厅觐见了一下当时的国王，一看人名Aragorn Dunadan懵了，大步佬不应该没出生吗，怎么白城还没改名就已经登基了？后续查阅得知：Aragorn是辛达语，意为“可敬的国王”，由表示荣耀的前缀ara和“可敬的”gorn组成，Dunadan则可以指“杜内丹”这一人种，所以王座上坐着的大概率不会是我们熟知的阿拉松之子，阿拉贡。\n此处对应的地城是The Paths of the Dead，死者之路，里面几乎全是亡灵和不死族，据说还有那兹古尔，攻略起来应该会被各种负面状态烦得不轻，另外光是第一层就有40级的强度，我下楼梯瞅了一眼立马跑了，根本惹不起。\n\n\n抛开到处都有的基础商店，白城这最为特殊的是有一家高级黑市，里面出售各种顶级的武器盔甲，威力惊人的法杖，甚至还有不同学派的法术全本，这些东西随便弄到一件咱都得抱着傻笑一晚上；然而它们的售价却也那么高不可攀，统统都是六位数起步，随便一本法典都要近四十万的金币，把我卖了估计都没这么多钱。\n服务点方面基本和刚多林相同，不过这里有座赌场，应该是游戏中唯一的一座，里面除了摇灌铅骰子猜点数之外，居然还可以下围棋，不过这种围棋变体的棋盘只有9x9大小；对手的难度等级分为8档，尽管ToMENet由于开源的关系一直保持着更新，但棋艺方面肯定比不过近几年的机器学习，而且浏览到玩家手册上一份零几年的围棋入门视频时，我更加断定了这种猜想。\n群里非常热烈地讨论了要用哪家的ai模型，然后……然后大家就忘了这茬，可能是我们道德素质太好，不忍心把赌场刷破产，最后连ascii的下棋界面长啥样都没见着（其实是不小心忘了）。\n\n\n人有悲欢离合，月有阴晴圆缺，再长的旅途也终将抵达尽头，我的故事到这里不得不告一段落了。\n那是在野外遇到的一大群Orge，虽然我个人更习惯将其译作“食人魔”，但这些家伙并不是比尔博在森林里遭遇的那三只食人妖，那三名怪物在《霍比特人》一书的原文中使用的是“Troll”一词，也就是我们现在常说的“巨魔”；纠结名称也不是很重要，只要分清我遇上的是Orge，老比尔博遇上的是Troll，而Ogre不会在阳光下变成石头就行。\n\n我低估了敌人的危险程度，也高估了自己的风险承受能力，对着这些食人魔就冲了上去，殊不知这却葬送了自己的后路，四面八方的攻击如潮水般涌来，血量急速下滑让我开始感觉不对劲，想后撤穿过地图边界离开，但背后早已被敌人包围得水泄不通。\n曾经无数次救过我的药水，在这次却成为了间接的死亡原因，因为在被包围的情况下，角色行动一轮，周围将存在整整8名敌人进行近战反击，更加不妙的是，这群食人魔还有大祭司远程施放魔法，以及前排偷窃戒指等装备的盗贼；致命伤治疗药水的回复量抵不上每轮损失的血量，根本无济于事，习惯性地不停喝药让我错失了最佳的逃跑时机，等我想起使用传送法杖时，hp早已岌岌可危。\n最后关头我记得有按过传送的快捷键，可不知道是网络延迟还是同时喝药导致操作冲突被吞，我最终未能脱困，不幸殒命当场。\n\n双拳难敌四手，更何况是十几名食人魔，由于即时制，事情从发生到结束不过短短十秒，很多东西没有反应过来，我便因为肉体损毁变成了鬼魂；虽然hp耗尽，但我这名3命角色还一次复活机会都没用过，故事本不该在此落幕，但游戏为了防止鬼魂被原先的敌人继续杀死，玩家死亡后会随机转移到同地图的其他位置，而我旁边不幸的出现了一头远古白龙。\n理解白色的字母D代表什么之后，我试图逃跑，但这条远古白龙的移动速度显然更快，何况它还能够喷吐超远距离的龙息，转眼间，我便再次失去了意识……\n\n“救命啊，训练塔里有好多老鼠”\n“来楼梯口蹲着揍老鼠”\n“啊？有什么怪把我的法术书烧了”\n“哪里有蠕虫啊——我这里一只都看不到”\n“二楼右下角的房间里有”\n“有人要这把剑吗，1d5”\n。。。。。。\n“四人小队，出发！”\n“楼梯在这里”\n“被麻痹了”\n“等一下队友过来”\n“我来啦”\n“&gt;&gt;”\n\n“法杖储能快没了，有没有人帮忙充一下”\n“我可以提供免费鉴定和充能”\n“把法杖彻底放干好像就不会爆”\n“下次试试”\n“（我也充爆过”\n“反正都不是什么值钱的杖”\n。。。。。。\n“ham ham，me dead”\n“u dead, dead where”\n“south of the volcanic hall”\n“aww”\n#dig#dig\n“found a vault，f**k wiz and volcano”\n\n“大灯哥画个果蝠”\n#SuperBulb瞬间出图\n“传神”\n“有艺术的”\n“用4090生成37000张表情包”\n“大灯哥拿到了4090脚就离开了地后边忘了”\n\n\n“imsb”\n“忘记带灯了”\n“还有武器”\n“ahhhh”\n“巧了，我下来也是”\n“帮我点个灯”\n\n“怎么房区还能刷果蝠，这不得给烦死”\n“业主呢，都出来维一下权”\n“什么业主，物业”\n“78物业，我在房区杀过兽人小队”\n“bree治安不是很好”\n“房子都建水里了”\n“有人要变银色小老鼠吗”\n\n“捡到张召唤死灵的卷轴，能拿来刷经验吗？”\n“应该可以（”\n“召唤怪物卷轴也刷经验”\n“就是 死灵可能会抽经验”\n“我刚刚差点用召唤杖把自己弄死”\n“出的东西很恐怖”\n“最好在走廊读，或者墙角”\n“死灵没脑子应该没法师吧”\n“巫妖也是死灵”\n“草，对哦”\n#捏着传送法杖在村长旁边读了然后发现安全区不能召唤怪物\n\n“v我50k”\n“等我把我的神器弓卖了就有了”\n“再去赚5k，hxh！”\n*Cap face\n*Whip*\n“唉，资本家”\n=。=\n“现在就给我去赚钱！”\n“v你5k先”\n\n“喝酒伤身，明儿见”\n#You notice Shinie the bat trying to steal from you！\n#Shinie the bat is seized by the guards and thrown into jail！\n“opz”\n“哈哈哈哈哈嗝”\n“等你出狱v你100”\n。。。。。。\n#翡翠鸟光速出狱买了酒\n“喝，你为什么不喝”\n“都给你买了一瓶了！”\n“喝酒掉san”\n“san掉没了脑死亡变植物人”\n“那这个酒有什么用”\n“谁喝谁倒霉”\n\n“happy”\n“i got slime mold juice”\n“want one？”\n“of course”\n“chicken nugget juice”\n“here”\n“that was a delicious slime mold”\n“thx，iden +1”\n\n\n无数回忆掠过脑海，但一切都结束了，眼前仿佛出现了远古白龙的寒霜吐息，将我的灵魂灼烧殆尽。\n哦，没错，我为了绘制地图走得太深，魂飞魄散无法复活了，此处的危险程度相当于地牢的51级，太过贪婪招致自身灭亡，某种意义上也是很符合矮人精神的死法。\n死亡时得分为168065，等级由于被吸掉一级，变成灵体又降了两级，最终显示为31级；另外由于鬼魂状态，没能在记录文件中留下装备信息，略有些遗憾，名下绝大部分财产都因物主死亡而烟消云散，仅剩的少量遗物放在“公厕”里留给了队友，这也是我最后能尽的绵薄之力。\n\n\n在生命活动终止前，我传回了最后的几张图像，并将它们拼在了之前测绘的地图上，往后的空白就要靠队友们填充了。\n起先为了搞清世界的具体大小，我将东南西北方向都探索至了尽头，然后再开始逐渐填充四个象限，所以最终留下的地图像个十字墓碑，或许我的归宿在这时就早已注定。\n\n\n逝者已矣，生者如斯，这只是些关于不知名探索者的事迹，至于成功讨伐魔苟斯凯旋而归的故事，则是一段应当由其他生还者讲述的传奇伟业了。\n这里长眠着刺猬，它为了探索广阔的世界而牺牲\n不能与大家一起挑战米尔寇了很抱歉，但那具魔像的残骸希望你们能拼好\n愿后世旅人的路途不再充满迷雾\n","plink":"http://www.ephesus.top/files/tomenet/BoMENet-arc/"},{"title":"","date":"2024-06-21T03:20:48.443Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.443Z","content":"Summarization\nNote: The process and results below are presented in our paper Bottom-Up Abstractive Summarization. Please consider citing it if you follow these instructions.\n1234567@inproceedings&#123;gehrmann2018bottom,  title=&#123;Bottom-Up Abstractive Summarization&#125;,  author=&#123;Gehrmann, Sebastian and Deng, Yuntian and Rush, Alexander&#125;,  booktitle=&#123;Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing&#125;,  pages=&#123;4098--4109&#125;,  year=&#123;2018&#125;&#125;\nThis document describes how to replicate summarization experiments on the CNN-DM and gigaword datasets using OpenNMT-py.\nIn the following, we assume access to a tokenized form of the corpus split into train/valid/test set. You can find the data here.\nAn example article-title pair from Gigaword should look like this:\nInput\naustralia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .\nOutput\naustralian current account deficit narrows sharply\nPreprocessing the data\nSince we are using copy-attention [1] in the model, we need to preprocess the dataset such that source and target are aligned and use the same dictionary. This is achieved by using the options dynamic_dict and share_vocab.\nWe additionally turn off truncation of the source to ensure that inputs longer than 50 words are not truncated.\nFor CNN-DM we follow See et al. [2] and additionally truncate the source length at 400 tokens and the target at 100. We also note that in CNN-DM, we found models to work better if the target surrounds sentences with tags such that a sentence looks like &lt;t&gt; w1 w2 w3 . &lt;/t&gt;. If you use this formatting, you can remove the tags after the inference step with the commands sed -i 's/ &lt;\\/t&gt;//g' FILE.txt and sed -i 's/&lt;t&gt; //g' FILE.txt.\nCommand used:\n(1) CNN-DM\n123456789101112onmt_preprocess -train_src data/cnndm/train.txt.src \\                -train_tgt data/cnndm/train.txt.tgt.tagged \\                -valid_src data/cnndm/val.txt.src \\                -valid_tgt data/cnndm/val.txt.tgt.tagged \\                -save_data data/cnndm/CNNDM \\                -src_seq_length 10000 \\                -tgt_seq_length 10000 \\                -src_seq_length_trunc 400 \\                -tgt_seq_length_trunc 100 \\                -dynamic_dict \\                -share_vocab \\                -shard_size 100000\n(2) Gigaword\n123456789onmt_preprocess -train_src data/giga/train.article.txt \\                -train_tgt data/giga/train.title.txt \\                -valid_src data/giga/valid.article.txt \\                -valid_tgt data/giga/valid.title.txt \\                -save_data data/giga/GIGA \\                -src_seq_length 10000 \\                -dynamic_dict \\                -share_vocab \\                -shard_size 100000\nTraining\nThe training procedure described in this section for the most part follows parameter choices and implementation similar to that of See et al. [2]. We describe notable options in the following list:\n\ncopy_attn: This is the most important option, since it allows the model to copy words from the source.\nglobal_attention mlp: This makes the model use the  attention mechanism introduced by Bahdanau et al. [3] instead of that by Luong et al. [4] (global_attention dot).\nshare_embeddings: This shares the word embeddings between encoder and decoder. This option drastically decreases the number of parameters a model has to learn. We did not find this option to helpful, but you can try it out by adding it to the command below.\nreuse_copy_attn: This option reuses the standard attention as copy attention. Without this, the model learns an additional attention that is only used for copying.\ncopy_loss_by_seqlength: This modifies the loss to divide the loss of a sequence by the number of tokens in it. In practice, we found this to generate longer sequences during inference. However, this effect can also be achieved by using penalties during decoding.\nbridge: This is an additional layer that uses the final hidden state of the encoder as input and computes an initial hidden state for the decoder. Without this, the decoder is initialized with the final hidden state of the encoder directly.\noptim adagrad: Adagrad outperforms SGD when coupled with the following option.\nadagrad_accumulator_init 0.1: PyTorch does not initialize the accumulator in adagrad with any values. To match the optimization algorithm with the Tensorflow version, this option needs to be added.\n\nWe are using using a 128-dimensional word-embedding, and 512-dimensional 1 layer LSTM. On the encoder side, we use a bidirectional LSTM (brnn), which means that the 512 dimensions are split into 256 dimensions per direction.\nWe additionally set the maximum norm of the gradient to 2, and renormalize if the gradient norm exceeds this value and do not use any dropout.\ncommands used:\n(1) CNN-DM\n12345678910111213141516171819202122onmt_train -save_model models/cnndm \\           -data data/cnndm/CNNDM \\           -copy_attn \\           -global_attention mlp \\           -word_vec_size 128 \\           -rnn_size 512 \\           -layers 1 \\           -encoder_type brnn \\           -train_steps 200000 \\           -max_grad_norm 2 \\           -dropout 0. \\           -batch_size 16 \\           -valid_batch_size 16 \\           -optim adagrad \\           -learning_rate 0.15 \\           -adagrad_accumulator_init 0.1 \\           -reuse_copy_attn \\           -copy_loss_by_seqlength \\           -bridge \\           -seed 777 \\           -world_size 2 \\           -gpu_ranks 0 1\n(2) CNN-DM Transformer\nThe following script trains the transformer model on CNN-DM\n12345678910111213141516171819202122232425262728onmt_train -data data/cnndm/CNNDM \\           -save_model models/cnndm \\           -layers 4 \\           -rnn_size 512 \\           -word_vec_size 512 \\           -max_grad_norm 0 \\           -optim adam \\           -encoder_type transformer \\           -decoder_type transformer \\           -position_encoding \\           -dropout 0\\.2 \\           -param_init 0 \\           -warmup_steps 8000 \\           -learning_rate 2 \\           -decay_method noam \\           -label_smoothing 0.1 \\           -adam_beta2 0.998 \\           -batch_size 4096 \\           -batch_type tokens \\           -normalization tokens \\           -max_generator_batches 2 \\           -train_steps 200000 \\           -accum_count 4 \\           -share_embeddings \\           -copy_attn \\           -param_init_glorot \\           -world_size 2 \\           -gpu_ranks 0 1\n(3) Gigaword\nGigaword can be trained equivalently. As a baseline, we show a model trained with the following command:\n12345onmt_train -data data/giga/GIGA \\           -save_model models/giga \\           -copy_attn \\           -reuse_copy_attn \\           -train_steps 200000\nInference\nDuring inference, we use beam-search with a beam-size of 10. We also added specific penalties that we can use during decoding, described in the following.\n\nstepwise_penalty: Applies penalty at every step\ncoverage_penalty summary: Uses a penalty that prevents repeated attention to the same source word\nbeta 5: Parameter for the Coverage Penalty\nlength_penalty wu: Uses the Length Penalty by Wu et al.\nalpha 0.8: Parameter for the Length Penalty.\nblock_ngram_repeat 3: Prevent the model from repeating trigrams.\nignore_when_blocking &quot;.&quot; &quot;&lt;/t&gt;&quot; &quot;&lt;t&gt;&quot;: Allow the model to repeat trigrams with the sentence boundary tokens.\n\ncommands used:\n(1) CNN-DM\n12345678910111213141516onmt_translate -gpu X \\               -batch_size 20 \\               -beam_size 10 \\               -model models/cnndm... \\               -src data/cnndm/test.txt.src \\               -output testout/cnndm.out \\               -min_length 35 \\               -verbose \\               -stepwise_penalty \\               -coverage_penalty summary \\               -beta 5 \\               -length_penalty wu \\               -alpha 0.9 \\               -verbose \\               -block_ngram_repeat 3 \\               -ignore_when_blocking &quot;.&quot; &quot;&lt;/t&gt;&quot; &quot;&lt;t&gt;&quot;\nEvaluation\nCNN-DM\nTo evaluate the ROUGE scores on CNN-DM, we extended the pyrouge wrapper with additional evaluations such as the amount of repeated n-grams (typically found in models with copy attention), found here. The repository includes a sub-repo called pyrouge. Make sure to clone the code with the git clone --recurse-submodules https://github.com/sebastianGehrmann/rouge-baselines command to check this out as well and follow the installation instructions on the pyrouge repository before calling this script.\nThe installation instructions can be found here. Note that on MacOS, we found that the pointer to your perl installation in line 1 of pyrouge/RELEASE-1.5.5/ROUGE-1.5.5.pl might be different from the one you have installed. A simple fix is to change this line to #!/usr/local/bin/perl -w if it fails.\nIt can be run with the following command:\n1python baseline.py -s testout/cnndm.out -t data/cnndm/test.txt.tgt.tagged -m sent_tag_verbatim -r\nThe sent_tag_verbatim option strips &lt;t&gt; and &lt;/t&gt; tags around sentences - when a sentence previously was &lt;t&gt; w w w w . &lt;/t&gt;, it becomes w w w w ..\nGigaword\nFor evaluation of large test sets such as Gigaword, we use the a parallel python wrapper around ROUGE, found here.\ncommand used:\nfiles2rouge giga.out test.title.txt --verbose\nScores and Models\nCNN-DM\n\n\nModel Type\nModel\nR1 R\nR1 P\nR1 F\nR2 R\nR2 P\nR2 F\nRL R\nRL P\nRL F\n\n\n\n\nPointer-Generator + Coverage [2]\nlink\n39.05\n43.02\n39.53\n17.16\n18.77\n17.28\n35.98\n39.56\n36.38\n\n\nPointer-Generator [2]\nlink\n37.76\n37.60\n36.44\n16.31\n16.12\n15.66\n34.66\n34.46\n33.42\n\n\nOpenNMT BRNN  (1 layer, emb 128, hid 512)\nlink\n40.90\n40.20\n39.02\n17.91\n17.99\n17.25\n37.76\n37.18\n36.05\n\n\nOpenNMT BRNN  (1 layer, emb 128, hid 512, shared embeddings)\nlink\n38.59\n40.60\n37.97\n16.75\n17.93\n16.59\n35.67\n37.60\n35.13\n\n\nOpenNMT BRNN (2 layer, emb 256, hid 1024)\nlink\n40.41\n40.94\n39.12\n17.76\n18.38\n17.35\n37.27\n37.83\n36.12\n\n\nOpenNMT Transformer\nlink\n40.31\n41.09\n39.25\n17.97\n18.46\n17.54\n37.41\n38.18\n36.45\n\n\nGigaword\n\n\nModel Type\nModel\nR1 R\nR1 P\nR1 F\nR2 R\nR2 P\nR2 F\nRL R\nRL P\nRL F\n\n\n\n\nOpenNMT, no penalties\nlink\n?\n?\n35.51\n?\n?\n17.35\n?\n?\n33.17\n\n\nReferences\n[1] Vinyals, O., Fortunato, M. and Jaitly, N., 2015. Pointer Network. NIPS\n[2] See, A., Liu, P.J. and Manning, C.D., 2017. Get To The Point: Summarization with Pointer-Generator Networks. ACL\n[3] Bahdanau, D., Cho, K. and Bengio, Y., 2014. Neural machine translation by jointly learning to align and translate. ICLR\n[4] Luong, M.T., Pham, H. and Manning, C.D., 2015. Effective approaches to attention-based neural machine translation. EMNLP\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/Summarization/"},{"title":"","date":"2024-06-21T03:20:47.973Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:47.973Z","content":"Translation\nThe example below uses the Moses tokenizer (http://www.statmt.org/moses/) to prepare the data and the moses BLEU script for evaluation. This example if for training for the WMT’16 Multimodal Translation task (http://www.statmt.org/wmt16/multimodal-task.html).\nStep 0. Download the data.\n1234mkdir -p data/multi30kwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz &amp;&amp;  tar -xf training.tar.gz -C data/multi30k &amp;&amp; rm training.tar.gzwget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz &amp;&amp; tar -xf validation.tar.gz -C data/multi30k &amp;&amp; rm validation.tar.gzwget http://www.quest.dcs.shef.ac.uk/wmt17_files_mmt/mmt_task1_test2016.tar.gz &amp;&amp; tar -xf mmt_task1_test2016.tar.gz -C data/multi30k &amp;&amp; rm mmt_task1_test2016.tar.gz\nStep 1. Preprocess the data.\n123for l in en de; do for f in data/multi30k/*.$l; do if [[ &quot;$f&quot; != *&quot;test&quot;* ]]; then sed -i &quot;$ d&quot; $f; fi;  done; donefor l in en de; do for f in data/multi30k/*.$l; do perl tools/tokenizer.perl -a -no-escape -l $l -q  &lt; $f &gt; $f.atok; done; doneonmt_preprocess -train_src data/multi30k/train.en.atok -train_tgt data/multi30k/train.de.atok -valid_src data/multi30k/val.en.atok -valid_tgt data/multi30k/val.de.atok -save_data data/multi30k.atok.low -lower\nStep 2. Train the model.\n1onmt_train -data data/multi30k.atok.low -save_model multi30k_model -gpu_ranks 0\nStep 3. Translate sentences.\n1onmt_translate -gpu 0 -model multi30k_model_*_e13.pt -src data/multi30k/test2016.en.atok -tgt data/multi30k/test2016.de.atok -replace_unk -verbose -output multi30k.test.pred.atok\nAnd evaluate\n1perl tools/multi-bleu.perl data/multi30k/test2016.de.atok &lt; multi30k.test.pred.atok\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/extended/"},{"title":"","date":"2024-06-21T03:20:48.023Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.023Z","content":"Image to Text\nA deep learning-based approach to learning the image-to-text conversion, built on top of the OpenNMT system. It is completely data-driven, hence can be used for a variety of image-to-text problems, such as image captioning, optical character recognition and LaTeX decompilation.\nTake LaTeX decompilation as an example, given a formula image:\n\nThe goal is to infer the LaTeX source that can be compiled to such an image:\n1d s _ &#123; 1 1 &#125; ^ &#123; 2 &#125; = d x ^ &#123; + &#125; d x ^ &#123; - &#125; + l _ &#123; p &#125; ^ &#123; 9 &#125; \\frac &#123; p _ &#123; - &#125; &#125; &#123; r ^ &#123; 7 &#125; &#125; \\delta ( x ^ &#123; - &#125; ) d x ^ &#123; - &#125; d x ^ &#123; - &#125; + d x _ &#123; 1 &#125; ^ &#123; 2 &#125; + \\; \\cdots \\; + d x _ &#123; 9 &#125; ^ &#123; 2 &#125; \nThe paper [What You Get Is What You See: A Visual Markup Decompiler] provides more technical details of this model.\nDependencies\n\ntorchvision: conda install torchvision\nPillow: pip install Pillow\n\nQuick Start\nTo get started, we provide a toy Math-to-LaTex example. We assume that the working directory is OpenNMT-py throughout this document.\nIm2Text consists of four commands:\n\nDownload the data.\n\n1wget -O data/im2text.tgz http://lstm.seas.harvard.edu/latex/im2text_small.tgz; tar zxf data/im2text.tgz -C data/\n\nPreprocess the data.\n\n123456789onmt_preprocess -data_type img \\                -src_dir da](../images/ \\                -train_src data/im2text/src-train.txt \\                -train_tgt data/im2text/tgt-train.txt -valid_src data/im2text/src-val.txt \\                -valid_tgt data/im2text/tgt-val.txt -save_data data/im2text/demo \\                -tgt_seq_length 150 \\                -tgt_words_min_frequency 2 \\                -shard_size 500 \\                -image_channel_size 1\n\nTrain the model.\n\n12345678910onmt_train -model_type img \\           -data data/im2text/demo \\           -save_model demo-model \\           -gpu_ranks 0 \\           -batch_size 20 \\           -max_grad_norm 20 \\           -learning_rate 0.1 \\           -word_vec_size 80 \\           -encoder_type brnn \\           -image_channel_size 1\n\nTranslate the images.\n\n123456789onmt_translate -data_type img \\               -model demo-model_acc_x_ppl_x_e13.pt \\               -src_dir da](../images \\               -src data/im2text/src-test.txt \\               -output pred.txt \\               -max_length 150 \\               -beam_size 5 \\               -gpu 0 \\               -verbose\nThe above dataset is sampled from the im2latex-100k-dataset. We provide a trained model [link] on this dataset.\nOptions\n\n\n-src_dir: The directory containing the images.\n\n\n-train_tgt: The file storing the tokenized labels, one label per line. It shall look like:\n\n\n1234&lt;label0_token0&gt; &lt;label0_token1&gt; ... &lt;label0_tokenN0&gt;&lt;label1_token0&gt; &lt;label1_token1&gt; ... &lt;label1_tokenN1&gt;&lt;label2_token0&gt; &lt;label2_token1&gt; ... &lt;label2_tokenN2&gt;...\n\n-train_src: The file storing the paths of the images (relative to src_dir).\n\n1234&lt;image0_path&gt;&lt;image1_path&gt;&lt;image2_path&gt;...\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/im2text/"},{"title":"","date":"2024-06-21T03:20:48.033Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.033Z","content":"… toctree::\n:maxdepth: 2\nindex.md\nquickstart.md\nextended.md\nThis portal provides a detailled documentation of the OpenNMT toolkit. It describes how to use the PyTorch project and how it works.\nInstallation\n1. Install PyTorch\n2. Clone the OpenNMT-py repository:\n12git clone https://github.com/OpenNMT/OpenNMT-pycd OpenNMT-py\n3. Install required libraries\n1pip install -r requirements.txt\nAnd you are ready to go! Take a look at the quickstart to familiarize yourself with the main training workflow.\nAlternatively you can use Docker to install with nvidia-docker. The main Dockerfile is included\nin the root directory.\nCitation\nWhen using OpenNMT for research please cite our\nOpenNMT technical report\n123456789101112@inproceedings&#123;opennmt,  author    = &#123;Guillaume Klein and               Yoon Kim and               Yuntian Deng and               Jean Senellart and               Alexander M. Rush&#125;,  title     = &#123;OpenNMT: Open-Source Toolkit for Neural Machine Translation&#125;,  booktitle = &#123;Proc. ACL&#125;,  year      = &#123;2017&#125;,  url       = &#123;https://doi.org/10.18653/v1/P17-4012&#125;,  doi       = &#123;10.18653/v1/P17-4012&#125;&#125;\nAdditional resources\nYou can find additional help or tutorials in the following resources:\n\nGitter channel\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/"},{"title":"","date":"2024-06-21T03:20:48.143Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.143Z","content":"Overview\nThis portal provides a detailed documentation of the OpenNMT toolkit. It describes how to use the PyTorch project and how it works.\nInstallation\nInstall from pip:\nInstall OpenNMT-py from pip:\n1pip install OpenNMT-py\nor from the sources:\n123git clone https://github.com/OpenNMT/OpenNMT-py.gitcd OpenNMT-pypython setup.py install\n(Optionnal) some advanced features (e.g. working audio, image or pretrained models) requires extra packages, you can install it with:\n1pip install -r requirements.opt.txt\nAnd you are ready to go! Take a look at the quickstart to familiarize yourself with the main training workflow.\nAlternatively you can use Docker to install with nvidia-docker. The main Dockerfile is included\nin the root directory.\nCitation\nWhen using OpenNMT for research please cite our\nOpenNMT technical report\n123456789101112@inproceedings&#123;opennmt,  author    = &#123;Guillaume Klein and               Yoon Kim and               Yuntian Deng and               Jean Senellart and               Alexander M. Rush&#125;,  title     = &#123;OpenNMT: Open-Source Toolkit for Neural Machine Translation&#125;,  booktitle = &#123;Proc. ACL&#125;,  year      = &#123;2017&#125;,  url       = &#123;https://doi.org/10.18653/v1/P17-4012&#125;,  doi       = &#123;10.18653/v1/P17-4012&#125;&#125;\nAdditional resources\nYou can find additional help or tutorials in the following resources:\n\n\nGitter channel\n\n\nForum\n\n\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/main/"},{"title":"","date":"2024-06-21T03:20:48.343Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.343Z","content":"Quickstart\nStep 1: Preprocess the data\n1onmt_preprocess -train_src data/src-train.txt -train_tgt data/tgt-train.txt -valid_src data/src-val.txt -valid_tgt data/tgt-val.txt -save_data data/demo\nWe will be working with some example data in data/ folder.\nThe data consists of parallel source (src) and target (tgt) data containing one sentence per line with tokens separated by a space:\n\nsrc-train.txt\ntgt-train.txt\nsrc-val.txt\ntgt-val.txt\n\nValidation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.\n1234$ head -n 3 data/src-train.txtIt is not acceptable that , with the help of the national bureaucracies , Parliament &amp;apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .Federal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .&amp;quot; Two soldiers came up to me and told me that if I refuse to sleep with them , they will kill me . They beat me and ripped my clothes .\nStep 2: Train the model\n1onmt_train -data data/demo -save_model demo-model\nThe main train command is quite simple. Minimally it takes a data file\nand a save file.  This will run the default model, which consists of a\n2-layer LSTM with 500 hidden units on both the encoder/decoder.\nIf you want to train on GPU, you need to set, as an example:\nCUDA_VISIBLE_DEVICES=1,3\n-world_size 2 -gpu_ranks 0 1 to use (say) GPU 1 and 3 on this node only.\nTo know more about distributed training on single or multi nodes, read the FAQ section.\nStep 3: Translate\n1onmt_translate -model demo-model_XYZ.pt -src data/src-test.txt -output pred.txt -replace_unk -verbose\nNow you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into pred.txt.\nNote:\nThe predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets! For example you can download millions of parallel sentences for translation or summarization.\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/quickstart/"},{"title":"","date":"2024-06-21T03:20:48.423Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:48.423Z","content":"Speech to Text\nA deep learning-based approach to learning the speech-to-text conversion, built on top of the OpenNMT system.\nGiven raw audio, we first apply short-time Fourier transform (STFT), then apply Convolutional Neural Networks to get the source features. Based on this source representation, we use an LSTM decoder with attention to produce the text character by character.\nDependencies\n\ntorchaudio: sudo apt-get install -y sox libsox-dev libsox-fmt-all; pip install git+https://github.com/pytorch/audio\nlibrosa: pip install librosa\n\nQuick Start\nTo get started, we provide a toy speech-to-text example. We assume that the working directory is OpenNMT-py throughout this document.\n\nDownload the data.\n\n1wget -O data/speech.tgz http://lstm.seas.harvard.edu/latex/speech.tgz; tar zxf data/speech.tgz -C data/\n\nPreprocess the data.\n\n1onmt_preprocess -data_type audio -src_dir data/speech/an4_dataset -train_src data/speech/src-train.txt -train_tgt data/speech/tgt-train.txt -valid_src data/speech/src-val.txt -valid_tgt data/speech/tgt-val.txt -shard_size 300 -save_data data/speech/demo\n\nTrain the model.\n\n1onmt_train -model_type audio -enc_rnn_size 512 -dec_rnn_size 512 -audio_enc_pooling 1,1,2,2 -dropout 0 -enc_layers 4 -dec_layers 1 -rnn_type LSTM -data data/speech/demo -save_model demo-model -global_attention mlp -gpu_ranks 0 -batch_size 8 -optim adam -max_grad_norm 100 -learning_rate 0.0003 -learning_rate_decay 0.8 -train_steps 100000\n\nTranslate the speechs.\n\n1onmt_translate -data_type audio -model demo-model_acc_x_ppl_x_e13.pt -src_dir data/speech/an4_dataset -src data/speech/src-val.txt -output pred.txt -gpu 0 -verbose\nOptions\n\n\n-src_dir: The directory containing the audio files.\n\n\n-train_tgt: The file storing the tokenized labels, one label per line. It shall look like:\n\n\n1234&lt;label0_token0&gt; &lt;label0_token1&gt; ... &lt;label0_tokenN0&gt;&lt;label1_token0&gt; &lt;label1_token1&gt; ... &lt;label1_tokenN1&gt;&lt;label2_token0&gt; &lt;label2_token1&gt; ... &lt;label2_tokenN2&gt;...\n\n-train_src: The file storing the paths of the audio files (relative to src_dir).\n\n1234&lt;speech0_path&gt;&lt;speech1_path&gt;&lt;speech2_path&gt;...\n\nsample_rate: Sample rate. Default: 16000.\nwindow_size: Window size for spectrogram in seconds. Default: 0.02.\nwindow_stride: Window stride for spectrogram in seconds. Default: 0.01.\nwindow: Window type for spectrogram generation. Default: hamming.\n\nAcknowledgement\nOur preprocessing and CNN encoder is adapted from deepspeech.pytorch.\n","plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/机器翻译/OpenNMT-py/docs/source/speech2text/"}]