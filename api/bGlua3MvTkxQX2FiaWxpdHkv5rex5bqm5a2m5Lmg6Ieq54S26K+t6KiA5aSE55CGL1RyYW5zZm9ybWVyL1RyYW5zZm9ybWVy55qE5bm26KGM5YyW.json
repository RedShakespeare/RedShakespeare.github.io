{"title":"","date":"2024-06-21T01:54:07.249Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2022-08-13T11:56:31.000Z","content":"<h3 id=\"transformer的并行化\">Transformer的并行化<a title=\"#transformer的并行化\" href=\"#transformer的并行化\"></a></h3>\n<h4 id=\"正文\">正文<a title=\"#正文\" href=\"#正文\"></a></h4>\n<p>本文主要谈一下关于 Transformer的并行化。文章比较短，适合大家碎片化阅读。</p>\n<p>Decoder不用多说，没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。</p>\n<p>对于Encoder侧：</p>\n<p>首先，6个大的模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。</p>\n<p>从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。</p>\n<p>当然对于注意力层在做attention的时候会依赖别的时刻的输入，不过这个只需要在计算之前就可以提供。</p>\n<p>然后注意力层和前馈神经层之间是串行，必须先完成注意力层计算再做前馈神经层。</p>\n<p>有点绕，不知道有没有讲清楚。</p>\n<p>简单讲，就是6个encoder之间是串行，每个encoder中的两个子模块之间是串行，子模块自身是可以并行的。</p>\n<h4 id=\"系列总结\">系列总结<a title=\"#系列总结\" href=\"#系列总结\"></a></h4>\n<p>整个Transformer这一块基本就是讲完了，基本上可以解决之前那个关于transformer面试题百分之八十的题目。</p>\n<p>至于剩下的题目会放在之后别的模块去讲，比如 wordpiece model 会在总结机器翻译知识点的时候写一下，然后 GPT 会在总结词向量知识点的时候写一下。</p>\n<p>写这个系列过程中，很多朋友也有私信我一些问题，交流过程中，对我自己帮助也很大，能回答的问题我都尽力回答了，也感谢大家的关注。平时工作挺忙的，尽量输出干货，也欢迎大家和我交流问题。</p>\n","link":"links/NLP_ability/深度学习自然语言处理/Transformer/Transformer的并行化","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/Transformer的并行化/","toc":[{"id":"transformer的并行化","title":"Transformer的并行化","index":"1"}],"reward":true}