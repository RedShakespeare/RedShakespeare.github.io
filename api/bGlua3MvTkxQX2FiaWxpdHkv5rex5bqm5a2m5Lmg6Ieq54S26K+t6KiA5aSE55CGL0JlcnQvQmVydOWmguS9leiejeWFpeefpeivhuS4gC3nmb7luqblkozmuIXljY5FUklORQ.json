{"title":"","date":"2024-06-21T03:48:13.735Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:13.735Z","content":"<p>Bert如何融入知识(一)-百度和清华ERINE</p>\n<p>首先想一下Bert是如何训练的？首先我获取无监督语料，随机mask掉一部分数据，去预测这部分信息。</p>\n<p>这个过程其实和W2C很类似，上下文相似的情况下，mask掉的单词的词向量很可能非常相近。</p>\n<p>比如说”今天米饭真好吃“和”今天苹果真好吃“，很有可能”米饭“和”苹果“学出来的向量就很相似。</p>\n<p>我在李如有一篇文章中《BERT句子表示的可视化》有这样一句话，contextual dependent词向量的一个缺点，就是上下文相似的情况下词向量也很接近。</p>\n<p>从这里，我觉得很容易就可以发现一个问题，就是Bert确实抽取能力非常的强，但是他也是在死记硬背的学这些知识。</p>\n<p>想一下，为什么我们需要在Bert中融入一些知识呢？</p>\n<p>我们考虑这么一个例子，比如我要对一个文本进行分类：”库克今日来北京进行商务洽谈活动“</p>\n<p>单从bert做一个文本分类，可能模型很难从语义角度进行决断。</p>\n<p>但是，我现在的知识图谱中有这样一个三元组：库克-CEO-苹果公司</p>\n<p>我把这个三元组的信息融入到我的模型之中，也就是我在文本分类的时候不仅仅使用了你的原始文本，还是使用了知识图谱中的三元组信息，相当于一种信息的增强，这个时候我的模型就可以文本分类为”IT公司“这个类别。</p>\n<p>一般来说，涉及到Bert中融入知识，大家都会涉及到两个文章：百度的 ERNIE from Baidu 和清华的ERNIE from THU</p>\n<p>我先从整体的思路说一下两者：</p>\n<p>ERNIE from Baidu 出发点是这样的，Bert 的mask只是 mask掉单字，放在中文中，一般来说词汇会带有比字更多的信息。</p>\n<p>比如说</p>\n<p>哈[mask]滨真冷啊 是Bert基础操作</p>\n<p>[mask][mask][mask]真冷啊 是ERNIE from Baidu的操作</p>\n<p>也就是，我预测的不仅仅是一个单字，而是一个实体词组。</p>\n<p>对于这个操作，我是这么想的，首先从难度来讲，去预测一个词组会比预测一个单字难，而且这个词组是一个实体，所以在学习的时候回学习到实体信息</p>\n<p>ERNIE from THU</p>\n<p>对于这个模型，我是这么想的，百度利用的是预测句子中的实体信息。而清华这边的操作是加入了外部的知识信息。</p>\n<p>就像最开始我们的例子，”库克-CEO-苹果公司“，这是外部知识，这个不是我文本中的信息，相当于显示的加入了外部信息。</p>\n<p>当然清华这边应该也只是使用到了实体信息（做了实体对齐）</p>\n<p>我们需要考虑两个问题：</p>\n<ol>\n<li>\n<p>如何抽取并且更好的表达知识图谱的信息：知识嵌入算法（如TransE）</p>\n</li>\n<li>\n<p>实体向量和Bert的向量在不同的空间，如何缓解两者之间的Gap：</p>\n</li>\n</ol>\n<p>对于这个问题，从模型架构上来解决，使用两种：</p>\n<p>textual encoder (T-Encoder)：类别Bert</p>\n<p>knowledgeable encoder (K-Encoder)：用于将外部的知识图谱的信息融入到模型中；</p>\n<p>对于Bert融入知识信息，主要是参考以下文章：</p>\n<p>站在BERT肩膀上的NLP新秀们（PART I） - kaiyuan的文章 - 知乎<br>\n<a href=\"https://zhuanlan.zhihu.com/p/68295881\" target=\"_blank\">https://zhuanlan.zhihu.com/p/68295881</a></p>\n<p>写的还不错，介绍了百度和清华的ERINE</p>\n<p>Bert 改进： 如何融入知识 - 老宋的茶书会的文章 - 知乎<br>\n<a href=\"https://zhuanlan.zhihu.com/p/69941989\" target=\"_blank\">https://zhuanlan.zhihu.com/p/69941989</a></p>\n<p>写的还不错，介绍了百度和清华的ERINE</p>\n<p>BERT与知识图谱的结合——ERNIE模型浅析 - 段易通的文章 - 知乎<br>\n<a href=\"https://zhuanlan.zhihu.com/p/75466388\" target=\"_blank\">https://zhuanlan.zhihu.com/p/75466388</a></p>\n<p>写的还不错，介绍了百度和清华的ERINE</p>\n","link":"links/NLP_ability/深度学习自然语言处理/Bert/Bert如何融入知识一-百度和清华ERINE","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Bert/Bert如何融入知识一-百度和清华ERINE/","reward":true}