{"title":"","date":"2024-06-21T03:48:19.575Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:19.575Z","content":"<p>今天分享的论文主要是讲Bert如何在文本分类上获得比较好的效果，比较简单：<a href=\"https://arxiv.org/abs/1905.05583,\" title=\"How to Fine-Tune BERT for Text Classification?\" target=\"_blank\">How to Fine-Tune BERT for Text Classification?</a>：不涉及什么复杂公式，也比较早了，里面很多东西对于当下已经司空见惯，我就直接就分享论文结论，攒个思路。</p>\n<h1 id=\"1.-如何处理长文本\">1. 如何处理长文本<a title=\"#1.-如何处理长文本\" href=\"#1.-如何处理长文本\"></a></h1>\n<p>我比较感兴趣的是一点是Bert处理长文本的思路。</p>\n<p>首先数据集是IMDB，文本分类任务，超过512个token的12.69%，最大长度为3045；</p>\n<h2 id=\"1.1-截断方法：\">1.1 截断方法：<a title=\"#1.1-截断方法：\" href=\"#1.1-截断方法：\"></a></h2>\n<ol>\n<li>保留头部：保留头部最开始的510个tokens</li>\n<li>保留尾部：保留最后的610个tokens</li>\n<li>头部加尾部：头部128+尾部382</li>\n</ol>\n<h2 id=\"1.2-分层的方法：\">1.2 分层的方法：<a title=\"#1.2-分层的方法：\" href=\"#1.2-分层的方法：\"></a></h2>\n<p>简单来说就是把文本分为 k = L/510个小段落，每个都喂进去Bert，然后得到的K个【CLS】的输出向量，我们对这个K个向量做：</p>\n<ol>\n<li>mean pooling</li>\n<li>max pooling</li>\n<li>self-attention</li>\n</ol>\n<p>直接看结果：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-02-070900.png\" alt=\"Bert处理长文本\" loading=\"lazy\" class=\"φbp\"></p>\n<p>看结果，我们知道，头部加尾部会获得更好的结果。</p>\n<h2 id=\"2.-其他结论\">2. 其他结论<a title=\"#2.-其他结论\" href=\"#2.-其他结论\"></a></h2>\n<ol>\n<li>BERT 顶层对于文本分类任务更加有效</li>\n<li><strong>每层适当的逐层降低学习速率</strong>，可以提高文本分类效果</li>\n<li>任务内和领域内（和任务内数据分布相似）的进一步预训练可以提升文本分类效果</li>\n</ol>\n<p>对于第二点，降低学习率来说，论文中是从顶层到底层逐渐降低，越靠近输出学习率越高，越靠近输入层，学习率越低，这一点还是挺有意思的。</p>\n<p>对于第三点，任务内数据和领域内数据，对提升效果都有用，通用领域基本没啥用，因为Bert本来就是在通用领域训练的。</p>\n<p>还有意思的一点是，并不是在任务内的数据训练的越多step越好，直接看图：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-02-070838.png\" alt=\"领域内数据进一步预训练\" loading=\"lazy\" class=\"φbp\"></p>\n<p>也就是说，在任务领域数据预训练可以提升效果，但是也有注意预训练的步数，不能是过分（有点过拟合的感觉，但是感觉说过拟合有点不准确）。</p>\n<h1 id=\"3.-总结\">3. 总结<a title=\"#3.-总结\" href=\"#3.-总结\"></a></h1>\n<p>掌握以下几点：</p>\n<ol>\n<li>如何处理长文本：head/tail/combine two</li>\n<li>不同层不同学习率提升效果，越靠近输入层学习率应该越低</li>\n<li>领域内和任务内数据进一步预训练提升效果：<strong>注意进一步预训练步数控制</strong></li>\n</ol>\n<p>参考链接：</p>\n<p>文本 × 分类：让 BERT 适配短句分类任务 - 小莲子的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/148501319\" target=\"_blank\">https://zhuanlan.zhihu.com/p/148501319</a></p>\n","link":"links/NLP_ability/深度学习自然语言处理/文本分类/在文本分类上微调Bert","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本分类/在文本分类上微调Bert/","toc":[{"id":"1.-如何处理长文本","title":"1. 如何处理长文本","index":"1","children":[{"id":"1.1-截断方法：","title":"1.1 截断方法：","index":"1.1"},{"id":"1.2-分层的方法：","title":"1.2 分层的方法：","index":"1.2"},{"id":"2.-其他结论","title":"2. 其他结论","index":"1.3"}]},{"id":"3.-总结","title":"3. 总结","index":"2"}],"reward":true}