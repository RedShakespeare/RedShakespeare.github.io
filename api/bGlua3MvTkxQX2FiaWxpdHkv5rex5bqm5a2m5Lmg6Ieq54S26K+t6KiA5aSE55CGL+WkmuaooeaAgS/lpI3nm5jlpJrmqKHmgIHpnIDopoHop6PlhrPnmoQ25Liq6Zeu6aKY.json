{"title":"","date":"2024-06-21T01:54:07.329Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2022-08-13T11:56:31.000Z","content":"<p>今天主要聊我在做多模态任务中的六个方面的介绍，如下：</p>\n<ol>\n<li>多模态业务简单介绍；</li>\n<li>多模态数据问题；</li>\n<li>如何确保多模态任务的预测速度；</li>\n<li>如何确定多模态任务确实起到了作用；</li>\n<li>多模态中多张图片如何处理；</li>\n<li>交互的时候哪种attention方式更好；</li>\n<li>训练的时候需要注意什么；</li>\n</ol>\n<p><strong>1.多模态业务简单介绍；</strong></p>\n<p>之前花了不少时间在多模态这块的落地工作，取得了一定的效果，今天分享一下我的经验；</p>\n<p>首先在调研多模态任务的时候大家可以看一下最近的论文，这两年的多模态任务基本上都在往Transformer上去靠，基本可以分为两种：单流网络和双流网络；</p>\n<p>双流网络就是文本过一个编码器，图片过一个编码器，然后两个编码器的输出进行一个交互；</p>\n<p>单流网络就是文本和图片先concat，然后直接输入到Transformer编码器中，然后输出；</p>\n<p>一般来说，这里的编码器使用的都是TRM结构；</p>\n<p>文本这块，输出的时候得到的是embedding就可以；图片这里，一般来说使用的是Faster-RCNN模型识别出图片中包含的多个物体及其对应的矩形位置信息，把这个作为TRM的输入；</p>\n<p>但是我在真正去做的时候，并没有按照这个思路去做，我是先按照自己的思路做了个baseline，然后有效果，之后再去看论文架构提升模型效果；</p>\n<p>我简单分享一下我的主体思路，文本过的BERT，图像过的Resnet,然后输出的两个表征向量之间做多头注意力，然后接全连接输出logits；</p>\n<p>按照分类，我这个架构应该属于双流网络；</p>\n<p>架构其实很简单，但是在真正去做的时候，真的是比较复杂，有很多细节，我在这里简单的梳理一下，一起探讨；</p>\n<p><strong>2.多模态数据问题；</strong></p>\n<p>多模态一般来说就是双模态数据，我主要接触的是文本+图片；很幸运，我有标注数据~~ 如果没有基于自己场景下的标注数据，还是不太建议强行上多模态任务；</p>\n<p><strong>3.如何确保多模态任务的预测速度；</strong></p>\n<p>为了保证我的预测速度，我不可能所有的case都过多模态网络；所以我做的策略很简答，就是单从文本输出结果置信度不高的而且含有图片信息的case走多模态任务；</p>\n<p><strong>4.如何确定多模态任务确实起到了作用；</strong></p>\n<p>这个问题其实很关键，首先我们当然可以做测试集，验证一下单走文本或者单走图片得到的f1以及做多模态得到的f1，两者一个比较就可以；</p>\n<p>当时确实也这么做了，但是我纠结点在于能不能使用一种可见的方式，告诉大家多模态度确实起到了作用？</p>\n<p>那么一个很有用的方法就是使用attention的可视化；这个方法可以可视化出文本和图片之间确实是有交互的，而且交互的部分是有意义的，比如有的单词就是对图片中的某个部分更加关注；</p>\n<p><strong>5.多张图片如何处理；</strong></p>\n<p>因为我图片过的是Resnet网络，所以输入是多张图片的数量是动态的，这是个问题；</p>\n<p>我们退一步说，按照现在bert多模态预训练中的方法，多张图片完全可以作为transformer中的输入tokens部分；或者把多张图片合并在一起生成一个图片再走正常流程；</p>\n<p>我这边处理的时候需要注意的细节就是resnet输出池化的时候k是个动态的池化就可以；</p>\n<p><strong>6.哪种attention方式更好；</strong></p>\n<p>一般来说做互相之间的交互更好，就是文本对图片做一次attention，图片对文本做一次attention，两者结合来做；</p>\n<p><strong>7.训练的时候需要注意什么；</strong></p>\n<p>bert和resnet网络架构不太一样，训练的时候容易不收敛，需要控制一下不同部分的学习率；</p>\n<p>如上，因为业务的原因，很多东西不能细说，所以我只是大体的介绍了一些自己的经验，希望能对大家有帮助；</p>\n<p>之后我会写一些BERT多模态预训练论文的解读文章，大体是<strong>LXMERT，ViLBERT，Unicoder-VL、VisualBERT、VL-VERT、UNITER</strong>等等；</p>\n<p><strong>求点赞，求在看，求转发，求一切，爱你们哦~ ~</strong></p>\n","link":"links/NLP_ability/深度学习自然语言处理/多模态/复盘多模态需要解决的6个问题","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/多模态/复盘多模态需要解决的6个问题/","reward":true}