{"title":"","date":"2024-06-21T01:54:07.419Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2022-08-13T11:56:31.000Z","content":"<p>DSSM</p>\n<p><a href=\"https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf\" target=\"_blank\">https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf</a></p>\n<h1 id=\"架构图\">架构图<a title=\"#架构图\" href=\"#架构图\"></a></h1>\n<p>架构图很简单，也有点老了</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2021-01-14-081908.jpg\" alt=\"image-20201223164854016\" loading=\"lazy\" class=\"φbp\"></p>\n<p>核心细节点有两个：一个是使用了cosine做了查询和文档的相似度量</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2021-01-14-81910.jpg\" alt=\"image-20201223165620044\" loading=\"lazy\" class=\"φbp\"></p>\n<p>第二个就是，softmax</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2021-01-14-081909.jpg\" alt=\"image-20201223165655726\" loading=\"lazy\" class=\"φbp\"></p>\n<p>第三个是损失函数，使用最大似然估计，只计算了正样本：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2021-01-14-081906.jpg\" alt=\"image-20201223165905179\" loading=\"lazy\" class=\"φbp\"></p>\n<p>对于DSSM，主要是想提几个小细节，也是我自己的思考，不准确的地方，欢迎拍砖。</p>\n<p>首先，为什么采用(Query,D+,D-1,D-2,D-3)的方式作为输入，而不是采用(Query,D+)；(Query,D-1)；(Query,D-2)；(Query,D-3)；作为单独的pair样本对作为输入；</p>\n<p>这个问题，其实还可以换个问法，为什么DSSM的损失函数，使用的是一个正样本多个负样本归一化之后对正样本求交叉熵，而不是单个pair对作为输入，去求二分类的交叉熵；</p>\n<p>我的理解是，这个其实适合业务场景相关的一个问题；参考下面这个回答的答案：</p>\n<p>DSSM 为什么以一个正样本几个负样本softmax归一化然后正样本交叉熵的方式算loss? - xSeeker的回答 - 知乎 <a href=\"https://www.zhihu.com/question/425436660/answer/1522163398\" target=\"_blank\">https://www.zhihu.com/question/425436660/answer/1522163398</a></p>\n<p>我直接截图过来：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2021-01-14-081907.jpg\" alt=\"image-20210114153620063\" loading=\"lazy\" class=\"φbp\"></p>\n<p>本质上，还是在学习一种顺序关系，正样本排在负样本之前</p>\n<h1 id=\"dssm在各大公司的实战\">DSSM在各大公司的实战<a title=\"#dssm在各大公司的实战\" href=\"#dssm在各大公司的实战\"></a></h1>\n<p>实践DSSM召回模型 - 王多鱼的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/136253355\" target=\"_blank\">https://zhuanlan.zhihu.com/p/136253355</a></p>\n<p>深度语义模型以及在淘宝搜索中的应用:<a href=\"https://developer.aliyun.com/article/422338\" target=\"_blank\">https://developer.aliyun.com/article/422338</a>  写的很好</p>\n<p>百度NLP | 神经网络语义匹配技术：<a href=\"https://www.jiqizhixin.com/articles/2017-06-15-5\" target=\"_blank\">https://www.jiqizhixin.com/articles/2017-06-15-5</a></p>\n<p>语义匹配 - 乐沐阳的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/57550660\" target=\"_blank\">https://zhuanlan.zhihu.com/p/57550660</a></p>\n<h1 id=\"损失函数\">损失函数<a title=\"#损失函数\" href=\"#损失函数\"></a></h1>\n<p>DSSM通过推导公式，可以得到最大化似然估计和交叉熵损失函数是一致的。</p>\n<p>【辩难】DSSM 损失函数是 Pointwise Loss 吗？ - xSeeker的文章 - 知乎 <a href=\"https://zhuanlan.zhihu.com/p/322065156\" target=\"_blank\">https://zhuanlan.zhihu.com/p/322065156</a></p>\n<p>交叉熵损失函数原理详解：</p>\n<p><a href=\"https://blog.csdn.net/b1055077005/article/details/100152102\" target=\"_blank\">https://blog.csdn.net/b1055077005/article/details/100152102</a></p>\n","link":"links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/DSSM论文-公司实战文章","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/DSSM论文-公司实战文章/","toc":[{"id":"架构图","title":"架构图","index":"1"},{"id":"dssm在各大公司的实战","title":"DSSM在各大公司的实战","index":"2"},{"id":"损失函数","title":"损失函数","index":"3"}],"reward":true}