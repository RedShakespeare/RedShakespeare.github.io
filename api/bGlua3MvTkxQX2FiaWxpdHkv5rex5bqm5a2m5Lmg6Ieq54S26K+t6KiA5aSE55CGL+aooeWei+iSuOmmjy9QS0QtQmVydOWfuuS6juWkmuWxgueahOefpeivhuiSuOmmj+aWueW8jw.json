{"title":"","date":"2024-06-21T01:54:07.499Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2022-08-13T11:56:31.000Z","content":"<p><a href=\"https://arxiv.org/pdf/1908.09355.pdf\" title=\"Patient Knowledge Distillation for BERT Model Compression\" target=\"_blank\">PKD</a>  核心点就是不仅仅从Bert（老师网络）的最后一层学习知识去做蒸馏，它还另加了一部分，就是从<strong>Bert的中间层去学习</strong>。</p>\n<p>简单说，PKD的知识来源有两部分：<strong>中间层+最后输出</strong>。</p>\n<p>它缓解了之前只用最后softmax输出层的蒸馏方式出现的过拟合而导致泛化能力降低的问题。</p>\n<p>接下来，我们从PKD模型的两个策略说起：PKD-Last 和 PKD-Skip。</p>\n<h1 id=\"1.pkd-last-and-pkd-skip\">1.PKD-Last and PKD-Skip<a title=\"#1.pkd-last-and-pkd-skip\" href=\"#1.pkd-last-and-pkd-skip\"></a></h1>\n<p>PKD的本质是从中间层学习知识，但是这个中间层如何去定义，就各式各样了。</p>\n<p>比如说，我完全可以定位我只要<strong>奇数层</strong>，或者我只要<strong>偶数层</strong>，或者说我只要<strong>最中间的两层</strong>，等等，不一而足。</p>\n<p>那么作者，主要是使用了这么多想法中的看起来比较合理的两种。</p>\n<p><strong>PKD-Last，就是把中间层定义为老师网络的最后k层</strong>。</p>\n<p>这样做是基于老师网络越靠后的层数含有更多更重要的信息。</p>\n<p>这样的想法其实和之前的蒸馏想法很类似，也就是只使用softmax层的输出去做蒸馏。但是从感官来看，有种尾大不掉的感觉，不均衡。</p>\n<p>另一个策略是 就是<strong>PKD-Skip，顾名思义，就是每跳几层学习一层</strong>。</p>\n<p>这么做是基于老师网络比较底层的层也含有一些重要性信息，这些信息不应该被错过。</p>\n<p>作者在后面的实验中，证明了，PKD-Skip 效果稍微好一点（slightly better）；</p>\n<p>作者认为PKD-Skip抓住了老师网络不同层的多样性信息。而PKD-Last抓住的更多相对来说同质化信息，因为集中在了最后几层。</p>\n<h1 id=\"2.-pkd\">2. PKD<a title=\"#2.-pkd\" href=\"#2.-pkd\"></a></h1>\n<h2 id=\"2.1架构图\">2.1架构图<a title=\"#2.1架构图\" href=\"#2.1架构图\"></a></h2>\n<p>两种策略的PKD的架构图如下所示，<strong>注意观察图，有个细节很容易忽视掉</strong>:</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-11-23-104752.jpg\" alt=\"PKD_Models\" loading=\"lazy\" class=\"φbp\"></p>\n<p>我们注意看这个图，<strong>Bert的最后一层（不是那个绿色的输出层）是没有被蒸馏的，这个细节一会会提到</strong>。</p>\n<h2 id=\"2.2-怎么蒸馏中间层\">2.2 怎么蒸馏中间层<a title=\"#2.2-怎么蒸馏中间层\" href=\"#2.2-怎么蒸馏中间层\"></a></h2>\n<p>这个时候，需要解决一个问题：我们怎么蒸馏中间层？</p>\n<p>仔细想一下Bert的架构，假设最大长度是128，那么我们每一层Transformer encoder的输出都应该是128个单元，每个单元是768维度。</p>\n<p>那么在对中间层进行蒸馏的时候，我们需要针对哪一个单元？是针对所有单元还是其中的部分单元？</p>\n<p>首先，我们想一下，正常KD进行蒸馏的时候，我们使用的是[CLS]单元Softmax的输出，进行蒸馏。</p>\n<p>我们可以把这个思想借鉴过来，一来，对所有单元进行蒸馏，计算量太大。二来，[CLS] 不严谨的说，可以看到整个句子的信息。</p>\n<p>为啥说是不严谨的说呢？因为[CLS]是不能代表整个句子的输出信息，这一点我记得Bert中有提到。</p>\n<h2 id=\"2.3蒸馏层数和学生网络的初始化\">2.3蒸馏层数和学生网络的初始化<a title=\"#2.3蒸馏层数和学生网络的初始化\" href=\"#2.3蒸馏层数和学生网络的初始化\"></a></h2>\n<p>接下来，我想说一个很小的细节点，对比着看上面的模型架构图：</p>\n<p><strong>Bert（老师网络）的最后一层 (Layer 12 for BERT-Base) 在蒸馏的时候是不予考虑</strong>；</p>\n<p>原因的话，其一可以这么理解，PKD创新点是从中间层学习知识，最后一层不属于中间层。当然这么说有点牵强附会。</p>\n<p>作者的解释是最后一层的隐层输出之后连接的就是Softmax层，而Softmax层的输出已经被KD Loss计算在内了。</p>\n<p>比如说，K=5，那么对于两种PKD的模式，被学习的中间层分别是：</p>\n<p>PKD-Skip: $I_{pt} = {2,4,6,8,10}$;</p>\n<p>PKD-Last: $I_{pt} = {7,8,9,10,11}$</p>\n<p>还有一个细节点需要注意，就是学生网络的初始化方式，直接使用老师网络的前几层去初始化学生网络的参数。</p>\n<h2 id=\"2.4-损失函数\">2.4 损失函数<a title=\"#2.4-损失函数\" href=\"#2.4-损失函数\"></a></h2>\n<p>首先需要注意的是中间层的损失，作者使用的是MSE损失。如下：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-11-23-104749.jpg\" alt=\"中间层损失计算\" loading=\"lazy\" class=\"φbp\"></p>\n<p>整个模型的损失主要是分为两个部分：KD损失和中间层的损失，如下：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-11-23-104750.jpg\" alt=\"Loss_of_PKD\" loading=\"lazy\" class=\"φbp\"></p>\n<p>超参数问题：</p>\n<ol>\n<li>$T:{5,10,20}$</li>\n<li>$\\alpha:{0.2,0.5,0.7}$</li>\n<li>$LR :{5e-5, 2e-5, 1e-5}$</li>\n<li>$\\beta :{10, 100, 500, 1000} $</li>\n</ol>\n<h1 id=\"3.-实验效果\">3. 实验效果<a title=\"#3.-实验效果\" href=\"#3.-实验效果\"></a></h1>\n<p>实验效果可以总结如下：</p>\n<ol>\n<li>PKD确实有效，而且Skip模型比Last效果稍微好一点。</li>\n<li>PKD模型减少了参数量，加快了推理速度，基本是线性关系，毕竟减少了层数</li>\n</ol>\n<p>除了这两点，作者还做了一个实验去验证：<strong>如果老师网络更大，PKD模型得到的学生网络会表现更好吗</strong>？</p>\n<p>这个实验我很感兴趣。</p>\n<p>直接上结果图：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-11-23-104753.jpg\" alt=\"Larger_Teacher\" loading=\"lazy\" class=\"φbp\"></p>\n<p>KD情况下，注意不是PKD模型，看#1 和#2，在老师网络增加的情况下，效果有好有坏。这个和训练数据大小有关。</p>\n<p>KD情况下，看#1和#3，在老师网络增加的情况下，学生网络明显变差。</p>\n<p>作者分析是因为，压缩比高了，学生网络获取的信息变少了。</p>\n<p>也就是大网络和小网络本身效果没有差多少，但是学生网络在老师是大网络的情况下压缩比大，学到的信息就少了。</p>\n<p>更有意思的是对比#2和#3，老师是大网络的情况下，学生网络效果差。</p>\n<p>这里刚开始没理解，后来仔细看了一下，注意#2 的学生网络是$Bert_{6}[Base]-KD$，也就是它的初始化是从$Bert_{12}[Base]$来的，占了一半的信息。</p>\n<p>好的，写到这里</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-11-23-104751.jpg\" alt=\"个人微信\" loading=\"lazy\" class=\"φbp\"></p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-11-21-%E5%85%AC%E4%BC%97%E5%8F%B7.png\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n","link":"links/NLP_ability/深度学习自然语言处理/模型蒸馏/PKD-Bert基于多层的知识蒸馏方式","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/模型蒸馏/PKD-Bert基于多层的知识蒸馏方式/","toc":[{"id":"1.pkd-last-and-pkd-skip","title":"1.PKD-Last and PKD-Skip","index":"1"},{"id":"2.-pkd","title":"2. PKD","index":"2","children":[{"id":"2.1架构图","title":"2.1架构图","index":"2.1"},{"id":"2.2-怎么蒸馏中间层","title":"2.2 怎么蒸馏中间层","index":"2.2"},{"id":"2.3蒸馏层数和学生网络的初始化","title":"2.3蒸馏层数和学生网络的初始化","index":"2.3"},{"id":"2.4-损失函数","title":"2.4 损失函数","index":"2.4"}]},{"id":"3.-实验效果","title":"3. 实验效果","index":"3"}],"reward":true}