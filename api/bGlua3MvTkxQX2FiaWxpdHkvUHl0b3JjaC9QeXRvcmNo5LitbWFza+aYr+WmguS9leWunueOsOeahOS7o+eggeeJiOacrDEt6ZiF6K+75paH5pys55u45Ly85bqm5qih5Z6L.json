{"title":"","date":"2024-06-21T01:54:07.129Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2022-08-13T11:56:31.000Z","content":"<p>Pytorch中mask是如何实现的代码版本1-阅读文本相似度模型</p>\n<p>代码参考链接: <a href=\"https://github.com/DA-southampton/TextMatch/tree/master/ESIM\" target=\"_blank\">https://github.com/DA-southampton/TextMatch/tree/master/ESIM</a></p>\n<p>最近在用Pytorch重新ESIM代码，其中关于attention的细节我自己重新梳理了一下，附上代码解读。</p>\n<p>我先在有一个batch的数据。Sentence1 维度为[256,32,300],Sentence2的维度为[256,33,300]</p>\n<p>维度含义为[batch_size,batch中最大长度，词向量维度]</p>\n<p>数据流转ESIM第一个Bilstm之后，维度变化为：Sentence1 维度为[256,32,600],Sentence2的维度为[256,33,600]（假设hidden为300）</p>\n<p>此时我们需要计算两个句子输出的attention矩阵，以便计算每个句子的加权和。</p>\n<p>我这里主要是梳理矩阵的计算方式细节。</p>\n<p>核心代码是这个：<br>\n<a href=\"https://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/model.py#L57\" target=\"_blank\">https://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/model.py#L57</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">q1_aligned, q2_aligned = self.attention(q1_encoded, q1_mask, q2_encoded, q2_mask)</span><br></pre></td></tr></table></figure>\n<p>self.attention 函数对应的是这个函数，如下：<br>\n<a href=\"https://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/layers.py#L59\" target=\"_blank\">https://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/layers.py#L59</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">SoftmaxAttention</span>(nn.Module):</span><br><span class=\"line\">        similarity_matrix = premise_batch.bmm(hypothesis_batch.transpose(<span class=\"number\">2</span>, <span class=\"number\">1</span>).contiguous())  <span class=\"comment\">## 256*32 *33</span></span><br><span class=\"line\">        <span class=\"comment\"># Softmax attention weights.</span></span><br><span class=\"line\">        prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)</span><br><span class=\"line\">        hyp_prem_attn = masked_softmax(similarity_matrix.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>).contiguous(), premise_mask)</span><br><span class=\"line\">        <span class=\"comment\"># Weighted sums of the hypotheses for the the premises attention,</span></span><br><span class=\"line\">        <span class=\"comment\"># and vice-versa for the attention of the hypotheses.</span></span><br><span class=\"line\">        attended_premises = weighted_sum(hypothesis_batch, prem_hyp_attn, premise_mask)</span><br><span class=\"line\">        attended_hypotheses = weighted_sum(premise_batch, hyp_prem_attn, hypothesis_mask)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> attended_premises, attended_hypotheses  </span><br></pre></td></tr></table></figure>\n<p>首先我们看一下输入：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">q1_encoded：<span class=\"number\">256</span>*<span class=\"number\">32</span>*<span class=\"number\">600</span> q2_encoded：<span class=\"number\">256</span>*<span class=\"number\">33</span>*<span class=\"number\">600</span>  q1mask torch.Size([<span class=\"number\">256</span>, <span class=\"number\">32</span>])  q2mask torch.Size([<span class=\"number\">256</span>, <span class=\"number\">33</span>])</span><br></pre></td></tr></table></figure>\n<p>然后对于这个函数，核心操作是这个：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">prem_hyp_attn = masked_softmax(similarity_matrix, hypothesis_mask)</span><br></pre></td></tr></table></figure>\n<p>similarity_matrix 维度为256*32 <em>33 hypothesis_mask 为256</em>33<br>\n我们去看一下masked_softmax这个函数：<br>\n<a href=\"https://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/utils.py#L29\" target=\"_blank\">https://github.com/DA-southampton/TextMatch/blob/54e24599ce2d4caaa16d68400dc6a404795d44e9/ESIM/utils.py#L29</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">masked_softmax</span>(<span class=\"params\">tensor, mask</span>):</span><br><span class=\"line\">    tensor_shape = tensor.size()  <span class=\"comment\">##torch.Size([256, 32, 33])</span></span><br><span class=\"line\">    reshaped_tensor = tensor.view(-<span class=\"number\">1</span>, tensor_shape[-<span class=\"number\">1</span>]) <span class=\"comment\">## torch.Size([7680, 33])</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> mask.dim() &lt; tensor.dim():</span><br><span class=\"line\">        mask = mask.unsqueeze(<span class=\"number\">1</span>)</span><br><span class=\"line\">    mask = mask.expand_as(tensor).contiguous().<span class=\"built_in\">float</span>()</span><br><span class=\"line\">    reshaped_mask = mask.view(-<span class=\"number\">1</span>, mask.size()[-<span class=\"number\">1</span>])  <span class=\"comment\">## torch.Size([7680, 33])</span></span><br><span class=\"line\"></span><br><span class=\"line\">    result = nn.functional.softmax(reshaped_tensor * reshaped_mask, dim=-<span class=\"number\">1</span>)  <span class=\"comment\">## 补长位置也就是置为零的位置之后进行softmax</span></span><br><span class=\"line\">    result = result * reshaped_mask <span class=\"comment\">## 再次置为零，因为上面这个对于补长位置还会有概率共现</span></span><br><span class=\"line\">    <span class=\"comment\"># 1e-13 is added to avoid divisions by zero.</span></span><br><span class=\"line\">    result = result / (result.<span class=\"built_in\">sum</span>(dim=-<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>) + <span class=\"number\">1e-13</span>) <span class=\"comment\">## 普通的求概率公式</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result.view(*tensor_shape)</span><br></pre></td></tr></table></figure>\n<p>简单总结一下：<br>\n整个mask的代码其实我读起来感觉比较奇怪，我印象中mask的操作，应该是补长的部分直接为负无穷（代码里写一个-1000就可以），但是他这里的代码，是补长的部位置为0，所以<br>\n在softmax的时候，虽然为1，但是也有贡献也有概率的输出，虽然很小。所以又把这些部分置为零，然后用每一行的值除以每一行的总和得到了新的概率值，这个概率和补长的部位就没有关系了。<br>\n还有一个细节点需要注意的是，比如我的输入是256<em>32</em>33 batch为256，那么我在计算每一行的的时候，完全可以把batch中的数据并起来，也就是变成(256*32)*33</p>\n<p>所以我简单总结一下，在这里的mask的操作分为两个步骤：首先补长位置置为零然后计算softmax，随后对softmax的结构补长位置继续置为零，计算简单的分值（各自除以每一行的总和），得到最后的概率值。</p>\n","link":"links/NLP_ability/Pytorch/Pytorch中mask是如何实现的代码版本1-阅读文本相似度模型","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/Pytorch/Pytorch中mask是如何实现的代码版本1-阅读文本相似度模型/","reward":true}