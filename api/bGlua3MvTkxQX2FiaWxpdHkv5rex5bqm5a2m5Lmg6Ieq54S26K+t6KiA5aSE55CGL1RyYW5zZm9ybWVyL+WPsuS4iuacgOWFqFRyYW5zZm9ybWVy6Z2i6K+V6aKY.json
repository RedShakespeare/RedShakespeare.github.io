{"title":"","date":"2024-06-21T03:48:16.045Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:48:16.045Z","content":"<p>史上最全Transformer面试题</p>\n<ol>\n<li>Transformer为何使用多头注意力机制？（为什么不使用一个头）</li>\n<li>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？<br>\n（注意和第一个问题的区别）</li>\n<li>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</li>\n<li>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解</li>\n<li>在计算attention score的时候如何对padding做mask操作？</li>\n<li>为什么在进行多头注意力的时候需要对每个head进行降维？（可以参考上面一个问题）</li>\n<li>大概讲一下Transformer的Encoder模块？</li>\n<li>为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？</li>\n<li>简单介绍一下Transformer的位置编码？有什么意义和优缺点？</li>\n<li>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</li>\n<li>简单讲一下Transformer中的残差结构以及意义。</li>\n<li>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</li>\n<li>简答讲一下BatchNorm技术，以及它的优缺点。</li>\n<li>简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</li>\n<li>Encoder端和Decoder端是如何进行交互的？（在这里可以问一下关于seq2seq的attention知识）</li>\n<li>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？（为什么需要decoder自注意力需要进行 sequence mask)</li>\n<li>Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？</li>\n<li>简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</li>\n<li>Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</li>\n<li>引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</li>\n</ol>\n","link":"links/NLP_ability/深度学习自然语言处理/Transformer/史上最全Transformer面试题","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/Transformer/史上最全Transformer面试题/","reward":true}