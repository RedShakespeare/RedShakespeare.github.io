{"title":"","date":"2024-06-21T01:54:07.439Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2022-08-13T11:56:31.000Z","content":"<p><a href=\"https://www.aclweb.org/anthology/P19-1465/,\" title=\"Simple and Effective Text Matching with Richer Alignment Features\" target=\"_blank\">RE2</a> 这个名称来源于该网络三个重要部分的合体：<strong>R</strong>esidual vectors；<strong>E</strong>mbedding vectors；<strong>E</strong>ncoded vectors;</p>\n<p>掌握这个论文，最重要的一个细节点就是<strong>了解如何将增强残差连接融入到模型之中</strong>。</p>\n<h1 id=\"1.架构图\">1.架构图<a title=\"#1.架构图\" href=\"#1.架构图\"></a></h1>\n<p>先来看架构图，如下：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-22-034023.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p>这个架构图很精简，所以不太容易理解。</p>\n<p>大体上区分可以分为三层。第一层就是输入层，第二个就是中间处理层，第三个就是输出层。</p>\n<p>中间处理层我们可以称之为block，就是画虚线的部分，可以被循环为n次，但是需要注意的是每个block不是共享的，参数是不同的，是独立的，这点需要注意。</p>\n<h1 id=\"2.增强残差连接\">2.增强残差连接<a title=\"#2.增强残差连接\" href=\"#2.增强残差连接\"></a></h1>\n<p>其实这个论文比较有意思的点就是增强残差连接这里。架构图在这里其实很精简，容易看糊涂，要理解还是要看代码和公式。</p>\n<h2 id=\"2.1-第一个残差\">2.1 第一个残差<a title=\"#2.1-第一个残差\" href=\"#2.1-第一个残差\"></a></h2>\n<p>首先假设我们的句子长度为$l$，然后对于第n个block（就是第n个虚线框的部分）。</p>\n<p>它的输入和输出分别是:$x^{(n)}=(x_{1}^{(n)},x_{2}^{(n)},…,x_{l}^{(n)})$ 和$o^{(n)}=(o_{1}^{(n)},o_{2}^{(n)},…,o_{l}^{(n)})$;</p>\n<p>首先对一第一个block，也就是$x^{(1)}$，它的输入是embedding层，注意这里仅仅是embedding层；</p>\n<p>对于第二个block，也就是$x^{(2)}$，它的输入是embedding层（就是初始的embedding层）和第一个block的输出$o^{(1)}$拼接在一起；</p>\n<p>紧接着对于n大于2的情况下，也就是对于第三个，第四个等等的block，它的输入形式是这样的;</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-22-034027.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p>理解的重点在这里：在每个block的输入，大体可以分为两个部分，第一部分就是初始的embedding层，这个永远不变，第二个部分就是此时block之前的两层的blocks的输出和；这两个部分进行拼接。</p>\n<p>这是第一个体现残差的部分。</p>\n<h2 id=\"2.2第二个残差\">2.2第二个残差<a title=\"#2.2第二个残差\" href=\"#2.2第二个残差\"></a></h2>\n<p>第二个残差的部分在block内部：</p>\n<p>alignment层之前的输入就有三个部分：第一部分就是embedding，第二部分就是前两层的输出，第三部分就是encoder的输出。</p>\n<p>这点结合着图就很好理解了。</p>\n<h1 id=\"3.alignment-layer\">3.Alignment Layer<a title=\"#3.alignment-layer\" href=\"#3.alignment-layer\"></a></h1>\n<p>attention这里其实操作比较常规，和ESIM很类似，大家可以去看之前这个文章。</p>\n<p>公式大概如下：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-22-34025.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-22-034024.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p>这里有一个细节点需要注意，在源码中计算softmax之前，也是做了类似TRM中的缩放，也就是参数，放个代码：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#核心代码</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, args, __</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.temperature = nn.Parameter(torch.tensor(<span class=\"number\">1</span> / math.sqrt(args.hidden_size)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">_attention</span>(<span class=\"params\">self, a, b</span>):</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.matmul(a, b.transpose(<span class=\"number\">1</span>, <span class=\"number\">2</span>)) * self.temperature</span><br></pre></td></tr></table></figure>\n<h1 id=\"4.fusion-layer\">4.Fusion Layer<a title=\"#4.fusion-layer\" href=\"#4.fusion-layer\"></a></h1>\n<p>融合层，就是对attentino之前和之后的特征进行一个融合，具体如下：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-22-034025.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p>三种融合方式分别是直接拼接，算了对位减法然后拼接，算了对位乘法然后拼接。最后是对三个融合结果进行拼接。</p>\n<p>有一个很有意思的点，作者说到减法强调了两句话的不同，而乘法强调了两句话相同的地方。</p>\n<h1 id=\"5.prediction-layer\">5.Prediction Layer<a title=\"#5.prediction-layer\" href=\"#5.prediction-layer\"></a></h1>\n<p>Pooling层之后两个句子分别得到向量表达：$v_{1}$和$v_{2}$</p>\n<p>三个表达方式，各取所需就可以：</p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-22-034022.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-22-034026.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<p><img src=\"https://picsfordablog.oss-cn-beijing.aliyuncs.com/2020-12-22-034028.jpg\" alt=\"\" loading=\"lazy\" class=\"φbp\"></p>\n<h1 id=\"6.-总结\">6. 总结<a title=\"#6.-总结\" href=\"#6.-总结\"></a></h1>\n<p>简单总结一下，这个论文最主要就是掌握残差连接。</p>\n<p>残差体现在模型两个地方，一个是block外，一个是block内；</p>\n<p>对于block，需要了解的是，每一个block的输入是有两部分拼接而成，一个是最初始的embeddding，一个是之前两层的输出和。</p>\n<p>对于block内，需要注意的是Alignment之前，有三个部分的输入一个是最初始的embeddding，一个是之前两层的输出和，还有一个是encoder的输出。</p>\n","link":"links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/阿里RE2-将残差连接和文本匹配模型融合","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/文本匹配和文本相似度/阿里RE2-将残差连接和文本匹配模型融合/","toc":[{"id":"1.架构图","title":"1.架构图","index":"1"},{"id":"2.增强残差连接","title":"2.增强残差连接","index":"2","children":[{"id":"2.1-第一个残差","title":"2.1 第一个残差","index":"2.1"},{"id":"2.2第二个残差","title":"2.2第二个残差","index":"2.2"}]},{"id":"3.alignment-layer","title":"3.Alignment Layer","index":"3"},{"id":"4.fusion-layer","title":"4.Fusion Layer","index":"4"},{"id":"5.prediction-layer","title":"5.Prediction Layer","index":"5"},{"id":"6.-总结","title":"6. 总结","index":"6"}],"reward":true}