{"title":"","date":"2024-06-21T03:20:42.593Z","date_formatted":{"ll":"Jun 21, 2024","L":"06/21/2024","MM-DD":"06-21"},"updated":"2024-06-20T19:20:42.593Z","content":"<h2 id=\"pytorch代码分析--如何让bert在finetune小数据集时更“稳”一点\">Pytorch代码分析–如何让Bert在finetune小数据集时更“稳”一点<a title=\"#pytorch代码分析--如何让bert在finetune小数据集时更“稳”一点\" href=\"#pytorch代码分析--如何让bert在finetune小数据集时更“稳”一点\"></a></h2>\n<p>前几天在知乎刷到邱震宇同学的一个文章，如何让 Bert 在 finetune 小数据集时更“稳”一点，主要是分析了一篇论文，感觉很有意思。</p>\n<p>小数据集的时候，很容易就训练震荡。作者主要是分析了两个点进行优化。一个是adam偏移修正的问题，一个是权重初始化的问题。</p>\n<p>作者通过实验给出的结论是：1.使用误差修正，训练效率提高（收敛变快），开发集效果变好。2.使用权重初始化（后6层），训练效率提高（收敛变快），开发集效果变化不大。</p>\n<p>我做了简单的测试，有一个小点和作者结论不太一样，adam的修正并没有加速模型收敛，具体的看正文分析吧。</p>\n<p>我基于 <font color=\"#666600\">huggingface</font> 的 Bert，做了个两个简单的实验，详细情况如下。</p>\n<h3 id=\"1.-adam偏移修正\">1. adam偏移修正<a title=\"#1.-adam偏移修正\" href=\"#1.-adam偏移修正\"></a></h3>\n<p>对于adam偏移修正的实验，我这边得到的结果是：使用误差修正，收敛并未变快（反而变慢），训练更加稳定，开发集效果变好。</p>\n<p>对于收敛速度这块和作者结论不太一样，多跑了几个种子，都是收敛速度并未变快，希望有关大佬解惑。</p>\n<h4 id=\"1.1-代码情况讲解\">1.1 代码情况讲解<a title=\"#1.1-代码情况讲解\" href=\"#1.1-代码情况讲解\"></a></h4>\n<p>首先说一下，在翻看源代码的时候，发现对于抱抱脸的 Bert 的实现，默认偏移修正是开启的，具体代码的位置在这里：</p>\n<p><a href=\"https://github.com/huggingface/transformers/blob/84be482f6698fac822a5113735f2242c6d3abc76/src/transformers/optimization.py#L107\" target=\"_blank\">https://github.com/huggingface/transformers/blob/84be482f6698fac822a5113735f2242c6d3abc76/src/transformers/optimization.py#L107</a></p>\n<p>抱抱脸使用的是 AdamW:  Adam algorithm with weight decay fix</p>\n<p>代码如下</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">AdamW</span>(<span class=\"title class_ inherited__\">Optimizer</span>):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self,...,correct_bias=<span class=\"literal\">True</span></span>):</span><br><span class=\"line\">    ...</span><br><span class=\"line\">    ...</span><br></pre></td></tr></table></figure>\n<p>所以在测试偏移修正的对比的实验的时候，一个是保持默认不变得出一个结果；一个是修改这个参数，你可以在调用函数的时候修改参数传入值，也可以修改源代码，如果是修改的源代码，记得做完实验把代码改回来，别对之后工作造成影响。</p>\n<h4 id=\"1.2-任务基本情况\">1.2 任务基本情况<a title=\"#1.2-任务基本情况\" href=\"#1.2-任务基本情况\"></a></h4>\n<p>任务基本情况是这样的：</p>\n<ul>\n<li>任务类别：文本分类/15个类别</li>\n<li>数据来源: 今日头条新闻数据</li>\n<li>数据量级：训练集1K/开发集50k</li>\n<li>训练参数：\n<ul>\n<li><font color=\"#0000dd\">Bert : chinese_l-12_h-768_a-12，使用原始版本未做修改</font></li>\n<li><font color=\"#0000dd\">batchsize：16</font></li>\n<li><font color=\"#0000dd\">max_seq_length ：128</font></li>\n<li><font color=\"#0000dd\">learning_rate：2e-5</font></li>\n<li><font color=\"#0000dd\">Epoches: 10</font></li>\n</ul>\n</li>\n</ul>\n<p>因为数据量较小，并且想要观察变化，没使用 earlly stopping，完整的跑完了10个epoch，一共是 600 steps 左右，文中所示图以每隔 10 steps 打点显示，所以最大显示为 60/30。</p>\n<h4 id=\"1.3-结果展示\">1.3 结果展示<a title=\"#1.3-结果展示\" href=\"#1.3-结果展示\"></a></h4>\n<p>结果展示分为两个，一个是 Loss 变化图，一个是在开发集 Acc 展示图。</p>\n<p>Loss 变化如下图：</p>\n<p><img src=\"/links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/images/loss_adam.png\" alt=\"loss_adam\" loading=\"lazy\" class=\"φbp\"></p>\n<p>可以看到，没有使用偏移纠正的 loss 收敛的更加的迅速一点，反而使用了修正的模型收敛的慢一点。但是同时可以观测到，修正之后，模型的收敛更加的稳定，相比较而言，并没有特别大的震荡。</p>\n<p>Acc变化如下图（后期没怎么变化，所以截取了前300steps）：</p>\n<p><img src=\"/links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/images/acc_corre.png\" alt=\"acc_corre\" loading=\"lazy\" class=\"φbp\"></p>\n<p>对于在开发集来说，经过修正的收敛速度慢，但是比较稳定，没有大幅度的震荡，准确度相比，有可观收益（图中看不不太明显，无修正最好效果:0.80，加入修正最好效果: 0.82）</p>\n<h3 id=\"2.-权重初始化\">2. 权重初始化<a title=\"#2.-权重初始化\" href=\"#2.-权重初始化\"></a></h3>\n<p>权重初始化比较简单，平常任务也试过，因为是文本分类任务，所以在这里只是简单的测试了一下重新初始化 Pooler 层。</p>\n<p>Loss结果如下图：</p>\n<p><img src=\"/links/NLP_ability/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Bert/images/loss_pool.png\" alt=\"loss_pool\" loading=\"lazy\" class=\"φbp\"></p>\n<p>从图中可以看出，重新初始化，收敛速度变快了，但是不明显。</p>\n<p>Acc没什么变化，就不上图了，没什么变化（主要是被我无意删了，懒得再重跑一次了，不影响大局）</p>\n<h3 id=\"3.-简单总结\">3. 简单总结<a title=\"#3.-简单总结\" href=\"#3.-简单总结\"></a></h3>\n<p>简单总结一下：</p>\n<p>与没有修正的adam之后，修正之后，模型收敛速度变慢，收敛过程变得稳定，效果提升比较明显。</p>\n<p>与没有重新初始化的模型相比，初始化最后一层pooler之后，模型收敛速度有所变快，但是不明显，效果也没有明显变化。</p>\n<p>上面这两个实验只是基于邱震宇同学的文章做的，在这里感谢作者。关于收敛速度这里，结果有一点不一样，希望有大佬可以解惑，我也会抽空去看看原论文，仔细研读一下，看论文还有没有值得挖的东西，有任何进展，我再和大家说。</p>\n<p>打完收工，看完我这么辛苦画图（真是累死了）的份上，点个赞再撤吧，鞠躬感谢。</p>\n","link":"links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/Pytorch代码分析-如何让Bert在finetune小数据集时更“稳”一点","comments":true,"plink":"http://www.ephesus.top/links/NLP_ability/深度学习自然语言处理/深度学习自然语言处理/Bert/Pytorch代码分析-如何让Bert在finetune小数据集时更“稳”一点/","toc":[{"id":"pytorch代码分析--如何让bert在finetune小数据集时更“稳”一点","title":"Pytorch代码分析–如何让Bert在finetune小数据集时更“稳”一点","index":"1","children":[{"id":"1.-adam偏移修正","title":"1. adam偏移修正","index":"1.1"},{"id":"2.-权重初始化","title":"2. 权重初始化","index":"1.2"},{"id":"3.-简单总结","title":"3. 简单总结","index":"1.3"}]}],"reward":true}